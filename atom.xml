<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KaiRen&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/5a84ea9a0afaca03be45b87dde58e51c</icon>
  <subtitle>Learn Anything, Anytime, Anywhere~</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://k2r2bai.com/"/>
  <updated>2019-12-05T10:36:39.997Z</updated>
  <id>https://k2r2bai.com/</id>
  
  <author>
    <name>Kyle Bai</name>
    <email>k2r2.bai@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2019 LINE Dev Day 議程心得 Part3</title>
    <link href="https://k2r2bai.com/2019/12/03/linedevday/part3/"/>
    <id>https://k2r2bai.com/2019/12/03/linedevday/part3/</id>
    <published>2019-12-02T16:00:00.000Z</published>
    <updated>2019-12-05T10:36:39.997Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>昨天分享了 LINE Infrastructure 團隊為何導入 Cloud Native 的原因，從中可以看出 LINE 面臨服務開發的資源需求快速增長問題，因此必須更加有效的利用資料中心的資源，而 Cloud Native 的導入對他們來說是個關鍵點，利用容器、服務網格(Service Mesh)、微服務、不可變基礎建設、宣告式 API 等等技術與概念來提升交付時間，並更加有效的利用與控管資源。然而 LINE Infrastructure 團隊不僅僅只是導入 Cloud Native 技術來解決面臨問題，在不同層面上也有非常多的著墨，如網路與儲存這種基礎建設最關鍵的部分也考慮的很多，且服務團隊也會基於應用選用 Cloud Native 專案來達到需求。</p><a id="more"></a><p>今天就是要針對 <a href="https://speakerdeck.com/line_devday2019/grpc-service-development-in-private-kubernetes-cluster" target="_blank" rel="noopener">gRPC service development in private Kubernetes cluster</a> 這個議程來分享心得，該議程講者是 LINE Live 成員，他們為了服務能支撐直播高峰值的問題，因此導入了 Kubernetes 與 Envoy。</p><h2 id="議程心得"><a href="#議程心得" class="headerlink" title="議程心得"></a>議程心得</h2><p>這個議程主要分享 LINE Live 團隊為何使用 Kubernetes 的原因，大體上 LINE Live 團隊需要一個辦法，來因應直播服務的流量峰值變化，因此需要擁有能快速 Scale In/Out 的能力，然而過去他們都服務都執行於 VM 之上，並且採用手動方式來進行擴展，比如說開個 VM 實例，然後用 Ansible 跑腳本來部署等等流程，而這樣過程無法應付瞬間爆量的狀況，因為 VM 交付與應用部署時間過久，因此導致無法快速達到需求。有鑒於上述問題，LIEN Live 團隊才選擇使用 Kubernetes 作為服務運行的平台。</p><p><img src="https://i.imgur.com/vy9PMdZ.png" alt></p><p>從當天聽到的內容，可以了解 LINE Verda 團隊會提供自研的 VKS(Verda Kubernetes Service) 平台，來讓 Service 團隊自助部署基於 OpenStack 之上的 Kubernetes 叢集，當 Service 團隊需要更多資源時，就可以以 VKS 來快速增減節點，或是利用 Kubernetes Deployment 特性來建立多個副本以支撐流量等等。</p><p><img src="https://i.imgur.com/XWTqJid.png" alt></p><p>接著講者介紹到團隊的服務架構，他們為了確保溝通的請求率，而以 gRPC 作為服務溝通的協定，更利用 Envoy 來協助轉換不同協定成 gRPC。</p><p><img src="https://i.imgur.com/BT2QpNk.png" alt></p><p>這邊有趣的是他們在 Mobile App 與服務交互時，採用了非常多層的 Load Balancer，像是 Mobile App 經由網際網路跟 LB 溝通時，會接著經由 Kubernetes NodePort 方式轉發到各節點上(這邊可能是 Envoy 的 Kubernetes Service)，然後再透過 Envoy 以 DNS 輪詢方式(利用 Headless service 來發現所有 Pod IP)轉發到 Spring 服務上。</p><blockquote><p>其實我覺得 NodePort 那一段怪怪的…。</p></blockquote><p><img src="https://i.imgur.com/dtXFNkl.png" alt></p><p>而由於他們內部服務有很多是 gRPC 協定在溝通，但如果以 Kubernetes Service(kube-proxy) 作為彼此路由的話，將會因為其是 TCP/IP(L4) LB 的關析，而無法達到 gPRC/HTTP2 協定的好處。因為這樣只能讓 gRPC 在每次建立連線時，有負載平衡效果，而之後每次 RPC 請求都保持原本連線，並跑到同一個 Pod 上。所以 LINE Live 團隊才會選擇 Envoy 來達到每次 RPC 請求都根據我們策略路由到合適 Pod 上。</p><p>而另外他們提到不使用 Istio 是因為 Spring 已經擁有一些相同功能，因此不需要用到整個 Istio 方案，只需要 Envoy 來處理 gPRC 轉發與轉換。至於效能問題，是他們無法容忍使用 Istio 帶來的延遲增加問題，因為他們是直播類型，所以延遲過高會影響服務品質。</p><p><img src="https://i.imgur.com/giJ92Ey.png" alt></p><blockquote><p>關於 Istio 效能可以參考這篇 <a href="https://istio.io/docs/ops/deployment/performance-and-scalability/" target="_blank" rel="noopener">Performance and Scalability</a></p></blockquote><p>接著講者提到 Kubernetes Pod 存取 MySQL 的 ACL 問題，因為過去無法應付 Cluster autoscaler 長出來的新節點，因此導致該節點上的 Pod 都會被阻擋存取，所以 LINE 團隊開發了一個 ACL Manager 來動態更新。基本上 ACL Manager 就是去觀察連接 DB 的 Pod 座落在哪些節點上，然後在動態調整 MySQL ACL</p><p><img src="https://i.imgur.com/3xAO3zw.png" alt></p><p>而在 Monitoring 部分，他們採用 Verda Team 提供的 Add-ons Manager 來部署 Prometheus，該 Prometheus 以 StatefulSet 形式運行在當前 Kubernetes 叢集中(這是目前典型的做法)，而不是透過外部 Prometheus 方式來拉取 Metrics，增加了叢集的安全性。當然這樣做法還是要依據應用來進行，並且要很好的規劃叢集節點的角色，若沒定義好相對的 Affinity 的話，就可能影響到正式服務的容器。</p><p><img src="https://i.imgur.com/S7SYYMW.png" alt></p><p>另外他們覺得用 Kubernetes 的 Persistent Volume 來儲存 Prometheus TSDB 很難使用，因此透過 <a href="https://prometheus.io/docs/prometheus/latest/storage/#remote-storage-integrations" target="_blank" rel="noopener">Remote storage</a> 來跟自家開發的 <a href="https://speakerdeck.com/line_devday2019/deep-dive-into-lines-time-series-database-flash" target="_blank" rel="noopener">TSDB - Flash</a> 整合。至於在視覺化部分，就是採用 Grafana 來處理 Prometheus 資料。</p><p><img src="https://i.imgur.com/UEql1JP.png" alt></p><p>在 Spring 應用部分，Metrics 會透過 Spring Boot Actuator 與 OpenCensus 來提取，其中 OpenCensus 還能用於對 Metrics 進行分散式追蹤(Distributed tracing)，以確保團隊在遇到問題時，能更快的解決，甚至是提前預防問題發生。</p><p><img src="https://i.imgur.com/wPEIvNS.png" alt></p><p>最後部分講者分享了該團隊的 Logging 系統，可以看出類似 Monitoring 一樣直接採用 Verda Team 提供的 Add-ons 形式部署 EFK(Fluentd, Elasticsearch and Kibana) stack 架構在 Kubernetes 叢集上來收集 Logs，另外透過 IMON 來實現 Log 的告警。</p><p><img src="https://i.imgur.com/1FIiaky.png" alt></p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>從這個議程可以看出來 LINE 在技術採納並不是隨便就使用，完全是對症下藥。另外 LINE 團隊在 Monitoring 跟 Logging 的軟體選擇都是目前 Kubernetes 社區典型部署方案，雖然在一些細節可能有額外開發東西，但是不難看出公司採納的架構比較偏向社區方案，或許是希望藉由社區的力量來加快平台的迭代，一方面也利用社區貢獻者力量來解決遇到問題吧。</p><p>總之 LINE 團隊正在全面導入 Cloud Native 技術，利用這些技術來解決各種問題!!</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.techbang.com/posts/73284-lines-first-expansion-will-be-held-on-november-20th-and-21st-for-two-day-line-developer-day-2019" target="_blank" rel="noopener">https://www.techbang.com/posts/73284-lines-first-expansion-will-be-held-on-november-20th-and-21st-for-two-day-line-developer-day-2019</a></li><li><a href="https://www.inside.com.tw/article/18162-line-dev-day" target="_blank" rel="noopener">https://www.inside.com.tw/article/18162-line-dev-day</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;昨天分享了 LINE Infrastructure 團隊為何導入 Cloud Native 的原因，從中可以看出 LINE 面臨服務開發的資源需求快速增長問題，因此必須更加有效的利用資料中心的資源，而 Cloud Native 的導入對他們來說是個關鍵點，利用容器、服務網格(Service Mesh)、微服務、不可變基礎建設、宣告式 API 等等技術與概念來提升交付時間，並更加有效的利用與控管資源。然而 LINE Infrastructure 團隊不僅僅只是導入 Cloud Native 技術來解決面臨問題，在不同層面上也有非常多的著墨，如網路與儲存這種基礎建設最關鍵的部分也考慮的很多，且服務團隊也會基於應用選用 Cloud Native 專案來達到需求。&lt;/p&gt;
    
    </summary>
    
      <category term="LINE Dev Day" scheme="https://k2r2bai.com/categories/LINE-Dev-Day/"/>
    
    
      <category term="LINE Dev Day" scheme="https://k2r2bai.com/tags/LINE-Dev-Day/"/>
    
  </entry>
  
  <entry>
    <title>2019 LINE Dev Day 議程心得 Part2</title>
    <link href="https://k2r2bai.com/2019/12/02/linedevday/part2/"/>
    <id>https://k2r2bai.com/2019/12/02/linedevday/part2/</id>
    <published>2019-12-01T16:00:00.000Z</published>
    <updated>2019-12-05T09:56:12.992Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>昨天主要分享 LINE Dev Day 當天開場的 Keynote 內容，而今天將針對我最想要了解的 Infrastructure &amp; Cloud Native 議程來分享，因為自己在這方面技術比較有著墨，因此想一探究竟 LINE 團隊是如何建構這麼大規模的平台，也非常想了解他們在技術選型時，是如何考量，而當遇到問題時，又是如何去解決，以支撐這麼大量的服務與系統營運。</p><a id="more"></a><p>今天將先焦距在 LINE Verda Team 的私有雲部分，會以其私有雲部門主管 Yuki 桑的 <a href="https://speakerdeck.com/line_devday2019/cloud-native-challenges-in-private-cloud-with-k8s-knative" target="_blank" rel="noopener">Cloud Native Challenges in Private Cloud with K8s, Knative</a> 議題來切入。</p><h2 id="議程心得"><a href="#議程心得" class="headerlink" title="議程心得"></a>議程心得</h2><p>本次 LINE 特別將 Infrastructure 作為重點放入 Keynote 分享，原因正是大多數 LINE 的服務與系統，都是由自家私有雲 Verda 所支撐，而其基礎建設 Verda Team 正是負責這項重責大任的團隊，因此可以看到關於 Infrastructure &amp; Cloud Native 都是來自該團隊。</p><p>首先分享這個議題是因為該講者是 Verda Team 的私有雲主管 Yuki 桑，Yuki 在過去非常活躍於技術社區，尤其是在 OpenStack Summit 與 Cloud Native Forum 都有他分享技術的足跡，而 LINE 的許多系統會如此茁壯，很多也是歸功於他與團隊的技術選擇，他這次從私有雲上的挑戰來說明為何選擇 Cloud Native 技術，他們又該如何解決遇到問題與貧頸。</p><p>目前 Verda Team 維護著整個 IaaS、PaaS 上的所有服務，如下圖所示:</p><p><img src="https://i.imgur.com/wjJIbXL.png" alt></p><p><img src="https://i.imgur.com/QwNJnFi.png" alt></p><p>可以看出 LINE 許多服務都是自己實現與建制，而這些功能很多都建構在 OpenStack 之上，因此他們擁有很高的擴展性與可客製化性，這是由於 OpenStack 架構能夠讓人選擇樣安裝哪些元件來提供哪些服務功能，如 <a href="https://docs.openstack.org/nova/latest/" target="_blank" rel="noopener">Nova</a> 提供 VM、<a href="https://docs.openstack.org/cinder/latest/" target="_blank" rel="noopener">Cinder</a> 提供 Block Storage 與 <a href="https://docs.openstack.org/neutron/pike/" target="_blank" rel="noopener">Neutron</a> 提供 Networking 等等，而也能聽到 LINE 在這之上又提供了各種服務，如 DBaaS(Database as a Service)、VKS(Verda Container Service)、FaaS(Function as a Service) 等等。而為何要擁有這麼多元的功能與需求呢?這是因為 LINE 從去年六月開始快速的成長，有越來越多的服務與系統需要被執行，更有越來越的團隊需要開發與測試用環境，因此面臨如此挑戰。</p><p><img src="https://i.imgur.com/KStTss8.png" alt></p><p>那麼面對這樣的需求，LINE 是如何去解決的呢?</p><p><img src="https://i.imgur.com/DcNR4BK.png" alt></p><p>沒錯!就是他們擁抱 <a href="https://github.com/cncf/toc/blob/master/DEFINITION.md" target="_blank" rel="noopener">Cloud Native</a> 相關技術。從 Yuki 標註的段落，可以了解 Cloud Native 旨在採用容器、服務網格(Service Mesh)、微服務、不可變基礎建設、宣告式 API 等等，來有助在雲端環境中，建構與執行彈性與可擴展的應用。且這些技術還擁有著良好的容錯性、易管理與松耦合系統等等特點，因此結合可靠自動化，將使工程師(系統管理員)能夠對當前系統做出頻繁與可預測的重大變更。</p><blockquote><p>有種簡單說就是盡可能用 Kubernetes 來降低服務維運與運行的困難性，畢竟對於 CNCF 來說 Cloud Native 定義的本質，很多都是再講 Kubernetes 本身。</p></blockquote><p>當導入 Cloud Native 時，可以讓他們更有效的控管不同團隊的資源狀況，而另外 Yuki 也說到未導入 Cloud Native 的服務開發遇到的問題，可以看到 LINE 很多專案的服務會開很多 VM 執行不同事情，比如說有些做為主服務執行用、有些執行週期性工作或者執行操作腳本等等，因為執行這些事情的都是 Application Engineers 在進行，因此無從得知哪些 VM 在幹什麼，又或者有沒有在使用，因此造成嚴重的資源浪費，像是這邊就提到他們有 60% 的機器 CPU 使用率落在 10% 左右。而為了解決這問題才導入 Cloud Native 來提升在服務開發週期中，能夠控管的資源範圍。</p><p><img src="https://i.imgur.com/zd8iKV8.png" alt></p><p>而他們究竟是怎麼解決的呢?實際上，Verda Team 實現了一個  Computing Resources 抽象層，用於描述應用被部署的形式，這樣 Infra Engineers 就可以識別哪些 VM 被執行哪種形式的程式，因此能夠對此做出資源的調整，這解決之前不知道資源狀況的問題，因此導致無法隨意調整 VM 資源。</p><p><img src="https://i.imgur.com/XCVYbK5.png" alt></p><p>另外 Yuki 也提到目前他們正在實現的新方式，就是採用 Kubernetes 與 Knative 來取代之前 VM 的形式，由於腳本服務，通常不太需要過多的資源來支撐，因此能夠以 Function 形式在容器中執行，除了有效的利用資源以外，更加快了交付的時間。</p><p><img src="https://i.imgur.com/9jZW4iR.png" alt></p><p>而 LINE 的 Kubernetes as a Service 主要基於 Rancher 實現而成，他們實現了一套 Verda APIs 用於管理多個 Kubernetes 叢集，一但有人呼叫建立叢集時，就以 Rancher 來自動化部屬 Kubernetes 叢集，而當發生不同叢集事件時，也會即時收到並執行 Rancher API 來解決對應問題，比如說節點掛了，就透過 Rancher 來開新節點，並取代掉。</p><p><img src="https://i.imgur.com/9u86Rgc.png" alt></p><blockquote><p>這邊可能使用 Rancher 的 OpenStack Driver 在 IaaS 上面部署 Kubernetes 叢集。</p></blockquote><p>而另外透過開發的 Add-on Manager 來維護與管理 Monitoring、Logging 等額外功能。</p><p><img src="https://i.imgur.com/6FZ7NYo.png" alt></p><blockquote><p>這邊目前 LINE 都是選用社區典型的工具，像是 Monitoring 使用 Prometheus + Grafana，而 Logging 則用 Elasticsearch + Fluentd + Kibana 來達成。</p></blockquote><p>接著 Yuki 提到為何使用 Knative，這除了減少透過 VM 執行腳本作業問題外，也想用 Knative 的 Eventing 與 Function 功能來達到各種事件與執行程序的連動，比如說某台 VM CPU 快用滿了，就觸發一個 Function 來傳遞資訊到 Slack 上，已通知大家。</p><p><img src="https://i.imgur.com/ROxdH16.png" alt></p><p>從這些內容來看，可以很明顯發現 LINE Infra 團隊正在努力地提升資源的利用率，以 Cloud Native 來確保不必要的資源浪費。</p><p>最後 Yuki 桑給出了他們希望提供的 Kubernetes 架構，我想這也是許多人常問的問題，到底是要用多叢集來區分服務，還是大叢集以 Namespace 切分服務?</p><p>我想這還是要依據應用情境來區別，因為每家公司對於服務分離的要求性不同。但是在 LINE 的服務有太多種了，因此兩者在他們部署情境是都會發生的。</p><p><img src="https://i.imgur.com/CDygMQk.png" alt></p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>雖然這次到 LINE Dev Day 聽到這場非常類似 Cloud Native Forum 講的內容，但這是比較像是概觀與未來展望，因此沒有太多細節內容，不過扔然讓人感受到 LINE 團隊對於技術的選擇與發展，是真的很勇於嘗試，想想 Knative 剛出現也不就一年時間，他們已經開始在內部系統採用，真的讓人很佩服。</p><blockquote><p>這邊想了解更多 LINE Verda 私有雲細節的話，可以看看 Yuki 桑在 Cloud Native Forum 分享的 <a href="https://drive.google.com/drive/folders/18ZIECXxpgN3rve_YvjHRGy2Dn3L4w32C?usp=sharing" target="_blank" rel="noopener">Managed Kubernetes Cluster Service In Private Cloud</a> 議程。</p></blockquote><p>而除了 Yuki 桑的議題外，也有幾場關於 Cloud Native 議程，分別為以下，大家有興趣也可以看簡報了解:</p><ul><li><a href="https://speakerdeck.com/line_devday2019/grpc-service-development-in-private-kubernetes-cluster" target="_blank" rel="noopener">gRPC service development in private Kubernetes cluster</a></li><li><a href="https://speakerdeck.com/line_devday2019/implementation-of-service-mesh-using-envoy-and-central-dogma" target="_blank" rel="noopener">Implementation of service mesh using Envoy and Central Dogma</a></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.techbang.com/posts/73284-lines-first-expansion-will-be-held-on-november-20th-and-21st-for-two-day-line-developer-day-2019" target="_blank" rel="noopener">https://www.techbang.com/posts/73284-lines-first-expansion-will-be-held-on-november-20th-and-21st-for-two-day-line-developer-day-2019</a></li><li><a href="https://www.inside.com.tw/article/18162-line-dev-day" target="_blank" rel="noopener">https://www.inside.com.tw/article/18162-line-dev-day</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;昨天主要分享 LINE Dev Day 當天開場的 Keynote 內容，而今天將針對我最想要了解的 Infrastructure &amp;amp; Cloud Native 議程來分享，因為自己在這方面技術比較有著墨，因此想一探究竟 LINE 團隊是如何建構這麼大規模的平台，也非常想了解他們在技術選型時，是如何考量，而當遇到問題時，又是如何去解決，以支撐這麼大量的服務與系統營運。&lt;/p&gt;
    
    </summary>
    
      <category term="LINE Dev Day" scheme="https://k2r2bai.com/categories/LINE-Dev-Day/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
      <category term="LINE Dev Day" scheme="https://k2r2bai.com/tags/LINE-Dev-Day/"/>
    
  </entry>
  
  <entry>
    <title>2019 LINE Dev Day 議程心得 Part1</title>
    <link href="https://k2r2bai.com/2019/12/01/linedevday/part1/"/>
    <id>https://k2r2bai.com/2019/12/01/linedevday/part1/</id>
    <published>2019-11-30T16:00:00.000Z</published>
    <updated>2019-12-04T17:29:23.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今年很榮幸被 LINE 以 Influencer 形式邀請到日本參加 LINE Dev Day，這也是人生第一次踏入那長年陪伴我長大的影片的國家 - <code>日本</code>。原本在這期間會參與 KubeCon NA 的活動，但因為一些變故，因此最後沒能去成，於是就來到了 LINE Dev Day，這次參與 LINE Dev Day 最大目的當然就是想要一探究竟 LINE 的基礎建設，過去在 OpenStack 與 Cloud Native 相關活動中，了解到 LINE 的 IaaS 是以 OpenStack、Ceph 等開源專案建置而成，並且提供一套自研 Kubernetes as a Service，提供給不同團隊開發與部署服務使用，而且規模無法想像的大，其維護團隊 Verda Team 更是這些專案社區的程式碼貢獻者，因此能夠來到日本現場真的讓人很興奮。</p><a id="more"></a><p><img src="https://i.imgur.com/XrYtUNJ.jpg" alt></p><h2 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h2><p>在這次 LINE Dev Day 中，一開場就是由 LINE CTO - Euivin Park 開場 <a href="https://speakerdeck.com/line_devday2019/line-devday-2019-keynote" target="_blank" rel="noopener">Keynote</a>，過程中說明目前 LINE 專注的幾個重點技術項目，分別為 AI、資料平台與基礎設施，以及資安與隱私，其中 AI 部分更是重頭戲，在過程中展示了許多成果，並且這些都被應用在 LINE 的各種應用與服務上，以增加更多優勢。</p><p><img src="https://i.imgur.com/S2saw2n.png" alt></p><p>另外 LINE 也展示了對於 AI 的展望，從下圖會發現，他們真的搞了很多項目，除了 Face Sign 外，還有語音辨識與分析、手寫辨識與分析、資料分析、自然語言處理等等。</p><p><img src="https://i.imgur.com/t1krEUW.png" alt></p><p>其實不難發現 LINE 的 AI 有許多成果，像是這次活動報到就是採用 Face Sign 形式進行，參加者只要在活動開始之前，上傳自己的照片後，就可以在當天以臉部辨識方式來完成報到，據我自己經驗來說，真的非常準確… 因為我上傳時沒戴眼鏡，但是到現場有戴眼鏡還是能識別出來，這讓我蠻訝異的。</p><p><img src="https://i.imgur.com/FHJVNJC.png" alt></p><p>當 AI 部分講完時，就來到了我參與本次活動最想了解部分 - <code>Data Platform &amp; Infrastructure</code>。這部分一開場就由 Verda Team 的 Yoshihiro san 說明 LINE 每天面臨的資料量及執行的 Spark/Hive 程序量等等，並指出當前的關鍵挑戰部分。</p><p><img src="https://i.imgur.com/RLS7B2F.png" alt></p><p>而因為這些問題，LINE 團隊採用了<code>Self-service Data Platform</code>平台，來讓內部資料分析團隊能自助建立使用，以提升分析跨服務的複雜性，並有效的運用。</p><p><img src="https://i.imgur.com/HbMYKXK.png" alt></p><p>另外在基礎建設(雲端系統)部分，Verda Team 在全球維護著 4 萬台以上的實體機器，用於支撐每天 50 億的訊息量，以及 1Tbps 的流量。而支撐這些服務的平台，就是 LINE 團隊透過各種開源技術搭建而成的私有雲，該私有雲涵蓋了 OpenStack、Ceph、Kubernetes 等等技術，其中不乏有許多 CNCF 組織的專案被採用在各種服務內，像是 gRPC、Prometheus 等等，另外 LINE 在網路技術上，採用 SRv6 來達到 Multi-Tenant Network，以解決許多分散的基礎建設網路、缺乏靈活性問題。最後有趣的是 Yoshihiro 說 LINE 目前機器推疊起來，有 2200 公尺高，相等於 3.5 座東京晴空塔的高度。</p><p><img src="https://i.imgur.com/8cv6VAL.jpg" alt></p><p>而 LINE 的 Verda Team 建構的私有雲平台，採用新的技術是希望提升開發流程與產品交付的效率，以因應目前許多服務快速發展的趨勢。</p><p><img src="https://i.imgur.com/ckTphEI.png" alt></p><p>當基礎建設結束後，就進入到 LINE CTO 提到的第三大重點 - 安全與隱私(Security and Privacy)，LINE 這幾年對於使用者資料與隱私的使用上，一直致力於合法運用，並完全確保不侵犯到使用者隱私。另外有趣的是 LINE 資安團隊導入機器學習後，有效的降低使用者帳戶被盜用的狀況，甚至在今年九月達到沒有任何在被盜用了，可以想像 LINE 團隊在這方面的積極程度。</p><p><img src="https://i.imgur.com/tEamvOV.png" alt></p><p>另外在資安方面，LINE 也將自己的資安漏洞回報機制轉到 HackerOne 上，以讓全球駭客能夠來修復與回報，既而加強服務的安全性。最後 LINE Dev Day 除了上述三大重點外，其議程還涵蓋了許多技術內容，如區塊鏈技術、網路銀行、自研發系統(DB, TSDB) 等等。</p><p><img src="https://i.imgur.com/3TEj0WB.png" alt></p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>從這次活動中，可以感受到 LINE 在基礎建設(雲端系統)採用開源專案已經非常成熟，從虛擬化、網路架構與儲存方案都少不了開源專案身影，其中又導入很多 Cloud Native 技術，如 Kubernetes、Knative 等等，且 LINE 團隊也是開源社區的貢獻者，可以發現他們為開源專案上游修復了許多 Patch，除了更完善自家私有雲平台外，也能與社區緊密接合，以快速反應當前遇到問題。</p><p>雖然本篇只提到 Keynote 部分，但是從概觀來看，就能感受到 LINE 對於自家技術的要求。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://speakerdeck.com/line_devday2019/line-devday-2019-keynote" target="_blank" rel="noopener">https://speakerdeck.com/line_devday2019/line-devday-2019-keynote</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;今年很榮幸被 LINE 以 Influencer 形式邀請到日本參加 LINE Dev Day，這也是人生第一次踏入那長年陪伴我長大的影片的國家 - &lt;code&gt;日本&lt;/code&gt;。原本在這期間會參與 KubeCon NA 的活動，但因為一些變故，因此最後沒能去成，於是就來到了 LINE Dev Day，這次參與 LINE Dev Day 最大目的當然就是想要一探究竟 LINE 的基礎建設，過去在 OpenStack 與 Cloud Native 相關活動中，了解到 LINE 的 IaaS 是以 OpenStack、Ceph 等開源專案建置而成，並且提供一套自研 Kubernetes as a Service，提供給不同團隊開發與部署服務使用，而且規模無法想像的大，其維護團隊 Verda Team 更是這些專案社區的程式碼貢獻者，因此能夠來到日本現場真的讓人很興奮。&lt;/p&gt;
    
    </summary>
    
      <category term="LINE Dev Day" scheme="https://k2r2bai.com/categories/LINE-Dev-Day/"/>
    
    
      <category term="LINE Dev Day" scheme="https://k2r2bai.com/tags/LINE-Dev-Day/"/>
    
  </entry>
  
  <entry>
    <title>動手實作 Kubernetes 自定義控制器 Part6</title>
    <link href="https://k2r2bai.com/2019/10/15/ironman2020/day30/"/>
    <id>https://k2r2bai.com/2019/10/15/ironman2020/day30/</id>
    <published>2019-10-14T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="https://k2r2bai.com/2019/10/14/ironman2020/day29/">動手實作 Kubernetes 自定義控制器 Part5</a> 文章結束後，基本上已經完成了這個自定義控制器範例的功能，這時若我們想要部署這個控制器的話，該怎麼辦呢?因為過去文章中，我們都是以 Go 語言指令直接建構程式進行測試，且使用 client-go 與 API Server 溝通時，都是以<code>cluster-admin</code>使用者來達成，這種作法如果是正式上線環境，必然會有很多疑慮，比如說控制器環境有安全問題，如果這些狀況被取得 Kubernetes cluster-admin 權限的話，就可能會危害到整個 Kubernetes 環境，因為 cluster-admin 可以操作任何 Kubernetes API 資源。基於這些問題，今天就是要來說明如何讓控制器正確的部署到 Kubernetes 叢集中執行。</p><a id="more"></a><h2 id="Deploy-in-the-cluster"><a href="#Deploy-in-the-cluster" class="headerlink" title="Deploy in the cluster"></a>Deploy in the cluster</h2><p>由於自定義控制器部署到 Kubernetes 叢集時，需要擁有特定使用者與權限操作 Kubernetes APIs，才能正常的執行控制器功能，因此必須利用 <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" rel="noopener">Service Account</a> 與 <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" rel="noopener">RBAC</a> 來讓控制器能在 Pod 中與 API Server 溝通。</p><blockquote><p>雖然 Kubernetes 在建立 Namespace 時，預設也會自動建立一個名稱為<code>default</code>的 Service Account，但這個 Service Account 通常會被用於該 Namespace 下的所有 Pod，因此不建議將 RBAC 權限賦予給這個 Service Account。</p></blockquote><p>而要提供部署到 Kubernetes 叢集中，通常需要準備以下幾個檔案:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">├── Dockerfile <span class="comment"># 將控制器建構成容器映像檔</span></span><br><span class="line">└── deploy <span class="comment"># 提供部署控制器的目錄</span></span><br><span class="line">    ├── crd.yml <span class="comment"># 自定義控制器的 CRDs</span></span><br><span class="line">    ├── deployment.yml <span class="comment"># 用於部署自定義控制器程式本身</span></span><br><span class="line">    ├── rbac.yml <span class="comment"># 自定義控制器的 API 存取權限</span></span><br><span class="line">    └── sa.yml <span class="comment"># 自定義控制器的服務帳戶，會與 RBAC 結合以限制 Pod 存取 API 的權限</span></span><br></pre></td></tr></table></figure><h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><p>由於今天的實作內容需要用到 Kubernetes 與 Docker，因此須完成以下需求與條件:</p><ul><li>一座 Kubernetes v1.10+ 叢集。透過 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 建立即可 <code>minikube start --kubernetes-version=v1.15.4</code>。</li><li>一個 Docker 環境，可以直接 Minikube 執行<code>eval $(minikube docker-env)</code>來取的 Docker 參數，並遠端操作。</li></ul><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>本部分將建立這些檔案，並執行所需的操作。首先由於要將控制器部署到 Kubernetes 中，因此必須將控制器程式建構成容器映像檔，這樣才能透過 Pod 的形式來部署。而建構映像檔可以透過 Dockerfile 來達到，如以下內容:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM kairen/golang-dep:1.12-alpine AS build</span><br><span class="line"></span><br><span class="line">ENV GOPATH <span class="string">"/go"</span></span><br><span class="line">ENV PROJECT_PATH <span class="string">"<span class="variable">$GOPATH</span>/src/github.com/cloud-native-taiwan/controller101"</span></span><br><span class="line">ENV GO111MODULE <span class="string">"on"</span></span><br><span class="line"></span><br><span class="line">COPY . <span class="variable">$PROJECT_PATH</span></span><br><span class="line">RUN <span class="built_in">cd</span> <span class="variable">$PROJECT_PATH</span> &amp;&amp; \</span><br><span class="line">  make &amp;&amp; mv out/controller /tmp/controller</span><br><span class="line"></span><br><span class="line">FROM alpine:3.7</span><br><span class="line">COPY --from=build /tmp/controller /bin/controller</span><br><span class="line">ENTRYPOINT [<span class="string">"controller"</span>]</span><br></pre></td></tr></table></figure><p>完成後，即可透過 Docker 指令來建構與推送到公有的 Container registry 上以便後續部署使用:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> $(minikube docker-env)</span><br><span class="line">$ docker build -t &lt;owner&gt;/controller101:v0.1.0 .</span><br><span class="line">$ docker push &lt;owner&gt;/controller101:v0.1.0</span><br></pre></td></tr></table></figure><p>接著我們將新增以下檔案用於部署控制器到 Kubernetes 上執行。</p><h4 id="deploy-crd-yml"><a href="#deploy-crd-yml" class="headerlink" title="deploy/crd.yml"></a>deploy/crd.yml</h4><p>用於以 CRD API 來新增自定義資源的檔案。在這檔案中，通常會定義一個或多個自定義資源內容。</p><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">virtualmachines.cloudnative.tw</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  group:</span> <span class="string">cloudnative.tw</span></span><br><span class="line"><span class="attr">  version:</span> <span class="string">v1alpha1</span></span><br><span class="line"><span class="attr">  names:</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">VirtualMachine</span></span><br><span class="line"><span class="attr">    singular:</span> <span class="string">virtualmachine</span></span><br><span class="line"><span class="attr">    plural:</span> <span class="string">virtualmachines</span></span><br><span class="line"><span class="attr">    shortNames:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">vm</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">vms</span></span><br><span class="line"><span class="attr">  scope:</span> <span class="string">Namespaced</span></span><br><span class="line"><span class="attr">  additionalPrinterColumns:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">Status</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">string</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.status.phase</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">CPU</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">number</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.status.server.usage.cpu</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">Memory</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">number</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.status.server.usage.memory</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">Age</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">date</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.metadata.creationTimestamp</span></span><br></pre></td></tr></table></figure><blockquote><p>有些自定義控制器會直接透過 <a href="https://github.com/kubernetes/apiextensions-apiserver" target="_blank" rel="noopener">apiextensions-apiserver</a> 在程式啟動時，將自定義資源自動新增到當前 Kubernetes 叢集中。</p></blockquote><h4 id="deploy-sa-yml"><a href="#deploy-sa-yml" class="headerlink" title="deploy/sa.yml"></a>deploy/sa.yml</h4><p>該檔案會定義一個 Service Account 用於提供給控制器程式在 Pod 中與 API Server 溝通的帳戶，這個帳戶會透過 RBAC 賦予特定的權限，以便控制器程式獲取操作所需 API 資源的權限。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">controller101</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><h4 id="deploy-rbac-yml"><a href="#deploy-rbac-yml" class="headerlink" title="deploy/rbac.yml"></a>deploy/rbac.yml</h4><p>該檔案會賦予指定 Service Account 相對應的 API 權限，由於這個自定義控制器需要對 VirtualMachines 資源做各種操作(如 Create、Update、Delete、Get 與 Watch 等等)，因此需要設定相對應的 API Group 與 Resources 來確保控制器程式能獲取權限。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">controller101-role</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">cloudnative.tw</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">"virtualmachines"</span></span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">"*"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">controller101-rolebinding</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">controller101-role</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">controller101</span></span><br></pre></td></tr></table></figure><blockquote><p>如果控制器需要存取其他 API 時，就必須在<code>rules</code>欄位額外新增。請務必確保給予最小可運作的權限。</p></blockquote><h4 id="deploy-deployment-yml"><a href="#deploy-deployment-yml" class="headerlink" title="deploy/deployment.yml"></a>deploy/deployment.yml</h4><p>該檔案定義自定義控制器的容器如何在 Kubernetes 部署，我們會在以 Deployment 方式進行部署，因為能利用 Deployment 機制來確保控制器在叢集中的可靠性。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">controller101</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> <span class="string">controller101</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Recreate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">controller101</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      priorityClassName:</span> <span class="string">system-cluster-critical</span> <span class="comment"># 由於控制器有可能是重要元件，因此要確保節點資源不足時，不會優先被驅逐</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">CriticalAddonsOnly</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">Exists</span></span><br><span class="line"><span class="attr">      - effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">        key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">controller101</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">controller</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">kairen/controller101:v0.1.0</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">DOCKER_HOST</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"tcp://192.168.99.159:2376"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">DOCKER_TLS_VERIFY</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"1"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">DOCKER_CERT_PATH</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"/etc/docker-certs"</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--v=2</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--logtostderr=true</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--vm-driver=docker</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--leader-elect=false</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">docker-certs</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">"/etc/docker-certs"</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">docker-certs</span></span><br><span class="line"><span class="attr">        secret:</span></span><br><span class="line"><span class="attr">          secretName:</span> <span class="string">docker-certs</span></span><br></pre></td></tr></table></figure><blockquote><ul><li>其中<code>env</code>部分，需要依據環境差異來改變。</li><li>若有多個副本時，需要透過<code>--leader-elect</code>來啟用 Leader Election 機制。</li></ul></blockquote><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p>當檔案都建立好後，就可以透過 kubectl 來部署到 Kubernetes 叢集。首先由於範例使用 Docker Driver 來測試，因此需要在控制器程式啟動時載入 Docker Certs，故這邊要先將這些檔案以 Secrets 方式存到 Kubernetes 中，之後再提供給 Deployment 掛載使用。</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ minikube docker-env</span><br><span class="line"><span class="built_in">export</span> DOCKER_TLS_VERIFY=<span class="string">"1"</span></span><br><span class="line"><span class="built_in">export</span> DOCKER_HOST=<span class="string">"tcp://192.168.99.159:2376"</span></span><br><span class="line"><span class="built_in">export</span> DOCKER_CERT_PATH=<span class="string">"/Users/test/.minikube/certs"</span></span><br><span class="line"></span><br><span class="line">$ kubectl -n kube-system create secret generic docker-certs \</span><br><span class="line">    --from-file=<span class="variable">$HOME</span>/.minikube/certs/ca.pem \</span><br><span class="line">    --from-file=<span class="variable">$HOME</span>/.minikube/certs/cert.pem \</span><br><span class="line">    --from-file=<span class="variable">$HOME</span>/.minikube/certs/key.pem</span><br></pre></td></tr></table></figure><p>建立好 Docker certs 後，就可以執行以下指令進行部署:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f deploy/</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/virtualmachines.cloudnative.tw created</span><br><span class="line">deployment.apps/controller101 created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/controller101-role created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/controller101-rolebinding created</span><br><span class="line">serviceaccount/controller101 created</span><br><span class="line"></span><br><span class="line">$ kubectl -n kube-system logs -f controller101-7858db7484-5bjvf</span><br><span class="line">I1015 11:14:45.159556       1 controller.go:77] Starting the controller</span><br><span class="line">I1015 11:14:45.159640       1 controller.go:78] Waiting <span class="keyword">for</span> the informer caches to sync</span><br><span class="line">I1015 11:14:45.262042       1 controller.go:86] Started workers</span><br></pre></td></tr></table></figure><blockquote><p>若 Minikube 的 Docker envs 資訊不同時，需要修改<code>deploy/deployment.yml</code>裡面 envs。</p></blockquote><p>接著嘗試建立一個 VirtualMachine 資源實例，並用 kubectl get 與 docker ps 查看狀態:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: cloudnative.tw/v1alpha1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-vm</span><br><span class="line">spec:</span><br><span class="line">  resource:</span><br><span class="line">    cpu: 2</span><br><span class="line">    memory: 4G</span><br><span class="line">EOF</span><br><span class="line">virtualmachine.cloudnative.tw/<span class="built_in">test</span>-vm created</span><br><span class="line"></span><br><span class="line">$ kubectl get vms</span><br><span class="line">NAME      STATUS   CPU   MEMORY                AGE</span><br><span class="line"><span class="built_in">test</span>-vm   Active   0     0.11359797976548552   22s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ docker ps --filter <span class="string">"name=test-vm"</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">5ac009171998        nginx:1.17.4        <span class="string">"nginx -g 'daemon of…"</span>   46 seconds ago      Up 45 seconds       80/tcp              <span class="built_in">test</span>-vm</span><br></pre></td></tr></table></figure><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>從今天實作中，可以了解到部署自定義控制器到 Kubernetes 叢集並非難事，一但控制器容器化後，並以 Kubernetes API 資源形式部署時，就能夠增加控制器的可攜帶性與維護性，往後有版本更新時，也可以利用 Kubernetes 的一些機制(如 Rolling Upgrade)來安全地更新控制器程式。</p><p>最後由於鐵人賽文章不夠用，因此關於 CKA/CKAD 認證、 Admission Controller 等文章，都會在 <a href="http://k2r2bai.com">KaiRen’s Blog</a> 上持續新增。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></li><li><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;https://k2r2bai.com/2019/10/14/ironman2020/day29/&quot;&gt;動手實作 Kubernetes 自定義控制器 Part5&lt;/a&gt; 文章結束後，基本上已經完成了這個自定義控制器範例的功能，這時若我們想要部署這個控制器的話，該怎麼辦呢?因為過去文章中，我們都是以 Go 語言指令直接建構程式進行測試，且使用 client-go 與 API Server 溝通時，都是以&lt;code&gt;cluster-admin&lt;/code&gt;使用者來達成，這種作法如果是正式上線環境，必然會有很多疑慮，比如說控制器環境有安全問題，如果這些狀況被取得 Kubernetes cluster-admin 權限的話，就可能會危害到整個 Kubernetes 環境，因為 cluster-admin 可以操作任何 Kubernetes API 資源。基於這些問題，今天就是要來說明如何讓控制器正確的部署到 Kubernetes 叢集中執行。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>動手實作 Kubernetes 自定義控制器 Part5</title>
    <link href="https://k2r2bai.com/2019/10/14/ironman2020/day29/"/>
    <id>https://k2r2bai.com/2019/10/14/ironman2020/day29/</id>
    <published>2019-10-13T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="https://k2r2bai.com/2019/10/13/ironman2020/day28/">動手實作 Kubernetes 自定義控制器 Part4</a> 文章結語部分，我有提到目前實作的自定義控制器還存在著問題(如下圖)，其中就是自定義資源 VirtualMachine 的實例被刪除前，未正確透過 VM Driver 刪除實際管理的虛擬機，這樣情況下的虛擬機都會變成失去控制器管理的殭屍(或孤兒)。基於此問題，今天將說明該如何修改程式以解決這樣問題。</p><a id="more"></a><p>在開始實作前，我們先來探討一些概念。在接觸 Kubernetes 時，相信大家都玩過 Deployment、Job 與 DaemonSet 等功能，這些功能有個共同點，那就是都管理著一個或多個 Pod 的生命週期，這表示當一個實例(比如 Deployment)被執行刪除時，其相關聯的 Pod 都會接著被刪除。而這種機制正是 Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/" target="_blank" rel="noopener">垃圾收集器</a>，在 v1.6+ 開始，Kubernetes 會自動對一些 API 資源物件(如 Deployment、ReplicaSet)引入<code>ownerReferences</code>欄位，這個欄位用來標示相依 API 資源物件的 Owner 是誰，而自己則為 Owner 的 Dependent，因此當 Owner 被刪除時，所有關聯的 API 資源物件就會被垃圾收集器回收(從叢集中刪除)，而這過程又稱<code>級聯刪除(Cascading deletion)</code>。</p><blockquote><p>雖然命名為垃圾收集器，但實際上它也是以控制器模式(Controller Pattern)的形式實作。</p></blockquote><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl run nginx --image nginx --port 80</span><br><span class="line">$ kubectl get po nginx-7c45b84548-gj999 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: <span class="string">"2019-10-14T14:43:07Z"</span></span><br><span class="line">  generateName: nginx-7c45b84548-</span><br><span class="line">  labels:</span><br><span class="line">    pod-template-hash: 7c45b84548</span><br><span class="line">    run: nginx</span><br><span class="line">  name: nginx-7c45b84548-gj999</span><br><span class="line">  namespace: default</span><br><span class="line">  ownerReferences: <span class="comment"># Deployment 管理著 ReplicaSet，而 ReplicaSet 管理著 Pod</span></span><br><span class="line">  - apiVersion: apps/v1</span><br><span class="line">    blockOwnerDeletion: <span class="literal">true</span></span><br><span class="line">    controller: <span class="literal">true</span></span><br><span class="line">    kind: ReplicaSet</span><br><span class="line">    name: nginx-7c45b84548</span><br><span class="line">    uid: 426961f6-b94d-49d2-a110-db17b3c50008</span><br></pre></td></tr></table></figure><blockquote><p>值得一提的是，<code>ownerReferences</code>是可以手動設定與修改的。比如說上述指令建立了一個 NGINX Pod，這時我們用 kubectl 刪除 Pod 的<code>ownerReferences</code>，就可以讓 Kubernetes 垃圾收集器無法處理到該 Pod。只是這個 Pod 也變成殭屍(或孤兒)，不會因為 Deployment 刪除而被殺掉，因此必須手動殺掉才能。</p></blockquote><p>那麼這樣的方式是否可以用來解決我們遇到的問題呢?</p><p>不幸的是，Kubernetes 垃圾收集器僅能用於刪除 Kubernetes API 資源，因此無法讓我們達到 VirtualMachine 資源實例被刪除前，確保所關聯的虛擬機已被刪除。那這樣該如何實現呢?</p><p>事實上 Kubernetes 也考慮到這樣問題，因此對於 API 資源的級聯刪除提供了兩種模式:</p><ul><li><strong>Background</strong>:在這模式下，Kubernetes 會直接刪除 Owner 資源物件，然後再由垃圾收集器在後台刪除相關的 API 資源物件。</li><li><strong>Foreground</strong>:在這模式下，Owner 資源物件會透過設定<code>metadta.deletionTimestamp</code>欄位來表示『正在刪除中』。這時 Owner 資源物件依然存在於叢集中，並且能透過 REST API 查看到相關資訊。該資源被刪除條件是當移除了<code>metadata.finalizers</code>欄位後，才會真正的從叢集中移除。這樣機制形成了預刪除掛鉤(Pre-delete hook)，因此我們能在正在刪除的期間，開始回收相關的資源(如虛擬機或其他 Kubernetes API 資源等等)，當回收完後，再將該物件刪除。</li></ul><p>其中 Foreground 模式能透過 Kubernetes Finalizers 機制與 OwnerReferences 機制完成。不過 OwnerReferences 只能用於內部 API 資源物件，當想要處理外部資源時，就必須利用 Finalizers 來達成。</p><p>而 Finalizers 機制只需要在 API 資源物件中的<code>metadata.finalizers</code>欄位塞入一個字串值即可，比如說以下範例:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: cloudnative.tw/v1alpha1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-vm-finalizer</span><br><span class="line">  finalizers:</span><br><span class="line">  - finalizer.cloudnative.tw</span><br><span class="line">spec:</span><br><span class="line">  resource:</span><br><span class="line">    cpu: 2</span><br><span class="line">    memory: 4G</span><br><span class="line">EOF</span><br><span class="line">virtualmachine.cloudnative.tw/<span class="built_in">test</span>-vm-finalizer created</span><br></pre></td></tr></table></figure><p>當建立時，接著透過 kubectl 來刪除這個資源實例:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl delete vms <span class="built_in">test</span>-vm-finalizer</span><br><span class="line">virtualmachine.cloudnative.tw <span class="string">"test-vm-finalizer"</span> deleted</span><br></pre></td></tr></table></figure><p>這時會發現 kubectl 卡在刪除指令，且不管怎麼執行都無法刪除。因為這樣情況，我們開啟另一個 Terminal 查看後，發現資源物件依然存在，但<code>metadata.deletionTimestamp</code>被下了時間，這表示該資源已經處於預刪除階段:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get vms <span class="built_in">test</span>-vm-finalizer -o yaml</span><br><span class="line">apiVersion: cloudnative.tw/v1alpha1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  ...</span><br><span class="line">  deletionGracePeriodSeconds: 0</span><br><span class="line">  deletionTimestamp: <span class="string">"2019-10-14T16:28:58Z"</span></span><br><span class="line">  finalizers:</span><br><span class="line">  - finalizer.cloudnative.tw</span><br><span class="line">  name: <span class="built_in">test</span>-vm-finalizer</span><br></pre></td></tr></table></figure><p>那麼該怎麼讓這個資源物件被刪除呢? 我們只要透過<code>kubectl edit</code>把<code>metadata.finalizers</code>欄位拔掉即可。不過這樣做法都是透過 kubectl 來達成，那麼我們該如何在自定義控制器程式中實現呢? 接下來將針對這部份進行說明。</p><h2 id="使用-Finalizer"><a href="#使用-Finalizer" class="headerlink" title="使用 Finalizer"></a>使用 Finalizer</h2><p>本部分將修改<code>controller.go</code>程式，以加入 Finalizers 機制來確保虛擬機被正確刪除。</p><h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><p>由於使用這個功能需要用到 Kubernetes 與 Go 語言，因此需要透過以下來完成條件:</p><ul><li>一座 Kubernetes v1.10+ 叢集。透過 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 建立即可 <code>minikube start --kubernetes-version=v1.15.4</code>。</li><li>一個 Docker 環境，可以直接 Minikube 執行<code>eval $(minikube docker-env)</code>來取的 Docker 參數，並遠端操作。</li><li>安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 <a href="https://golang.org/doc/install" target="_blank" rel="noopener">Go Getting Started</a>。</li></ul><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>從前言的過程中，可以發現 Finalizers 能在<code>metadata.finalizers</code>欄位手動加入來實現預刪除掛鉤。而在程式的實作中，要加入 Finalizers 機制以避免虛擬機變成殭屍(或孤兒)並不難，只要在 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/controller/controller.go" target="_blank" rel="noopener">controller.go</a> 的<code>createServer()</code>函式中，對 VirtualMachine 物件設定<code>metadata.finalizers</code>即可，只不過加入前提需要確保虛擬機已被正確建立，且 VirtualMachine 一定會進入 Active 狀態下進行。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">createServer</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vmCopy := vm.DeepCopy()</span><br><span class="line">ok, _ := c.vm.IsServerExist(vm.Name)</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">...</span><br><span class="line">addFinalizer(&amp;vmCopy.ObjectMeta, finalizerName)</span><br><span class="line"><span class="keyword">if</span> err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineActive, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><ul><li><code>...</code> 表示不更改內容。完整程式請參考 <a href="https://github.com/cloud-native-taiwan/controller101/blob/34bcecb2ae43e3eb9a981da17c76e21f001f06b0/pkg/controller/controller.go#L187-L214" target="_blank" rel="noopener">controller.go L187-L214</a></li><li>其中<code>finalizerName</code>被定義在成 const 變數，其內容為<code>finalizer.cloudnative.tw</code>。</li><li>而<code>addFinalizer()</code>函式而在 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/controller/util.go" target="_blank" rel="noopener">util.go</a> 中被實作。基本上就是傳入 API 物件的 ObjectMeta(metadata) 與 Finalizer 名稱來設定。</li></ul></blockquote><p>修改完成後，當控制器依據 VirtualMachine 資源實例正確建立虛擬機時，就會自動塞入 Finalizers。而當擁有 Finalizers 的資源實例被執行刪除時，Kubernetes API Server 會透過 Update 操作修改<code>metadata.deletionTimestamp</code>欄位，但不會執行 Delete 操作，因此 Informer 實際上收到會是 Update 事件。基於此改變，我們必須在 controller.go 中修改一些流程與內容。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">syncHandler</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> vm.Status.Phase &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">case</span> v1alpha1.VirtualMachineActive:</span><br><span class="line"><span class="keyword">if</span> !vm.ObjectMeta.DeletionTimestamp.IsZero() &#123;</span><br><span class="line"><span class="keyword">if</span> err := c.makeTerminatingPhase(vm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := c.updateUsage(vm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> v1alpha1.VirtualMachineTerminating:</span><br><span class="line"><span class="keyword">if</span> err := c.deleteServer(vm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>...</code> 表示不更改內容。完整程式請參考 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/controller/controller.go" target="_blank" rel="noopener">controller.go</a>。</p></blockquote><p>當 VirtualMachine 實例 Active，且被執行刪除時，可以透過判斷<code>metadata.deletionTimestamp</code>來確認是否進入預刪除階段，若是的話，則將 VirtualMachine 資源實例更新成 Terminating 階段，若不是的話，則持續更新狀態。其中<code>makeTerminatingPhase()</code>的程式實現如下:</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">makeTerminatingPhase</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vmCopy := vm.DeepCopy()</span><br><span class="line"><span class="keyword">return</span> c.updateStatus(vmCopy, v1alpha1.VirtualMachineTerminating, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接著當控制器接收到一個處於<code>Terminating</code>的資源物件時，就會執行<code>deleteServer()</code>來刪除虛擬機，並且直到刪除成功後，才將 Finalizers 從資源實例中移除。一但被 Finalizers 時，Kubernetes 就會在經過<code>deletionGracePeriodSeconds</code>設定的秒數後，將該資源實例從叢集中刪除。<code>deleteServer()</code>的程式實現如下:</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">deleteServer</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vmCopy := vm.DeepCopy()</span><br><span class="line"><span class="keyword">if</span> err := c.vm.DeleteServer(vmCopy.Name); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="comment">// Requeuing object to workqueue for retrying</span></span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">removeFinalizer(&amp;vmCopy.ObjectMeta, finalizerName)</span><br><span class="line"><span class="keyword">if</span> err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineTerminating, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最後由於採用 Finalizers 機制，因此 Informer 並不會觸發<code>DeleteFunc</code>對應的<code>deleteObject()</code>函式，因此我們可以在 Controller 的建構子中註解掉。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(clientset cloudnative.Interface, informer cloudnativeinformer.SharedInformerFactory, vm driver.Interface)</span> *<span class="title">Controller</span></span> &#123;</span><br><span class="line">...</span><br><span class="line">vmInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">AddFunc: controller.enqueue,</span><br><span class="line">UpdateFunc: <span class="function"><span class="keyword">func</span><span class="params">(old, <span class="built_in">new</span> <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">controller.enqueue(<span class="built_in">new</span>)</span><br><span class="line">&#125;,</span><br><span class="line"><span class="comment">// DeleteFunc: controller.deleteObject,</span></span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">return</span> controller</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>...</code> 表示不更改內容。完整程式請參考 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/controller/controller.go" target="_blank" rel="noopener">controller.go</a></p></blockquote><h3 id="Running"><a href="#Running" class="headerlink" title="Running"></a>Running</h3><p>當上述功能實現後，且已有新增完 VirtualMachine CRD 的 Kubernetes 環境時，就能執行以下指令啟動控制器:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> $(minikube docker-env)</span><br><span class="line">$ go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config \</span><br><span class="line">    -v=3 --logtostderr \</span><br><span class="line">    --leader-elect=<span class="literal">false</span> \</span><br><span class="line">    --vm-driver=docker</span><br><span class="line">...</span><br><span class="line">I1015 16:02:57.180484   62884 controller.go:77] Starting the controller</span><br><span class="line">I1015 16:02:57.180665   62884 controller.go:78] Waiting <span class="keyword">for</span> the informer caches to sync</span><br><span class="line">I1015 16:02:57.285693   62884 controller.go:86] Started workers</span><br></pre></td></tr></table></figure><p>接著開啟另一個 Terminal 來建立 VirtualMachine 資源實例。當建立時，會發現控制器更新了 test-vm 資源實例，這時可以利用 kubectl 與 docker 查看狀態:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: cloudnative.tw/v1alpha1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-vm</span><br><span class="line">spec:</span><br><span class="line">  resource:</span><br><span class="line">    cpu: 2</span><br><span class="line">    memory: 4G</span><br><span class="line">EOF</span><br><span class="line">virtualmachine.cloudnative.tw/<span class="built_in">test</span>-vm created</span><br><span class="line"></span><br><span class="line">$ kubectl get vms</span><br><span class="line">NAME      STATUS   CPU   MEMORY                AGE</span><br><span class="line"><span class="built_in">test</span>-vm   Active   0     0.10977787071142493   44s</span><br><span class="line"></span><br><span class="line">$ docker ps --filter <span class="string">"name=test-vm"</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">347f8626f36a        nginx:1.17.4        <span class="string">"nginx -g 'daemon of…"</span>   4 seconds ago       Up 3 seconds        80/tcp              <span class="built_in">test</span>-vm</span><br></pre></td></tr></table></figure><p>接著我們利用 kubectl 觀察 test-vm 的<code>metadata</code>變化:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get vms <span class="built_in">test</span>-vm -o=jsonpath=<span class="string">'&#123;.metadata.finalizers&#125;'</span></span><br><span class="line">[finalizer.cloudnative.tw]</span><br></pre></td></tr></table></figure><p>而當執行<code>kubectl delete vm test-vm</code>時，就會發現這樣變化:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get vms -w</span><br><span class="line">NAME      STATUS   CPU   MEMORY                AGE</span><br><span class="line">test-vm   Active   0     0.10595776165736437   4m4s</span><br><span class="line">test-vm   Terminating   0     0.10595776165736437   4m6s</span><br></pre></td></tr></table></figure><p>這時查看 API 資源與 Container 時，都會被正確移除。另外也可以嘗試把控制器暫時關閉，並執行刪除一個 VirtualMachine 資源實例的操作，這時會看到該操作卡在刪除指令下，並且資源實例還存在於叢集中。而當重新啟動控制器時，才會停止這樣狀況。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天簡單認識 Kubernetes 的垃圾收集器與 Finalizers 的機制，並且在自定義控制器實作 Finalizers 來確保外部資源能夠在 Kubernetes 內部關聯的 API 資源被刪除時，優先被回收。</p><p>到這邊一個自定義控制器大致上已完成，而接下來我們將說明如何讓控制器部署到 Kubernetes 叢集中，並且能夠實現哪些功能來加強這個控制器(Expose Metrics、Admission Controller 與 Fake client testing)。</p><blockquote><p>由於鐵人賽文章不夠用，因此之後都會以 <a href="http://k2r2bai.com">KaiRen’s Blog</a> 來新增這些內容。</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/</a></li><li><a href="https://book.kubebuilder.io/reference/using-finalizers.html" target="_blank" rel="noopener">https://book.kubebuilder.io/reference/using-finalizers.html</a></li><li><a href="https://draveness.me/kubernetes-garbage-collector" target="_blank" rel="noopener">https://draveness.me/kubernetes-garbage-collector</a></li><li><a href="https://blog.openshift.com/garbage-collection-custom-resources-available-kubernetes-1-8/" target="_blank" rel="noopener">https://blog.openshift.com/garbage-collection-custom-resources-available-kubernetes-1-8/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;https://k2r2bai.com/2019/10/13/ironman2020/day28/&quot;&gt;動手實作 Kubernetes 自定義控制器 Part4&lt;/a&gt; 文章結語部分，我有提到目前實作的自定義控制器還存在著問題(如下圖)，其中就是自定義資源 VirtualMachine 的實例被刪除前，未正確透過 VM Driver 刪除實際管理的虛擬機，這樣情況下的虛擬機都會變成失去控制器管理的殭屍(或孤兒)。基於此問題，今天將說明該如何修改程式以解決這樣問題。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>動手實作 Kubernetes 自定義控制器 Part4</title>
    <link href="https://k2r2bai.com/2019/10/13/ironman2020/day28/"/>
    <id>https://k2r2bai.com/2019/10/13/ironman2020/day28/</id>
    <published>2019-10-12T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="https://k2r2bai.com/2019/10/12/ironman2020/day27/">動手實作 Kubernetes 自定義控制器 Part3</a> 文章中，了解如何實現自定義控制器的高可靠架構，而今天將延續之前位完成的部分，會簡單以 Docker 實作一個虛擬機驅動來提供給自定義控制器使用，控制器會依據自定義資源<code>VirtualMachine</code>的內容，來協調完成預期結果的事情。如下架構圖所示。</p><a id="more"></a><p><img src="https://i.imgur.com/3xhW8am.png" alt></p><p>由於為了方便大家在 Minikube 上執行這個自定義控制器，因此這邊實作了一個 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/driver/driver.go" target="_blank" rel="noopener">VM Driver</a> 的 Golang 介面，並以該介面實現一個 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/driver/docker.go" target="_blank" rel="noopener">Docker Driver</a> 來作為使用。這個 Docker Driver 會以 Docker 預設的系統環境變數來載入 Endpoint、Certs 等等 Docker client 需要的資訊，接著透過這些資訊建立一個 client 與 Docker API 溝通進行各種操作。這邊介面只簡單實現以下函式來完成範例:</p><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Interface <span class="keyword">interface</span> &#123;</span><br><span class="line">CreateServer(*CreateRequest) (*CreateReponse, error)</span><br><span class="line">DeleteServer(name <span class="keyword">string</span>) error</span><br><span class="line">IsServerExist(name <span class="keyword">string</span>) (<span class="keyword">bool</span>, error)</span><br><span class="line">GetServerStatus(name <span class="keyword">string</span>) (*GetStatusReponse, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>當控制器收到 VirtualMachine 的實例建立時，控制器會在<code>syncHandler()</code>函式依據接受到的資源物件資訊來呼叫 VM Driver 進行處理相關事情(如建立虛擬機環境、更新虛擬機使用率等等)，當處理完成後，再依據回應的內容更新到 VirtualMachine 資源實例的<code>.status</code>內容。而當控制器收到有個 VirtualMachine 實例被刪除時，就會呼叫 Informer 的<code>DeleteFunc</code>來進行處理實際虛擬機移除的事情。</p><blockquote><ul><li>由於這只是為了說明如何開發控制器，因此該範例使用的 Docker Driver 在建立容器時，只會以 NGINX 映像檔為基礎來建立。</li><li>原本規劃 Fake Driver 與 KVM 來模擬，但因為時間關析，只能之後再補上。</li></ul></blockquote><h2 id="協調資源"><a href="#協調資源" class="headerlink" title="協調資源"></a>協調資源</h2><p>本部分將修改<code>controller.go</code>程式，以實現自定義資源 VirtualMachine 管理虛擬機的機制。</p><h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><p>由於使用這個功能需要用到 Kubernetes 與 Go 語言，因此需要透過以下來完成條件:</p><ul><li>一座 Kubernetes v1.10+ 叢集。透過 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 建立即可 <code>minikube start --kubernetes-version=v1.15.4</code>。</li><li>一個 Docker 環境，可以直接 Minikube 執行<code>eval $(minikube docker-env)</code>來取的 Docker 參數，並遠端操作。</li><li>安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 <a href="https://golang.org/doc/install" target="_blank" rel="noopener">Go Getting Started</a>。</li></ul><h3 id="管理虛擬機邏輯實現"><a href="#管理虛擬機邏輯實現" class="headerlink" title="管理虛擬機邏輯實現"></a>管理虛擬機邏輯實現</h3><p>前幾天我們在實作控制器時，有提到主要處理 API 資源實例的函式是<code>syncHandler()</code>，因此大部分邏輯會在這邊實現。但由於該控制器需要透過一些方法管理實際的虛擬機，因此這邊以 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/driver/driver.go" target="_blank" rel="noopener">VM Driver</a> 實現 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/pkg/driver/docker.go" target="_blank" rel="noopener">Docker Driver</a> 方式進行模擬。這時要修改控制器結構與建構子如下。</p><h4 id="pkg-controller-controller-go"><a href="#pkg-controller-controller-go" class="headerlink" title="pkg/controller/controller.go"></a>pkg/controller/controller.go</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Controller <span class="keyword">struct</span> &#123;</span><br><span class="line">...</span><br><span class="line">vm        driver.Interface <span class="comment">// 管理實際虛擬機的驅動程式</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(clientset cloudnative.Interface, informer cloudnativeinformer.SharedInformerFactory, vm driver.Interface)</span> *<span class="title">Controller</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line">controller := &amp;Controller&#123;</span><br><span class="line">        ...</span><br><span class="line">        vm:        vm,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">return</span> controller</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>...</code> 表示不更改內容。完整程式請參考 <a href="https://github.com/cloud-native-taiwan/controller101/blob/9669f5e1e744fdc33baf5c0c229c9cb27f095b20/pkg/controller/controller.go#L44-L72" target="_blank" rel="noopener">controller.go L44-L72</a></p></blockquote><p>完成後，就可以透過上面物件，在這個控制器結構的函式操作虛擬機。接著在<code>syncHandler()</code>實現協調循環的邏輯:</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">syncHandler</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">namespace, name, err := cache.SplitMetaNamespaceKey(key)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"invalid resource key: %s"</span>, key))</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vm, err := c.lister.VirtualMachines(namespace).Get(name)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> errors.IsNotFound(err) &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"virtualmachine '%s' in work queue no longer exists"</span>, key))</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> vm.Status.Phase &#123;</span><br><span class="line"><span class="keyword">case</span> v1alpha1.VirtualMachineNone:</span><br><span class="line"><span class="keyword">if</span> err := c.makeCreatingPhase(vm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> v1alpha1.VirtualMachinePending, v1alpha1.VirtualMachineFailed:</span><br><span class="line"><span class="keyword">if</span> err := c.createServer(vm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> v1alpha1.VirtualMachineActive:</span><br><span class="line"><span class="keyword">if</span> err := c.updateUsage(vm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用於更新 VirtualMachine 資源狀態的通用函式</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">updateStatus</span><span class="params">(vm *v1alpha1.VirtualMachine, phase v1alpha1.VirtualMachinePhase, reason error)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vm.Status.Reason = <span class="string">""</span></span><br><span class="line"><span class="keyword">if</span> reason != <span class="literal">nil</span> &#123;</span><br><span class="line">vm.Status.Reason = reason.Error()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vm.Status.Phase = phase</span><br><span class="line">vm.Status.LastUpdateTime = metav1.NewTime(time.Now())</span><br><span class="line">_, err := c.clientset.CloudnativeV1alpha1().VirtualMachines(vm.Namespace).Update(vm)</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用於將虛擬機狀態新增到 VirtualMachine 資源的通用函式</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">appendServerStatus</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">status, err := c.vm.GetServerStatus(vm.Name)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vm.Status.Server.Usage.CPU = status.CPUPercentage</span><br><span class="line">vm.Status.Server.Usage.Memory = status.MemoryPercentage</span><br><span class="line">vm.Status.Server.State = status.State</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>當收到 Informer 的 Add/Update 事件時，會將資源實例的物件放到 Workqueue，然後控制器的 Workers 會呼叫<code>processNextWorkItem()</code>函式來持續消化 Workqueue 中的物件，並在取出物件的 Key 後，將其丟到<code>syncHandler()</code>函式處理。而<code>syncHandler()</code>函式會透過 Lister 從本地快取中獲取資源實例的內容，這時我們就能透過內容的狀態來處理對應事情。以上面程式為例，我們分成以下幾個狀態來處理。</p><blockquote><p>這邊使用不同狀態來處理不同過程，其目的是確保控制器不會因為實例的狀態更新，而一直觸發 Update 事件導致無限循環，因此以狀態來做收斂的點。</p></blockquote><ul><li><strong>VirtualMachineNone</strong>: 由於 VirtualMachine 資源實例被建立時，並不會有任何資源狀態，因此該狀態可用於判斷是否是第一次建立，若是的話則將狀態更新為 Creating，這樣可以讓該資源被標示為即將建立虛擬機。程式內容如下:</li></ul><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">makeCreatingPhase</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vmCopy := vm.DeepCopy()</span><br><span class="line"><span class="keyword">return</span> c.updateStatus(vmCopy, v1alpha1.VirtualMachineCreating, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>VirtualMachineCreating</strong>: 當處於 Creating 時，控制器會呼叫 VM Driver 以建立虛擬機，若成功的話，則更新 VirtualMachine 資源的<code>.status</code>為 Active 狀態;若失敗的話，則標示為 Failed，狀態，並提供失敗原因的訊息。程式內容如下:</li></ul><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">createServer</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vmCopy := vm.DeepCopy()</span><br><span class="line">ok, _ := c.vm.IsServerExist(vm.Name)</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">req := &amp;driver.CreateRequest&#123;</span><br><span class="line">Name:   vm.Name,</span><br><span class="line">CPU:    vm.Spec.Resource.Cpu().Value(),</span><br><span class="line">Memory: vm.Spec.Resource.Memory().Value(),</span><br><span class="line">&#125;</span><br><span class="line">resp, err := c.vm.CreateServer(req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineFailed, err); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">vmCopy.Status.Server.ID = resp.ID</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := c.appendServerStatus(vmCopy); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineActive, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>VirtualMachineFailed</strong>: 類似 Creating 狀態，當嘗試透過 VM Driver 建立虛擬機失敗時，會讓資源 Requeuing 到 Workqueue 中，並繼續在協調循環中重新嘗試建立虛擬機，直到建立成功或 VirtualMachine 資源被刪除。程式內容同 Creating 狀態。</li><li><strong>VirtualMachineActive</strong>: 當虛擬機被正確建立，並且能夠取得狀態後，就會進入 Active 狀態。而當資源一直處於 Active 時，就能夠持續透過 VM Driver 獲取當前虛擬機狀態，並更新到 API 資源上。程式內容如下:</li></ul><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">updateUsage</span><span class="params">(vm *v1alpha1.VirtualMachine)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">vmCopy := vm.DeepCopy()</span><br><span class="line">t := subtractTime(vmCopy.Status.LastUpdateTime.Time)</span><br><span class="line"><span class="keyword">if</span> t.Seconds() &gt; periodSec &#123;</span><br><span class="line"><span class="keyword">if</span> err := c.appendServerStatus(vmCopy); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineActive, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>這邊<code>subtractTime()</code>用於避免控制器一直執行<code>updateStatus()</code>，而導致無限循環。</p></blockquote><p>而當 API 資源物件被刪除時，Informer 會呼叫<code>DeleteFunc</code>的對應函式<code>deleteObject()</code>來刪除虛擬機。程式內容如下:</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">deleteObject</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">vm := obj.(*v1alpha1.VirtualMachine)</span><br><span class="line"><span class="keyword">if</span> err := c.vm.DeleteServer(vm.Name); err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Errorf(<span class="string">"Failed to delete the '%s' server: %v"</span>, vm.Name, err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cmd-main-go"><a href="#cmd-main-go" class="headerlink" title="cmd/main.go"></a>cmd/main.go</h4><p>當 Controller 與 VM Driver 程式都完成後，就可以修改主程式來反映功能改變:</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> (</span><br><span class="line">...</span><br><span class="line">driverName         <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">parseFlags</span><span class="params">()</span></span> &#123;</span><br><span class="line">...</span><br><span class="line">flag.StringVarP(&amp;driverName, <span class="string">"vm-driver"</span>, <span class="string">""</span>, <span class="string">""</span>, <span class="string">"Driver is one of: [fake docker]."</span>)</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">var</span> vmDriver driver.Interface</span><br><span class="line"><span class="keyword">switch</span> driverName &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">docker, err := driver.NewDockerDriver()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to create docker driver: %s"</span>, err.Error())</span><br><span class="line">&#125;</span><br><span class="line">vmDriver = docker</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">klog.Fatalf(<span class="string">"The driver '%s' is not supported."</span>, driverName)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">controller := controller.New(clientset, informer, vmDriver)</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>...</code> 表示不更改內容。完整程式請參考 <a href="https://github.com/cloud-native-taiwan/controller101/blob/master/cmd/main.go" target="_blank" rel="noopener">main.co</a></p></blockquote><h3 id="執行"><a href="#執行" class="headerlink" title="執行"></a>執行</h3><p>當上述功能實現後，且已有新增完 VirtualMachine CRD 的 Kubernetes 環境時，就可以執行以下指令來啟動控制器:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> $(minikube docker-env)</span><br><span class="line">$ go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config \</span><br><span class="line">    -v=3 --logtostderr \</span><br><span class="line">    --leader-elect=<span class="literal">false</span> \</span><br><span class="line">    --vm-driver=docker</span><br><span class="line">...</span><br><span class="line">I1015 16:02:57.180484   62884 controller.go:77] Starting the controller</span><br><span class="line">I1015 16:02:57.180665   62884 controller.go:78] Waiting <span class="keyword">for</span> the informer caches to sync</span><br><span class="line">I1015 16:02:57.285693   62884 controller.go:86] Started workers</span><br></pre></td></tr></table></figure><p>接著開啟另一個 Terminal 來建立 VirtualMachine 資源實例。當建立時，會發現控制器更新了 test-vm 資源實例，這時可以利用 kubectl 查看狀態:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: cloudnative.tw/v1alpha1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-vm</span><br><span class="line">spec:</span><br><span class="line">  resource:</span><br><span class="line">    cpu: 2</span><br><span class="line">    memory: 4G</span><br><span class="line">EOF</span><br><span class="line">virtualmachine.cloudnative.tw/<span class="built_in">test</span>-vm created</span><br><span class="line"></span><br><span class="line">$ kubectl get vms</span><br><span class="line">NAME      STATUS   CPU   MEMORY                AGE</span><br><span class="line"><span class="built_in">test</span>-vm   Active   0     0.10977787071142493   44s</span><br></pre></td></tr></table></figure><p>由於本範例使用 Docker 作為虛擬機驅動程式，因此該資源實際上是建立一個容器。我們可以利用 docker 指令來查看:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ docker ps --filter <span class="string">"name=test-vm"</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">b0d7f2be48e5        nginx:1.17.4        <span class="string">"nginx -g 'daemon of…"</span>   8 seconds ago       Up 6 seconds        80/tcp              <span class="built_in">test</span>-vm</span><br><span class="line"></span><br><span class="line">$ docker inspect <span class="built_in">test</span>-vm -f <span class="string">"&#123;&#123;.HostConfig.Memory&#125;&#125;"</span></span><br><span class="line">4000000000</span><br><span class="line"></span><br><span class="line">$ docker inspect <span class="built_in">test</span>-vm -f <span class="string">"&#123;&#123;.HostConfig.NanoCpus&#125;&#125;"</span></span><br><span class="line">2</span><br></pre></td></tr></table></figure><p>接著我們來增加這個 NIGNX 的工作負載，以查看 CPU 變化:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ IP=$(docker inspect <span class="built_in">test</span>-vm -f <span class="string">"&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;"</span>)</span><br><span class="line">$ docker run --rm -it busybox /bin/sh -c <span class="string">"while :; do wget -O- <span class="variable">$&#123;IP&#125;</span>; done"</span></span><br></pre></td></tr></table></figure><p>開啟新 Terminal 以 kubectl 指令來查看:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get vms -w</span><br><span class="line">NAME      STATUS   CPU   MEMORY                AGE</span><br><span class="line">test-vm   Active   0     0.11279374628042013   5m30s</span><br><span class="line">test-vm   Active   15.637706179775282   0.11279374628042013   5m43s</span><br><span class="line">test-vm   Active   15.688157325581395   0.11299480465168647   6m13s</span><br><span class="line">test-vm   Active   15.55665426966292    0.11279374628042013   6m43s</span><br></pre></td></tr></table></figure><blockquote><p>由於控制器設計關析，CPU 與 Memory 只會每 30s 同步一次。</p></blockquote><p><img src="https://i.imgur.com/lWMxYl7.png" alt></p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天將控制器管理 VirtualMachine 資源實例的邏輯完成。一但完成，就能利用 Kubernetes-like API 來管理虛擬機的生命週期，或取得虛擬機狀態等等事情。然而今天實作部分，事實上還有一些問題存在，比如說我們先把自定義控制器暫時關閉，然後執行<code>kubectl delete vm test-vm</code>指令來將該資源實例從 Kubernetes 中刪除，這時查看虛擬機列表(因為 VM Driver 為 Docker，因此對應查看為 <code>docker ps</code>)時，就會發現被管理的虛擬機依然存在，並且當重新啟動控制器時，也會因為該資源實例已經被刪除，因此無法讓控制器來協助刪除，這樣就會形成殭屍虛擬機問題。如下圖所示。</p><p><img src="https://i.imgur.com/6qecfOu.png" alt></p><blockquote><p>從 Controller 程式碼中也可以從<code>deleteObject()</code>看出問題，因為這邊若發生刪除錯誤，就會造成外部資源變成殭屍(或孤兒)。</p></blockquote><p>那麼當遇到這個問題時，該怎麼解決呢?明天我們將針對這部份來實作，以確保 API 資源實例一定要先刪除所管理的虛擬機後，才能從 Kubernetes 叢集中刪除。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://godoc.org/github.com/docker/docker/client" target="_blank" rel="noopener">https://godoc.org/github.com/docker/docker/client</a></li><li><a href="https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc" target="_blank" rel="noopener">https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;https://k2r2bai.com/2019/10/12/ironman2020/day27/&quot;&gt;動手實作 Kubernetes 自定義控制器 Part3&lt;/a&gt; 文章中，了解如何實現自定義控制器的高可靠架構，而今天將延續之前位完成的部分，會簡單以 Docker 實作一個虛擬機驅動來提供給自定義控制器使用，控制器會依據自定義資源&lt;code&gt;VirtualMachine&lt;/code&gt;的內容，來協調完成預期結果的事情。如下架構圖所示。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>動手實作 Kubernetes 自定義控制器 Part3</title>
    <link href="https://k2r2bai.com/2019/10/12/ironman2020/day27/"/>
    <id>https://k2r2bai.com/2019/10/12/ironman2020/day27/</id>
    <published>2019-10-11T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="https://k2r2bai.com/2019/10/11/ironman2020/day26/">動手實作 Kubernetes 自定義控制器 Part2</a> 文章中，我們利用 client-go 與產生的 Client 函式庫實作了一個控制器功能。而今天想在控制器實現協調預期狀態之前，探討一下 Kubernetes 自定義控制器的高可靠(Highly Available，HA)如何實現。</p><p>在 Kubernetes 中，許多系統相關元件都是以 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/#extension-patterns" target="_blank" rel="noopener">Controller Pattern</a> 方式實現，比如說: Scheduler 與 Controller Manager。這些元件通常負責 Kubernetes 中的某一環核心功能，像是 Scheduler 負責 Pod 的節點分配， Controller Manager 提供許多 Kubernetes API 資源的功能協調與關聯。那麼如果這些元件發生故障了，就可能造成某部分功能無法正常運行，進而影響到整個叢集的健康，這樣該如何解決呢?</p><a id="more"></a><p>如果有閱讀過<a href="https://k2r2bai.com/2019/09/19/ironman2020/day04/">淺談 Kubernetes 高可靠架構</a>與<a href="https://k2r2bai.com/2019/09/20/ironman2020/day05/">實現 Kubernetes 高可靠架構部署</a>文章的人，可以從中知道 Kubernetes 的核心元件都支援 HA 的部署實現。而其中的兩個重要控制器 Scheduler 與 Controller Manager 是以 <a href="https://en.wikipedia.org/wiki/Lease_(computer_science)" target="_blank" rel="noopener">Lease</a> 機制實現 Active-Passive 架構。這表示一個環境中，有多個相同元件運作時，只會有一個作為 Leader 負責程式的功能，而其餘則會等待 Leader 發生錯時，才接手工作。</p><p>而這機制的實踐方式有很多種，比如基於 Redis、Zookeeper、Consul、etcd，或是資料庫的分散式鎖(Distributed Lock)。而 Kubernetes 則是是採用資源鎖(Resource Lock)概念來實現，基本上就是建立 Kubernetes API 資源 ConfigMap、 Endpoint 或 <a href="https://github.com/kubernetes/api/blob/master/coordination/v1/types.go#L27" target="_blank" rel="noopener">Lease</a> 來維護分散式鎖的狀態。</p><blockquote><p>Kubernetes 從 v1.15 版本開始推薦使用 Lease 資源實現，而 ConfigMap、 Endpoint 已經被棄用。</p></blockquote><p>分散式鎖最常見的實現方式就是搶資源的擁有權，搶到的人就是 Leader，接著 Leader 開始定期更新鎖狀態，以表示自己處於活躍狀態，以確保其他人沒辦法搶走擁有權。而 Kubernetes 也類似這樣概念，基本上就是搶 API 上的某個資源，當搶到時，就在該資源中標示自己是擁有者，並持續更新時間來表示自己還處於活躍狀態;而其他則持續取得資源鎖中的更新時間進行比對，以確認原擁有者是否已經死亡，若是的話，則更新資源鎖來標示自己為擁有者。</p><p>這邊看一下 Kubernetes Controller Manager 實際運作狀況，當 Controller Manager 被啟動時，預設會透過<code>--leader-elect=true</code>來開啟 HA 功能。當正確啟動後，在 kube-system 底下，就會看到被新增了一個用於維護分散式鎖狀態的 Endpoint 資源:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl -n kube-system get ep kube-controller-manager -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: <span class="string">'&#123;"holderIdentity":"k8s-m3_9f51cc32-679e-4cc3-951e-c35f8688bbc3","leaseDurationSeconds":15,"acquireTime":"2019-09-25T05:39:17Z","renewTime":"2019-10-09T09:37:48Z","leaderTransitions":6&#125;'</span></span><br><span class="line">  creationTimestamp: <span class="string">"2019-09-20T14:00:55Z"</span></span><br><span class="line">  name: kube-controller-manager</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><p>然後可以在該資源的<code>metadata.annotations</code>看到用於儲存狀態的<code>control-plane.alpha.kubernetes.io/leader</code>欄位。其中<code>holderIdentity</code>用於表示當前擁有者，<code>acquireTime</code>為擁有者取得持有權的時間，<code>renewTime</code>為當前擁有者上一次活躍時間。而更換 Leader 條件是當 renewTime 與自己當下時間計算超過<code>leaseDurationSeconds</code>時進行。</p><p>當確認 Leader 後，即可透過 kubectl logs 來查看元件執行結果:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Leader</span></span><br><span class="line">$ kubectl -n kube-system logs -f kube-controller-manager-k8s-m3</span><br><span class="line">I0923 14:02:27.809016       1 serving.go:319] Generated self-signed cert <span class="keyword">in</span>-memory</span><br><span class="line">I0923 14:02:28.214820       1 controllermanager.go:161] Version: v1.16.0</span><br><span class="line">I0923 14:02:28.215142       1 secure_serving.go:123] Serving securely on 127.0.0.1:10257</span><br><span class="line">I0923 14:02:28.215415       1 deprecated_insecure_serving.go:53] Serving insecurely on [::]:10252</span><br><span class="line">I0923 14:02:28.215453       1 leaderelection.go:241] attempting to acquire leader lease  kube-system/kube-controller-manager...</span><br><span class="line">I0925 05:39:17.506983       1 leaderelection.go:251] successfully acquired lease kube-system/kube-controller-manager</span><br><span class="line">I0925 05:39:17.507091       1 event.go:255] Event(v1.ObjectReference&#123;Kind:<span class="string">"Endpoints"</span>, Namespace:<span class="string">"kube-system"</span>, Name:<span class="string">"kube-controller-manager"</span>, UID:<span class="string">"b6627d30-c879-449f-99ea-f94d536f2516"</span>, APIVersion:<span class="string">"v1"</span>, ResourceVersion:<span class="string">"794261"</span>, FieldPath:<span class="string">""</span>&#125;): <span class="built_in">type</span>: <span class="string">'Normal'</span> reason: <span class="string">'LeaderElection'</span> k8s-m3_9f51cc32-679e-4cc3-951e-c35f8688bbc3 became leader</span><br><span class="line">I0925 05:39:17.766322       1 plugins.go:100] No cloud provider specified.</span><br><span class="line">I0925 05:39:17.767107       1 shared_informer.go:197] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> tokens</span><br><span class="line">I0925 05:39:17.778474       1 controllermanager.go:534] Started <span class="string">"daemonset"</span></span><br><span class="line">I0925 05:39:17.778485       1 daemon_controller.go:267] Starting daemon sets controller</span><br><span class="line">I0925 05:39:17.778505       1 shared_informer.go:197] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> daemon sets</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Not leader</span></span><br><span class="line">$ kubectl -n kube-system logs -f kube-controller-manager-k8s-m1</span><br><span class="line">I0925 05:39:06.784042       1 serving.go:319] Generated self-signed cert <span class="keyword">in</span>-memory</span><br><span class="line">I0925 05:39:07.932147       1 controllermanager.go:161] Version: v1.16.0</span><br><span class="line">I0925 05:39:07.932782       1 secure_serving.go:123] Serving securely on 127.0.0.1:10257</span><br><span class="line">I0925 05:39:07.933364       1 deprecated_insecure_serving.go:53] Serving insecurely on [::]:10252</span><br><span class="line">I0925 05:39:07.933418       1 leaderelection.go:241] attempting to acquire leader lease  kube-system/kube-controller-manager...</span><br></pre></td></tr></table></figure><p>講了這麼多，那究竟該如何在自己的控制器實現同樣功能呢?</p><p>事實上，Kubernetes client-go 提供了 <a href="https://github.com/kubernetes/client-go/tree/master/tools/leaderelection" target="_blank" rel="noopener">Leader Election</a> 功能，因此我們能夠透過這個 Package 輕易實作。 </p><h2 id="Use-Leader-Election-Package"><a href="#Use-Leader-Election-Package" class="headerlink" title="Use Leader Election Package"></a>Use Leader Election Package</h2><p>在 client-go 中，以提供了 <a href="https://github.com/kubernetes/client-go/tree/master/examples/leader-election" target="_blank" rel="noopener">Leader Election Example</a> 讓大家可以了解如何實現。因此可以下載 client-go 來進行測試，或是依據範例在控制器中實作。</p><h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><p>由於使用這個功能需要用到 Kubernetes 與 Go 語言，因此需要透過以下來完成條件:</p><ul><li>一座 Kubernetes v1.10+ 叢集。透過 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 建立即可 <code>minikube start --kubernetes-version=v1.15.4</code>。</li><li>安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 <a href="https://golang.org/doc/install" target="_blank" rel="noopener">Go Getting Started</a>。</li></ul><h3 id="在控制器實作"><a href="#在控制器實作" class="headerlink" title="在控制器實作"></a>在控制器實作</h3><p>要在自定義控制器中，應用 Leader Election 機制其實不難，只要參考 client-go 的範例，在<code>OnStartedLeading()</code>函式中執行控制器程式實例的啟動函式即可，而當觸發<code>OnStoppedLeading()</code>時，就關閉控制器程式的運作。如以下程式，我們修改 <a href="https://github.com/cloud-native-taiwan/controller101" target="_blank" rel="noopener">Controller101</a> 的 main.go。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"context"</span></span><br><span class="line">goflag <span class="string">"flag"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line"><span class="string">"os/signal"</span></span><br><span class="line"><span class="string">"syscall"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line"></span><br><span class="line"><span class="string">"github.com/cloud-native-taiwan/controller101/pkg/controller"</span></span><br><span class="line">cloudnative <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/clientset/versioned"</span></span><br><span class="line">cloudnativeinformer <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/informers/externalversions"</span></span><br><span class="line"><span class="string">"github.com/cloud-native-taiwan/controller101/pkg/version"</span></span><br><span class="line">flag <span class="string">"github.com/spf13/pflag"</span></span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line">clientset <span class="string">"k8s.io/client-go/kubernetes"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/rest"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/clientcmd"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/leaderelection"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/leaderelection/resourcelock"</span></span><br><span class="line"><span class="string">"k8s.io/klog"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> defaultSyncTime = time.Second * <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">kubeconfig         <span class="keyword">string</span></span><br><span class="line">showVersion        <span class="keyword">bool</span></span><br><span class="line">threads            <span class="keyword">int</span></span><br><span class="line">leaderElect        <span class="keyword">bool</span></span><br><span class="line">id                 <span class="keyword">string</span></span><br><span class="line">leaseLockName      <span class="keyword">string</span></span><br><span class="line">leaseLockNamespace <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">parseFlags</span><span class="params">()</span></span> &#123;</span><br><span class="line">flag.StringVarP(&amp;kubeconfig, <span class="string">"kubeconfig"</span>, <span class="string">""</span>, <span class="string">""</span>, <span class="string">"Absolute path to the kubeconfig file."</span>)</span><br><span class="line">flag.IntVarP(&amp;threads, <span class="string">"threads"</span>, <span class="string">""</span>, <span class="number">2</span>, <span class="string">"Number of worker threads used by the controller."</span>)</span><br><span class="line">flag.StringVarP(&amp;id, <span class="string">"holder-identity"</span>, <span class="string">""</span>, os.Getenv(<span class="string">"POD_NAME"</span>), <span class="string">"the holder identity name"</span>)</span><br><span class="line">flag.BoolVarP(&amp;leaderElect, <span class="string">"leader-elect"</span>, <span class="string">""</span>, <span class="literal">true</span>, <span class="string">"Start a leader election client and gain leadership before executing the main loop. "</span>)</span><br><span class="line">flag.StringVar(&amp;leaseLockName, <span class="string">"lease-lock-name"</span>, <span class="string">"controller101"</span>, <span class="string">"the lease lock resource name"</span>)</span><br><span class="line">flag.StringVar(&amp;leaseLockNamespace, <span class="string">"lease-lock-namespace"</span>, <span class="string">""</span>, <span class="string">"the lease lock resource namespace"</span>)</span><br><span class="line">flag.BoolVarP(&amp;showVersion, <span class="string">"version"</span>, <span class="string">""</span>, <span class="literal">false</span>, <span class="string">"Display the version."</span>)</span><br><span class="line">flag.CommandLine.AddGoFlagSet(goflag.CommandLine)</span><br><span class="line">flag.Parse()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">restConfig</span><span class="params">(kubeconfig <span class="keyword">string</span>)</span> <span class="params">(*rest.Config, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> kubeconfig != <span class="string">""</span> &#123;</span><br><span class="line">cfg, err := clientcmd.BuildConfigFromFlags(<span class="string">""</span>, kubeconfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cfg, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cfg, err := rest.InClusterConfig()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cfg, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">parseFlags()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> showVersion &#123;</span><br><span class="line">fmt.Fprintf(os.Stdout, <span class="string">"%s\n"</span>, version.GetVersion())</span><br><span class="line">os.Exit(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k8scfg, err := restConfig(kubeconfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to build rest config: %s"</span>, err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k8sclientset := clientset.NewForConfigOrDie(k8scfg)</span><br><span class="line">clientset, err := cloudnative.NewForConfig(k8scfg)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to build cloudnative clientset: %s"</span>, err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">informer := cloudnativeinformer.NewSharedInformerFactory(clientset, defaultSyncTime)</span><br><span class="line">controller := controller.New(clientset, informer)</span><br><span class="line">ctx, cancel := context.WithCancel(context.Background())</span><br><span class="line">signalChan := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> leaderElect &#123;</span><br><span class="line">lock := &amp;resourcelock.LeaseLock&#123;</span><br><span class="line">LeaseMeta: metav1.ObjectMeta&#123;</span><br><span class="line">Name:      leaseLockName,</span><br><span class="line">Namespace: leaseLockNamespace,</span><br><span class="line">&#125;,</span><br><span class="line">Client: k8sclientset.CoordinationV1(),</span><br><span class="line">LockConfig: resourcelock.ResourceLockConfig&#123;</span><br><span class="line">Identity: id,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">go</span> leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig&#123;</span><br><span class="line">Lock:            lock,</span><br><span class="line">ReleaseOnCancel: <span class="literal">true</span>,</span><br><span class="line">LeaseDuration:   <span class="number">60</span> * time.Second,</span><br><span class="line">RenewDeadline:   <span class="number">15</span> * time.Second,</span><br><span class="line">RetryPeriod:     <span class="number">5</span> * time.Second,</span><br><span class="line">Callbacks: leaderelection.LeaderCallbacks&#123;</span><br><span class="line">OnStartedLeading: <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := controller.Run(ctx, threads); err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to run the controller instance: %s."</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">klog.Infof(<span class="string">"%s: leading"</span>, id)</span><br><span class="line">&#125;,</span><br><span class="line">OnStoppedLeading: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">controller.Stop()</span><br><span class="line">klog.Infof(<span class="string">"%s: lost"</span>, id)</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := controller.Run(ctx, threads); err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to run the controller instance: %s."</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&lt;-signalChan</span><br><span class="line">cancel()</span><br><span class="line">controller.Stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="執行"><a href="#執行" class="headerlink" title="執行"></a>執行</h3><p>當程式開發完成後，就可以開啟三個單獨的 Terminal 來測試，其中每個 Terminal 會輸入一個唯一的 POD Name 來驗證:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># first terminal </span></span><br><span class="line">$ POD_NAME=test1 go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config -v=3 --logtostderr --lease-lock-namespace=default</span><br><span class="line"></span><br><span class="line"><span class="comment"># second terminal </span></span><br><span class="line">$ POD_NAME=test2 go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config -v=3 --logtostderr --lease-lock-namespace=default</span><br><span class="line"></span><br><span class="line"><span class="comment"># third terminal</span></span><br><span class="line">$ POD_NAME=test1 go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config -v=3 --logtostderr --lease-lock-namespace=default</span><br></pre></td></tr></table></figure><p>當三個控制器都啟動後，就會看到其中一個行程被選擇 Leader，這時如果停止該控制器，並經過一段時間後，就會發現新的 Leader 已經由其他行程接手。</p><p><img src="https://i.imgur.com/8Kff5Gh.png" alt></p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天主要透過 client-go 為自定義控制器實現高可靠機制，以確保控制器在發生問題時，能由其他節點上的控制器接手處理，這樣功能很適合以 Static Pod 部署的控制器。</p><blockquote><p>自定義控制器其實也可以用 Kubernetes Deployment 來達到高可靠，但在一些場景下並不適用，且若 Deployment 因為一些原因同時有多個副本在執行時，有可能會發生多個控制器寫入同一個 API 資源，造成資訊不一致問題。</p></blockquote><p>明天我們將回到 VM 控制器程式，深入了解如何實現核心功能，以讓我們透過 API 資源管理虛擬機。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/" target="_blank" rel="noopener">https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/</a></li><li><a href="https://tunein.engineering/implementing-leader-election-for-kubernetes-pods-2477deef8f13" target="_blank" rel="noopener">https://tunein.engineering/implementing-leader-election-for-kubernetes-pods-2477deef8f13</a></li><li><a href="https://medium.com/michaelbi-22303/deep-dive-into-kubernetes-simple-leader-election-3712a8be3a99" target="_blank" rel="noopener">https://medium.com/michaelbi-22303/deep-dive-into-kubernetes-simple-leader-election-3712a8be3a99</a></li><li><a href="http://liubin.org/blog/2018/04/28/how-to-build-controller-manager-high-available/" target="_blank" rel="noopener">http://liubin.org/blog/2018/04/28/how-to-build-controller-manager-high-available/</a></li><li><a href="https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/" target="_blank" rel="noopener">https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/</a></li><li><a href="https://mathspanda.github.io/2017/05/11/k8s-leader-election/" target="_blank" rel="noopener">https://mathspanda.github.io/2017/05/11/k8s-leader-election/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;https://k2r2bai.com/2019/10/11/ironman2020/day26/&quot;&gt;動手實作 Kubernetes 自定義控制器 Part2&lt;/a&gt; 文章中，我們利用 client-go 與產生的 Client 函式庫實作了一個控制器功能。而今天想在控制器實現協調預期狀態之前，探討一下 Kubernetes 自定義控制器的高可靠(Highly Available，HA)如何實現。&lt;/p&gt;
&lt;p&gt;在 Kubernetes 中，許多系統相關元件都是以 &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/#extension-patterns&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Controller Pattern&lt;/a&gt; 方式實現，比如說: Scheduler 與 Controller Manager。這些元件通常負責 Kubernetes 中的某一環核心功能，像是 Scheduler 負責 Pod 的節點分配， Controller Manager 提供許多 Kubernetes API 資源的功能協調與關聯。那麼如果這些元件發生故障了，就可能造成某部分功能無法正常運行，進而影響到整個叢集的健康，這樣該如何解決呢?&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>動手實作 Kubernetes 自定義控制器 Part2</title>
    <link href="https://k2r2bai.com/2019/10/11/ironman2020/day26/"/>
    <id>https://k2r2bai.com/2019/10/11/ironman2020/day26/</id>
    <published>2019-10-10T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="https://k2r2bai.com/2019/10/10/ironman2020/day25/">動手實作 Kubernetes 自定義控制器 Part1</a>文章中，我們透過定義 API 資源結構，以及使用 code-generator 產生了用於開發自定義控制器的程式函式庫。今天將延續範例，利用昨天產生的函式庫(apis, clientsets)建立一個控制器程式，以監聽自定義資源<code>VirtualMachine</code>的 API 事件。</p><a id="more"></a><h2 id="實現控制器程式"><a href="#實現控制器程式" class="headerlink" title="實現控制器程式"></a>實現控制器程式</h2><p>當有了自定義資源的 api 與 client 的函式庫後，我們就能利用這些來撰寫控制器程式。延續 <a href="https://github.com/cloud-native-taiwan/controller101" target="_blank" rel="noopener">Controller101</a>，我們將新增一些檔案來完成，如下所示:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">├── cmd</span><br><span class="line">│   └── main.go</span><br><span class="line">├── example</span><br><span class="line">│   └── <span class="built_in">test</span>-vm.yml </span><br><span class="line">└── pkg</span><br><span class="line">    ├── controller</span><br><span class="line">    │   └── controller.go</span><br><span class="line">    └── version</span><br><span class="line">        └── version.go</span><br></pre></td></tr></table></figure><ul><li><strong>cmd/main.go</strong>: 為控制器的主程式。</li><li><strong>example/test-vm.yml</strong>: 用於測試控制器的 VirtualMachine 資源的範例檔。(optional)</li><li><strong>pkg/controller/controller.go</strong>: VirtualMachine 控制器核心程式。</li><li><strong>pkg/version/version.go</strong>: 用於 Go build 時加入版本號。(optional)</li></ul><blockquote><p>目前 GitHub 範例已經新增這些程式，若不想看這累死人沒排版文章，可以直接透過 git 抓下來跑。</p></blockquote><h3 id="pkg-controller-controller-go"><a href="#pkg-controller-controller-go" class="headerlink" title="pkg/controller/controller.go"></a>pkg/controller/controller.go</h3><p>該檔案會利用 Kubernetes client-go 函式庫，以及 code-generator 產生的程式函式庫來實現控制器核心功能。通常撰寫一個控制器時，會建立一個 Controller struct，並包含以下元素:</p><ul><li><strong>Clientset</strong>: 擁有 VirtualMachine 的客戶端介面，讓控制器與 Kubernetes API Server 進行互動，以操作 VirtualMachine 資源。</li><li><strong>Informer</strong>: 控制器的 SharedInformer，用於接收 API 事件，並呼叫回呼函式。</li><li><strong>InformerSynced</strong>: 確認 SharedInformer 的儲存是否以獲得至少一次完整 LIST 通知。</li><li><strong>Lister</strong>: 用於列出或獲取快取中的 VirtualMachine 資源。</li><li><strong>Workqueue</strong>: 控制器的資源處理佇列，都 Informer 收到事件時，會將物件推到這個佇列，並在協調程式取出處理。當發生錯誤時，可以用於 Requeue 當前物件。</li></ul><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> controller</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"context"</span></span><br><span class="line"><span class="string">"encoding/json"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line"></span><br><span class="line">cloudnative <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/clientset/versioned"</span></span><br><span class="line">cloudnativeinformer <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/informers/externalversions"</span></span><br><span class="line">listerv1alpha1 <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/listers/cloudnative/v1alpha1"</span></span><br><span class="line"><span class="string">"github.com/golang/glog"</span></span><br><span class="line"><span class="string">"k8s.io/apimachinery/pkg/api/errors"</span></span><br><span class="line">utilruntime <span class="string">"k8s.io/apimachinery/pkg/util/runtime"</span></span><br><span class="line"><span class="string">"k8s.io/apimachinery/pkg/util/wait"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/cache"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/util/workqueue"</span></span><br><span class="line"><span class="string">"k8s.io/klog"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">resouceName = <span class="string">"VirtualMachine"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Controller <span class="keyword">struct</span> &#123;</span><br><span class="line">clientset cloudnative.Interface</span><br><span class="line">informer  cloudnativeinformer.SharedInformerFactory</span><br><span class="line">lister    listerv1alpha1.VirtualMachineLister</span><br><span class="line">synced    cache.InformerSynced</span><br><span class="line">queue     workqueue.RateLimitingInterface</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(clientset cloudnative.Interface, informer cloudnativeinformer.SharedInformerFactory)</span> *<span class="title">Controller</span></span> &#123;</span><br><span class="line">vmInformer := informer.Cloudnative().V1alpha1().VirtualMachines()</span><br><span class="line">controller := &amp;Controller&#123;</span><br><span class="line">clientset: clientset,</span><br><span class="line">informer:  informer,</span><br><span class="line">lister:    vmInformer.Lister(),</span><br><span class="line">synced:    vmInformer.Informer().HasSynced,</span><br><span class="line">queue:     workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), resouceName),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vmInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">AddFunc: controller.enqueue,</span><br><span class="line">UpdateFunc: <span class="function"><span class="keyword">func</span><span class="params">(old, <span class="built_in">new</span> <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">controller.enqueue(<span class="built_in">new</span>)</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">return</span> controller</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">Run</span><span class="params">(ctx context.Context, threadiness <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">go</span> c.informer.Start(ctx.Done())</span><br><span class="line">klog.Info(<span class="string">"Starting the controller"</span>)</span><br><span class="line">klog.Info(<span class="string">"Waiting for the informer caches to sync"</span>)</span><br><span class="line"><span class="keyword">if</span> ok := cache.WaitForCacheSync(ctx.Done(), c.synced); !ok &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to wait for caches to sync"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; threadiness; i++ &#123;</span><br><span class="line"><span class="keyword">go</span> wait.Until(c.runWorker, time.Second, ctx.Done())</span><br><span class="line">&#125;</span><br><span class="line">klog.Info(<span class="string">"Started workers"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">Stop</span><span class="params">()</span></span> &#123;</span><br><span class="line">glog.Info(<span class="string">"Stopping the controller"</span>)</span><br><span class="line">c.queue.ShutDown()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">runWorker</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> utilruntime.HandleCrash()</span><br><span class="line"><span class="keyword">for</span> c.processNextWorkItem() &#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">processNextWorkItem</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">obj, shutdown := c.queue.Get()</span><br><span class="line"><span class="keyword">if</span> shutdown &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">err := <span class="function"><span class="keyword">func</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> c.queue.Done(obj)</span><br><span class="line">key, ok := obj.(<span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">c.queue.Forget(obj)</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"Controller expected string in workqueue but got %#v"</span>, obj))</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := c.syncHandler(key); err != <span class="literal">nil</span> &#123;</span><br><span class="line">c.queue.AddRateLimited(key)</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"Controller error syncing '%s': %s, requeuing"</span>, key, err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">c.queue.Forget(obj)</span><br><span class="line">glog.Infof(<span class="string">"Controller successfully synced '%s'"</span>, key)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;(obj)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">enqueue</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">key, err := cache.MetaNamespaceKeyFunc(obj)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">c.queue.Add(key)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">syncHandler</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">namespace, name, err := cache.SplitMetaNamespaceKey(key)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"invalid resource key: %s"</span>, key))</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vm, err := c.lister.VirtualMachines(namespace).Get(name)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> errors.IsNotFound(err) &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"virtualmachine '%s' in work queue no longer exists"</span>, key))</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data, err := json.Marshal(vm)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">klog.Infof(<span class="string">"Controller get %s/%s object: %s"</span>, namespace, name, <span class="keyword">string</span>(data))</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="cmd-main-go"><a href="#cmd-main-go" class="headerlink" title="cmd/main.go"></a>cmd/main.go</h3><p>該檔案為控制器主程式，主要提供 Flags 來設定控制器參數、初始化所有必要的程式功能(如 REST Client、K8s Clientset、K8s Informer 等等)，以及執行控制器核心程式。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"context"</span></span><br><span class="line">goflag <span class="string">"flag"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"os"</span></span><br><span class="line"><span class="string">"os/signal"</span></span><br><span class="line"><span class="string">"syscall"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line"></span><br><span class="line"><span class="string">"github.com/cloud-native-taiwan/controller101/pkg/controller"</span></span><br><span class="line">cloudnative <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/clientset/versioned"</span></span><br><span class="line">cloudnativeinformer <span class="string">"github.com/cloud-native-taiwan/controller101/pkg/generated/informers/externalversions"</span></span><br><span class="line"><span class="string">"github.com/cloud-native-taiwan/controller101/pkg/version"</span></span><br><span class="line">flag <span class="string">"github.com/spf13/pflag"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/rest"</span></span><br><span class="line"><span class="string">"k8s.io/client-go/tools/clientcmd"</span></span><br><span class="line"><span class="string">"k8s.io/klog"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> defaultSyncTime = time.Second * <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">kubeconfig  <span class="keyword">string</span></span><br><span class="line">threads     <span class="keyword">int</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">parseFlags</span><span class="params">()</span></span> &#123;</span><br><span class="line">flag.StringVarP(&amp;kubeconfig, <span class="string">"kubeconfig"</span>, <span class="string">""</span>, <span class="string">""</span>, <span class="string">"Absolute path to the kubeconfig file."</span>)</span><br><span class="line">flag.IntVarP(&amp;threads, <span class="string">"threads"</span>, <span class="string">""</span>, <span class="number">2</span>, <span class="string">"Number of worker threads used by the controller."</span>)</span><br><span class="line">flag.BoolVarP(&amp;showVersion, <span class="string">"version"</span>, <span class="string">""</span>, <span class="literal">false</span>, <span class="string">"Display the version."</span>)</span><br><span class="line">flag.CommandLine.AddGoFlagSet(goflag.CommandLine)</span><br><span class="line">flag.Parse()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">restConfig</span><span class="params">(kubeconfig <span class="keyword">string</span>)</span> <span class="params">(*rest.Config, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> kubeconfig != <span class="string">""</span> &#123;</span><br><span class="line">cfg, err := clientcmd.BuildConfigFromFlags(<span class="string">""</span>, kubeconfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cfg, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cfg, err := rest.InClusterConfig()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cfg, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">parseFlags()</span><br><span class="line"></span><br><span class="line">k8scfg, err := restConfig(kubeconfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to build rest config: %s"</span>, err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clientset, err := cloudnative.NewForConfig(k8scfg)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to build cloudnative clientset: %s"</span>, err.Error())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">informer := cloudnativeinformer.NewSharedInformerFactory(clientset, defaultSyncTime)</span><br><span class="line">controller := controller.New(clientset, informer)</span><br><span class="line">ctx, cancel := context.WithCancel(context.Background())</span><br><span class="line">signalChan := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := controller.Run(ctx, threads); err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Fatalf(<span class="string">"Error to run the controller instance: %s."</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&lt;-signalChan</span><br><span class="line">cancel()</span><br><span class="line">controller.Stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中<code>restConfig()</code>函式用於建立 RESTClient Config，如果有指定 Kubeconfig 檔案時，會透過<code>client-go/tools/clientcmd</code>解析 Kubeconfig 內容以產生 Config 內容;若沒有的話，則表示該控制器可能被透過 Pod 部署在 Kubernetes 中，因此使用 <a href="https://github.com/kubernetes/client-go/blob/master/rest/config.go#L406" target="_blank" rel="noopener">InClusterConfig</a> 方式建立 Config。</p><h3 id="執行"><a href="#執行" class="headerlink" title="執行"></a>執行</h3><p>當控制器程式實現完成，且已經擁有一座安裝好 VirtualMachine CRD 的 Kubernetes 時，就能透過以下指令來執行:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config -v=2 --logtostderr</span><br><span class="line">I1008 15:38:30.350446   52017 controller.go:68] Starting the controller</span><br><span class="line">I1008 15:38:30.350543   52017 controller.go:69] Waiting <span class="keyword">for</span> the informer caches to sync</span><br><span class="line">I1008 15:38:30.454799   52017 controller.go:77] Started workers</span><br></pre></td></tr></table></figure><p>接著開啟另一個 Terminal 來建立 VirtualMachine 實例:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: cloudnative.tw/v1alpha1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-vm</span><br><span class="line">spec:</span><br><span class="line">  resource:</span><br><span class="line">    cpu: 2</span><br><span class="line">    memory: 4G</span><br><span class="line">EOF</span><br><span class="line">virtualmachine.cloudnative.tw/<span class="built_in">test</span>-vm created</span><br></pre></td></tr></table></figure><p>這時觀察控制器，會看到以下資訊:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ go run cmd/main.go --kubeconfig=<span class="variable">$HOME</span>/.kube/config -v=3 --logtostderr</span><br><span class="line">...</span><br><span class="line">I1008 17:28:18.775656   56945 controller.go:156] Controller get default/<span class="built_in">test</span>-vm object: &#123;<span class="string">"metadata"</span>:&#123;<span class="string">"name"</span>:<span class="string">"test-vm"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>,<span class="string">"selfLink"</span>:<span class="string">"/apis/cloudnative.tw/v1alpha1/namespaces/default/virtualmachines/test-vm"</span>,<span class="string">"uid"</span>:<span class="string">"a1acb111-c71e-4d2b-a2f4-62605e616dfc"</span>,<span class="string">"resourceVersion"</span>:<span class="string">"52295"</span>,<span class="string">"generation"</span>:1,<span class="string">"creationTimestamp"</span>:<span class="string">"2019-10-08T09:28:18Z"</span>,<span class="string">"annotations"</span>:&#123;<span class="string">"kubectl.kubernetes.io/last-applied-configuration"</span>:<span class="string">"&#123;\"apiVersion\":\"cloudnative.tw/v1alpha1\",\"kind\":\"VirtualMachine\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"name\":\"test-vm\",\"namespace\":\"default\"&#125;,\"spec\":&#123;\"action\":\"active\",\"resource\":&#123;\"cpu\":2,\"memory\":\"4G\",\"rootDisk\":\"40G\"&#125;&#125;&#125;\n"</span>&#125;&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"action"</span>:<span class="string">"active"</span>,<span class="string">"resource"</span>:&#123;<span class="string">"cpu"</span>:<span class="string">"2"</span>,<span class="string">"memory"</span>:<span class="string">"4G"</span>,<span class="string">"rootDisk"</span>:<span class="string">"40G"</span>&#125;&#125;,<span class="string">"status"</span>:&#123;<span class="string">"phase"</span>:<span class="string">""</span>,<span class="string">"server"</span>:&#123;<span class="string">"state"</span>:<span class="string">""</span>,<span class="string">"usage"</span>:&#123;<span class="string">"cpu"</span>:0,<span class="string">"memory"</span>:0&#125;&#125;,<span class="string">"lastUpdateTime"</span>:null&#125;&#125;</span><br><span class="line">I1008 17:28:18.775687   56945 controller.go:115] Controller successfully synced <span class="string">'default/test-vm'</span></span><br></pre></td></tr></table></figure><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>透過今天的實作，可以發現使用 code-generator 產生的相關程式碼操作自定義資源，就如同 Kubernetes client-go 的原生 API clientsets 一樣簡單，只要根據 <a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">sample-controller</a> 內容做些調整，就能實現特定 API 資源的控制器程式。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">https://github.com/kubernetes/sample-controller</a></li><li><a href="https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc" target="_blank" rel="noopener">https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;https://k2r2bai.com/2019/10/10/ironman2020/day25/&quot;&gt;動手實作 Kubernetes 自定義控制器 Part1&lt;/a&gt;文章中，我們透過定義 API 資源結構，以及使用 code-generator 產生了用於開發自定義控制器的程式函式庫。今天將延續範例，利用昨天產生的函式庫(apis, clientsets)建立一個控制器程式，以監聽自定義資源&lt;code&gt;VirtualMachine&lt;/code&gt;的 API 事件。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>動手實作 Kubernetes 自定義控制器 Part1</title>
    <link href="https://k2r2bai.com/2019/10/10/ironman2020/day25/"/>
    <id>https://k2r2bai.com/2019/10/10/ironman2020/day25/</id>
    <published>2019-10-09T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>昨天了解到 Kubernetes 官方的 Sample Controller，是如何讓一個自定義 API 資源被自定義控制器管理。雖然這個範例僅僅只是管理一個 Deployment 資源，但可以讓人認識到一個自定義控制器是如何運作的。而接下來的文章，我將每天撰寫一小部分程式內容，來重頭慢慢實作一個管理自定義資源<code>VirtualMachine</code>的控制器，並隨時間推移新增更多功能(如: LeaseLock、Metrics、Fake client 與 Finalizer、Admission Controller 等等)來完善這個控制器範例。</p><a id="more"></a><p>本文章的自定義控制器，會實作監聽自定義資源 VirtualMachine 的狀態，並依據 VirtualMachine 的<code>.spec</code>內容來操作私有雲中的虛擬機。另外該控制器也會持續同步私有雲虛擬機的狀態，並更新至子資源<code>.status</code>中。架構如下圖所示。</p><p><img src="https://i.imgur.com/WD3qr51.png" alt></p><p>而今天文章先把重點放在自定義 API 資源的資料結構，以及如何透過 <a href="https://github.com/kubernetes/code-generator" target="_blank" rel="noopener">code-generator</a> 產生控制器所需的程式碼。</p><h2 id="開發環境設置"><a href="#開發環境設置" class="headerlink" title="開發環境設置"></a>開發環境設置</h2><p>由於開發中，會使用到 Kubernetes 與 Go 語言，因此需要安裝這些到開發環境中。</p><ul><li>一座 Kubernetes v1.10+ 叢集。透過 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 建立即可 <code>minikube start --kubernetes-version=v1.15.4</code>。</li><li>安裝 kubectl v1.10+ 工具，安裝請參考 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">Install and Set Up kubectl</a></li><li>安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 <a href="https://golang.org/doc/install" target="_blank" rel="noopener">Go Getting Started</a>。</li></ul><h2 id="產生自定義資源程式碼"><a href="#產生自定義資源程式碼" class="headerlink" title="產生自定義資源程式碼"></a>產生自定義資源程式碼</h2><p>在開始定義 API 資源結構與使用 code-generator 前，需要先初始化存放控制器程式的目錄，其結構如下所示:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">controller101</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── cmd    <span class="comment"># Controller 主程式 main.go 檔</span></span><br><span class="line">├── deploy <span class="comment"># 部署 Controller 的相關檔案，如 Deployment、CRD、RBAC。</span></span><br><span class="line">├── go.mod <span class="comment"># Go mod package 檔案</span></span><br><span class="line">├── go.sum <span class="comment"># Go mod package 檔案</span></span><br><span class="line">├── hack   <span class="comment"># 存放一些常使用到的腳本</span></span><br><span class="line">└── pkg    <span class="comment"># 控制器相關程式碼</span></span><br></pre></td></tr></table></figure><p>目錄結構完成後，就能開始撰寫自定義資源，以及 code-generator 所需的內容。而要達到這個目的，我們必須涉及兩個步驟:</p><ul><li>定義 CRD 內容與資源類型程式結構。</li><li>透過 code-generator 腳本產生自定義資源的 Clientset、Informers、Listers 程式碼。</li></ul><h3 id="Defining-types-and-scripts"><a href="#Defining-types-and-scripts" class="headerlink" title="Defining types and scripts"></a>Defining types and scripts</h3><p>如前言所述，本範例希望透過新增一個自定義 API 資源，用於提供給自定義控制器實現功能。故我們必須在開始撰寫控制器前，先依據 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md" target="_blank" rel="noopener">Kubernetes-style API types</a> 的規範，來定義 API 群組(Group)與資源類型(Type)，並透過 CRD API 來建立自定義資源。所以假設控制器是所屬組織<code>Cloud Native Taiwan</code>想開發，那麼 API 群組就能定義為<code>cloudnative.tw</code>，而 API 資源則為<code>VirtualMachine</code>，因此 CRD 就會形成如下所示:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># File name: deploy/crd.yml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">virtualmachines.cloudnative.tw</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  group:</span> <span class="string">cloudnative.tw</span></span><br><span class="line"><span class="attr">  version:</span> <span class="string">v1alpha1</span></span><br><span class="line"><span class="attr">  names:</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">VirtualMachine</span></span><br><span class="line"><span class="attr">    singular:</span> <span class="string">virtualmachine</span></span><br><span class="line"><span class="attr">    plural:</span> <span class="string">virtualmachines</span></span><br><span class="line"><span class="attr">    shortNames:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">vm</span></span><br><span class="line"><span class="attr">  scope:</span> <span class="string">Namespaced</span></span><br></pre></td></tr></table></figure><p>當 CRD 定義完後，就能透過 CRD API 來新增自定義資源:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f deploy/crd.yml</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/virtualmachines.cloudnative.tw created</span><br><span class="line"></span><br><span class="line">$ kubectl get crd</span><br><span class="line">NAME                             CREATED AT</span><br><span class="line">virtualmachines.cloudnative.tw   2019-10-07T14:50:47Z</span><br><span class="line"></span><br><span class="line">$ kubectl get vm</span><br><span class="line">No resources found</span><br></pre></td></tr></table></figure><p>到這裡，會發現我們只是新增 API 而已，這個 VirtualMachine 資源完全沒有規範內容，這使我們無法描述一個 VirtualMachine 的預期內容，以提供給控制器處理。基於此，必須依據實作的功能假設自定義資源結構，如本範例想用 VirtualMachine 來管理虛擬機器，那麼就需要定義如下:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">cloudnative.tw/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualMachine</span> </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-vm</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  action:</span> <span class="string">active</span></span><br><span class="line"><span class="attr">  resource:</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">4</span><span class="string">G</span></span><br><span class="line"><span class="attr">    rootDisk:</span> <span class="number">40</span><span class="string">G</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  phase:</span> <span class="string">synchronized</span></span><br><span class="line"><span class="attr">  server:</span></span><br><span class="line"><span class="attr">    state:</span> <span class="string">active</span></span><br><span class="line"><span class="attr">    usage:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">13.3</span> </span><br><span class="line"><span class="attr">      memory:</span> <span class="number">40.1</span></span><br><span class="line"><span class="attr">  lastUpdateTime:</span> <span class="number">2019</span><span class="bullet">-10</span><span class="bullet">-07</span><span class="attr">T14:50:47Z</span></span><br></pre></td></tr></table></figure><p>雖然這個 YAML 能夠透過 kubectl 進行各種 API 操作，但如果想在自定義控制器操作的話，那該怎麼辦呢?</p><p>有接觸過 Kubernetes clinet-go 的人，可能會想到用 <a href="https://github.com/kubernetes/client-go/tree/master/dynamic" target="_blank" rel="noopener">Dynamic client</a> 來解決。但這不是好做法，因為以下原因:</p><ol><li>Dynamic client 無法很方便實現 API 資源類型的操作、轉換與驗證等等事情。</li><li>Dynamic client 不像原生 API 資源類型，提供了很方便的 <a href="https://github.com/kubernetes/client-go/tree/master/kubernetes" target="_blank" rel="noopener">Typed client</a> 可以使用。</li></ol><p>因此過去版本(v1.7 或更舊版本)的控制器管理自定義資源較為複雜，但這問題在 v1.8 版中被解決，Kubernetes 官方引入 code-generator 專案，用於產生如同原生 API 資源類型一樣功能的 Typed client 程式碼，這樣當我們在自定義控制器使用時，就如同使用 client-go 一樣。而要達到這樣事情，必須在專案建立產生程式碼所需的所有檔案，其檔案結構如下所示:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">├── hack </span><br><span class="line">│   └── k8s <span class="comment"># Code-generator 腳本</span></span><br><span class="line">│       ├── boilerplate.go.txt</span><br><span class="line">│       ├── tools.go</span><br><span class="line">│       ├── update-generated.sh</span><br><span class="line">│       └── verify-codegen.sh</span><br><span class="line">│   </span><br><span class="line">└── pkg</span><br><span class="line">    └── apis <span class="comment"># APIs 定義</span></span><br><span class="line">        └── cloudnative <span class="comment"># 提供該 Package 的 API Group Name。</span></span><br><span class="line">            ├── register.go</span><br><span class="line">            └── v1alpha1 <span class="comment"># API 各版本結構定義。Kubernetes API 是支援多版本的。</span></span><br><span class="line">                ├── doc.go</span><br><span class="line">                ├── register.go</span><br><span class="line">                └── types.go</span><br></pre></td></tr></table></figure><h4 id="v1alpha1-doc-go"><a href="#v1alpha1-doc-go" class="headerlink" title="v1alpha1/doc.go"></a>v1alpha1/doc.go</h4><p>該檔案用於定義 code-generator 的 Global tags。可標示當前版本 Package 中的每個類型，想要透過 code-generator 產生哪些程式碼(如 Deepcopy, Client)。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// +k8s:deepcopy-gen=package</span></span><br><span class="line"><span class="comment">// +groupName=cloudnative.tw</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Package v1alpha1 is the v1alpha1 version of the API.</span></span><br><span class="line"><span class="keyword">package</span> v1alpha1 <span class="comment">// import "github.com/cloud-native-taiwan/controller101/pkg/apis/cloudnative/v1alpha1"</span></span><br></pre></td></tr></table></figure><p>如上述內容，透過<code>+k8s:deepcopy-gen=package</code>標示這個 package 要建立 Deepcopy 方法(Method)。另外<code>+groupName=cloudnative.tw</code>定義整個 API Group 名稱，以確保在 code-generator 發生錯誤時，能產生可識別的錯誤代號。</p><h4 id="v1alpha1-types-go"><a href="#v1alpha1-types-go" class="headerlink" title="v1alpha1/types.go"></a>v1alpha1/types.go</h4><p>該檔案用於定義資源類型的資料結構，以及定義 code-generator 的 Local tags。可標示哪些資源類型想透過 code-generator 產生 Client 程式碼。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> v1alpha1</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">corev1 <span class="string">"k8s.io/api/core/v1"</span></span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// +genclient</span></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> VirtualMachine <span class="keyword">struct</span> &#123;</span><br><span class="line">metav1.TypeMeta   <span class="string">`json:",inline"`</span></span><br><span class="line">metav1.ObjectMeta <span class="string">`json:"metadata,omitempty"`</span></span><br><span class="line"></span><br><span class="line">Spec   VirtualMachineSpec   <span class="string">`json:"spec"`</span></span><br><span class="line">Status VirtualMachineStatus <span class="string">`json:"status"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> VirtualMachineSpec <span class="keyword">struct</span> &#123;</span><br><span class="line">Resource corev1.ResourceList <span class="string">`json:"resource"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> VirtualMachinePhase <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">VirtualMachineNone        VirtualMachinePhase = <span class="string">""</span></span><br><span class="line">VirtualMachineCreating    VirtualMachinePhase = <span class="string">"Creating"</span></span><br><span class="line">VirtualMachineActive      VirtualMachinePhase = <span class="string">"Active"</span></span><br><span class="line">VirtualMachineFailed      VirtualMachinePhase = <span class="string">"Failed"</span></span><br><span class="line">VirtualMachineTerminating VirtualMachinePhase = <span class="string">"Terminating"</span></span><br><span class="line">VirtualMachineUnknown     VirtualMachinePhase = <span class="string">"Unknown"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ResourceUsage <span class="keyword">struct</span> &#123;</span><br><span class="line">CPU    <span class="keyword">float64</span> <span class="string">`json:"cpu"`</span></span><br><span class="line">Memory <span class="keyword">float64</span> <span class="string">`json:"memory"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ServerStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">ID    <span class="keyword">string</span>        <span class="string">`json:"id"`</span></span><br><span class="line">State <span class="keyword">string</span>        <span class="string">`json:"state"`</span></span><br><span class="line">Usage ResourceUsage <span class="string">`json:"usage"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> VirtualMachineStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">Phase          VirtualMachinePhase <span class="string">`json:"phase"`</span></span><br><span class="line">Reason         <span class="keyword">string</span>              <span class="string">`json:"reason,omitempty"`</span></span><br><span class="line">Server         ServerStatus        <span class="string">`json:"server,omitempty"`</span></span><br><span class="line">LastUpdateTime metav1.Time         <span class="string">`json:"lastUpdateTime"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> VirtualMachineList <span class="keyword">struct</span> &#123;</span><br><span class="line">metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line">metav1.ListMeta <span class="string">`json:"metadata"`</span></span><br><span class="line"></span><br><span class="line">Items []VirtualMachine <span class="string">`json:"items"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>這邊的<code>+genclient</code> tag 是表示在執行 code-generator 時，會對這個類型建立 Client 程式碼。而<code>+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</code>則表示產生的 Deepcopy 使用 runtime.Object 介面實現。</p><h4 id="v1alpha1-register-go"><a href="#v1alpha1-register-go" class="headerlink" title="v1alpha1/register.go"></a>v1alpha1/register.go</h4><p>用於將剛建立的新 API 版本與新資源類型註冊到 API Group Schema 中，以便 API Server 能夠識別。</p><blockquote><p>Scheme: 用於 API 資源群組之間的序列化、反序列化與版本轉換。</p></blockquote><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> v1alpha1</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line"><span class="string">"k8s.io/apimachinery/pkg/runtime"</span></span><br><span class="line"><span class="string">"k8s.io/apimachinery/pkg/runtime/schema"</span></span><br><span class="line"></span><br><span class="line"><span class="string">"github.com/cloud-native-taiwan/controller101/pkg/apis/cloudnative"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> SchemeGroupVersion = schema.GroupVersion&#123;Group: cloudnative.GroupName, Version: <span class="string">"v1alpha1"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Kind</span><span class="params">(kind <span class="keyword">string</span>)</span> <span class="title">schema</span>.<span class="title">GroupKind</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> SchemeGroupVersion.WithKind(kind).GroupKind()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Resource</span><span class="params">(resource <span class="keyword">string</span>)</span> <span class="title">schema</span>.<span class="title">GroupResource</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> SchemeGroupVersion.WithResource(resource).GroupResource()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)</span><br><span class="line">AddToScheme = SchemeBuilder.AddToScheme</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">addKnownTypes</span><span class="params">(scheme *runtime.Scheme)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">scheme.AddKnownTypes(SchemeGroupVersion,</span><br><span class="line">&amp;VirtualMachine&#123;&#125;,</span><br><span class="line">&amp;VirtualMachineList&#123;&#125;,</span><br><span class="line">)</span><br><span class="line">metav1.AddToGroupVersion(scheme, SchemeGroupVersion)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="hack-k8s-boilerplate-go-txt"><a href="#hack-k8s-boilerplate-go-txt" class="headerlink" title="hack/k8s/boilerplate.go.txt"></a>hack/k8s/boilerplate.go.txt</h4><p>code-generator 自動將 boilerplate.go.txt 的文字，新增到產生的程式碼檔案內容的最上層。通常為 License 樣板。如以下範例:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line">Copyright © 2019 The controller101 Authors.</span><br><span class="line"></span><br><span class="line">Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">you may not use this file except in compliance with the License.</span><br><span class="line">You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">   http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">Unless required by applicable law or agreed to in writing, software</span><br><span class="line">distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">See the License for the specific language governing permissions and</span><br><span class="line">limitations under the License.</span><br><span class="line">*/</span><br></pre></td></tr></table></figure><h4 id="hack-k8s-tools-go"><a href="#hack-k8s-tools-go" class="headerlink" title="hack/k8s/tools.go"></a>hack/k8s/tools.go</h4><p>確保<code>go mod</code>能夠 code-generator 視為相依套件。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// +build tools</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// This package imports things required by build scripts, to force `go mod` to see them as dependencies</span></span><br><span class="line"><span class="comment">// See https://github.com/golang/go/issues/25922</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> tools</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">_ <span class="string">"k8s.io/code-generator"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="hack-k8s-update-generated-sh"><a href="#hack-k8s-update-generated-sh" class="headerlink" title="hack/k8s/update-generated.sh"></a>hack/k8s/update-generated.sh</h4><p>用於執行 code-generator 腳本，以產生自定義資源的 Deepcopy、Client、Informer 與 Lister 程式碼。</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -o errexit</span><br><span class="line"><span class="built_in">set</span> -o nounset</span><br><span class="line"><span class="built_in">set</span> -o pipefail</span><br><span class="line"></span><br><span class="line">SCRIPT_ROOT=$(dirname <span class="string">"<span class="variable">$&#123;BASH_SOURCE[0]&#125;</span>"</span>)/../..</span><br><span class="line">CODEGEN_PKG=<span class="variable">$&#123;CODEGEN_PKG:-$(cd "$&#123;SCRIPT_ROOT&#125;</span><span class="string">"; ls -d -1 ./vendor/k8s.io/code-generator 2&gt;/dev/null || echo ../code-generator)&#125;</span></span><br><span class="line"><span class="string">bash "</span><span class="variable">$&#123;CODEGEN_PKG&#125;</span><span class="string">"/generate-groups.sh "</span>deepcopy,client,informer,lister<span class="string">" \</span></span><br><span class="line"><span class="string">  github.com/cloud-native-taiwan/controller101/pkg/generated \</span></span><br><span class="line"><span class="string">  github.com/cloud-native-taiwan/controller101/pkg/apis \</span></span><br><span class="line"><span class="string">  "</span>cloudnative:v1alpha1<span class="string">" \</span></span><br><span class="line"><span class="string">  --output-base "</span>$(dirname <span class="variable">$&#123;BASH_SOURCE&#125;</span>)/../../../../../<span class="string">" \</span></span><br><span class="line"><span class="string">  --go-header-file <span class="variable">$&#123;SCRIPT_ROOT&#125;</span>/hack/k8s/boilerplate.go.txt</span></span><br></pre></td></tr></table></figure><h4 id="hack-verify-codegen-sh"><a href="#hack-verify-codegen-sh" class="headerlink" title="hack/verify-codegen.sh"></a>hack/verify-codegen.sh</h4><p>透過 diff 檢查當前的程式碼是否已經依據 apis 定義的內容產生。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -o errexit</span><br><span class="line"><span class="built_in">set</span> -o nounset</span><br><span class="line"><span class="built_in">set</span> -o pipefail</span><br><span class="line"></span><br><span class="line">SCRIPT_ROOT=$(dirname <span class="string">"<span class="variable">$&#123;BASH_SOURCE[0]&#125;</span>"</span>)/../..</span><br><span class="line"></span><br><span class="line">DIFFROOT=<span class="string">"<span class="variable">$&#123;SCRIPT_ROOT&#125;</span>/pkg"</span></span><br><span class="line">TMP_DIFFROOT=<span class="string">"<span class="variable">$&#123;SCRIPT_ROOT&#125;</span>/_tmp/pkg"</span></span><br><span class="line">_tmp=<span class="string">"<span class="variable">$&#123;SCRIPT_ROOT&#125;</span>/_tmp"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">cleanup</span></span>() &#123;</span><br><span class="line">  rm -rf <span class="string">"<span class="variable">$&#123;_tmp&#125;</span>"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">trap</span> <span class="string">"cleanup"</span> EXIT SIGINT</span><br><span class="line"></span><br><span class="line">cleanup</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="string">"<span class="variable">$&#123;TMP_DIFFROOT&#125;</span>"</span></span><br><span class="line">cp -a <span class="string">"<span class="variable">$&#123;DIFFROOT&#125;</span>"</span>/* <span class="string">"<span class="variable">$&#123;TMP_DIFFROOT&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="string">"<span class="variable">$&#123;SCRIPT_ROOT&#125;</span>/hack/k8s/update-generated.sh"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"diffing <span class="variable">$&#123;DIFFROOT&#125;</span> against freshly generated codegen"</span></span><br><span class="line">ret=0</span><br><span class="line">diff -Naupr <span class="string">"<span class="variable">$&#123;DIFFROOT&#125;</span>"</span> <span class="string">"<span class="variable">$&#123;TMP_DIFFROOT&#125;</span>"</span> || ret=$?</span><br><span class="line">cp -a <span class="string">"<span class="variable">$&#123;TMP_DIFFROOT&#125;</span>"</span>/* <span class="string">"<span class="variable">$&#123;DIFFROOT&#125;</span>"</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$ret</span> -eq 0 ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;DIFFROOT&#125;</span> up to date."</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;DIFFROOT&#125;</span> is out of date. Please run hack/k8s/update-generated.sh"</span></span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h3 id="Generate-codes"><a href="#Generate-codes" class="headerlink" title="Generate codes"></a>Generate codes</h3><p>當所有用於產生程式碼的檔案都建立後，就能透過 code-generator 依據定義的內容，來產生相關程式碼。由於開發使用 Go mod 管理套件，因此需要執行以下指令來完成程式碼產生:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ go mod vendor</span><br><span class="line">$ ./hack/k8s/update-generated.sh</span><br><span class="line">Generating deepcopy funcs</span><br><span class="line">Generating clientset <span class="keyword">for</span> cloudnative:v1alpha1 at github.com/cloud-native-taiwan/controller101/pkg/generated/clientset</span><br><span class="line">Generating listers <span class="keyword">for</span> cloudnative:v1alpha1 at github.com/cloud-native-taiwan/controller101/pkg/generated/listers</span><br><span class="line">Generating informers <span class="keyword">for</span> cloudnative:v1alpha1 at github.com/cloud-native-taiwan/controller101/pkg/generated/informers</span><br></pre></td></tr></table></figure><p>完成後，即可看到以下檔案:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pkg/generated</span><br><span class="line">├── clientset</span><br><span class="line">│   └── versioned</span><br><span class="line">│       ├── fake</span><br><span class="line">│       ├── scheme</span><br><span class="line">│       └── typed</span><br><span class="line">│           └── cloudnative</span><br><span class="line">│               └── v1alpha1</span><br><span class="line">│                   └── fake</span><br><span class="line">├── informers</span><br><span class="line">│   └── externalversions</span><br><span class="line">│       ├── cloudnative</span><br><span class="line">│       │   └── v1alpha1</span><br><span class="line">│       └── internalinterfaces</span><br><span class="line">└── listers</span><br><span class="line">    └── cloudnative</span><br><span class="line">        └── v1alpha1</span><br></pre></td></tr></table></figure><p>這樣就能夠在自定義控制器中，透過 Client 程式碼直接操作 VirtualMachine API 了。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天主要了解如何自己定義 API 資源類型，並利用 code-generator 產生相關程式碼，以利我們在自定義控制器中使用。明天將透過這些定義與建立的程式碼，實際進行操作自定義 API 資源，並撰寫控制器程式。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">https://github.com/kubernetes/sample-controller</a></li><li><a href="https://github.com/kubernetes/code-generator" target="_blank" rel="noopener">https://github.com/kubernetes/code-generator</a></li><li><a href="https://itnext.io/how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd-b4b9907769ba" target="_blank" rel="noopener">https://itnext.io/how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd-b4b9907769ba</a></li><li><a href="https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/" target="_blank" rel="noopener">https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/</a></li><li><a href="http://blog.xbblfz.site/2018/09/19/k8s%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E7%9A%84%E8%A7%A3%E6%9E%90/" target="_blank" rel="noopener">http://blog.xbblfz.site/2018/09/19/k8s%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E7%9A%84%E8%A7%A3%E6%9E%90/</a></li><li><a href="https://rancher.com/blog/2018/2018-07-09-rancher-management-plane-architecture/" target="_blank" rel="noopener">https://rancher.com/blog/2018/2018-07-09-rancher-management-plane-architecture/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;昨天了解到 Kubernetes 官方的 Sample Controller，是如何讓一個自定義 API 資源被自定義控制器管理。雖然這個範例僅僅只是管理一個 Deployment 資源，但可以讓人認識到一個自定義控制器是如何運作的。而接下來的文章，我將每天撰寫一小部分程式內容，來重頭慢慢實作一個管理自定義資源&lt;code&gt;VirtualMachine&lt;/code&gt;的控制器，並隨時間推移新增更多功能(如: LeaseLock、Metrics、Fake client 與 Finalizer、Admission Controller 等等)來完善這個控制器範例。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>探討 Kubernetes 自定義控制器是如何運作 Part2</title>
    <link href="https://k2r2bai.com/2019/10/09/ironman2020/day24/"/>
    <id>https://k2r2bai.com/2019/10/09/ironman2020/day24/</id>
    <published>2019-10-08T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在前幾天文章中，認識了開發 Kubernetes 自定義控制器的知識與概念，如: API 函式庫、client-go 函式庫、CRD 與自定義控制器本身等等。但講了這麼多，卻都沒有實際執行一個自定義控制器，因此今天將以 Kubernetes 社區提供的 <a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">sample-controller</a> 範例為主，來說明如何運行與實作。</p><a id="more"></a><h2 id="Sample-Controller-架構與運作流程"><a href="#Sample-Controller-架構與運作流程" class="headerlink" title="Sample Controller 架構與運作流程"></a>Sample Controller 架構與運作流程</h2><p>Sample Controller 是 Kubernetes 官方提供的範例，該範例實現了針對<code>Foo</code>自定義資源的控制器，當建立自定義資源 Foo 物件時，該控制器將使用 nginx:latest 容器映像檔與指定的副本數建立 Deployment，換句話說，控制器會確保每個 Foo 資源都有一個對應的 Deployment，其中 Foo 資源的<code>.spec</code>內容會與 Deployment 關聯，控制器會在協調循環中依據 Foo 資源的<code>.spec</code>內容處理預期的結果。另外該範例也提供了 CRD 的一些功能展示，如: Validation 與 Sub Resources。</p><p><img src="https://i.imgur.com/J9lmgVa.png" alt></p><p>這個控制器流程大致如下:</p><ol><li>Sample Controller 使用 <a href="https://github.com/kubernetes/client-go/" target="_blank" rel="noopener">client-go</a> 與 <a href="https://github.com/kubernetes/sample-controller/tree/master/pkg/generated" target="_blank" rel="noopener">Foo clientset</a> 函式庫來與 Kubernetes API Server 溝通，並建立一個控制循環(Control Loop)。</li><li>在控制循環中，Sample Controller 使用 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/reflector.go" target="_blank" rel="noopener">Reflector</a>(實現這功能的是<code>ListAndWatch()</code>) 監視 Kubernetes API 中的 Foo 與 Deployment 資源類型(Kind)，以確保兩者持續同步。</li><li>當 Reflector 透過 Watch API，收到有關新<code>Foo</code>與<code>Deployment</code>資源實例存在的事件通知時，它將使用 List API 取得新建立的 API 資源物件，並將其放入<code>watchHandler</code>函式內的 DeltaFIFO 佇列中。</li><li>接著 Sample Controller 使用 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go" target="_blank" rel="noopener">SharedInformer</a>(<code>DeploymentInformer</code> 與 <code>FooInformer</code>) 從 DeltaFIFO 佇列中取出 API 資源物件。這邊，SharedInformer 提供了<code>AddEventHandler()</code>函式，用於註冊事件處理程式，並將 API 資源物件新增到 Workqueue 中。這邊為 Foo 與 Deployment 傳遞了不同的資源處理函式(<a href="https://github.com/kubernetes/sample-controller/blob/master/controller.go#L116-L141" target="_blank" rel="noopener">L116-L141</a>)，以處理 API 資源在<code>add</code>、<code>update</code>與<code>delete</code>的事件。如果需要進行某些處理時，這些函式會負責將 Foo 資源物件的鍵(Key)排入 Workqueue。</li></ol><blockquote><ul><li>在本範例中，Workqueue 採用<code>RateLimitQueue</code>來進行 API 物件的處理速率限制。</li><li>當事件函式被呼叫時，控制器透過<code>MetaNamespaceKeyFunc()</code>函式，將當前處理的 API 資源物件轉換為<code>namespace/name</code>或<code>name</code>(如果沒有 Namespace 的話)的格式作為 Key，然後將這些 Key 添加到 Workqueue。或者透過<code>DeletionHandlingMetaNamespaceKeyFunc()</code>來處理刪除事件的 API 資源物件。</li></ul></blockquote><ol start="5"><li>這時，Sample Controller 會運行幾個 Worker(完成這個操作的函式是<code>runWorker()</code>)，它們會透過不斷的呼叫<code>processNextWorkItem()</code>來消耗 Workqueue 中要被處理的 API 物件。這時會進入<code>syncHandler()</code>以協調 Foo 當前狀態至預期狀態。而在<code>syncHandler()</code>中，控制器會將 Key 恢復成 Namespace 與 Name，並用<code>Lister</code>來取得 API 物件內容。</li></ol><blockquote><p>在<code>syncHandler()</code>中，部分功能會使用 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/index.go" target="_blank" rel="noopener">Indexer</a> 引用或者是一個 Listing 封裝器(Wrapper)來檢索對應 Key 的 API 資源物件內容。</p></blockquote><ol start="6"><li>在呼叫<code>syncHandler()</code>時，控制器會使用指定名稱(Foo 的<code>.spec.deploymentName</code>)與副本數(Foo 的<code>.spec.replicas</code>)建立一個 Deployment，並同步 Foo 資源的<code>.status</code>狀態。</li></ol><blockquote><p>在這些過程中，控制器會建立一個 EventRecorder 來紀錄事件的變化過程到 Kubernetes API 中。</p></blockquote><p>而 Sample Controller 整個目錄結構用意如下所示:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sample-controller</span><br><span class="line">├── Godeps </span><br><span class="line">├── artifacts</span><br><span class="line">│   └── examples <span class="comment"># 存放 Foo 範例，以及 CRD 檔案。</span></span><br><span class="line">├── docs</span><br><span class="line">│   └── images</span><br><span class="line">├── hack <span class="comment"># 存放</span></span><br><span class="line">└── pkg</span><br><span class="line">    ├── apis</span><br><span class="line">    │   └── samplecontroller</span><br><span class="line">    │       └── v1alpha1 <span class="comment"># 自定義資源 Foo 資料結構，會用於 code-generator 產生 client libraries。</span></span><br><span class="line">    ├── generated <span class="comment"># 透過 code-generator 產生的 client libraries。用於跟 API Server 溝通操作 Foo 資源。</span></span><br><span class="line">    │   ├── clientset</span><br><span class="line">    │   │   └── versioned</span><br><span class="line">    │   │       ├── fake</span><br><span class="line">    │   │       ├── scheme</span><br><span class="line">    │   │       └── typed</span><br><span class="line">    │   │           └── samplecontroller</span><br><span class="line">    │   │               └── v1alpha1</span><br><span class="line">    │   │                   └── fake</span><br><span class="line">    │   ├── informers </span><br><span class="line">    │   │   └── externalversions</span><br><span class="line">    │   │       ├── internalinterfaces</span><br><span class="line">    │   │       └── samplecontroller</span><br><span class="line">    │   │           └── v1alpha1</span><br><span class="line">    │   └── listers</span><br><span class="line">    │       └── samplecontroller</span><br><span class="line">    │           └── v1alpha1</span><br><span class="line">    └── signals <span class="comment"># 實現 Windows 與 POSIX 的 OS shutdown signal</span></span><br></pre></td></tr></table></figure><h2 id="測試環境部署"><a href="#測試環境部署" class="headerlink" title="測試環境部署"></a>測試環境部署</h2><p>由於本次文章將實際執行自定義控制器範例，因此需要建立測試用環境來觀察，這邊請依據需求來完成。</p><ul><li>需要一座 Kubernetes 叢集。透過 <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 建立即可 <code>minikube start --kubernetes-version=v1.15.4</code>。</li><li>安裝 kubectl 工具，請參考 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">Install and Set Up kubectl</a></li><li>安裝 Go 語言 v1.11+ 開發環境，請參考 <a href="https://golang.org/doc/install" target="_blank" rel="noopener">Go Getting Started</a>。</li><li>安裝 Git 工具，請參考 <a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a>。</li></ul><h3 id="設定-Sample-Controller"><a href="#設定-Sample-Controller" class="headerlink" title="設定 Sample Controller"></a>設定 Sample Controller</h3><p>首先透過 Git(或 Go) 取得 sample-controller 原始碼:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/kubernetes/sample-controller.git</span><br><span class="line">$ <span class="built_in">cd</span> sample-controller</span><br></pre></td></tr></table></figure><p>透過 kubectl 建立該自定義控制器的 CRD 到當前叢集中:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f artifacts/examples/crd.yaml</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created</span><br><span class="line"></span><br><span class="line">$ kubectl get crd</span><br><span class="line">NAME                           CREATED AT</span><br><span class="line">foos.samplecontroller.k8s.io   2019-10-06T12:55:18Z</span><br></pre></td></tr></table></figure><h3 id="執行-Sample-Controller"><a href="#執行-Sample-Controller" class="headerlink" title="執行 Sample Controller"></a>執行 Sample Controller</h3><p>當 CRD 建立好後，就可以透過 Go 指令直接執行這個控制器。首先透過 go mod 下載相依函示庫:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> GO111MODULE=on</span><br><span class="line">$ go mod download</span><br></pre></td></tr></table></figure><p>載完後，即可透過以下指令來執行控制器:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ go run $(ls -1 *.go | grep -v _test.go) -kubeconfig=<span class="variable">$HOME</span>/.kube/config -v=3 -logtostderr</span><br><span class="line">I1006 21:14:32.364765   73322 controller.go:114] Setting up event handlers</span><br><span class="line">I1006 21:14:32.364892   73322 controller.go:155] Starting Foo controller</span><br><span class="line">I1006 21:14:32.364905   73322 controller.go:158] Waiting <span class="keyword">for</span> informer caches to sync</span><br><span class="line">I1006 21:14:32.365031   73322 reflector.go:150] Starting reflector *v1.Deployment (30s) from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105</span><br><span class="line">I1006 21:14:32.365724   73322 reflector.go:185] Listing and watching *v1.Deployment from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105</span><br><span class="line">I1006 21:14:32.365032   73322 reflector.go:150] Starting reflector *v1alpha1.Foo (30s) from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105</span><br><span class="line">I1006 21:14:32.365796   73322 reflector.go:185] Listing and watching *v1alpha1.Foo from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105</span><br><span class="line">I1006 21:14:32.467616   73322 controller.go:163] Starting workers</span><br><span class="line">I1006 21:14:32.467660   73322 controller.go:169] Started workers</span><br></pre></td></tr></table></figure><h3 id="建立-Foo-資源實例"><a href="#建立-Foo-資源實例" class="headerlink" title="建立 Foo 資源實例"></a>建立 Foo 資源實例</h3><p>當執行了控制器後，就可以開一個新 Terminal 來建立 Foo 實例，以觀察控制器執行的結果:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> sample-controller</span><br><span class="line">$ kubectl apply -f artifacts/examples/example-foo.yaml</span><br><span class="line">foo.samplecontroller.k8s.io/example-foo created</span><br><span class="line"></span><br><span class="line">$ kubectl get foo</span><br><span class="line">NAME          AGE</span><br><span class="line">example-foo   48s</span><br><span class="line"></span><br><span class="line">$ kubectl get deploy,po</span><br><span class="line">NAME                                READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.extensions/example-foo   1/1     1            1           5m22s</span><br><span class="line"></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/example-foo-d75d8587c-wlqsz   1/1     Running   0          5m22s</span><br></pre></td></tr></table></figure><p>從範例中，可以發現 Foo 實際功能是管理著一個 Deployment 的建立，因此會看到上述的結果。當嘗試修改 Foo 中的<code>.spec.replicas</code>時，會發現 Foo 管理的 Deployment 會跟著變動副本數。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>Sample Controller 雖然只是一個非常簡單的自定義控制器範例，但從中卻可以學習到很多觀念，從最基本的 API 資源結構定義、透過 code-generator 產生客戶端函式庫、管理原有的 Kubernetes API 資源等等。雖然範例交代了很多基礎觀念，但是在實際應用上，還是缺乏了一些元素，比如說:多個相同控制器的 HA 實現、如何取得控制器 Metrics、怎麼使用 Finalizer 實作垃圾資源回收、實際部署方式與 RBAC 設定等等問題與功能。</p><p>在接下來章節中，我將針對這部份一一說明，我會透過每天實作一小部分來完成一個自定義控制器，在過程中再把一些觀念進一步釐清。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html" target="_blank" rel="noopener">https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html</a></li><li><a href="https://engineering.bitnami.com/articles/kubewatch-an-example-of-kubernetes-custom-controller.html" target="_blank" rel="noopener">https://engineering.bitnami.com/articles/kubewatch-an-example-of-kubernetes-custom-controller.html</a></li><li><a href="https://itnext.io/building-an-operator-for-kubernetes-with-the-sample-controller-b4204be9ad56" target="_blank" rel="noopener">https://itnext.io/building-an-operator-for-kubernetes-with-the-sample-controller-b4204be9ad56</a></li><li><a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">https://github.com/kubernetes/sample-controller</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在前幾天文章中，認識了開發 Kubernetes 自定義控制器的知識與概念，如: API 函式庫、client-go 函式庫、CRD 與自定義控制器本身等等。但講了這麼多，卻都沒有實際執行一個自定義控制器，因此今天將以 Kubernetes 社區提供的 &lt;a href=&quot;https://github.com/kubernetes/sample-controller&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;sample-controller&lt;/a&gt; 範例為主，來說明如何運行與實作。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>探討 Kubernetes 自定義控制器是如何運作 Part1</title>
    <link href="https://k2r2bai.com/2019/10/08/ironman2020/day23/"/>
    <id>https://k2r2bai.com/2019/10/08/ironman2020/day23/</id>
    <published>2019-10-07T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Kubernetes 控制器主要目的是協調(Reconcile)某個 API 資源的狀態，從實際狀態轉換為期望狀態，換句話說，在 Kubernetes API 資源上的說法，就是讓 API 資源的<code>.status</code>狀態，達到<code>.spec</code>所定義內容。而為了達到這點，控制器會透過 client-go 一直監視這兩種狀態的變化，並在發現變化時，觸發協調邏輯的循環，以更改當前狀態所需的任何操作，並使其往資源的預期狀態進展。Kubernetes 為了實現這樣機制，在 API 提供了一組 List 與 Watch 方法，用於監視任何 API 資源的事件。而自定義控制器就是以 client-go 操作這些 API 方法，監視其主要的自定義資源與其他任何相關的資源。</p><a id="more"></a><p>舉例，想要實現管理 TensorFlow 分散式訓練的控制器。這個控制器不僅要監視 TensorFlowJob(這是自定義資源) 物件的變化，而必須響應 Pod 事件，以確保 Pod 能再發生狀況時，處理對應狀況。當然這種追蹤 API 資源之間關聯的機制，也能利用 Kubernetes 的 Owner references 機制達成。這機制允許控制器在任何 API 資源上，設定資源的父子關析(如 Deployment 與 Pod 關聯這樣)，而當子資源事件發生時，就能反應給控制器，以知道哪個 TensorFlowJob 物件已受到影響，這時再由控制器的檢查與協調循環，來解決狀態的變化。</p><p>但講這麼多，一個控制器內部究竟是如何運作呢?今天就是要來聊聊這個內容。</p><h2 id="控制器如何運作"><a href="#控制器如何運作" class="headerlink" title="控制器如何運作?"></a>控制器如何運作?</h2><p>這部分將解釋 Kubernetes 自定義控制器運作流程，如下圖所示。一個自定義控制器是由 client-go 中的幾個主要功能實現，並結合自己實現的邏輯來達成。因此在開發前，必須先理解這些名詞與功能是做什麼用的。</p><p><img src="https://i.imgur.com/NYo9CsO.png" alt="Kubernetes controller diagram"><br>(圖片擷取自：<a href="https://github.com/kubernetes/sample-controller/blob/master/docs/images/client-go-controller-interaction.jpeg" target="_blank" rel="noopener">Kubernetes sample-controller</a>)</p><p>從架構圖來看，主要的 client-go 會有以下三者處理:</p><ul><li><strong>Reflector</strong>: 會透過 List/Watch API 監視著 Kubernetes 中指定的資源類型(Kind)，而這些資源可以是既有的資源(如 Pod、Deployment)，也可以是自定義資源。當 Reflector 透過 Watch API 收到新資源實例建立通知時，會將透過該資源的 List API 取得新建立的物件，並將物件放到 Delta Fifo 佇列中。</li><li><strong>Informer</strong>: 是控制器機制的基礎，它會在協調循環中，從 Delta Fifo 佇列取出 API 資源，將其儲存成物件提供給 Indexer 快取，並提供物件事件的處理介面(Add、Update 與 Delete)。</li><li><strong>Indexer</strong>: 為 API 資源物件提供檢索功能。利用執行緒安全的資料儲存中，將物件儲存成鍵(Key)/值(Value)形式，其 Key 的格式為 <code>namespace/name</code>。</li></ul><p>而除了上面 client-go 元件外，在開發一個自定義控制器時，還會有以下幾個元件會被使用到:</p><ul><li><strong>Informer reference</strong>: 在自定義控制器使用的 Informer 引用。在開發自定義控制器時，通常會宣告一個 Informer 實例用於多個 API 資源監聽使用，但自定義控制器有可能會需要監聽不同 API 資源，因此可能有多個子控制器，這時就可以用工廠模式(Factory Pattern)傳遞進去，並設定該控制器監聽的 API 資源。</li><li><strong>Indexer reference</strong>: 同 Informer 概念。用於處理不同 API 資源的檢索。</li><li><strong>Resource Event Handlers</strong>: 這些 Informer 的回呼函式(callback functions)，分別有<code>onAdd()</code>、<code>onUpdate()</code>與<code>onDelete()</code>。當 Informer 收到 API 資源物件事件時，就會呼叫這些函式，這時自定義控制器就能在函式中處理接下來事情。</li><li><strong>Work queue</strong>: 當 Resource Event Handlers 被呼叫時，會將 Key 寫到這個佇列中，以確保 API 資源物件能被依序處理。在 client-go 支援不同的 Workqueue 類型，而這邊通常會以 RateLimiting 為主。</li><li><strong>Process Item</strong>: 從 Workqueue 中取出物件 Key 的過程。</li><li><strong>Handle Object</strong>: 處理物件的實際邏輯，在開發自定義控制器時，通常會在這邊撰寫功能邏輯。一般來說會利用一個 Indexer reference 從取得的 Key 來檢索指定 API 資源物件的內容，並依據內容當前狀態與預期狀態處理。</li></ul><blockquote><p>TODO: 補充更多細節</p></blockquote><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天理解了自定義控制器使用到的元件，以及其運作方式，從中可以發現 Kubernetes client-go 幾乎是整個控制器的核心，許多功能實現都是圍繞著該函式庫。明天將部署 <a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">sample-controller</a> 到 Minikube 上，以釐清一些功能運作流程，這些知識與觀念將用於後續範例開發中。</p><p>雖然 Kubernetes 控制器看起來似乎很複雜，事實上簡化來看，大概也就長這個樣子:</p><p><img src="https://i.imgur.com/RZAuO0K.png" alt></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc" target="_blank" rel="noopener">https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc</a></li><li><a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">https://github.com/kubernetes/sample-controller</a></li><li><a href="https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/" target="_blank" rel="noopener">https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/</a></li><li><a href="http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/" target="_blank" rel="noopener">http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/</a></li><li><a href="https://kubernetes.io/docs/reference/using-api/client-libraries/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/using-api/client-libraries/</a></li><li><a href="https://speakerdeck.com/chanyilin/k8s-metacontroller" target="_blank" rel="noopener">https://speakerdeck.com/chanyilin/k8s-metacontroller</a></li><li><a href="https://toutiao.io/posts/4rnwh6/preview" target="_blank" rel="noopener">https://toutiao.io/posts/4rnwh6/preview</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources</a></li><li><a href="https://www.oreilly.com/library/view/cloud-native-infrastructure/9781491984291/ch04.html" target="_blank" rel="noopener">https://www.oreilly.com/library/view/cloud-native-infrastructure/9781491984291/ch04.html</a></li><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/</a></li><li><a href="https://github.com/opsnull/kubernetes-dev-docs/tree/master/client-go" target="_blank" rel="noopener">https://github.com/opsnull/kubernetes-dev-docs/tree/master/client-go</a></li><li><a href="https://blog.csdn.net/weixin_42663840/article/details/81482553" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42663840/article/details/81482553</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Kubernetes 控制器主要目的是協調(Reconcile)某個 API 資源的狀態，從實際狀態轉換為期望狀態，換句話說，在 Kubernetes API 資源上的說法，就是讓 API 資源的&lt;code&gt;.status&lt;/code&gt;狀態，達到&lt;code&gt;.spec&lt;/code&gt;所定義內容。而為了達到這點，控制器會透過 client-go 一直監視這兩種狀態的變化，並在發現變化時，觸發協調邏輯的循環，以更改當前狀態所需的任何操作，並使其往資源的預期狀態進展。Kubernetes 為了實現這樣機制，在 API 提供了一組 List 與 Watch 方法，用於監視任何 API 資源的事件。而自定義控制器就是以 client-go 操作這些 API 方法，監視其主要的自定義資源與其他任何相關的資源。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>開發自定義控制器前，需要先了解的東西 Part3</title>
    <link href="https://k2r2bai.com/2019/10/07/ironman2020/day22/"/>
    <id>https://k2r2bai.com/2019/10/07/ironman2020/day22/</id>
    <published>2019-10-06T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面文章中，了解到 Kubernetes 的架構一直以擴展性與靈活性為主，從 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/extend-cluster/#extension-points" target="_blank" rel="noopener">Extension Points</a> 可以看到上至 API 層，下至基礎建設層都有各種擴展的介面、標準與 API，其中 API 的擴展，我們在介紹自定義資源(Custom Resource)時，有提及能透過<code>CustomResourceDefinition(CRD)</code>與<code>API Aggregation</code>來達成，一方面在開發自定義控制器時，很多情況下，會透過增加新的 API 資源來讓控制器實現功能，因此我們必須先瞭解這兩者擴展 API 的作法與選擇。</p><a id="more"></a><p>在 Kubernetes API 擴展中，不管是使用 CRD 或 API Aggregation，旨都是希望在不修改 Kubernetes 核心程式碼的情況下，讓新的 API 能夠被註冊或新增到 Kubernetes 叢集中。但兩者在用意上有一點小差異:</p><ul><li><strong>API Aggregation</strong>: 用於將第三方服務資源註冊到 Kubernetes API 中，以統一透過 Kubernetes API 來存取外部服務。</li><li><strong>CRD</strong>: 在當前叢集中新增 API 資源，並沿用 Kubernetes 原有的 API(如 Pod) 操作方式來管理這些新 API 資源。</li></ul><p>從描述中，可以看到差別在於『把既有註冊』跟『直接增加新的』。而這兩者又是如何運作呢?</p><h3 id="CRD-CustomResourceDefinitions"><a href="#CRD-CustomResourceDefinitions" class="headerlink" title="CRD(CustomResourceDefinitions)"></a>CRD(CustomResourceDefinitions)</h3><p>CRD 是 Kubernetes 在 v1.7 版本新增的 API 資源，被用於新增自定義 API 資源上，一但 CRD 物件建立時，Kubernetes API Server 就會幫你處理自定義資源的 API 請求、狀態儲存等。而這過程中，完全不需要撰寫任何程式碼。</p><blockquote><p>事實上，在 CRD 之前還有個 ThirdPartyResources(TPR) 被用於擴展 API，但因為一些限制關析，TPR 被 CRD 取代了，並在 v1.8 就被棄用。</p></blockquote><ul><li>不用撰寫任何程式碼，就能輕易擴展新 API。</li><li>能夠沿用熟悉的 Kubernetes UX 工具來管理。如 kubectl。</li><li>支援 SubResources、Multiple versions、Defaulting、Additional Properties 與 Validation 等等功能。</li><li>擴展 API 方式簡單，但相對靈活性差。</li></ul><p>那要如何新增呢?在跑一個範例前，先來看一下 CRD 是如何定義 API URL。假設想實現在 Kubernetes 上管理 KVM 虛擬機時，我們需要先定義一個用於管理虛擬機的 API 類型 - VM，並且這個 API 是 kairen (或公司與組織)開發，這時 CRD 會透過資源類型名稱+域名來定義，如:域名為<code>kairen.io</code>，那麼 CRD 名稱就會是<code>vms.kairen.io</code>，而完整 API 資源路徑則是<code>/apis/kairen.io/&lt;version&gt;/namespaces/&lt;namespace&gt;/vms/..</code>。</p><p><img src="https://i.imgur.com/gRkisOJ.png" alt></p><p>基於上述，我們會這樣定義 CRD 內容。當這個範例被建立時，Kubernetes API 伺服器會增加新的 API 資源端點<code>/apis/kairen.io/v1alpha1/namespaces/&lt;namespace&gt;/vms/..</code>提供給客戶端存取。而當我們新增一個 VM 實例時，Kubernetes API 伺服器就會幫你管理整個 VMs API 的資源狀態儲存。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">vms.kairen.io</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  group:</span> <span class="string">kairen.io</span></span><br><span class="line"><span class="attr">  version:</span> <span class="string">v1alpha1</span></span><br><span class="line"><span class="attr">  names:</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">VM</span></span><br><span class="line"><span class="attr">    plural:</span> <span class="string">vms</span> </span><br><span class="line"><span class="attr">  scope:</span> <span class="string">Namespaced</span> </span><br><span class="line"><span class="attr">  additionalPrinterColumns:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">Status</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">string</span></span><br><span class="line"><span class="attr">    description:</span> <span class="string">The</span> <span class="string">VM</span> <span class="string">Phase</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.status.phase</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">CPU</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">integer</span></span><br><span class="line"><span class="attr">    description:</span> <span class="string">CPU</span> <span class="string">Usage</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.status.cpuUtilization</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">MEMORY</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">integer</span></span><br><span class="line"><span class="attr">    description:</span> <span class="string">Memory</span> <span class="string">Usage</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.status.memoryUtilization</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">Age</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">date</span></span><br><span class="line"><span class="attr">    JSONPath:</span> <span class="string">.metadata.creationTimestamp</span></span><br><span class="line"><span class="attr">  validation:</span></span><br><span class="line"><span class="attr">    openAPIV3Schema:</span></span><br><span class="line"><span class="attr">      properties:</span></span><br><span class="line"><span class="attr">        spec:</span></span><br><span class="line"><span class="attr">          properties:</span></span><br><span class="line"><span class="attr">            vmName:</span></span><br><span class="line"><span class="attr">              type:</span> <span class="string">string</span></span><br><span class="line"><span class="attr">              pattern:</span> <span class="string">'^[-_a-zA-Z0-9]+$'</span></span><br><span class="line"><span class="attr">            cpu:</span></span><br><span class="line"><span class="attr">              type:</span> <span class="string">integer</span></span><br><span class="line"><span class="attr">              minimum:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            memory:</span></span><br><span class="line"><span class="attr">              type:</span> <span class="string">integer</span></span><br><span class="line"><span class="attr">              minimum:</span> <span class="number">128</span></span><br><span class="line"><span class="attr">            diskSize:</span></span><br><span class="line"><span class="attr">              type:</span> <span class="string">integer</span></span><br><span class="line"><span class="attr">              minimum:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><blockquote><ul><li><strong>scope</strong>: 分為 Cluster 與 Namespaced。前者為叢集面資源(如 PV)，這種 API 通常只有管理員才能操作。</li><li><em>additionalPrinterColumns*</em>: 在 kubectl get 時，額外顯示的資訊。能夠以 Json Path 來取得 API 資源內容。</li><li><em>validation*</em>: 在呼叫 API 時，能基於 <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject" target="_blank" rel="noopener">OpenAPI v3 schema</a> 來驗證 VM 這個物件內容是否符合要求。</li></ul></blockquote><p>當我們利用這個範例在 Kubernetes 建立時，會如下所示:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get crd</span><br><span class="line">NAME            CREATED AT</span><br><span class="line">vms.kairen.io   2019-10-03T12:30:55Z</span><br><span class="line"></span><br><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: kairen.io/v1alpha1</span><br><span class="line">kind: VM</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-1</span><br><span class="line">spec:</span><br><span class="line">  cpu: 1</span><br><span class="line">  memory: 2048 <span class="comment"># MB</span></span><br><span class="line">  diskSize: 10 <span class="comment"># GiB</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ kubectl get vms</span><br><span class="line">NAME     STATUS   CPU   MEMORY   AGE</span><br><span class="line"><span class="built_in">test</span>-1                           2m31s</span><br></pre></td></tr></table></figure><p>這邊會看到 Status 跟 CPU/Memory 使用率都沒更新，因為 CRD 只幫你管理與儲存 API 狀態，若背後沒有一個機制去處理這個 API 時，就不會有實際作用。當然要模擬也是可以，利用 kubectl edit 嘗試修改<code>.status</code>內容即可。</p><h3 id="API-Aggregation"><a href="#API-Aggregation" class="headerlink" title="API Aggregation"></a>API Aggregation</h3><p>在 Kubernetes 架構下，每個資源都是由 API 伺服器處理 REST 操作請求，然後管理每個資源物件的狀態儲存。但有些情況下，擴展 API 的開發者，希望自行實現處理 REST API 的所有請求時，就無法透過 CRD 機制來達成。在這種需求下，就要用 API Aggregation 來解決，因為 API Aggregation 能利用一些機制，讓 Kubernetes API 伺服器知道如何委託自定義 API 的請求給第三方 API 伺服器處理。</p><p>雖然這種方式能處理更多 API 相關的事情，但相對的程式開發要求較高。那怎麼開發呢?我們能會利用 <a href="https://github.com/kubernetes/apiserver" target="_blank" rel="noopener">k8s.io/apiserver</a> 函式庫來實現。</p><ul><li>需要程式開發能力，通常建構在 k8s.io/apiserver 函式庫之上。</li><li>客製化程度非常高。如新增 HTTP verb、實現 Hooks。</li><li>能完成 CRD 所有能做到的事情。</li><li>支援 protobuf 與 OpenAPI schema 等等。</li></ul><blockquote><p>TODO: 補充更多細節</p></blockquote><h3 id="CRD-vs-API-Aggregation"><a href="#CRD-vs-API-Aggregation" class="headerlink" title="CRD vs API Aggregation"></a>CRD vs API Aggregation</h3><table><thead><tr><th>CRD</th><th>API Aggregation</th></tr></thead><tbody><tr><td>不用寫程式</td><td>要用 Go 語言來開發 API 伺服器</td></tr><tr><td>不需要額外服務處理 API 請求與儲存，但還是需要一個控制器來實現資源功能邏輯</td><td>需要獨立的第三方服務處理 API 各種事情</td></tr><tr><td>任何問題都是由 Kubernetes 社區處理與修復</td><td>需要同步 Kubernetes 社區問題修復方法，並重新建構 API 伺服器</td></tr><tr><td>無需額外處理 API 多版本機制</td><td>需要自行處理 API 多版本機制</td></tr></tbody></table><blockquote><ul><li>詳細英文描述，可以參考 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#comparing-ease-of-use" target="_blank" rel="noopener">Comparing ease of use</a>。</li><li>更多詳細的功能支援比較，可以參考 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#advanced-features-and-flexibility" target="_blank" rel="noopener">Advanced features and flexibility</a>。</li></ul></blockquote><p>看完這邊大家會發現 CRD 能達到的功能，API Aggregation 也都能達到。但 CRD 上手簡單又快速，API Aggregation 相對要處理事情更多。因此後續開發我會以 CRD 為主。</p><blockquote><p>TODO: 補充更多比較</p></blockquote><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>Kubernetes 提供了非常彈性的方式來擴展 API 功能，開發者能夠依據需求自行選擇擴展方式。也因為這些機制的完善，漸漸地越來越多在 Kubernetes 上新增自定義資源，並開發自家的控制器來管理這些資源，以實現各種在 Kubernetes 的新功能。</p><p>到這邊大致上了解一些開發自定義控制器前相關的知識，明天將說明一個 Kubernetes 自定義控制器是如何運作。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://hackmd.io/@onyiny-ang/HyxVWsS6z?type=view" target="_blank" rel="noopener">https://hackmd.io/@onyiny-ang/HyxVWsS6z?type=view</a></li><li><a href="https://programming-kubernetes.info/" target="_blank" rel="noopener">https://programming-kubernetes.info/</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;前面文章中，了解到 Kubernetes 的架構一直以擴展性與靈活性為主，從 &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/extend-cluster/#extension-points&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Extension Points&lt;/a&gt; 可以看到上至 API 層，下至基礎建設層都有各種擴展的介面、標準與 API，其中 API 的擴展，我們在介紹自定義資源(Custom Resource)時，有提及能透過&lt;code&gt;CustomResourceDefinition(CRD)&lt;/code&gt;與&lt;code&gt;API Aggregation&lt;/code&gt;來達成，一方面在開發自定義控制器時，很多情況下，會透過增加新的 API 資源來讓控制器實現功能，因此我們必須先瞭解這兩者擴展 API 的作法與選擇。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>開發自定義控制器前，需要先了解的東西 Part2</title>
    <link href="https://k2r2bai.com/2019/10/06/ironman2020/day21/"/>
    <id>https://k2r2bai.com/2019/10/06/ironman2020/day21/</id>
    <published>2019-10-05T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>昨天提到 Kubernetes GitHub 組織上，有許多豐富的程式函式庫可以使用，除了昨天介紹的一些關於 API 的函式庫外，還有用於跟 Kubernetes API 伺服器溝通的客戶端函式庫，如: client-go，而這些客戶端函式庫在開發 Kubernetes 自定義控制器時，是幾乎避免不了，甚至整個 Kubernetes 控制器架構都是圍繞這些函式庫上實現，而今天就是要針對這些客戶端函式庫做初步認識。</p><a id="more"></a><h3 id="client-go"><a href="#client-go" class="headerlink" title="client-go"></a>client-go</h3><p>由於 Kubernetes 是以 Go 語言打造的專案，因此相關函式庫都是以 Go 語言來提供使用，因此將 Kubernetes 客戶端函式庫稱之為 <a href="https://github.com/kubernetes/client-go" target="_blank" rel="noopener">client-go</a>。而 client-go 是一套典型的 Web service 函式庫，它支援了所有 Kubernetes 正式的 API 資源類型，並對它進行以下 REST 操作:</p><ul><li>Create</li><li>Get</li><li>List</li><li>Update</li><li>Delete</li><li>Patch</li></ul><p>這些 REST 操作動詞都是基於 API 伺服器的 HTTP 介面實現。另外 client-go 也支援了 Watch API，這是透過 HTTP Streaming 機制來監聽資源在叢集中的變化事件。透過 client-go 我們能在程式做到什麼事情呢?這邊列舉幾個:</p><ul><li>允許操作資源狀態(如:新增 Pod、修改 ConfigMap 或刪除 Persistent Volume)。</li><li>列出所有資源。</li><li>獲取有關當前資源狀態的詳細訊息。</li><li>開發自定義控制器。</li></ul><p>而 Kubernetes 的 client-go 背後會使用到上述提到的 api、api-machinery 等等函式庫，因此在導入時，需要注意一下版本相容性，如下圖。</p><p><img src="https://i.imgur.com/aXjpQfT.png" alt></p><p><img src="https://i.imgur.com/FHb4taS.png" alt="Compatibility matrix"></p><p>client-go 是官方最主要的 API client 函式庫，它在許多地方被使用，如 kubectl、kubeadm 與各種控制器等等。而目前 client-go 整體以目錄分成以下功能:</p><ul><li><strong>kubernetes</strong>: 提供原生 Kubernetes API 的資源 REST 操作方法與結構，如 Pod。</li><li><strong>informers</strong>: 提供原生 Kubernetes API 的資源 List/Watch API 機制與功能。經常被用於實現控制器中。</li><li><strong>listers</strong>: 提供原生 Kubernetes API 的資源從 Local cache 取得 API 等功能。經常被用於實現控制器中。</li><li><strong>discovery</strong>: 用於發現 Kubernetes API 伺服器支援哪些的 API 群組、版本與資源方法。</li><li><strong>dynamic</strong>: 提供一個動態的客戶端，能用於操作任何 Kubernetes 上的 API 資源。功能與<code>kubernetes</code>套件類似，差別在於<code>kubernetes</code>是針對每種 API 資源提供自己的操作方法。</li><li><strong>transport</strong>: 提供 TCP 授權/連接、Stream(如: exec、logs 與 portforward 等)、Websocket 等等功能。如果沒有明確選擇協定的話，預設會使用 HTTP2 進行溝通。其中 Stream 部分，若不支援 HTTP2 的話，則採用 <a href="https://zh.wikipedia.org/wiki/SPDY" target="_blank" rel="noopener">SPDY</a> 實現。</li><li><strong>rest</strong>: 提供 REST 客戶端的介面與實現，為 client-go 的 <code>kubernetes</code> package 基礎。</li><li><strong>plugin</strong>: 提供雲端供應商(Cloud Provider)的身份認證插件。</li><li><strong>tools</strong>: 提供各種方便使用的功能與工具，如 Cache、LeaseLock、Metrics 等等。</li><li><strong>scale</strong>: 提供 Auto Scaling 相關的客戶端。</li><li><strong>util</strong>: 提供各種方便使用的程式功能，如 Workqueue、Flow control、Certificate 等等。</li><li><strong>examples</strong>: 提供各種範例，如 Workqueue、Fake Client 等等操作。</li></ul><blockquote><p>TODO: 補範例</p></blockquote><h3 id="apiextensions-apiserver"><a href="#apiextensions-apiserver" class="headerlink" title="apiextensions-apiserver"></a>apiextensions-apiserver</h3><p><a href="https://github.com/kubernetes/apiextensions-apiserver" target="_blank" rel="noopener">apiextensions-apiserver</a> 類似 client-go 功能，但主要為 CRD(CustomResourceDefinitions) API 的資源結構，以及用於操作該資源的 Client 函式庫。如下面範例。</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> customResource <span class="keyword">struct</span> &#123;</span><br><span class="line">Name       <span class="keyword">string</span></span><br><span class="line">Kind       <span class="keyword">string</span></span><br><span class="line">Group      <span class="keyword">string</span></span><br><span class="line">Plural     <span class="keyword">string</span></span><br><span class="line">Version    <span class="keyword">string</span></span><br><span class="line">Scope      apiextensionsv1beta1.ResourceScope</span><br><span class="line">ShortNames []<span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createCRD</span><span class="params">(clientset apiextensionsclientset.Interface, resource customResource)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">crdName := fmt.Sprintf(<span class="string">"%s.%s"</span>, resource.Plural, resource.Group)</span><br><span class="line">crd := &amp;apiextensionsv1beta1.CustomResourceDefinition&#123;</span><br><span class="line">ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">Name: crdName,</span><br><span class="line">&#125;,</span><br><span class="line">Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec&#123;</span><br><span class="line">Group:   resource.Group,</span><br><span class="line">Version: resource.Version,</span><br><span class="line">Scope:   resource.Scope,</span><br><span class="line">Names: apiextensionsv1beta1.CustomResourceDefinitionNames&#123;</span><br><span class="line">Singular:   resource.Name,</span><br><span class="line">Plural:     resource.Plural,</span><br><span class="line">Kind:       resource.Kind,</span><br><span class="line">ShortNames: resource.ShortNames,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line">_, err := clientset.ApiextensionsV1beta1().CustomResourceDefinitions().Create(crd)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> !errors.IsAlreadyExists(err) &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to create %s CRD. %+v"</span>, resource.Name, err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    res := customResource&#123;</span><br><span class="line">&#123;</span><br><span class="line">Name:    <span class="string">"security"</span>,</span><br><span class="line">Plural:  <span class="string">"securities"</span>,</span><br><span class="line">Kind:    reflect.TypeOf(blendedv1.Security&#123;&#125;).Name(),</span><br><span class="line">Group:   blendedv1.CustomResourceGroup,</span><br><span class="line">Version: blendedv1.Version,</span><br><span class="line">Scope:   apiextensionsv1beta1.NamespaceScoped,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line">createCRD(extensionsClient, res)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>範例並未提供完整內容，僅擷取部分資訊用以說明。</p></blockquote><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>client-go 與 apiextensions-apiserver 是開發一個原生 Kubernetes 控制器的主要函式庫，整個控制器的工作流程與功能，都會利用這兩個客戶端函式庫來完成。而 client-go 除了提供各種 API 資源物件以外，也有各種方便的介面(Interface)、功能(Function)與方法(Method)，如: LeaseLock、Metrics 等等，能讓我們使用。</p><p>有了這些函式庫後，就能夠以程式實現各種操作功能，比如說 <a href="https://github.com/kairen/websocket-exec" target="_blank" rel="noopener">Websocket Pod Exec</a> 這個範例，就是利用 client-go transport 實作。</p><p>今天主要分享有關客戶端的函式庫，明天將認識擴展 API 時，需要知道的兩個方法。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/" target="_blank" rel="noopener">https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/</a></li><li><a href="http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/" target="_blank" rel="noopener">http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/</a></li><li><a href="https://kubernetes.io/docs/reference/using-api/client-libraries/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/using-api/client-libraries/</a></li><li><a href="https://github.com/kubernetes-client/gen" target="_blank" rel="noopener">https://github.com/kubernetes-client/gen</a></li><li><a href="https://speakerdeck.com/chanyilin/k8s-metacontroller" target="_blank" rel="noopener">https://speakerdeck.com/chanyilin/k8s-metacontroller</a></li><li><a href="https://toutiao.io/posts/4rnwh6/preview" target="_blank" rel="noopener">https://toutiao.io/posts/4rnwh6/preview</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;昨天提到 Kubernetes GitHub 組織上，有許多豐富的程式函式庫可以使用，除了昨天介紹的一些關於 API 的函式庫外，還有用於跟 Kubernetes API 伺服器溝通的客戶端函式庫，如: client-go，而這些客戶端函式庫在開發 Kubernetes 自定義控制器時，是幾乎避免不了，甚至整個 Kubernetes 控制器架構都是圍繞這些函式庫上實現，而今天就是要針對這些客戶端函式庫做初步認識。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>開發自定義控制器前，需要先了解的東西 Part1</title>
    <link href="https://k2r2bai.com/2019/10/05/ironman2020/day20/"/>
    <id>https://k2r2bai.com/2019/10/05/ironman2020/day20/</id>
    <published>2019-10-04T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由於 Kubernetes 控制器是主動調和(Reconciliation)資源過程的程式，它會透過與 API 伺服器溝通，以監視叢集的資源狀態，並依據 API 物件的當前狀態，嘗試將其推向預期狀態。而本系列文章是說明如何採用官方 API client 函式庫來編寫 Kubernetes 自定義控制器。因此需要在開發之前，先了解會使用到的函式庫與工具等等。</p><p>Kubernetes 組織在 GitHub 上，維護了許多可以使用的程式函式庫，如: api、client 與 api-machinery 等等都被用於不同的功能實現。而要使用這些函式庫只需要以<code>k8s.io/..</code>方式，在 Go 語言的專案下導入即可。在接下來個小部分中，我將介紹一些會用於開發自定義控制器的 API 相關函式庫。</p><a id="more"></a><p>這部分包含以下:</p><ul><li>API Machinery</li><li>API</li><li>gengo</li><li>code-generator</li></ul><h3 id="apimachinery"><a href="#apimachinery" class="headerlink" title="apimachinery"></a>apimachinery</h3><p><a href="https://github.com/kubernetes/apimachinery" target="_blank" rel="noopener">API Machinery</a> 是定義 API 級別的 Scheme、類型(Typing)、編碼(Encoding)、解碼(Decoding)、驗證(Validate)、類型轉換與相關工具等等功能。當我們要實現一個新的 API 資源時，就必須透過 API Machinery 來註冊 Scheme，另外 API Machinery 也定義了 TypeMeta、ObjectMeta、ListMeta、 Labels 與 Selector 等等物件，而這些物件幾乎在每個 Kubernetes API 資源中都會使用到，比如下面 YAML 所示。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span> <span class="comment"># TypeMeta</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span> <span class="comment"># TypeMeta</span></span><br><span class="line"><span class="attr">metadata:</span> <span class="comment"># ObjectMeta</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">memory-demo</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">mem-example</span></span><br><span class="line"><span class="attr">  labels:</span> <span class="comment"># Labels</span></span><br><span class="line"><span class="attr">    tt:</span> <span class="string">xx</span> </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">memory-demo-ctr</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">polinux/stress</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">["stress"]</span></span><br><span class="line"><span class="attr">    args:</span> <span class="string">["--vm",</span> <span class="string">"1"</span><span class="string">,</span> <span class="string">"--vm-bytes"</span><span class="string">,</span> <span class="string">"150M"</span><span class="string">,</span> <span class="string">"--vm-hang"</span><span class="string">,</span> <span class="string">"1"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><h3 id="api"><a href="#api" class="headerlink" title="api"></a>api</h3><p><a href="https://github.com/kubernetes/api" target="_blank" rel="noopener">API</a> 主要提供 Kubernetes 原生的 API 資源類型的 Scheme，這包含 Namespace、Pod 等等。該函式庫也提供了每個 API 資源類型，當前所支援的版本，如:v1、v1beta1。而每種 API 資源都依功能取向被群組化，如下圖所示。</p><p><img src="https://i.imgur.com/y5KxOlT.png" alt="Kubernetes API Resources"></p><h3 id="gengo"><a href="#gengo" class="headerlink" title="gengo"></a>gengo</h3><p><a href="https://github.com/kubernetes/gengo" target="_blank" rel="noopener">gengo</a> 主要用於透過 Go 語言檔案產生各種系統與 API 所需的文件，比如說 Protobuf。而該專案也包含了 Set、Deep-copy、Defaulter 等等產生器(Generator)，這些會被用於產生客製化 Client 函式庫。</p><blockquote><p>大家在看 Kubernetes 源碼時，一定會看到這樣一段註解<code>// Code generated by xxx. DO NOT EDIT.</code>。事實上 Kubernetes 有許多程式碼是基於該專案產生出來的，因為 Kubernetes 有很多 API 資源類型，若每一種都寫套維護的話，會非常複雜，因此 Kubernetes 定義了一套標準(Interface 與 Scheme 等等)來維護，並透過 Generator 來產生一些程式碼。</p></blockquote><h3 id="code-generator"><a href="#code-generator" class="headerlink" title="code-generator"></a>code-generator</h3><p><a href="https://github.com/kubernetes/code-generator" target="_blank" rel="noopener">Code Generator</a> 是基於 gengo 開發的程式碼產生器，主要用來實現產生 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md" target="_blank" rel="noopener">Kubernetes-style API types</a> 的 Client、Deep-copy、Informer、Lister 等等功能的程式碼。這是因為 Go 語言中沒有泛型(Generic)概念，因此不同的 API 資源類型，若都要寫一次上述這些功能的話，會有大量重複的程式碼，因此 Kubernetes 採用定義好<code>類型結構</code>後，再透過該專案提供的工具產生相關程式碼。下面舉個例子。</p><blockquote><p>其他語言的 Generator 可以參考 <a href="https://github.com/kubernetes-client/gen" target="_blank" rel="noopener">gen</a>。</p></blockquote><p>假設要實作一個 LINE Bot 的 API 資源，並產生 Client 程式時，我們必需先定義結構在 Go 檔案中。然後接著用<code>註解</code>方式，在程式碼標示物件結構要產生程式碼。比範例會產生 Bot 物件的 client 程式碼跟 Deep-copy 方法:</p><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> v1alpha1</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"github.com/line/line-bot-sdk-go/linebot"</span></span><br><span class="line">metav1 <span class="string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// +genclient</span></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Bot <span class="keyword">struct</span> &#123;</span><br><span class="line">metav1.TypeMeta   <span class="string">`json:",inline"`</span></span><br><span class="line">metav1.ObjectMeta <span class="string">`json:"metadata"`</span></span><br><span class="line"></span><br><span class="line">Spec   BotSpec   <span class="string">`json:"spec"`</span></span><br><span class="line">Status BotStatus <span class="string">`json:"status,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> BotExposeType <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">NgrokExpose        BotExposeType = <span class="string">"Ngrok"</span></span><br><span class="line">IngressExpose      BotExposeType = <span class="string">"Ingress"</span></span><br><span class="line">LoadBalancerExpose BotExposeType = <span class="string">"LoadBalancer"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> BotExpose <span class="keyword">struct</span> &#123;</span><br><span class="line">Type           BotExposeType <span class="string">`json:"type"`</span></span><br><span class="line">DomainName     <span class="keyword">string</span>        <span class="string">`json:"domainName"`</span></span><br><span class="line">LoadBalanceIPs []<span class="keyword">string</span>      <span class="string">`json:"loadBalanceIPs,omitempty"`</span></span><br><span class="line">NgrokToken     <span class="keyword">string</span>        <span class="string">`json:"ngrokToken"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> BotSpec <span class="keyword">struct</span> &#123;</span><br><span class="line">Selector          *metav1.LabelSelector <span class="string">`json:"selector"`</span></span><br><span class="line">ChannelSecretName <span class="keyword">string</span>                <span class="string">`json:"channelSecretName"`</span></span><br><span class="line">Expose            BotExpose             <span class="string">`json:"expose"`</span></span><br><span class="line">Version           <span class="keyword">string</span>                <span class="string">`json:"version"`</span></span><br><span class="line">LogLevel          <span class="keyword">int</span>                   <span class="string">`json:"logLevel"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> BotPhase <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">BotPending     BotPhase = <span class="string">"Pending"</span></span><br><span class="line">BotActive      BotPhase = <span class="string">"Active"</span></span><br><span class="line">BotFailed      BotPhase = <span class="string">"Failed"</span></span><br><span class="line">BotTerminating BotPhase = <span class="string">"Terminating"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> BotStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">Phase          BotPhase    <span class="string">`json:"phase"`</span></span><br><span class="line">Reason         <span class="keyword">string</span>      <span class="string">`json:"reason,omitempty"`</span></span><br><span class="line">LastUpdateTime metav1.Time <span class="string">`json:"lastUpdateTime"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> BotList <span class="keyword">struct</span> &#123;</span><br><span class="line">metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line">metav1.ListMeta <span class="string">`json:"metadata"`</span></span><br><span class="line"></span><br><span class="line">Items []Bot <span class="string">`json:"items"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>當完成定義與註解描述後，我們會以這樣的目錄方式放在開發專案中，其中<code>types.go</code>就是上述檔案。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pkg/apis</span><br><span class="line">└── line</span><br><span class="line">    ├── register.go</span><br><span class="line">    └── v1alpha1</span><br><span class="line">        ├── doc.go</span><br><span class="line">        ├── register.go</span><br><span class="line">        ├── types.go</span><br><span class="line">        └── zz_generated.deepcopy.go</span><br></pre></td></tr></table></figure><blockquote><p>其他檔案會在後續開發時，詳細說明。</p></blockquote><p>接著利用 code-generator 工具來指向 API 物件結構位置，以讓 code-generator 解析，並產生對應的程式碼。下面是執行腳本範例:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -o errexit</span><br><span class="line"><span class="built_in">set</span> -o nounset</span><br><span class="line"><span class="built_in">set</span> -o pipefail</span><br><span class="line"></span><br><span class="line">SCRIPT_ROOT=$(dirname <span class="string">"<span class="variable">$&#123;BASH_SOURCE[0]&#125;</span>"</span>)/..</span><br><span class="line">CODEGEN_PKG=<span class="variable">$&#123;CODEGEN_PKG:-$(cd "$&#123;SCRIPT_ROOT&#125;</span><span class="string">"; ls -d -1 ./vendor/k8s.io/code-generator 2&gt;/dev/null || echo ../code-generator)&#125;</span></span><br><span class="line"><span class="string">bash "</span><span class="variable">$&#123;CODEGEN_PKG&#125;</span><span class="string">"/generate-groups.sh "</span>deepcopy,client,informer,lister<span class="string">" \</span></span><br><span class="line"><span class="string">  github.com/kairen/line-bot-operator/pkg/generated \</span></span><br><span class="line"><span class="string">  github.com/kairen/line-bot-operator/pkg/apis \</span></span><br><span class="line"><span class="string">  "</span>line:v1alpha1<span class="string">" \</span></span><br><span class="line"><span class="string">  --output-base "</span>$(dirname <span class="variable">$&#123;BASH_SOURCE&#125;</span>)/../../../../<span class="string">" \</span></span><br><span class="line"><span class="string">  --go-header-file <span class="variable">$&#123;SCRIPT_ROOT&#125;</span>/hack/boilerplate.go.txt</span></span><br></pre></td></tr></table></figure><blockquote><p>這邊<code>boilerplate.go.txt</code>為 Go 檔案的 License 內容。用於在產生程式碼時，自動塞在檔案內容的頭。</p></blockquote><p>當完成後，我們會在指定輸出的目錄看到產生的程式碼，如</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pkg/generated</span><br><span class="line">├── clientset</span><br><span class="line">│   └── versioned</span><br><span class="line">│       ├── clientset.go</span><br><span class="line">│       ├── doc.go</span><br><span class="line">│       ├── fake</span><br><span class="line">│       │   ├── clientset_generated.go</span><br><span class="line">│       │   ├── doc.go</span><br><span class="line">│       │   └── register.go</span><br><span class="line">│       ├── scheme</span><br><span class="line">│       │   ├── doc.go</span><br><span class="line">│       │   └── register.go</span><br><span class="line">│       └── typed</span><br><span class="line">│           └── line</span><br><span class="line">│               └── v1alpha1</span><br><span class="line">│                   ├── bot.go</span><br><span class="line">│                   ├── doc.go</span><br><span class="line">│                   ├── event.go</span><br><span class="line">│                   ├── eventbinding.go</span><br><span class="line">│                   ├── fake</span><br><span class="line">│                   │   ├── doc.go</span><br><span class="line">│                   │   ├── fake_bot.go</span><br><span class="line">│                   │   ├── fake_event.go</span><br><span class="line">│                   │   ├── fake_eventbinding.go</span><br><span class="line">│                   │   └── fake_line_client.go</span><br><span class="line">│                   ├── generated_expansion.go</span><br><span class="line">│                   └── line_client.go</span><br><span class="line">├── informers</span><br><span class="line">│   └── externalversions</span><br><span class="line">│       ├── factory.go</span><br><span class="line">│       ├── generic.go</span><br><span class="line">│       ├── internalinterfaces</span><br><span class="line">│       │   └── factory_interfaces.go</span><br><span class="line">│       └── line</span><br><span class="line">│           ├── interface.go</span><br><span class="line">│           └── v1alpha1</span><br><span class="line">│               ├── bot.go</span><br><span class="line">│               ├── event.go</span><br><span class="line">│               ├── eventbinding.go</span><br><span class="line">│               └── interface.go</span><br><span class="line">└── listers</span><br><span class="line">    └── line</span><br><span class="line">        └── v1alpha1</span><br><span class="line">            ├── bot.go</span><br><span class="line">            ├── event.go</span><br><span class="line">            ├── eventbinding.go</span><br><span class="line">            └── expansion_generated.go</span><br></pre></td></tr></table></figure><p>如此一來，我們就能在開發時，使用程式碼來操作自定義資源的 CRUD。</p><blockquote><p>TODO: 補全部 Generator 細節與設定</p></blockquote><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天主要初步了解 Kubernetes GitHub 組織上關於 API 的函式庫，在開發 Kubernetes 自定義控制器時，有可能因為跟原本 Kubernetes 的功能整合，因此會很頻繁地使用到這些函式庫。然而對這些函式庫有出不了的話，對於後續在自定義資源實作時，也能比較清楚 Kubernetes 的一些設計架構。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/" target="_blank" rel="noopener">https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/</a></li><li><a href="http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/" target="_blank" rel="noopener">http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/</a></li><li><a href="https://kubernetes.io/docs/reference/using-api/client-libraries/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/using-api/client-libraries/</a></li><li><a href="https://github.com/kubernetes-client/gen" target="_blank" rel="noopener">https://github.com/kubernetes-client/gen</a></li><li><a href="https://speakerdeck.com/chanyilin/k8s-metacontroller" target="_blank" rel="noopener">https://speakerdeck.com/chanyilin/k8s-metacontroller</a></li><li><a href="https://toutiao.io/posts/4rnwh6/preview" target="_blank" rel="noopener">https://toutiao.io/posts/4rnwh6/preview</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由於 Kubernetes 控制器是主動調和(Reconciliation)資源過程的程式，它會透過與 API 伺服器溝通，以監視叢集的資源狀態，並依據 API 物件的當前狀態，嘗試將其推向預期狀態。而本系列文章是說明如何採用官方 API client 函式庫來編寫 Kubernetes 自定義控制器。因此需要在開發之前，先了解會使用到的函式庫與工具等等。&lt;/p&gt;
&lt;p&gt;Kubernetes 組織在 GitHub 上，維護了許多可以使用的程式函式庫，如: api、client 與 api-machinery 等等都被用於不同的功能實現。而要使用這些函式庫只需要以&lt;code&gt;k8s.io/..&lt;/code&gt;方式，在 Go 語言的專案下導入即可。在接下來個小部分中，我將介紹一些會用於開發自定義控制器的 API 相關函式庫。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>淺談 Kubernetes 自定義資源(Custom Resource)與自定義控制器(Custom Controller)</title>
    <link href="https://k2r2bai.com/2019/10/04/ironman2020/day19/"/>
    <id>https://k2r2bai.com/2019/10/04/ironman2020/day19/</id>
    <published>2019-10-03T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>使用 Kubernetes 時，大家都能感受到其容器編配能力，當有一個容器發生異常時，Kubernetes 會透過自身機制幫你把容器遷移或重新啟動，或者能利用副本機制讓容器同時存在於叢集的不同節點上，甚至提供滾動升級(Rolling Update)容器機制。這些酷炫的功能，大家肯定都知道如何去使用，因為 Kubernetes 透過一些方式，將複雜的功能進行了抽象化與封裝，因此使用者只需要了解如何操作 API 物件，就能完成需要的功能，比如:Deployment 修改參數就會進行滾動升級。然而這些『抽象化』與『封裝』的過程究竟是如何實現呢?今天文章就是要針對這個部分進行探討。</p><a id="more"></a><p>Kubernetes 是個非常容易擴展的系統。Kubernetes 提供了多種方法讓我們能夠自定義 API，或擴展功能，比如:</p><ul><li><strong>Cloud providers</strong>: 提供自定義雲平台整合的控制器。比如說跟 IaaS 進行整合，當建立一個 LoadBalancer Service 時，自動呼叫 IaaS 負載平衡 API 進行建立。</li></ul><blockquote><p>過去 Cloud providers 屬於 kube-controller-manager 的部分控制器，現在已從核心程式碼移出。</p></blockquote><ul><li><strong>Admission control webhooks</strong>: 提供 kube-apiserver 存取擴展的 Webhook API。</li><li><strong>kubelet plugins</strong>: 提供容器 Runtime(CRI)、網路(CNI)、儲存(CSI)與裝置(Device Plugins)等等介面。</li><li><strong>kubectl plugins</strong>: 提供擴展 kubectl。如 <a href="https://github.com/kubernetes-sigs/krew" target="_blank" rel="noopener">krew</a>。</li><li><strong>Custom resources 與 Custom controllers</strong>: 提供自定義 API 資源(物件)，以及執行這些客製化 API 資源(物件)的邏輯程式。</li><li><strong>Custom API servers</strong>: 提供透過 <a href="https://github.com/kubernetes/apiserver" target="_blank" rel="noopener">apiserver</a> 函式庫開發用於 API Aggregation 的 API servers。</li><li><strong>Custom schedulers</strong>: 提供能實現自定義的排程演算法與機制，並在 Pod 指定使用。</li><li><strong>Authentication webhooks</strong>: 提供擴展 Kubernetes 身份認證機制，可以與外部系統整合。如: LDAP、OCID。</li></ul><p><img src="https://i.imgur.com/iz2YEOb.png" alt></p><p>而今天我們重點就是要放在探討<code>自定義資源(Custom Resource)</code>與<code>自定義控制器(Custom Controller)</code>上。</p><h3 id="自定義資源-Custom-Resource"><a href="#自定義資源-Custom-Resource" class="headerlink" title="自定義資源(Custom Resource)"></a>自定義資源(Custom Resource)</h3><p>在 Kubernetes API 中，一個端點(Endpoint)就是一個資源，這個資源是被用於儲存某個類型的 API 物件的集合。比如說 Pod 有 /api/v1/pods API 端點。 </p><p><img src="https://i.imgur.com/IdC8aKc.png" alt="Kubernetes API Resources"></p><p>一個 API 端點的組成如下所示:</p><ul><li><strong>API Group</strong>: 是邏輯上相關的種類集合，如 Job 與 CronJob 都屬於批次處理功能相關。</li><li><strong>Version</strong>: 每個 API Group 存在多個版本，這些版本區分不同穩定度層級，一般功能會從 v1alpha1 升級到 v1beta1，然後在 v1 成為穩定版本。</li><li><strong>Resource</strong>: 資源是透過 HTTP 發送與檢索的 API 物件實體，其以 JSON 來表示。可以是單一或者多個資源。</li></ul><p>其中每個 API 都可能存在著不同版本，其意味著不同層級穩定度與支援度:</p><ul><li><strong>Alpha Level</strong>: 在預設下是大多情況禁止使用狀態，這些功能有可能隨時在下一版本被遺棄，因此只適用於測試用，如: v1alpha1。</li><li><strong>Beta Level</strong>: 在這級別一般預設會啟用，這表示該功能已經過很好的測試項目，但是物件內容可能會在後續版本或穩定版本發生變化。如: v1beta2。</li><li><strong>Stable Level</strong>: 在這級別表示該功能已經穩定，會很長的時間一直存在。如: v1。</li></ul><p><img src="https://i.imgur.com/znt3I3m.png" alt></p><p>而自定義資源就是預設不存在於 Kubernetes 原生的額外 API 資源，這包含了從當前叢集擴展新的資源物件(如:原本沒有 DaemonJob，我透過一些機制新增了)，以及其他系統元件本身使用的(如: KubeadmConfig)。目前 Kubernetes 提供了兩種方式來新增自定義資源:</p><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/" target="_blank" rel="noopener">CRD(CustomResourceDefinitions)</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/" target="_blank" rel="noopener">API Aggregation</a></li></ul><blockquote><p>TODO: 補充更多細節</p></blockquote><h3 id="自定義控制器-Controller"><a href="#自定義控制器-Controller" class="headerlink" title="自定義控制器(Controller)"></a>自定義控制器(Controller)</h3><p>當利用 kubectl 建立一個 Pod 時，客戶端會透過 kubectl 與 kube-apiserver 進行溝通呼叫 Pod APIs，這時 API 物件經過驗證後，被成功建立到 Kubernetes 上，最後儲存到 etcd 中，然後過不久後，就會發現這個 Pod 在叢集中的某個節點上被執行了。到這邊一定會疑惑中間的過程，是怎麼判定建立到哪個節點上的，又是怎麼在該節點建立的呢?實際上，這需要由 Kubernetes 的<code>kube-scheduler</code>與<code>kubelet</code>元件完成的，其流程如下圖所示。</p><p><img src="https://i.imgur.com/kOPShpV.png" alt="Pod workflow"></p><ol><li>使用者透過客戶端工具與 kube-apiserver 以 REST API 方式進行溝通建立 Pod 物件，然後 kube-apiserver 進行各種驗證通過後，將其寫入 etcd 中。</li><li>這時 kube-scheduler 會透過監聽 kube-apiserver 的 Pod 物件變化事件，獲取到 Pod 物件的內容，而當觀察到該 Pod 的<code>.spec.nodeName</code>欄位沒有被分配節點名稱時，kube-scheduler就會透過<code>過濾(Filter)</code>與<code>排名(Rank)</code>演算法來計算所有節點的權重，並從中找出一個最佳的節點，接著在 Pod 的<code>.spec.nodeName</code>更新被選取的名稱，然後該狀態會被儲存到 etcd 中。</li><li>這時 kubelet 也一直監聽著 kube-apiserver 的 Pod 物件變化事件，當發現有一個 Pod 的<code>.spec.nodeName</code>欄位是這個節點時，kubelet 就會呼叫容器 Runtime 來啟動這個 Pod 所定義的相關功能，如:掛載儲存、透過 system call 寫入環境變數等等。</li><li>一方面 kubelet 會監控容器 Runtime 執行的 Pod 狀態。並隨著情況的變化，同步將內容更新到 Pod API 物件上，以讓使用者能夠了解當前狀態。</li></ol><p>從這點了解 kube-scheduler 與 kubelet，才是實際上負責執行 Pod 邏輯的角色之一，而這些<code>邏輯</code>就是所謂的<code>控制器(Controller)</code>。因此儘管 Kubernetes 原生提供了許多的 API 資源可以使用(如下圖)，如果當前 Kubernetes 叢集並沒有啟動或安裝相關的控制器，以執行實際的邏輯的話，這些 API 資源就形同空殼般存在於叢集中。這邊再舉幾個例子，比如:實現 Service 功能的就是 kube-proxy 與 kube-controller-manager 的 <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/endpoint" target="_blank" rel="noopener">Endpoint 控制器</a>(或 <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/endpointslice" target="_blank" rel="noopener">Endpoint Slice 控制器</a>)，這兩者分別監聽 Service 設定 NAT rules 與同步綁定 Service 與 Pod 的 IP。</p><p><img src="https://i.imgur.com/y5KxOlT.png" alt="Kubernetes API Resources"></p><p>那麼自定義控制器又跟自定義資源有什麼關析呢?就如同上面提到範例的 API 資源一樣，自定義資源本身只提供儲存與檢索結構化內容，因此當擴展時，並不會有實際功能，而這時就需要結合自定義控制器來完成功能的邏輯事情，並持續同步更新自定義資源。一般來說自定義控制器會有一個 Control Loop 邏輯，會持續監聽自定義資源在 API server 的事件變化(Create、Update 與 Delete)，一但收到變化後，取出 API 物件內容，並執行預期結果。</p><p><img src="https://i.imgur.com/vTzezjA.png" alt="The Controller Loop"><br>(圖片擷取自：<a href="https://github.com/programming-kubernetes" target="_blank" rel="noopener">Programming Kubernetes</a>)</p><blockquote><p>TODO: 補充更多細節</p></blockquote><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>在 Kubernetes 生態中，幾乎所有 API 物件功能都是以這樣形式來完成，讓使用者以宣告式 API(Declarative API)方式先定義該物件<code>預期執行的需求</code>，最後再由控制器想辦法<code>執行到預期的結果</code>。而在過去，這種模式並沒有盛行於開發者上，是直到 v1.7 版本 CRD(CustomResourceDefinitions) 的出現(當然 Kubernetes 成功也是原因)，才出現越來越多基於此概念的各種控制器出現，甚至出現了新的名詞『Operator』。</p><p>自定義控制器除了能夠讓開發人員擴展與添加新功能以外，事實上也能替換現有的功能來優化(如利用 kbue-router 取代 kube-proxy)。當然也能用於執行一些自動化管理任務。接下來我將用一系列文章說明如何實作自定義控制器，並了解一些技巧。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/master/pkg/controller</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/extend-kubernetes/</a></li><li><a href="https://speakerdeck.com/thockin/kubernetes-what-is-reconciliation" target="_blank" rel="noopener">https://speakerdeck.com/thockin/kubernetes-what-is-reconciliation</a></li><li><a href="https://medium.com/speechmatics/how-to-write-kubernetes-custom-controllers-in-go-8014c4a04235" target="_blank" rel="noopener">https://medium.com/speechmatics/how-to-write-kubernetes-custom-controllers-in-go-8014c4a04235</a></li><li><a href="https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc" target="_blank" rel="noopener">https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc</a></li><li><a href="https://github.com/kubeflow/tf-operator/issues/300" target="_blank" rel="noopener">https://github.com/kubeflow/tf-operator/issues/300</a></li><li><a href="https://admiralty.io/blog/kubernetes-custom-resource-controller-and-operator-development-tools/" target="_blank" rel="noopener">https://admiralty.io/blog/kubernetes-custom-resource-controller-and-operator-development-tools/</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/</a></li><li><a href="https://zhuanlan.zhihu.com/p/59660536" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59660536</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;使用 Kubernetes 時，大家都能感受到其容器編配能力，當有一個容器發生異常時，Kubernetes 會透過自身機制幫你把容器遷移或重新啟動，或者能利用副本機制讓容器同時存在於叢集的不同節點上，甚至提供滾動升級(Rolling Update)容器機制。這些酷炫的功能，大家肯定都知道如何去使用，因為 Kubernetes 透過一些方式，將複雜的功能進行了抽象化與封裝，因此使用者只需要了解如何操作 API 物件，就能完成需要的功能，比如:Deployment 修改參數就會進行滾動升級。然而這些『抽象化』與『封裝』的過程究竟是如何實現呢?今天文章就是要針對這個部分進行探討。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>利用 Device Plugins 提供硬體加速</title>
    <link href="https://k2r2bai.com/2019/10/01/ironman2020/day16/"/>
    <id>https://k2r2bai.com/2019/10/01/ironman2020/day16/</id>
    <published>2019-09-30T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank" rel="noopener">Device Plugins</a> 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">Device Plugins 介面</a>來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。</p><p>P.S. 傳統的<code>alpha.kubernetes.io/nvidia-gpu</code>將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。</p><a id="more"></a><h2 id="Device-Plugins-原理"><a href="#Device-Plugins-原理" class="headerlink" title="Device Plugins 原理"></a>Device Plugins 原理</h2><p>Device  Plugins 主要提供了一個 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">gRPC 介面</a>來給廠商實現<code>ListAndWatch()</code>與<code>Allocate()</code>等 gRPC 方法，並監聽節點的<code>/var/lib/kubelet/device-plugins/</code>目錄中的 gRPC Server Unix Socket，這邊可以參考官方文件 <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank" rel="noopener">Device Plugins</a>。一旦啟動 Device Plugins 時，透過 Kubelet Unix Socket 註冊，並提供該 plugin 的 Unix Socket 名稱、API 版本號與插件資源名稱(vendor-domain/resource，例如 nvidia.com/gpu)，接著 Kubelet 會將這些曝露到 Node 狀態以便 Scheduler 使用。</p><p>Unix Socket 範例：</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ ls /var/lib/kubelet/device-plugins/</span><br><span class="line">kubelet_internal_checkpoint  kubelet.sock  nvidia.sock</span><br></pre></td></tr></table></figure><p>一些 Device Plugins 列表：</p><ul><li><a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="noopener">NVIDIA GPU</a></li><li><a href="https://github.com/hustcat/k8s-rdma-device-plugin" target="_blank" rel="noopener">RDMA</a></li><li><a href="https://github.com/kubevirt/kubernetes-device-plugins" target="_blank" rel="noopener">Kubevirt</a></li><li><a href="https://github.com/vikaschoudhary16/sfc-device-plugin" target="_blank" rel="noopener">SFC</a></li><li><a href="https://github.com/intel/intel-device-plugins-for-kubernetes" target="_blank" rel="noopener">Intel Device Plugins</a></li><li><a href="https://github.com/intel/sriov-network-device-plugin" target="_blank" rel="noopener">SR-IOV</a></li></ul><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為<code>Ubuntu 18.04+</code>:</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th><th>Role</th><th>Extra Device</th></tr></thead><tbody><tr><td>172.22.132.11</td><td>k8s-m1</td><td>4</td><td>16G</td><td>Master</td><td>None</td></tr><tr><td>172.22.132.12</td><td>k8s-m2</td><td>4</td><td>16G</td><td>Master</td><td>None</td></tr><tr><td>172.22.132.13</td><td>k8s-m3</td><td>4</td><td>16G</td><td>Master</td><td>None</td></tr><tr><td>172.22.132.21</td><td>k8s-n1</td><td>4</td><td>16G</td><td>Node</td><td>None</td></tr><tr><td>172.22.132.22</td><td>k8s-n2</td><td>4</td><td>16G</td><td>Node</td><td>None</td></tr><tr><td>172.22.132.32</td><td>k8s-g2</td><td>4</td><td>16G</td><td>Node</td><td>GTX 1060 3G *2</td></tr></tbody></table><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝 Device Plugin 前，需要確保以下條件達成：</p><ul><li>所有節點需要安裝 <a href="https://docs.docker.com/v17.09/engine/installation/#cloud" target="_blank" rel="noopener">Docker</a>。</li></ul><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ curl -fsSL <span class="string">"https://get.docker.com/"</span> | sh</span><br></pre></td></tr></table></figure><ul><li>GPU 節點需正確安裝指定版本的 NVIDIA Driver 與 CUDA。</li></ul><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb</span><br><span class="line">$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb</span><br><span class="line">$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub</span><br><span class="line">$ sudo apt-get update </span><br><span class="line">$ sudo apt-get install -y linux-headers-$(uname -r)</span><br><span class="line">$ sudo apt-get -o Dpkg::Options::=<span class="string">"--force-overwrite"</span> install -y cuda-10-0 cuda-drivers</span><br></pre></td></tr></table></figure><ul><li>GPU 節點需正確安裝指定版本的 <a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">NVIDIA Docker 2</a>。</li></ul><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -</span><br><span class="line">$ curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">$ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><ul><li>部署一座 Kubernetes v1.10+ 叢集。請參考 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener">kubeadm 部署 Kubernetes 叢集</a>。</li></ul><h2 id="安裝-NVIDIA-Device-Plugin"><a href="#安裝-NVIDIA-Device-Plugin" class="headerlink" title="安裝 NVIDIA Device Plugin"></a>安裝 NVIDIA Device Plugin</h2><p>若上述要求以符合，再開始前需要在<code>每台 GPU worker 節點</code>修改<code>/lib/systemd/system/docker.service</code>檔案，將 Docker default runtime 改成 nvidia，依照以下內容來修改:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia</span><br><span class="line">...</span><br></pre></td></tr></table></figure><blockquote><p>這邊也可以修改<code>/etc/docker/daemon.json</code>檔案，請參考 <a href="https://docs.docker.com/config/daemon/" target="_blank" rel="noopener">Configure and troubleshoot the Docker daemon</a>。</p></blockquote><p>完成後儲存，並重新啟動 Docker：</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>確認上述完成，接著在主節點透過 kubectl 來部署 NVIDIA Device Plugins:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl create -f https://gist.githubusercontent.com/kairen/bf967d566d35edda381edb9ba8659f7b/raw/ccc18711bf016d5b836280226785c1ad0282c035/nvidia-device-plugin.yml</span><br><span class="line">daemonset <span class="string">"nvidia-device-plugin-daemonset"</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl -n kube-system get po -o wide</span><br><span class="line">NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class="line">...</span><br><span class="line">nvidia-device-plugin-daemonset-nwx2s       1/1     Running   0          49s    10.244.255.80   k8s-g2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><blockquote><p>由於目前 NVIDIA Device Plugin 的 beta3 有問題，因此以 beat1 為主。</p></blockquote><h2 id="測試-GPU"><a href="#測試-GPU" class="headerlink" title="測試 GPU"></a>測試 GPU</h2><p>首先執行以下指令確認是否可被分配資源:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes <span class="string">"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu"</span></span><br><span class="line">NAME     GPU</span><br><span class="line">k8s-g2   2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>當 NVIDIA Device Plugins 部署完成後，即可建立一個簡單範例來進行測試:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: gpu-pod</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">  containers:</span><br><span class="line">  - image: nvidia/cuda</span><br><span class="line">    name: cuda</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">"nvidia-smi"</span>]</span><br><span class="line">    resources:</span><br><span class="line">      limits:</span><br><span class="line">        nvidia.com/gpu: 1</span><br><span class="line">EOF</span><br><span class="line">pod <span class="string">"gpu-pod"</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl get po -o wide</span><br><span class="line">NAME                     READY   STATUS      RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">gpu-pod                  0/1     Completed   0          21m     10.244.255.81   k8s-g2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl logs gpu-pod</span><br><span class="line">Sat Oct 01 15:28:38 2019</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GTX 106...  Off  | 00000000:05:00.0 Off |                  N/A |</span><br><span class="line">|  0%   38C    P8     6W / 120W |      0MiB /  3019MiB |      1%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>從上面結果可以看到 Kubernetes Pod 正確的使用到 NVIDIA GPU。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>Kubernetes 提供了 Device Plugin Interface 來讓硬體供應商實現自家硬體與 Kubernetes 整合的功能，這使 Kubernetes 社區不在需要維護各種廠商的硬體整合程式，以減少核心程式碼的複雜性，一方面能更加專注在規範 Device Plugin 標準的事情。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://medium.com/@maniac.tw/ubuntu-18-04-%E5%AE%89%E8%A3%9D-nvidia-driver-418-cuda-10-tensorflow-1-13-a4f1c71dd8e5" target="_blank" rel="noopener">https://medium.com/@maniac.tw/ubuntu-18-04-%E5%AE%89%E8%A3%9D-nvidia-driver-418-cuda-10-tensorflow-1-13-a4f1c71dd8e5</a></li><li><a href="https://www.mvps.net/docs/install-nvidia-drivers-ubuntu-18-04-lts-bionic-beaver-linux/" target="_blank" rel="noopener">https://www.mvps.net/docs/install-nvidia-drivers-ubuntu-18-04-lts-bionic-beaver-linux/</a></li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Device Plugins&lt;/a&gt; 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 &lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Device Plugins 介面&lt;/a&gt;來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。&lt;/p&gt;
&lt;p&gt;P.S. 傳統的&lt;code&gt;alpha.kubernetes.io/nvidia-gpu&lt;/code&gt;將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>自建私有容器儲存庫(Container Registry)與實現內容信任(Content Trust)</title>
    <link href="https://k2r2bai.com/2019/09/30/ironman2020/day15/"/>
    <id>https://k2r2bai.com/2019/09/30/ironman2020/day15/</id>
    <published>2019-09-29T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在地端的環境中，有許多原本能透過網路取的資源(如: 系統套件、容器映像檔等等)，有可能會基於公司一些考量(如:安全、網路等)，而將這些資源建立在本地端，然後再提供給叢集使用。其中容器儲存庫(Container Registry)是最常見的需求，因為有些團隊會要求公司測試與服務的容器映像檔，都必須從公司內部取得，這時自建一套私有容器儲存庫就非常重要。尤其是基於安全考量，還需要對映像檔進行安全掃描，或對映像檔內容進行加密等等。</p><p>而今天將說明如何自建一套容器儲存庫，並實現映像檔內容信任功能，以確保叢集使用的映像檔處於安全受信任的。在開始前，先來了解一下今天要使用到的開源軟體吧。</p><a id="more"></a><h3 id="Harbor"><a href="#Harbor" class="headerlink" title="Harbor"></a>Harbor</h3><p><a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">Harbor</a> 是 CNCF Incubating 專案，該專案是基於 Docker Distribution 擴展功能的 Container Registry，提供映像檔儲存、簽署、漏洞掃描等功能。另外增加了安全、身份認證與 Web-based 管理介面等功能。</p><ul><li>整合 LDAP/Active Directory、OIDC 進行使用者認證</li><li>整合 Clair 以實現容器映像檔安全掃描</li><li>整合 Notary 以實現容器映像檔簽署(Content trust)</li><li>支援 S3、Cloud Storage 等儲存後端</li><li>支援映像檔副本機制</li><li>提供使用者管理(User managment)UI</li><li>提供基於角色存取控制(Role-based access control)和活動稽核(Activity auditing)機制</li></ul><h3 id="Clair"><a href="#Clair" class="headerlink" title="Clair"></a>Clair</h3><p><a href="https://github.com/coreos/clair" target="_blank" rel="noopener">Clair</a> 是 CoreOS 開源的容器映像檔安全掃描專案，其提供 API 式的分析服務，透過比對公開漏洞資料庫 CVE（Common Vulnerabilities and Exposures）的漏洞資料，並發送關於容器潛藏漏洞的有用和可操作資訊給管理者。</p><p><img src="https://i.imgur.com/bW4smc9.png" alt></p><h3 id="Notary"><a href="#Notary" class="headerlink" title="Notary"></a>Notary</h3><p><a href="https://github.com/theupdateframework/notary" target="_blank" rel="noopener">Notary</a> 是 CNCF Incubating 專案，該專案是 Docker 對安全模組重構時，抽離的獨立專案。Notary 是用於建立內容信任的平台，目標是確保 Server 與 Client 之間交互使用已經相互信任的連線，並保證在 Internet 上的內容發佈安全性，該專案在容器應用時，能夠對映像檔、映像檔完整性等安全需求提供內容信任支援。</p><blockquote><p>TODO: 補 Portieris。</p></blockquote><h2 id="部署環境"><a href="#部署環境" class="headerlink" title="部署環境"></a>部署環境</h2><p>本部分將說明如何部署 Harbor，並設定啟用 Clair 與 Notary。</p><h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為<code>Ubuntu 18.04+</code>:</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th><th>Role</th></tr></thead><tbody><tr><td>172.22.132.11</td><td>k8s-m1</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.12</td><td>k8s-m2</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.13</td><td>k8s-m3</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.21</td><td>k8s-n1</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.22</td><td>k8s-n2</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.31</td><td>k8s-g1</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.32</td><td>k8s-g2</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.253</td><td>deploy-node</td><td>4</td><td>16G</td><td>Harbor</td></tr></tbody></table><blockquote><p>k8s 節點不需要這麼多，這邊只是沿用。</p></blockquote><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>在開始部署時，請確保滿足以下條件:</p><ul><li><code>deploy-node</code>節點需要安裝容器引擎:</li></ul><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ curl -fsSL <span class="string">"https://get.docker.com/"</span> | sh</span><br></pre></td></tr></table></figure><ul><li><code>deploy-node</code>節點需要安裝 docker-compose 工具:</li></ul><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">$ chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br></pre></td></tr></table></figure><ul><li>部署一座 Kubernetes v1.10+ 叢集。可參考<a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li></ul><h3 id="Harbor-部署"><a href="#Harbor-部署" class="headerlink" title="Harbor 部署"></a>Harbor 部署</h3><p>Harbor 會由多個容器部署而成，因此我們需要在部署前取得安裝檔案。這邊可以利用 wget 下載 Offline 安裝的檔案:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ wget https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz</span><br><span class="line">$ tar xvf harbor-offline-installer-v1.9.0.tgz &amp;&amp; \</span><br><span class="line">    rm harbor-offline-installer-v1.9.0.tgz</span><br><span class="line">$ <span class="built_in">cd</span> harbor</span><br></pre></td></tr></table></figure><p>這邊安裝透過 Offline 進行，因此在部署前，需要透過 Docker 載入映像檔:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ docker load &lt; harbor.v1.9.0.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成後，透過 images 指令查看</span></span><br><span class="line">$ docker images</span><br></pre></td></tr></table></figure><p>接著由於部署的 Harbor 使用 HTTPS，因此需要提供憑證。這邊由於測試用，因此以自簽(Self signed)來處理:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 產生 CA crt </span></span><br><span class="line">$ openssl req \</span><br><span class="line">    -newkey rsa:4096 -nodes -sha256 -keyout ca.key \</span><br><span class="line">    -x509 -days 365 -out ca.crt \</span><br><span class="line">    -subj <span class="string">"/C=TW/ST=New Taipei/L=New Taipei/O=test_company/OU=IT/CN=test"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 產生 Harbor csr </span></span><br><span class="line">$ openssl req \</span><br><span class="line">    -newkey rsa:4096 -nodes -sha256 -keyout harbor-registry.key \</span><br><span class="line">    -out harbor-registry.csr \</span><br><span class="line">    -subj <span class="string">"/C=TW/ST=New Taipei/L=New Taipei/O=test_company/OU=IT/CN=172.22.132.253"</span></span><br><span class="line"></span><br><span class="line">$ cat &gt; v3.ext &lt;&lt;-EOF</span><br><span class="line">authorityKeyIdentifier=keyid,issuer</span><br><span class="line">basicConstraints=CA:FALSE</span><br><span class="line">keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment</span><br><span class="line">extendedKeyUsage = serverAuth </span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line"></span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1=harbor-server</span><br><span class="line">IP.1=172.22.132.253</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ openssl x509 -req -sha512 -days 3650 \</span><br><span class="line">    -extfile v3.ext \</span><br><span class="line">    -CA ca.crt -CAkey ca.key -CAcreateserial \</span><br><span class="line">    -<span class="keyword">in</span> harbor-registry.csr \</span><br><span class="line">    -out harbor-registry.crt</span><br></pre></td></tr></table></figure><p>複製檔案至<code>/data/cert/</code>目錄底下:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ mkdir -p /data/cert/</span><br><span class="line">$ cp -rp ./&#123;ca.key,ca.crt,harbor-registry.key,harbor-registry.crt&#125; /data/cert/</span><br></pre></td></tr></table></figure><p>修改 Harbor 組態檔案<code>harbor.yml</code>內容，如以下所示:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">hostname:</span> <span class="number">172.22</span><span class="number">.132</span><span class="number">.253</span></span><br><span class="line"><span class="attr">https:</span></span><br><span class="line">  <span class="comment"># https port for harbor, default is 443</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">443</span></span><br><span class="line">  <span class="comment"># The path of cert and key files for nginx</span></span><br><span class="line"><span class="attr">  certificate:</span> <span class="string">/data/cert/harbor-registry.crt</span></span><br><span class="line"><span class="attr">  private_key:</span> <span class="string">/data/cert/harbor-registry.key</span></span><br><span class="line"><span class="attr">harbor_admin_password:</span> <span class="string">p@ssw0rd</span></span><br></pre></td></tr></table></figure><blockquote><p>這邊僅修改需要欄位，其餘則保持不變。</p></blockquote><p>完成後，即可執行腳本進行部署 Harbor:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ ./prepare</span><br><span class="line">...</span><br><span class="line">Clean up the input dir</span><br><span class="line"></span><br><span class="line">$ ./install.sh --with-notary --with-clair</span><br></pre></td></tr></table></figure><h3 id="在-Docker-存取映像檔"><a href="#在-Docker-存取映像檔" class="headerlink" title="在 Docker 存取映像檔"></a>在 Docker 存取映像檔</h3><p>首先複製 ca 憑證到 Docker certs 目錄，以確保 HTTPs 能夠授權:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ mkdir -p /etc/docker/certs.d/172.22.132.253</span><br><span class="line">$ cp /data/cert/ca.crt /etc/docker/certs.d/172.22.132.253/</span><br></pre></td></tr></table></figure><p>取得測試用映像檔，並推送映像檔到 Harbor 中:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ docker pull alpine:3.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸入帳密登入</span></span><br><span class="line">$ docker login 172.22.132.253</span><br><span class="line">$ docker tag alpine:3.7 172.22.132.253/library/alpine:3.7</span><br><span class="line">$ docker push 172.22.132.253/library/alpine:3.7</span><br></pre></td></tr></table></figure><p>完成後，可以在 UI 上查看，如同下圖所示:</p><p><img src="https://i.imgur.com/nR9kkGE.png" alt></p><h3 id="在-Kubernetes-上存取映像檔"><a href="#在-Kubernetes-上存取映像檔" class="headerlink" title="在 Kubernetes 上存取映像檔"></a>在 Kubernetes 上存取映像檔</h3><p>首先在所有 K8s 節點上，複製 ca 憑證到 Docker certs 目錄，以確保 HTTPS 能夠授權:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ mkdir -p /etc/docker/certs.d/172.22.132.253</span><br><span class="line">$ scp /data/cert/ca.crt &lt;HOST&gt;:/etc/docker/certs.d/172.22.132.253/</span><br></pre></td></tr></table></figure><blockquote><p>這邊建議用 Ansible 這種工具複製。</p></blockquote><p>在任一能操作叢集的節點上，執行以下指令建立 Pull Secret:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl create secret docker-registry regcred \</span><br><span class="line">    --docker-server=&quot;172.22.132.253&quot; \</span><br><span class="line">    --docker-username=admin \</span><br><span class="line">    --docker-password=p@ssw0rd \</span><br><span class="line">    --docker-email=admin@example.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ kubectl apply -f /vagrant/harbor</span><br><span class="line">$ kubectl get po</span><br></pre></td></tr></table></figure><p>接著建立一個測試用 Pod:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span></span><br><span class="line">spec:</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: regcred</span><br><span class="line">  containers:</span><br><span class="line">  - name: alpine</span><br><span class="line">    image: 172.22.132.253/library/alpine:3.7</span><br><span class="line">    <span class="built_in">command</span>: [<span class="string">"/bin/sh"</span>, <span class="string">"-c"</span>]</span><br><span class="line">    args:</span><br><span class="line">    - <span class="string">"while :; do sleep 1; done"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ kubectl get po</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>                     1/1     Running   0          8s</span><br></pre></td></tr></table></figure><h3 id="映像檔-Content-trust"><a href="#映像檔-Content-trust" class="headerlink" title="映像檔 Content trust"></a>映像檔 Content trust</h3><p>首先透過 UI 建立新的 Project 來測試內容信任功能。</p><p><img src="https://i.imgur.com/m2ARRFp.png" alt="New Project"></p><p><img src="https://i.imgur.com/z0AIluw.png" alt="Enable content trust"></p><p>在 Harbor 節點上，複製 ca 憑證到 Notary certs 目錄，以確保 HTTPS 能夠授權:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ mkdir -p <span class="variable">$HOME</span>/.docker/tls/172.22.132.253:4443/</span><br><span class="line">$ cp /data/cert/ca.crt <span class="variable">$HOME</span>/.docker/tls/172.22.132.253:4443/</span><br></pre></td></tr></table></figure><p>在 Docker 客戶端啟用 Content Trust，並推送一個簽署的映像檔到 Harbor:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> DOCKER_CONTENT_TRUST=1</span><br><span class="line">$ <span class="built_in">export</span> DOCKER_CONTENT_TRUST_SERVER=https://172.22.132.253:4443</span><br><span class="line">$ docker tag alpine:3.7 172.22.132.253/trust/alpine:3.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 這邊會需要輸入密碼短語資訊</span></span><br><span class="line">$ docker push 172.22.132.253/trust/alpine:3.7</span><br><span class="line">...</span><br><span class="line">Enter passphrase <span class="keyword">for</span> new root key with ID 93f1593:</span><br><span class="line">Repeat passphrase <span class="keyword">for</span> new root key with ID 93f1593:</span><br><span class="line">Enter passphrase <span class="keyword">for</span> new repository key with ID 224d9cd:</span><br><span class="line">Repeat passphrase <span class="keyword">for</span> new repository key with ID 224d9cd:</span><br><span class="line">Finished initializing <span class="string">"172.22.132.253/trust/alpine"</span></span><br><span class="line">Successfully signed 172.22.132.253/trust/alpine:3.7</span><br></pre></td></tr></table></figure><p>上傳完成後，即可以查看 UI。結果如下圖所示:</p><p><img src="https://i.imgur.com/rDHeDiw.png" alt></p><p>當 Docker 啟用 Content Trust 時，也可測試 pull 未簽署的映像檔:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ docker rmi 172.22.132.253/library/alpine:3.7</span><br><span class="line">$ docker pull 172.22.132.253/library/alpine:3.7</span><br><span class="line">Error: remote trust data does not exist <span class="keyword">for</span> 172.22.132.253/library/alpine: 172.22.132.253:4443 does not have trust data <span class="keyword">for</span> 172.22.132.253/library/alpine</span><br></pre></td></tr></table></figure><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天簡單部署 Harbor 作為私有容器儲存庫使用，可以看到 Harbor 整合了許多有用的系統與工具，如:掃描映像檔 CVE、提供 Notary 映像檔內容信任、Web-based UI 等等功能。且 Harbor 也提供身份認證系統與後端儲存的整合，這讓我們擁有 Enterpise 級的功能。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://docs.docker.com/notary/getting_started/" target="_blank" rel="noopener">https://docs.docker.com/notary/getting_started/</a></li><li><a href="https://goharbor.io/" target="_blank" rel="noopener">https://goharbor.io/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在地端的環境中，有許多原本能透過網路取的資源(如: 系統套件、容器映像檔等等)，有可能會基於公司一些考量(如:安全、網路等)，而將這些資源建立在本地端，然後再提供給叢集使用。其中容器儲存庫(Container Registry)是最常見的需求，因為有些團隊會要求公司測試與服務的容器映像檔，都必須從公司內部取得，這時自建一套私有容器儲存庫就非常重要。尤其是基於安全考量，還需要對映像檔進行安全掃描，或對映像檔內容進行加密等等。&lt;/p&gt;
&lt;p&gt;而今天將說明如何自建一套容器儲存庫，並實現映像檔內容信任功能，以確保叢集使用的映像檔處於安全受信任的。在開始前，先來了解一下今天要使用到的開源軟體吧。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>實作 Kubernetes 外部認證系統整合: 以 LDAP 為例</title>
    <link href="https://k2r2bai.com/2019/09/29/ironman2020/day14/"/>
    <id>https://k2r2bai.com/2019/09/29/ironman2020/day14/</id>
    <published>2019-09-28T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在一座 Kubernetes 叢集中，通常都會透過不同的使用者來給予不同的存取權限，因為若讓任何人擁有叢集最高權限的話，有可能帶來一些風險。而在 Kubernetes 中都會有兩種類型的使用者:</p><ul><li>由 Kubernetes 管理的服務帳號(Service Account)。</li><li>普通使用者。</li></ul><p>假設普通使用者是由外部獨立系統進行管理(如 LDAP)，那麼管理員分散私鑰、儲存使用者資訊等等功能，都必須由外部系統處理，因為在這方面，Kubernetes 並沒有普通使用者的 API 物件可以使用，因此無法透過 API 將普通使用者資訊添加到叢集中。</p><a id="more"></a><p>但在 Kubernetes 生產環境中，管理普通使用者需求是很常見的需求，假設公司又希望讓管理使用者事情，由既有的帳戶系統管理的話，就會面臨問題。好在 Kubernetes 在這方面也都考慮到了，Kubernetes 提供了 <a href="https://kubernetes.io/docs/admin/authentication/#webhook-token-authentication" target="_blank" rel="noopener">Webhook Token Authentication</a> 與 <a href="https://kubernetes.io/docs/admin/authentication/#authenticating-proxy" target="_blank" rel="noopener">Authenticating Proxy</a> 機制讓我們可以跟既有系統整合。</p><blockquote><p>TODO: 補 Webhook 細節。</p></blockquote><h2 id="以-LDAP-作為-Kubernetes-身份認證"><a href="#以-LDAP-作為-Kubernetes-身份認證" class="headerlink" title="以 LDAP 作為 Kubernetes 身份認證"></a>以 LDAP 作為 Kubernetes 身份認證</h2><p>本節以 LDAP 為例來實現身份認證整合。由於 Kubernetes 官方並沒有針對 LDAP/AD 的整合，因此需要藉由 Webhook Token 方式來達成。這邊概念上會開發一個 HTTP Server 提供認證 APIs，當 Kubernetes API Server 收到認證請求時，會轉發至認證用的 HTTP Server 上，這時 HTTP Server 會利用 LDAP client 檢索符合認證的 User 資訊，並將該 User 的 Group 回傳給 API Server，最後 API Server 以該資訊來進行認證授權。</p><p><img src="https://i.imgur.com/7Fb4saO.png" alt></p><h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為<code>Ubuntu 18.04+</code>:</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th><th>Role</th></tr></thead><tbody><tr><td>172.22.132.11</td><td>k8s-m1</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.12</td><td>k8s-m2</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.13</td><td>k8s-m3</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.21</td><td>k8s-n1</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.22</td><td>k8s-n2</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.31</td><td>k8s-g1</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.32</td><td>k8s-g2</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.150</td><td>deploy-node</td><td>4</td><td>16G</td><td>LDAP Server</td></tr></tbody></table><blockquote><p>節點不需要這麼多，這邊只是沿用。</p></blockquote><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>在開始部署時，請確保滿足以下條件:</p><ul><li><code>LDAP Server</code>節點需要安裝 Docker 容器引擎:</li></ul><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ curl -fsSL <span class="string">"https://get.docker.com/"</span> | sh</span><br></pre></td></tr></table></figure><ul><li>部署一座 Kubernetes v1.10+ 叢集。可參考<a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/" target="_blank" rel="noopener">用 kubeadm 部署 Kubernetes 叢集</a>。</li></ul><h3 id="OpenLDAP-與-phpLDAPadmin-部署"><a href="#OpenLDAP-與-phpLDAPadmin-部署" class="headerlink" title="OpenLDAP 與 phpLDAPadmin 部署"></a>OpenLDAP 與 phpLDAPadmin 部署</h3><p>本部分說明如何部署、設定與操作 OpenLDAP。首先進入<code>ldap-server</code>節點，接著利用容器部署 OpenLDAP 與 phpLDAPadmin:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ docker run -d \</span><br><span class="line">    -p 389:389 -p 636:636 \</span><br><span class="line">    --env LDAP_ORGANISATION=<span class="string">"Kubernetes LDAP"</span> \</span><br><span class="line">    --env LDAP_DOMAIN=<span class="string">"k8s.com"</span> \</span><br><span class="line">    --env LDAP_ADMIN_PASSWORD=<span class="string">"password"</span> \</span><br><span class="line">    --env LDAP_CONFIG_PASSWORD=<span class="string">"password"</span> \</span><br><span class="line">    --name openldap-server \</span><br><span class="line">    osixia/openldap:1.2.0</span><br><span class="line"></span><br><span class="line">$ docker run -d \</span><br><span class="line">    -p 443:443 \</span><br><span class="line">    --env PHPLDAPADMIN_LDAP_HOSTS=172.22.132.150 \</span><br><span class="line">    --name phpldapadmin \</span><br><span class="line">    osixia/phpldapadmin:0.7.1</span><br></pre></td></tr></table></figure><blockquote><ul><li>這邊的<code>cn=admin,dc=k8s,dc=com</code>為<code>admin</code> DN，而<code>cn=admin,cn=config</code>為<code>config</code> DN。</li><li>另外這邊僅做測試用，故沒有使用 Persistent Volumes，若需要的話，可以參考 <a href="https://github.com/osixia/docker-openldap" target="_blank" rel="noopener">Docker OpenLDAP</a> 來設定。</li></ul></blockquote><p>執行完成後，就可以透過瀏覽器來 <a href="https://172.22.132.150/" target="_blank" rel="noopener">phpLDAPadmin</a>。這邊點選<code>Login</code>輸入 DN 與 Password。成功登入後畫面，就可以自行新增其他資訊。</p><p><img src="https://i.imgur.com/JBJ86LQ.png" alt></p><p>雖然可以直接利用 phpLDAPadmin 來新增跟 Kubernetes 整合的資訊，但為了操作快速，這邊以指令方式進行。</p><h4 id="建立-Kubenretes-Token-Schema"><a href="#建立-Kubenretes-Token-Schema" class="headerlink" title="建立 Kubenretes Token Schema"></a>建立 Kubenretes Token Schema</h4><p>在<code>ldap-server</code>節點透過 Docker 進入<code>openldap-server</code>容器，然後執行以下指令建立 Kubernetes token schema 設定:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -ti openldap-server sh</span><br><span class="line">$ mkdir ~/kubernetes_tokens</span><br><span class="line">$ cat &lt;&lt;EOF &gt; ~/kubernetes_tokens/kubernetesToken.schema</span><br><span class="line">attributeType ( 1.3.6.1.4.1.18171.2.1.8</span><br><span class="line">        NAME <span class="string">'kubernetesToken'</span></span><br><span class="line">        DESC <span class="string">'Kubernetes authentication token'</span></span><br><span class="line">        EQUALITY caseExactIA5Match</span><br><span class="line">        SUBSTR caseExactIA5SubstringsMatch</span><br><span class="line">        SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )</span><br><span class="line"></span><br><span class="line">objectClass ( 1.3.6.1.4.1.18171.2.3</span><br><span class="line">        NAME <span class="string">'kubernetesAuthenticationObject'</span></span><br><span class="line">        DESC <span class="string">'Object that may authenticate to a Kubernetes cluster'</span></span><br><span class="line">        AUXILIARY</span><br><span class="line">        MUST kubernetesToken )</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"include /root/kubernetes_tokens/kubernetesToken.schema"</span> &gt; ~/kubernetes_tokens/schema_convert.conf</span><br><span class="line">$ slaptest -f ~/kubernetes_tokens/schema_convert.conf -F ~/kubernetes_tokens</span><br><span class="line">config file testing succeeded</span><br></pre></td></tr></table></figure><p>然後執行以下指令來修改內容:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ vim ~/kubernetes_tokens/cn=config/cn=schema/cn\=\&#123;0\&#125;kubernetestoken.ldif</span><br><span class="line"><span class="comment"># AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.</span></span><br><span class="line"><span class="comment"># CRC32 e502306e</span></span><br><span class="line">dn: cn=kubernetestoken,cn=schema,cn=config</span><br><span class="line">objectClass: olcSchemaConfig</span><br><span class="line">cn: kubernetestoken</span><br><span class="line">olcAttributeTypes: &#123;0&#125;( 1.3.6.1.4.1.18171.2.1.8 NAME <span class="string">'kubernetesToken'</span> DESC</span><br><span class="line"> <span class="string">'Kubernetes authentication token'</span> EQUALITY caseExactIA5Match SUBSTR caseExa</span><br><span class="line"> ctIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )</span><br><span class="line">olcObjectClasses: &#123;0&#125;( 1.3.6.1.4.1.18171.2.3 NAME <span class="string">'kubernetesAuthenticationO</span></span><br><span class="line"><span class="string"> bject'</span> DESC <span class="string">'Object that may authenticate to a Kubernetes cluster'</span> AUXILIAR</span><br><span class="line"> Y MUST kubernetesToken )</span><br></pre></td></tr></table></figure><p>接著利用 ldapadd 指令將 Kubernetes token schema 物件新增到當前 LDAP 伺服器中:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/kubernetes_tokens/cn=config/cn=schema</span><br><span class="line">$ ldapadd -c -Y EXTERNAL -H ldapi:/// -f cn\=\&#123;0\&#125;kubernetestoken.ldif</span><br><span class="line">SASL/EXTERNAL authentication started</span><br><span class="line">SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth</span><br><span class="line">SASL SSF: 0</span><br><span class="line">adding new entry <span class="string">"cn=kubernetestoken,cn=schema,cn=config"</span></span><br></pre></td></tr></table></figure><p>完成後，透過 ldapsearch 指令查詢是否有正確新增 Entry:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ ldapsearch -x -H ldap:/// -LLL -D <span class="string">"cn=admin,cn=config"</span> -w password -b <span class="string">"cn=schema,cn=config"</span> <span class="string">"(objectClass=olcSchemaConfig)"</span> dn -Z</span><br><span class="line">Enter LDAP Password:</span><br><span class="line">dn: cn=schema,cn=config</span><br><span class="line">...</span><br><span class="line">dn: cn=&#123;14&#125;kubernetestoken,cn=schema,cn=config</span><br></pre></td></tr></table></figure><h4 id="新增測試用-LDAP-Groups-與-Users"><a href="#新增測試用-LDAP-Groups-與-Users" class="headerlink" title="新增測試用 LDAP Groups 與 Users"></a>新增測試用 LDAP Groups 與 Users</h4><p>一但 Kubernetes token schema 建立完成後，就能夠新增一些測試用 Groups 來模擬。這邊一樣在<code>openldap-server</code>容器中執行:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF &gt; groups.ldif</span><br><span class="line">dn: ou=People,dc=k8s,dc=com</span><br><span class="line">ou: People</span><br><span class="line">objectClass: top</span><br><span class="line">objectClass: organizationalUnit</span><br><span class="line">description: Parent object of all UNIX accounts</span><br><span class="line"></span><br><span class="line">dn: ou=Groups,dc=k8s,dc=com</span><br><span class="line">ou: Groups</span><br><span class="line">objectClass: top</span><br><span class="line">objectClass: organizationalUnit</span><br><span class="line">description: Parent object of all UNIX groups</span><br><span class="line"></span><br><span class="line">dn: cn=kubernetes,ou=Groups,dc=k8s,dc=com</span><br><span class="line">cn: kubernetes</span><br><span class="line">gidnumber: 100</span><br><span class="line">memberuid: user1</span><br><span class="line">memberuid: user2</span><br><span class="line">objectclass: posixGroup</span><br><span class="line">objectclass: top</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ ldapmodify -x -a -H ldap:// -D <span class="string">"cn=admin,dc=k8s,dc=com"</span> -w password -f groups.ldif</span><br><span class="line">adding new entry <span class="string">"ou=People,dc=k8s,dc=com"</span></span><br><span class="line"></span><br><span class="line">adding new entry <span class="string">"ou=Groups,dc=k8s,dc=com"</span></span><br><span class="line"></span><br><span class="line">adding new entry <span class="string">"cn=kubernetes,ou=Groups,dc=k8s,dc=com"</span></span><br></pre></td></tr></table></figure><p>當 Group 建立完成後，再接著建立 Users 資訊:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF &gt; users.ldif</span><br><span class="line">dn: uid=user1,ou=People,dc=k8s,dc=com</span><br><span class="line">cn: user1</span><br><span class="line">gidnumber: 100</span><br><span class="line">givenname: user1</span><br><span class="line">homedirectory: /home/users/user1</span><br><span class="line">loginshell: /bin/sh</span><br><span class="line">objectclass: inetOrgPerson</span><br><span class="line">objectclass: posixAccount</span><br><span class="line">objectclass: top</span><br><span class="line">objectClass: shadowAccount</span><br><span class="line">objectClass: organizationalPerson</span><br><span class="line">sn: user1</span><br><span class="line">uid: user1</span><br><span class="line">uidnumber: 1000</span><br><span class="line">userpassword: user1</span><br><span class="line"></span><br><span class="line">dn: uid=user2,ou=People,dc=k8s,dc=com</span><br><span class="line">homedirectory: /home/users/user2</span><br><span class="line">loginshell: /bin/sh</span><br><span class="line">objectclass: inetOrgPerson</span><br><span class="line">objectclass: posixAccount</span><br><span class="line">objectclass: top</span><br><span class="line">objectClass: shadowAccount</span><br><span class="line">objectClass: organizationalPerson</span><br><span class="line">cn: user2</span><br><span class="line">givenname: user2</span><br><span class="line">sn: user2</span><br><span class="line">uid: user2</span><br><span class="line">uidnumber: 1001</span><br><span class="line">gidnumber: 100</span><br><span class="line">userpassword: user2</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ ldapmodify -x -a -H ldap:// -D <span class="string">"cn=admin,dc=k8s,dc=com"</span> -w password -f users.ldif</span><br><span class="line">adding new entry <span class="string">"uid=user1,ou=People,dc=k8s,dc=com"</span></span><br><span class="line"></span><br><span class="line">adding new entry <span class="string">"uid=user2,ou=People,dc=k8s,dc=com"</span></span><br></pre></td></tr></table></figure><p>完成後，就可以透過指令或是登入 phpLDAPadmin 頁面查看資訊。如下圖所示。</p><p><img src="https://i.imgur.com/mRqk5F8.png" alt></p><h4 id="新增-Kubernetes-Token-至-Users"><a href="#新增-Kubernetes-Token-至-Users" class="headerlink" title="新增 Kubernetes Token 至 Users"></a>新增 Kubernetes Token 至 Users</h4><p>當 Users 建立完成後，就可以透過執行以下指令來新增每個 User 的 Kubernetes Token:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF &gt; users.txt</span><br><span class="line">dn: uid=user1,ou=People,dc=k8s,dc=com</span><br><span class="line">dn: uid=user2,ou=People,dc=k8s,dc=com</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 token 腳本指令</span></span><br><span class="line">$ <span class="keyword">while</span> <span class="built_in">read</span> -r user; <span class="keyword">do</span></span><br><span class="line">fname=$(<span class="built_in">echo</span> <span class="variable">$user</span> | grep -E -o <span class="string">"uid=[a-z0-9]+"</span> | cut -d<span class="string">"="</span> -f2)</span><br><span class="line">token=$(dd <span class="keyword">if</span>=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d <span class="string">"=+/"</span> | dd bs=32 count=1 2&gt;/dev/null)</span><br><span class="line">cat &lt;&lt; EOF &gt; <span class="string">"<span class="variable">$&#123;fname&#125;</span>.ldif"</span></span><br><span class="line"><span class="variable">$user</span></span><br><span class="line">changetype: modify</span><br><span class="line">add: objectClass</span><br><span class="line">objectclass: kubernetesAuthenticationObject</span><br><span class="line">-</span><br><span class="line">add: kubernetesToken</span><br><span class="line">kubernetesToken: <span class="variable">$token</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">ldapmodify -a -H ldapi:/// -D <span class="string">"cn=admin,dc=k8s,dc=com"</span> -w password  -f <span class="string">"<span class="variable">$&#123;fname&#125;</span>.ldif"</span></span><br><span class="line"><span class="keyword">done</span> &lt; users.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Enter LDAP Password:</span><br><span class="line">modifying entry <span class="string">"uid=user1,ou=Users,dc=k8s,dc=com"</span></span><br><span class="line"></span><br><span class="line">Enter LDAP Password:</span><br><span class="line">modifying entry <span class="string">"uid=user2,ou=Users,dc=k8s,dc=com"</span></span><br></pre></td></tr></table></figure><h3 id="部署-LDAP-Webhook"><a href="#部署-LDAP-Webhook" class="headerlink" title="部署 LDAP Webhook"></a>部署 LDAP Webhook</h3><p>當 OpenLDAP 都完成，且 Kubernetes 叢集也建立完成後，就可以進入任一 Kubernetes 主節點部署 LDAP Webhook，這邊透過 Git 取得:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/kairen/kube-ldap-authn.git</span><br><span class="line">$ <span class="built_in">cd</span> kube-ldap-authn</span><br></pre></td></tr></table></figure><blockquote><p>Golang 版本可以參考 <a href="https://github.com/kairen/kube-ldap-webhook" target="_blank" rel="noopener">kube-ldap-webhook</a>。</p></blockquote><p>新增一個<code>config.py</code>檔案，並設定查詢時需要的相關內容：</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">LDAP_URL=<span class="string">'ldap://172.22.132.150/ ldap://172.22.132.150'</span></span><br><span class="line">LDAP_START_TLS = False</span><br><span class="line">LDAP_BIND_DN = <span class="string">'cn=admin,dc=k8s,dc=com'</span></span><br><span class="line">LDAP_BIND_PASSWORD = <span class="string">'password'</span></span><br><span class="line">LDAP_USER_NAME_ATTRIBUTE = <span class="string">'uid'</span></span><br><span class="line">LDAP_USER_UID_ATTRIBUTE = <span class="string">'uidNumber'</span></span><br><span class="line">LDAP_USER_SEARCH_BASE = <span class="string">'ou=People,dc=k8s,dc=com'</span></span><br><span class="line">LDAP_USER_SEARCH_FILTER = <span class="string">"(&amp;(kubernetesToken=&#123;token&#125;))"</span></span><br><span class="line">LDAP_GROUP_NAME_ATTRIBUTE = <span class="string">'cn'</span></span><br><span class="line">LDAP_GROUP_SEARCH_BASE = <span class="string">'ou=Groups,dc=k8s,dc=com'</span></span><br><span class="line">LDAP_GROUP_SEARCH_FILTER = <span class="string">'(|(&amp;(objectClass=posixGroup)(memberUid=&#123;username&#125;))(&amp;(member=&#123;dn&#125;)(objectClass=groupOfNames)))'</span></span><br></pre></td></tr></table></figure><blockquote><p>可以參考 <a href="https://github.com/kairen/kube-ldap-authn/blob/master/config.py.example" target="_blank" rel="noopener">Config example</a> 查看詳細變數說明。</p></blockquote><p>接著將上述的設定檔以 Secret 方式上傳至 Kubernetes 叢集中，然後部署 LDAP webhook 的 DaemonSet 到所有主節點上:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl -n kube-system create secret generic ldap-authn-config --from-file=config.py=config.py</span><br><span class="line">$ kubectl create -f daemonset.yaml</span><br><span class="line">$ kubectl -n kube-system get po -l app=kube-ldap-authn -o wide</span><br><span class="line">NAME                    READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">kube-ldap-authn-sx994   1/1       Running   0          13s       192.16.35.11   k8s-m1</span><br><span class="line">...</span><br></pre></td></tr></table></figure><blockquote><p>部署到所有主節點是在 HA 架構中進行，因為呼叫 API 時，有可能會因為負載平衡關析，而導到不同節點上，這時若沒有在每個節點設定 Webhook 的話，就會認證失敗。</p></blockquote><p>部署成功後，就可以透過 cURL 工具來測試:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ curl -X POST -H <span class="string">"Content-Type: application/json"</span> \</span><br><span class="line">    -d <span class="string">'&#123;"apiVersion": "authentication.k8s.io/v1beta1", "kind": "TokenReview",  "spec": &#123;"token": "&lt;LDAP_K8S_TOKEN&gt;"&#125;&#125;'</span> \</span><br><span class="line">    http://localhost:8087/authn</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"authentication.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"TokenReview"</span>,</span><br><span class="line">  <span class="string">"status"</span>: &#123;</span><br><span class="line">    <span class="string">"authenticated"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"user"</span>: &#123;</span><br><span class="line">      <span class="string">"groups"</span>: [</span><br><span class="line">        <span class="string">"kubernetes"</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="string">"uid"</span>: <span class="string">"1000"</span>,</span><br><span class="line">      <span class="string">"username"</span>: <span class="string">"user1"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>確認沒問題後，接著在所有主節點上，新增<code>/srv/kubernetes/webhook-authn</code>檔案，並加入以下內容:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ mkdir /srv/kubernetes</span><br><span class="line">$ cat &lt;&lt;EOF &gt; /srv/kubernetes/webhook-authn</span><br><span class="line">clusters:</span><br><span class="line">  - name: ldap-authn</span><br><span class="line">    cluster:</span><br><span class="line">      server: http://localhost:8087/authn</span><br><span class="line">users:</span><br><span class="line">  - name: apiserver</span><br><span class="line">current-context: webhook</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: ldap-authn</span><br><span class="line">    user: apiserver</span><br><span class="line">  name: webhook</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>完成後，修改所有主節點的<code>/etc/kubernetes/manifests</code>目錄底下的<code>kube-apiserver.yaml</code>檔案，其內容修改成如下:</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--runtime-config=authentication.k8s.io/v1beta1=true</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--authentication-token-webhook-config-file=/srv/kubernetes/webhook-authn</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--authentication-token-webhook-cache-ttl=5m</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line"><span class="attr">    - mountPath:</span> <span class="string">/srv/kubernetes/webhook-authn</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">webhook-authn</span></span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">  - hostPath:</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">/srv/kubernetes/webhook-authn</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">File</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">webhook-authn</span></span><br></pre></td></tr></table></figure><blockquote><p>這邊<code>...</code>表示已存在的內容，請不要刪除與變更。</p></blockquote><h3 id="測試功能"><a href="#測試功能" class="headerlink" title="測試功能"></a>測試功能</h3><p>都完成部署後，就可以進入任一主節點進行測試。這邊建立一個綁定在 user1 Namespace 的 Role 與 RoleBinding 來提供權限測試:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl create ns user1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 Role</span></span><br><span class="line">$ cat &lt;&lt;EOF | kubectl create -f -</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">readonly</span>-role</span><br><span class="line">  namespace: user1</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [<span class="string">""</span>]</span><br><span class="line">  resources: [<span class="string">"pods"</span>]</span><br><span class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 RoleBinding</span></span><br><span class="line">$ cat &lt;&lt;EOF | kubectl create -f -</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">readonly</span>-role-binding</span><br><span class="line">  namespace: user1</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: kubernetes</span><br><span class="line">  apiGroup: <span class="string">""</span></span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: <span class="built_in">readonly</span>-role</span><br><span class="line">  apiGroup: <span class="string">""</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><blockquote><p>在 RoleBinding 中的 subjects 需要對應於 LDAP 中的 Group 資訊。</p></blockquote><p>接著在任一台操作端設定 Kubeconfig 來以 user1 使用者，存取叢集:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span></span><br><span class="line">$ kubectl config <span class="built_in">set</span>-credentials user1 --kubeconfig=.kube/config --token=&lt;user-ldap-token&gt;</span><br><span class="line">$ kubectl config <span class="built_in">set</span>-context user1-context \</span><br><span class="line">    --kubeconfig=.kube/config \</span><br><span class="line">    --cluster=kubernetes \</span><br><span class="line">    --namespace=user1 --user=user1</span><br></pre></td></tr></table></figure><p>透過 kubectl 來測試權限是否正確設定:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl --context=user1-context get po</span><br><span class="line">No resources found</span><br><span class="line"></span><br><span class="line">$ kubectl --context=user1-context run nginx --image nginx --port 80</span><br><span class="line">Error from server (Forbidden): deployments.extensions is forbidden: User <span class="string">"user1"</span> cannot create deployments.extensions <span class="keyword">in</span> the namespace <span class="string">"user1"</span></span><br><span class="line"></span><br><span class="line">$ kubectl --context=user1-context get po -n default</span><br><span class="line">Error from server (Forbidden): pods is forbidden: User <span class="string">"user1"</span> cannot list pods <span class="keyword">in</span> the namespace <span class="string">"default"</span></span><br></pre></td></tr></table></figure><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天簡單實作了 Kubernetes 整合外部認證系統的功能，讓我們能夠以 LDAP 方式來管理 Kubernetes 的普通使用者。可以發現 Kubernetes 在各種方面都考慮了許多擴充方式，不只是網路、儲存等等，在認證與授權部分也提供了一些 API 與機制來實現。現在也有很多開源專案實作了 Auth Webhook 來整合認證，如以下:</p><ul><li><a href="https://github.com/dexidp/dex" target="_blank" rel="noopener">Dex</a></li><li><a href="https://github.com/planetlabs/kubehook" target="_blank" rel="noopener">Kubehook</a></li><li><a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-keystone-webhook-authenticator-and-authorizer.md" target="_blank" rel="noopener">OpenStack Keystone</a></li><li><a href="https://github.com/appscode/guard" target="_blank" rel="noopener">Guard</a></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/osixia/docker-openldap" target="_blank" rel="noopener">https://github.com/osixia/docker-openldap</a></li><li><a href="https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/" target="_blank" rel="noopener">https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/</a></li><li><a href="https://github.com/torchbox/kube-ldap-authn" target="_blank" rel="noopener">https://github.com/torchbox/kube-ldap-authn</a></li><li><a href="https://superuser.openstack.org/articles/strengthening-open-infrastructure-integrating-openstack-and-kubernetes/" target="_blank" rel="noopener">https://superuser.openstack.org/articles/strengthening-open-infrastructure-integrating-openstack-and-kubernetes/</a></li><li><a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/access-authn-authz/webhook/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在一座 Kubernetes 叢集中，通常都會透過不同的使用者來給予不同的存取權限，因為若讓任何人擁有叢集最高權限的話，有可能帶來一些風險。而在 Kubernetes 中都會有兩種類型的使用者:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由 Kubernetes 管理的服務帳號(Service Account)。&lt;/li&gt;
&lt;li&gt;普通使用者。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假設普通使用者是由外部獨立系統進行管理(如 LDAP)，那麼管理員分散私鑰、儲存使用者資訊等等功能，都必須由外部系統處理，因為在這方面，Kubernetes 並沒有普通使用者的 API 物件可以使用，因此無法透過 API 將普通使用者資訊添加到叢集中。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>實作 Kubernetes 裸機 Load Balancer Part3</title>
    <link href="https://k2r2bai.com/2019/09/28/ironman2020/day13/"/>
    <id>https://k2r2bai.com/2019/09/28/ironman2020/day13/</id>
    <published>2019-09-27T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在公有雲環境中，負載平衡器建立與外部 IP 位址分配都能由雲平台完成，且 Kubernetes 也能輕易地用 Cloud Provider 來進行整合。但在地端(或裸機)環境中，原生 Kubernetes 就無法達到這樣功能，必須額外開發系統才能達到目的。而慶幸的是前 Google 工程師也看到這樣問題，因此開發了 MetalLB 來協助非雲平台 Kubernetes 能實現網路負載平衡的提供。且 MetalLB 以 Kubernetes 原生方式，直接在 Kubernetes Service 描述<code>LoadBalancer</code> 類型來要求分配負載平衡器 IP 位址。雖然 MetalLB 確實帶來了好處，但它使用起來沒問題嗎?另外它究竟是怎麼實作的?會不會影響目前叢集網路環境呢?</p><p>基於這些問題，今天想透過深入了解 MetalLB 功能與實作原理，以確保發生問題時，能夠快速解決。</p><a id="more"></a><h2 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h2><p>MetalLB 是基於標準<a href="https://en.wikipedia.org/wiki/Routing_protocol" target="_blank" rel="noopener">路由協定</a>實作的 Kubernetes 叢集負載平衡專案。該專案主要以兩個元件實現裸機負載平衡功能，分別為:</p><ul><li><strong>Controller</strong>:是叢集內的 MetalLB 控制器，主要負責分配 IP 給 Kubernetes Service 資源。該元件會監聽 Kubernetes Service 資源的事件，一但叢集有 LoadBalancer 類型的 Service 被新增時，就依據內容從一個 IP 位址池分配負載平衡 IP 給 Service 使用。</li><li><strong>Speaker</strong>:利用網路協定(L2: ARP/NDP, L3: BGP)告知負載平衡 IP 的目的位址在何處，並且如何路由。Speaker 是一個被安裝在所有節點上的 Controller，而這些叢集上的 Speaker 只會有一個負責處理事情。</li></ul><blockquote><p>TODO: 需補架構、流程圖跟程式細節說明。</p></blockquote><p>在 MetalLB 中，實現了 L2(ARP/NDP) 與 L3(BGP) 的模式，使用者可以透過在 MetalLB 組態檔設定。而這種模式差異在哪邊呢?</p><h3 id="Layer-2"><a href="#Layer-2" class="headerlink" title="Layer 2"></a>Layer 2</h3><p>在 L2 模式下，MetalLB Speaker 會在叢集中，選出一個節點以標準地址發現協定(IPv4 用 <a href="https://en.wikipedia.org/wiki/Address_Resolution_Protocol" target="_blank" rel="noopener">ARP</a>、IPv6 用 <a href="https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol" target="_blank" rel="noopener">NDP</a>)讓已分配的負載平衡 IP，透過 ARP/NDP 讓本地網路能夠得知目的位址。</p><blockquote><p>TODO: 需補架構、流程圖跟程式細節說明。</p></blockquote><p>這種模式好處在於簡單，且不需要外部硬體或配置。但受限於 L2 網路協定。</p><h3 id="BGP"><a href="#BGP" class="headerlink" title="BGP"></a>BGP</h3><p>在這種模式下，叢集節點的 MetalLB Speaker 會與外部路由器建立 BGP 對等互連，並告訴路由器如何將流量轉發到Service IP，然後藉由 BGP 策略機制，在多個節點之間實現負載平衡，以及細粒度的流量控制。</p><blockquote><p>TODO: 需補架構、流程圖與程式細節說明。</p></blockquote><p>這種模式適合生產環境，但需要更多的外部硬體與配置來達成。但要確保實作 BGP 的 CNI 不會衝突。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>從了解 MetalLB 原理後，可以更清楚知道一個 Kubernetes 裸機負載平衡該如何實現。但是除了 MetalLB 以外，還有其他方法可以實現嗎?當然有!大家可以參考我之前在社群分享的投影片 <a href="https://speakerdeck.com/kairen/how-to-impletement-kubernetes-bare-metal-load-balancer" target="_blank" rel="noopener">How to impletement Kubernetes Bare metal Load Balancer</a>。</p><blockquote><p>TODO: 需加入 IPVS 實現架構</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://metallb.universe.tf/" target="_blank" rel="noopener">https://metallb.universe.tf/</a></li><li><a href="https://github.com/danderson/metallb" target="_blank" rel="noopener">https://github.com/danderson/metallb</a></li><li><a href="https://blog.cybozu.io/entry/2019/03/25/093000" target="_blank" rel="noopener">https://blog.cybozu.io/entry/2019/03/25/093000</a></li><li><a href="https://www.objectif-libre.com/en/blog/2019/06/11/metallb/" target="_blank" rel="noopener">https://www.objectif-libre.com/en/blog/2019/06/11/metallb/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在公有雲環境中，負載平衡器建立與外部 IP 位址分配都能由雲平台完成，且 Kubernetes 也能輕易地用 Cloud Provider 來進行整合。但在地端(或裸機)環境中，原生 Kubernetes 就無法達到這樣功能，必須額外開發系統才能達到目的。而慶幸的是前 Google 工程師也看到這樣問題，因此開發了 MetalLB 來協助非雲平台 Kubernetes 能實現網路負載平衡的提供。且 MetalLB 以 Kubernetes 原生方式，直接在 Kubernetes Service 描述&lt;code&gt;LoadBalancer&lt;/code&gt; 類型來要求分配負載平衡器 IP 位址。雖然 MetalLB 確實帶來了好處，但它使用起來沒問題嗎?另外它究竟是怎麼實作的?會不會影響目前叢集網路環境呢?&lt;/p&gt;
&lt;p&gt;基於這些問題，今天想透過深入了解 MetalLB 功能與實作原理，以確保發生問題時，能夠快速解決。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>實作 Kubernetes 裸機 Load Balancer Part2</title>
    <link href="https://k2r2bai.com/2019/09/27/ironman2020/day12/"/>
    <id>https://k2r2bai.com/2019/09/27/ironman2020/day12/</id>
    <published>2019-09-26T16:00:00.000Z</published>
    <updated>2019-12-02T01:49:42.388Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>昨天文章中，我們提到想要讓同一個叢集能夠支援兩個同樣的 TCP/UDP 曝露給外部存取，雖然能夠利用 Service LoadBalancer 或 NodePort 類型來達到需求，但是這兩者依然存在著限制，比如說 NodePort 使用叢集節點 IP:Port 方式來提供存取，這存在著單點故障問題，且建立一個 Port 就會在所有節點綁定;而 LoadBalancer 則不支援地端分配負載平衡 IP 的機制，只能透過手動在<code>externalIPs</code>欄位指定，若沒指定的話，其功能只是繼承 NodePort 機制，多了個 Target Port 能夠直接存取而已，而且儘管能夠在<code>externalIPs</code>指定 IP，但這些 IP 又該從哪邊來呢?又怎麼分配呢?那該怎麼解決呢?</p><p>很慶幸的是有人開發了一個開源專案 <a href="https://metallb.universe.tf/" target="_blank" rel="noopener">MetalLB</a> 來幫助我們解決這些問題，而今天就是要來探討這個專案如何使用。</p><a id="more"></a><h2 id="環境部署"><a href="#環境部署" class="headerlink" title="環境部署"></a>環境部署</h2><p>本部分將說明如何部署與使用 MetalLB，並用於後續架構分使用。</p><h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為<code>Ubuntu 18.04+</code>:</p><table><thead><tr><th>IP Address</th><th>Hostname</th><th>CPU</th><th>Memory</th><th>Role</th></tr></thead><tbody><tr><td>172.22.132.11</td><td>k8s-m1</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.12</td><td>k8s-m2</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.13</td><td>k8s-m3</td><td>4</td><td>16G</td><td>Master</td></tr><tr><td>172.22.132.21</td><td>k8s-n1</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.22</td><td>k8s-n2</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.31</td><td>k8s-g1</td><td>4</td><td>16G</td><td>Node</td></tr><tr><td>172.22.132.32</td><td>k8s-g2</td><td>4</td><td>16G</td><td>Node</td></tr></tbody></table><blockquote><p>另外所有 Master 節點將透過 Keepalived 提供一個 Virtual IP <code>172.22.132.10</code> 作為使用。</p></blockquote><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>在開始部署時，請確保滿足以下條件:</p><ul><li>確保擁有一座版本為 v1.13.0+ 的 Kubernetes 叢集。</li><li>使用能夠與 MetalLB 共存的 <a href="https://metallb.universe.tf/installation/network-addons/" target="_blank" rel="noopener">Network Plugins</a>。</li><li>準備一些用於 IPv4 的 IP 位址。必須確保 L2/L3 網路能夠通。這邊將使用<code>172.22.132.150-172.22.132.200</code>。</li><li>若使用到 L3 功能，則還需要 BGP 路由。</li></ul><h3 id="MetalLB-安裝"><a href="#MetalLB-安裝" class="headerlink" title="MetalLB 安裝"></a>MetalLB 安裝</h3><p>MetalLB 提供了以容器方式部署到 Kubernetes，且官方也有提供 YAML 讓我們執行:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml</span><br><span class="line">namespace/metallb-system created</span><br><span class="line">podsecuritypolicy.policy/speaker created</span><br><span class="line">serviceaccount/controller created</span><br><span class="line">serviceaccount/speaker created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/metallb-system:controller created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created</span><br><span class="line">role.rbac.authorization.k8s.io/config-watcher created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/config-watcher created</span><br><span class="line">daemonset.apps/speaker created</span><br><span class="line">deployment.apps/controller created</span><br></pre></td></tr></table></figure><blockquote><p>使用 Helm 也能參考這邊 <a href="https://metallb.universe.tf/installation/" target="_blank" rel="noopener">Installation With Helm</a> 安裝 </p></blockquote><p>只要執行上面指令後，即可完成部署。這時可以透過 kubectl 來查看<code>metallb-system</code>的 Namespace:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl -n metallb-system get po</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">controller-6bcfdfd677-q9fzp   1/1     Running   0          5m</span><br><span class="line">speaker-8648w                 1/1     Running   0          5m</span><br><span class="line">speaker-8h4gs                 1/1     Running   0          5m</span><br><span class="line">speaker-f9zh4                 1/1     Running   0          5m</span><br><span class="line">speaker-dc134                 1/1     Running   0          5m</span><br><span class="line">speaker-xnkt5                 1/1     Running   0          5m</span><br><span class="line">speaker-zczp5                 1/1     Running   0          5m</span><br><span class="line">speaker-zzn5v                 1/1     Running   0          5m</span><br></pre></td></tr></table></figure><p>若這邊沒問題，就表示已經完成 MetalLB 安裝。接著需要新增 IP Pools 設定，以讓 MetalLB 能夠自動分配 IP 給 Service 的 LoadBalancer 類型使用，下面為一個 L2 的範例:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  namespace: metallb-system</span><br><span class="line">  name: config</span><br><span class="line">data:</span><br><span class="line">  config: |</span><br><span class="line">    address-pools:</span><br><span class="line">    - name: default</span><br><span class="line">      protocol: layer2</span><br><span class="line">      auto-assign: <span class="literal">true</span></span><br><span class="line">      addresses:</span><br><span class="line">      - 172.22.132.150-172.22.132.200</span><br><span class="line">    <span class="comment"># - name: production</span></span><br><span class="line">    <span class="comment">#   auto-assign: false</span></span><br><span class="line">    <span class="comment">#   avoid-buggy-ips: true</span></span><br><span class="line">    <span class="comment">#   addresses:</span></span><br><span class="line">    <span class="comment">#   - 172.22.131.0/24</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><blockquote><ul><li>這邊也可以用 CIDR 來表示。更多的設定可以參考 <a href="https://metallb.universe.tf/configuration/" target="_blank" rel="noopener">MetalLB Configuration</a></li><li>另外由於測試環境限制，僅以 L2 範例為主。</li></ul></blockquote><h2 id="功能驗證"><a href="#功能驗證" class="headerlink" title="功能驗證"></a>功能驗證</h2><p>當安裝與設定完成後，即可新增一個 Service 來驗證功能:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl run nginx --image nginx --port 80</span><br><span class="line">deployment.apps/nginx created</span><br><span class="line"></span><br><span class="line">$ kubectl expose deploy nginx --port 8080 --target-port 80 --<span class="built_in">type</span> LoadBalancer</span><br><span class="line">service/nginx exposed</span><br></pre></td></tr></table></figure><p>建立好後，透過 kubectl 來查看 Service:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get svc</span><br><span class="line">NAME         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)          AGE</span><br><span class="line">nginx        LoadBalancer   10.111.63.50   172.22.132.153   8080:31827/TCP   59s</span><br></pre></td></tr></table></figure><p>這時會看到，MetalLB 在 Service 為 LoadBalancer 時，會自動從前面設定的<code>default</code> Pool 中，分配一個 IP 給 Service 使用。當有 IP 時，可以嘗試利用 cURL 來存取看看:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ curl 172.22.132.153:8080</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接著若再新建一個不同 Port 的 Service 會怎樣呢?</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl expose deploy nginx --name nginx-80 --port 80 --target-port 80 --<span class="built_in">type</span> LoadBalancer</span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME         TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)          AGE</span><br><span class="line">nginx        LoadBalancer   10.111.63.50    172.22.132.153   8080:31827/TCP   5m16s</span><br><span class="line">nginx-80     LoadBalancer   10.97.188.162   172.22.132.154   80:30218/TCP     4s</span><br></pre></td></tr></table></figure><p>大家會發現 MetalLB 又分配了另一個 IP 來使用，這時肯定會覺得這樣是不是每個 IP 只能使用一個 Port，事實上 MetalLB 能夠在 Service 的 Annotation 中，新增<code>metallb.universe.tf/allow-shared-ip</code>欄位來達到 IP Sharing 功能，這邊可以參考 <a href="https://metallb.universe.tf/usage/" target="_blank" rel="noopener">IP Address Sharing</a>。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>今天利用 MetalLB 達成了裸機負載平衡功能，而明天我將針對該專案進行原理分析。</p><blockquote><p>最近都沒啥時間好好寫，所以一些缺少內容後續會再慢慢補齊。只能說一天寫一篇真的不簡單…</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://metallb.universe.tf/" target="_blank" rel="noopener">https://metallb.universe.tf/</a></li><li><a href="https://medium.com/@JockDaRock/metalloadbalancer-kubernetes-on-prem-baremetal-loadbalancing-101455c3ed48" target="_blank" rel="noopener">https://medium.com/@JockDaRock/metalloadbalancer-kubernetes-on-prem-baremetal-loadbalancing-101455c3ed48</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;昨天文章中，我們提到想要讓同一個叢集能夠支援兩個同樣的 TCP/UDP 曝露給外部存取，雖然能夠利用 Service LoadBalancer 或 NodePort 類型來達到需求，但是這兩者依然存在著限制，比如說 NodePort 使用叢集節點 IP:Port 方式來提供存取，這存在著單點故障問題，且建立一個 Port 就會在所有節點綁定;而 LoadBalancer 則不支援地端分配負載平衡 IP 的機制，只能透過手動在&lt;code&gt;externalIPs&lt;/code&gt;欄位指定，若沒指定的話，其功能只是繼承 NodePort 機制，多了個 Target Port 能夠直接存取而已，而且儘管能夠在&lt;code&gt;externalIPs&lt;/code&gt;指定 IP，但這些 IP 又該從哪邊來呢?又怎麼分配呢?那該怎麼解決呢?&lt;/p&gt;
&lt;p&gt;很慶幸的是有人開發了一個開源專案 &lt;a href=&quot;https://metallb.universe.tf/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MetalLB&lt;/a&gt; 來幫助我們解決這些問題，而今天就是要來探討這個專案如何使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/categories/Kubernetes/"/>
    
      <category term="IT Ironman" scheme="https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"/>
    
    
      <category term="Kubernetes" scheme="https://k2r2bai.com/tags/Kubernetes/"/>
    
  </entry>
  
</feed>
