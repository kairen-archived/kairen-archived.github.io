{"meta":{"title":"KaiRen's Blog","subtitle":"Learn Anything, Anytime, Anywhere~","description":"","author":"Kyle Bai","url":"https://k2r2bai.com"},"pages":[{"title":"","date":"2019-12-04T17:28:17.522Z","updated":"2019-12-02T01:49:42.376Z","comments":true,"path":"404.html","permalink":"https://k2r2bai.com/404.html","excerpt":"","text":""},{"title":"Archives","date":"2019-01-03T14:45:59.000Z","updated":"2019-12-02T01:49:42.403Z","comments":true,"path":"archive/index.html","permalink":"https://k2r2bai.com/archive/index.html","excerpt":"","text":""},{"title":"Categories","date":"2019-12-02T01:49:42.403Z","updated":"2019-12-02T01:49:42.403Z","comments":true,"path":"categories/index.html","permalink":"https://k2r2bai.com/categories/index.html","excerpt":"","text":""},{"title":"About Me","date":"2019-12-02T01:49:42.403Z","updated":"2019-12-02T01:49:42.403Z","comments":false,"path":"about/index.html","permalink":"https://k2r2bai.com/about/index.html","excerpt":"","text":"Who is Kyle Bai?一名喜愛開源專案的工程師，喜歡在開源社區與在地社群中，貢獻、分享與討論相關技術議題。另外，他目前在 Cloud Native Taiwan User Group 社群擔任活動共同舉辦人/講者，並定期的籌辦 Meetup 來推廣與分享雲原生(Cloud Native)的生態圈。 Site Reliability Engineer 3+ years of experience with OpenStack, Ceph, Kubernetes, which includes deployment, development, and trouble-shooting. 4+ years of experience with backend development, and mobile development. Contributor to multiple OSS including OpenStack, Kubernetes, Kubeflow, …, and etc. Certified Kubernetes Administrator. Co-organizer of Cloud Native Taiwan User Group. Technical speaker, with experience at technical talks, meetup, and conferences. See slides."},{"title":"Tags","date":"2019-12-02T01:49:42.553Z","updated":"2019-12-02T01:49:42.553Z","comments":true,"path":"tags/index.html","permalink":"https://k2r2bai.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-12-02T01:49:42.405Z","updated":"2019-12-02T01:49:42.405Z","comments":true,"path":"files/openstack/keystone/webhook-policy.json","permalink":"https://k2r2bai.com/files/openstack/keystone/webhook-policy.json","excerpt":"","text":"[{\"resource\":{\"verbs\":[\"get\",\"list\",\"watch\"],\"resources\":[\"pods\"],\"version\":\"*\",\"namespace\":\"default\"},\"match\":[{\"type\":\"role\",\"values\":[\"k8s-admin\",\"k8s-viewer\",\"k8s-editor\"]}]},{\"resource\":{\"verbs\":[\"create\",\"update\",\"delete\"],\"resources\":[\"pods\"],\"version\":\"*\",\"namespace\":\"default\"},\"match\":[{\"type\":\"role\",\"values\":[\"k8s-admin\",\"k8s-editor\"]},{\"type\":\"project\",\"values\":[\"PROJECT_ID\"]}]}]"}],"posts":[{"title":"2019 LINE Dev Day 議程心得 Part3","slug":"linedevday/part3","date":"2019-12-02T16:00:00.000Z","updated":"2019-12-05T10:36:39.997Z","comments":true,"path":"2019/12/03/linedevday/part3/","link":"","permalink":"https://k2r2bai.com/2019/12/03/linedevday/part3/","excerpt":"前言昨天分享了 LINE Infrastructure 團隊為何導入 Cloud Native 的原因，從中可以看出 LINE 面臨服務開發的資源需求快速增長問題，因此必須更加有效的利用資料中心的資源，而 Cloud Native 的導入對他們來說是個關鍵點，利用容器、服務網格(Service Mesh)、微服務、不可變基礎建設、宣告式 API 等等技術與概念來提升交付時間，並更加有效的利用與控管資源。然而 LINE Infrastructure 團隊不僅僅只是導入 Cloud Native 技術來解決面臨問題，在不同層面上也有非常多的著墨，如網路與儲存這種基礎建設最關鍵的部分也考慮的很多，且服務團隊也會基於應用選用 Cloud Native 專案來達到需求。","text":"前言昨天分享了 LINE Infrastructure 團隊為何導入 Cloud Native 的原因，從中可以看出 LINE 面臨服務開發的資源需求快速增長問題，因此必須更加有效的利用資料中心的資源，而 Cloud Native 的導入對他們來說是個關鍵點，利用容器、服務網格(Service Mesh)、微服務、不可變基礎建設、宣告式 API 等等技術與概念來提升交付時間，並更加有效的利用與控管資源。然而 LINE Infrastructure 團隊不僅僅只是導入 Cloud Native 技術來解決面臨問題，在不同層面上也有非常多的著墨，如網路與儲存這種基礎建設最關鍵的部分也考慮的很多，且服務團隊也會基於應用選用 Cloud Native 專案來達到需求。 今天就是要針對 gRPC service development in private Kubernetes cluster 這個議程來分享心得，該議程講者是 LINE Live 成員，他們為了服務能支撐直播高峰值的問題，因此導入了 Kubernetes 與 Envoy。 議程心得這個議程主要分享 LINE Live 團隊為何使用 Kubernetes 的原因，大體上 LINE Live 團隊需要一個辦法，來因應直播服務的流量峰值變化，因此需要擁有能快速 Scale In/Out 的能力，然而過去他們都服務都執行於 VM 之上，並且採用手動方式來進行擴展，比如說開個 VM 實例，然後用 Ansible 跑腳本來部署等等流程，而這樣過程無法應付瞬間爆量的狀況，因為 VM 交付與應用部署時間過久，因此導致無法快速達到需求。有鑒於上述問題，LIEN Live 團隊才選擇使用 Kubernetes 作為服務運行的平台。 從當天聽到的內容，可以了解 LINE Verda 團隊會提供自研的 VKS(Verda Kubernetes Service) 平台，來讓 Service 團隊自助部署基於 OpenStack 之上的 Kubernetes 叢集，當 Service 團隊需要更多資源時，就可以以 VKS 來快速增減節點，或是利用 Kubernetes Deployment 特性來建立多個副本以支撐流量等等。 接著講者介紹到團隊的服務架構，他們為了確保溝通的請求率，而以 gRPC 作為服務溝通的協定，更利用 Envoy 來協助轉換不同協定成 gRPC。 這邊有趣的是他們在 Mobile App 與服務交互時，採用了非常多層的 Load Balancer，像是 Mobile App 經由網際網路跟 LB 溝通時，會接著經由 Kubernetes NodePort 方式轉發到各節點上(這邊可能是 Envoy 的 Kubernetes Service)，然後再透過 Envoy 以 DNS 輪詢方式(利用 Headless service 來發現所有 Pod IP)轉發到 Spring 服務上。 其實我覺得 NodePort 那一段怪怪的…。 而由於他們內部服務有很多是 gRPC 協定在溝通，但如果以 Kubernetes Service(kube-proxy) 作為彼此路由的話，將會因為其是 TCP/IP(L4) LB 的關析，而無法達到 gPRC/HTTP2 協定的好處。因為這樣只能讓 gRPC 在每次建立連線時，有負載平衡效果，而之後每次 RPC 請求都保持原本連線，並跑到同一個 Pod 上。所以 LINE Live 團隊才會選擇 Envoy 來達到每次 RPC 請求都根據我們策略路由到合適 Pod 上。 而另外他們提到不使用 Istio 是因為 Spring 已經擁有一些相同功能，因此不需要用到整個 Istio 方案，只需要 Envoy 來處理 gPRC 轉發與轉換。至於效能問題，是他們無法容忍使用 Istio 帶來的延遲增加問題，因為他們是直播類型，所以延遲過高會影響服務品質。 關於 Istio 效能可以參考這篇 Performance and Scalability 接著講者提到 Kubernetes Pod 存取 MySQL 的 ACL 問題，因為過去無法應付 Cluster autoscaler 長出來的新節點，因此導致該節點上的 Pod 都會被阻擋存取，所以 LINE 團隊開發了一個 ACL Manager 來動態更新。基本上 ACL Manager 就是去觀察連接 DB 的 Pod 座落在哪些節點上，然後在動態調整 MySQL ACL 而在 Monitoring 部分，他們採用 Verda Team 提供的 Add-ons Manager 來部署 Prometheus，該 Prometheus 以 StatefulSet 形式運行在當前 Kubernetes 叢集中(這是目前典型的做法)，而不是透過外部 Prometheus 方式來拉取 Metrics，增加了叢集的安全性。當然這樣做法還是要依據應用來進行，並且要很好的規劃叢集節點的角色，若沒定義好相對的 Affinity 的話，就可能影響到正式服務的容器。 另外他們覺得用 Kubernetes 的 Persistent Volume 來儲存 Prometheus TSDB 很難使用，因此透過 Remote storage 來跟自家開發的 TSDB - Flash 整合。至於在視覺化部分，就是採用 Grafana 來處理 Prometheus 資料。 在 Spring 應用部分，Metrics 會透過 Spring Boot Actuator 與 OpenCensus 來提取，其中 OpenCensus 還能用於對 Metrics 進行分散式追蹤(Distributed tracing)，以確保團隊在遇到問題時，能更快的解決，甚至是提前預防問題發生。 最後部分講者分享了該團隊的 Logging 系統，可以看出類似 Monitoring 一樣直接採用 Verda Team 提供的 Add-ons 形式部署 EFK(Fluentd, Elasticsearch and Kibana) stack 架構在 Kubernetes 叢集上來收集 Logs，另外透過 IMON 來實現 Log 的告警。 結語從這個議程可以看出來 LINE 在技術採納並不是隨便就使用，完全是對症下藥。另外 LINE 團隊在 Monitoring 跟 Logging 的軟體選擇都是目前 Kubernetes 社區典型部署方案，雖然在一些細節可能有額外開發東西，但是不難看出公司採納的架構比較偏向社區方案，或許是希望藉由社區的力量來加快平台的迭代，一方面也利用社區貢獻者力量來解決遇到問題吧。 總之 LINE 團隊正在全面導入 Cloud Native 技術，利用這些技術來解決各種問題!! Reference https://www.techbang.com/posts/73284-lines-first-expansion-will-be-held-on-november-20th-and-21st-for-two-day-line-developer-day-2019 https://www.inside.com.tw/article/18162-line-dev-day","categories":[{"name":"LINE Dev Day","slug":"LINE-Dev-Day","permalink":"https://k2r2bai.com/categories/LINE-Dev-Day/"}],"tags":[{"name":"LINE Dev Day","slug":"LINE-Dev-Day","permalink":"https://k2r2bai.com/tags/LINE-Dev-Day/"}]},{"title":"2019 LINE Dev Day 議程心得 Part2","slug":"linedevday/part2","date":"2019-12-01T16:00:00.000Z","updated":"2019-12-05T09:56:12.992Z","comments":true,"path":"2019/12/02/linedevday/part2/","link":"","permalink":"https://k2r2bai.com/2019/12/02/linedevday/part2/","excerpt":"前言昨天主要分享 LINE Dev Day 當天開場的 Keynote 內容，而今天將針對我最想要了解的 Infrastructure &amp; Cloud Native 議程來分享，因為自己在這方面技術比較有著墨，因此想一探究竟 LINE 團隊是如何建構這麼大規模的平台，也非常想了解他們在技術選型時，是如何考量，而當遇到問題時，又是如何去解決，以支撐這麼大量的服務與系統營運。","text":"前言昨天主要分享 LINE Dev Day 當天開場的 Keynote 內容，而今天將針對我最想要了解的 Infrastructure &amp; Cloud Native 議程來分享，因為自己在這方面技術比較有著墨，因此想一探究竟 LINE 團隊是如何建構這麼大規模的平台，也非常想了解他們在技術選型時，是如何考量，而當遇到問題時，又是如何去解決，以支撐這麼大量的服務與系統營運。 今天將先焦距在 LINE Verda Team 的私有雲部分，會以其私有雲部門主管 Yuki 桑的 Cloud Native Challenges in Private Cloud with K8s, Knative 議題來切入。 議程心得本次 LINE 特別將 Infrastructure 作為重點放入 Keynote 分享，原因正是大多數 LINE 的服務與系統，都是由自家私有雲 Verda 所支撐，而其基礎建設 Verda Team 正是負責這項重責大任的團隊，因此可以看到關於 Infrastructure &amp; Cloud Native 都是來自該團隊。 首先分享這個議題是因為該講者是 Verda Team 的私有雲主管 Yuki 桑，Yuki 在過去非常活躍於技術社區，尤其是在 OpenStack Summit 與 Cloud Native Forum 都有他分享技術的足跡，而 LINE 的許多系統會如此茁壯，很多也是歸功於他與團隊的技術選擇，他這次從私有雲上的挑戰來說明為何選擇 Cloud Native 技術，他們又該如何解決遇到問題與貧頸。 目前 Verda Team 維護著整個 IaaS、PaaS 上的所有服務，如下圖所示: 可以看出 LINE 許多服務都是自己實現與建制，而這些功能很多都建構在 OpenStack 之上，因此他們擁有很高的擴展性與可客製化性，這是由於 OpenStack 架構能夠讓人選擇樣安裝哪些元件來提供哪些服務功能，如 Nova 提供 VM、Cinder 提供 Block Storage 與 Neutron 提供 Networking 等等，而也能聽到 LINE 在這之上又提供了各種服務，如 DBaaS(Database as a Service)、VKS(Verda Container Service)、FaaS(Function as a Service) 等等。而為何要擁有這麼多元的功能與需求呢?這是因為 LINE 從去年六月開始快速的成長，有越來越多的服務與系統需要被執行，更有越來越的團隊需要開發與測試用環境，因此面臨如此挑戰。 那麼面對這樣的需求，LINE 是如何去解決的呢? 沒錯!就是他們擁抱 Cloud Native 相關技術。從 Yuki 標註的段落，可以了解 Cloud Native 旨在採用容器、服務網格(Service Mesh)、微服務、不可變基礎建設、宣告式 API 等等，來有助在雲端環境中，建構與執行彈性與可擴展的應用。且這些技術還擁有著良好的容錯性、易管理與松耦合系統等等特點，因此結合可靠自動化\b，將使工程師(系統管理員)能夠對當前系統做出頻繁與可預測的重大變更。 有種簡單說就是盡可能用 Kubernetes 來降低服務維運與運行的困難性，畢竟對於 CNCF 來說 Cloud Native 定義的本質，很多都是再講 Kubernetes 本身。 當導入 Cloud Native 時，可以讓他們更有效的控管不同團隊的資源狀況，而另外 Yuki 也說到未導入 Cloud Native 的服務開發遇到的問題，可以看到 LINE 很多專案的服務會開很多 VM 執行不同事情，比如說有些做為主服務執行用、有些執行週期性工作或者執行操作腳本等等，因為執行這些事情的都是 Application Engineers 在進行，因此無從得知哪些 VM 在幹什麼，又或者有沒有在使用，因此造成嚴重的資源浪費，像是這邊就提到他們有 60% 的機器 CPU 使用率落在 10% 左右。而為了解決這問題才導入 Cloud Native 來提升在服務開發週期中，能夠控管的資源範圍。 而他們究竟是怎麼解決的呢?實際上，Verda Team 實現了一個 Computing Resources 抽象層，用於描述應用被部署的形式，這樣 Infra Engineers 就可以識別哪些 VM 被執行哪種形式的程式，因此能夠對此做出資源的調整，這解決之前不知道資源狀況的問題，因此導致無法隨意調整 VM 資源。 另外 Yuki 也提到目前他們正在實現的新方式，就是採用 Kubernetes 與 Knative 來取代之前 VM 的形式，由於腳本服務，通常不太需要過多的資源來支撐，因此能夠以 Function 形式在容器中執行，除了有效的利用資源以外，更加快了交付的時間。 而 LINE 的 Kubernetes as a Service 主要基於 Rancher 實現而成，他們實現了一套 Verda APIs 用於管理多個 Kubernetes 叢集，一但有人呼叫建立叢集時，就以 Rancher 來自動化部屬 Kubernetes 叢集，而當發生不同叢集事件時，也會即時收到並執行 Rancher API 來解決對應問題，比如說節點掛了，就透過 Rancher 來開新節點，並取代掉。 這邊可能使用 Rancher 的 OpenStack Driver 在 IaaS 上面部署 Kubernetes 叢集。 而另外透過開發的 Add-on Manager 來維護與管理 Monitoring、Logging 等額外功能。 這邊目前 LINE 都是選用社區典型的工具，像是 Monitoring 使用 Prometheus + Grafana，而 Logging 則用 Elasticsearch + Fluentd + Kibana 來達成。 接著 Yuki 提到為何使用 Knative，這除了減少透過 VM 執行腳本作業問題外，也想用 Knative 的 Eventing 與 Function 功能來達到各種事件與執行程序的連動，比如說某台 VM CPU 快用滿了，就觸發一個 Function 來傳遞資訊到 Slack 上，已通知大家。 從這些內容來看，可以很明顯發現 LINE Infra 團隊正在努力地提升資源的利用率，以 Cloud Native 來確保不必要的資源浪費。 最後 Yuki 桑給出了他們希望提供的 Kubernetes 架構，我想這也是許多人常問的問題，到底是要用多叢集來區分服務，還是大叢集以 Namespace 切分服務? 我想這還是要依據應用情境來區別，因為每家公司對於服務分離的要求性不同。但是在 LINE 的服務有太多種了，因此兩者在他們部署情境是都會發生的。 結語雖然這次到 LINE Dev Day 聽到這場非常類似 Cloud Native Forum 講的內容，但這是比較像是概觀與未來展望，因此沒有太多細節內容，不過扔然讓人感受到 LINE 團隊對於技術的選擇與發展，是真的很勇於嘗試，想想 Knative 剛出現也不就一年時間，他們已經開始在內部系統採用，真的讓人很佩服。 這邊想了解更多 LINE Verda 私有雲細節的話，可以看看 Yuki 桑在 Cloud Native Forum 分享的 Managed Kubernetes Cluster Service In Private Cloud 議程。 而除了 Yuki 桑的議題外，也有幾場關於 Cloud Native 議程，分別為以下，大家有興趣也可以看簡報了解: gRPC service development in private Kubernetes cluster Implementation of service mesh using Envoy and Central Dogma Reference https://www.techbang.com/posts/73284-lines-first-expansion-will-be-held-on-november-20th-and-21st-for-two-day-line-developer-day-2019 https://www.inside.com.tw/article/18162-line-dev-day","categories":[{"name":"LINE Dev Day","slug":"LINE-Dev-Day","permalink":"https://k2r2bai.com/categories/LINE-Dev-Day/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"LINE Dev Day","slug":"LINE-Dev-Day","permalink":"https://k2r2bai.com/tags/LINE-Dev-Day/"}]},{"title":"2019 LINE Dev Day 議程心得 Part1","slug":"linedevday/part1","date":"2019-11-30T16:00:00.000Z","updated":"2019-12-04T17:29:23.322Z","comments":true,"path":"2019/12/01/linedevday/part1/","link":"","permalink":"https://k2r2bai.com/2019/12/01/linedevday/part1/","excerpt":"前言今年很榮幸被 LINE 以 Influencer 形式邀請到日本參加 LINE Dev Day，這也是人生第一次踏入那長年陪伴我長大的影片的國家 - 日本。原本在這期間會參與 KubeCon NA 的活動，但因為一些變故，因此最後沒能去成，於是就來到了 LINE Dev Day，這次參與 LINE Dev Day 最大目的當然就是想要一探究竟 LINE 的基礎建設，過去在 OpenStack 與 Cloud Native 相關活動中，了解到 LINE 的 IaaS 是以 OpenStack、Ceph 等開源專案建置而成，並且提供一套自研 Kubernetes as a Service，提供給不同團隊開發與部署服務使用，而且規模無法想像的大，其維護團隊 Verda Team 更是這些專案社區的程式碼貢獻者，因此能夠來到日本現場真的讓人很興奮。","text":"前言今年很榮幸被 LINE 以 Influencer 形式邀請到日本參加 LINE Dev Day，這也是人生第一次踏入那長年陪伴我長大的影片的國家 - 日本。原本在這期間會參與 KubeCon NA 的活動，但因為一些變故，因此最後沒能去成，於是就來到了 LINE Dev Day，這次參與 LINE Dev Day 最大目的當然就是想要一探究竟 LINE 的基礎建設，過去在 OpenStack 與 Cloud Native 相關活動中，了解到 LINE 的 IaaS 是以 OpenStack、Ceph 等開源專案建置而成，並且提供一套自研 Kubernetes as a Service，提供給不同團隊開發與部署服務使用，而且規模無法想像的大，其維護團隊 Verda Team 更是這些專案社區的程式碼貢獻者，因此能夠來到日本現場真的讓人很興奮。 心得在這次 LINE Dev Day 中，一開場就是由 LINE CTO - Euivin Park 開場 Keynote，過程中說明目前 LINE 專注的幾個重點技術項目，分別為 AI、資料平台與基礎設施，以及資安與隱私，其中 AI 部分更是重頭戲，在過程中展示了許多成果，並且這些都被應用在 LINE 的各種應用與服務上，以增加更多優勢。 另外 LINE 也展示了對於 AI 的展望，從下圖會發現，他們真的搞了很多項目，除了 Face Sign 外，還有語音辨識與分析、手寫辨識與分析、資料分析、自然語言處理等等。 其實不難發現 LINE 的 AI 有許多成果，像是這次活動報到就是採用 Face Sign 形式進行，參加者只要在活動開始之前，上傳自己的照片後，就可以在當天以臉部辨識方式來完成報到，\b\b據我自己經驗來說，真的非常準確… 因為我上傳時沒戴眼鏡，但是到現場有戴眼鏡還是能識別出來，這讓我蠻訝異的。 當 AI 部分講完時，就來到了我參與本次活動最想了解部分 - Data Platform &amp; Infrastructure。這部分一開場就由 Verda Team 的 Yoshihiro san 說明 LINE 每天面臨的資料量及執行的 Spark/Hive 程序量等等，並指出當前的關鍵挑戰部分。 而因為這些問題，LINE 團隊採用了Self-service Data Platform平台，來讓內部資料分析團隊能自助建立使用，以提升分析跨服務的複雜性，並有效的運用。 另外在基礎建設(雲端系統)部分，Verda Team 在全球維護著 4 萬台以上的實體機器，\b用於支撐每天 50 億的訊息量，以及 1Tbps 的流量。而支撐這些服務的平台，就是 LINE 團隊透過各種開源技術搭建而成的私有雲，該私有雲涵蓋了 OpenStack、Ceph、Kubernetes 等等技術，其中不乏有許多 CNCF 組織的專案被採用在各種服務內，像是 gRPC、Prometheus 等等，另外 LINE 在網路技術上，採用 SRv6 來達到 Multi-Tenant Network，以解決許多分散的基礎建設網路、缺乏靈活性問題。最後有趣的是 Yoshihiro 說 LINE 目前機器推疊起來，有 2200 公尺高，相等於 3.5 座東京晴空塔的高度。 而 LINE 的 Verda Team 建構的私有雲平台，採用新的技術是希望提升開發流程與產品交付的效率，以因應目前許多服務快速發展的趨勢。 當基礎建設結束後，就進入到 LINE CTO 提到的第三大重點 - 安全與隱私(Security and Privacy)，LINE 這幾年對於使用者資料與隱私的使用上，一直致力於合法運用，並完全確保不侵犯到使用者隱私。另外有趣的是 LINE 資安團隊導入機器學習後，有效的降低使用者帳戶被盜用的狀況，甚至在今年九月達到沒有任何在被盜用了，可以想像 LINE 團隊在這方面的積極程度。 另外在資安方面，LINE 也將自己的資安漏洞回報機制轉到 HackerOne 上，以讓全球駭客能夠來修復與回報，既而加強服務的安全性。最後 LINE Dev Day 除了上述三大重點外，其議程還涵蓋了許多技術內容，如區塊鏈技術、網路銀行、自研發系統(DB, TSDB) 等等。 結語從這次活動中，可以感受到 LINE 在基礎建設(雲端系統)採用開源專案已經非常成熟，從虛擬化、網路架構與儲存方案都少不了開源專案身影，其中又導入很多 Cloud Native 技術，如 Kubernetes、Knative 等等，且 LINE 團隊也是開源社區的貢獻者，可以發現他們為開源專案上游修復了許多 Patch，除了更完善自家私有雲平台外，也能與社區緊密接合，以快速反應當前遇到問題。 雖然本篇只提到 Keynote 部分，但是從概觀來看，就能感受到 LINE 對於自家技術的要求。 Reference https://speakerdeck.com/line_devday2019/line-devday-2019-keynote","categories":[{"name":"LINE Dev Day","slug":"LINE-Dev-Day","permalink":"https://k2r2bai.com/categories/LINE-Dev-Day/"}],"tags":[{"name":"LINE Dev Day","slug":"LINE-Dev-Day","permalink":"https://k2r2bai.com/tags/LINE-Dev-Day/"}]},{"title":"動手實作 Kubernetes 自定義控制器 Part6","slug":"ironman2020/day30","date":"2019-10-14T16:00:00.000Z","updated":"2019-12-02T01:49:42.391Z","comments":true,"path":"2019/10/15/ironman2020/day30/","link":"","permalink":"https://k2r2bai.com/2019/10/15/ironman2020/day30/","excerpt":"前言在動手實作 Kubernetes 自定義控制器 Part5 文章結束後，基本上已經完成了這個自定義控制器範例的功能，這時若我們想要部署這個控制器的話，該怎麼辦呢?因為過去文章中，我們都是以 Go 語言指令直接建構程式進行測試，且使用 client-go 與 API Server 溝通時，都是以cluster-admin使用者來達成，這種作法如果是正式上線環境，必然會有很多疑慮，比如說控制器環境有安全問題，如果這些狀況被取得 Kubernetes cluster-admin 權限的話，就可能會危害到整個 Kubernetes 環境，因為 cluster-admin 可以操作任何 Kubernetes API 資源。基於這些問題，今天就是要來說明如何讓控制器正確的部署到 Kubernetes 叢集中執行。","text":"前言在動手實作 Kubernetes 自定義控制器 Part5 文章結束後，基本上已經完成了這個自定義控制器範例的功能，這時若我們想要部署這個控制器的話，該怎麼辦呢?因為過去文章中，我們都是以 Go 語言指令直接建構程式進行測試，且使用 client-go 與 API Server 溝通時，都是以cluster-admin使用者來達成，這種作法如果是正式上線環境，必然會有很多疑慮，比如說控制器環境有安全問題，如果這些狀況被取得 Kubernetes cluster-admin 權限的話，就可能會危害到整個 Kubernetes 環境，因為 cluster-admin 可以操作任何 Kubernetes API 資源。基於這些問題，今天就是要來說明如何讓控制器正確的部署到 Kubernetes 叢集中執行。 Deploy in the cluster由於自定義控制器部署到 Kubernetes 叢集時，需要擁有特定使用者與權限操作 Kubernetes APIs，才能正常的執行控制器功能，因此必須利用 Service Account 與 RBAC 來讓控制器能在 Pod 中與 API Server 溝通。 雖然 Kubernetes 在建立 Namespace 時，預設也會自動建立一個名稱為default的 Service Account，但這個 Service Account 通常會被用於該 Namespace 下的所有 Pod，因此不建議將 RBAC 權限賦予給這個 Service Account。 而要提供部署到 Kubernetes 叢集中，通常需要準備以下幾個檔案: ├── Dockerfile # 將控制器建構成容器映像檔└── deploy # 提供部署控制器的目錄 ├── crd.yml # 自定義控制器的 CRDs ├── deployment.yml # 用於部署自定義控制器程式本身 ├── rbac.yml # 自定義控制器的 API 存取權限 └── sa.yml # 自定義控制器的服務帳戶，會與 RBAC 結合以限制 Pod 存取 API 的權限 環境準備由於今天的實作內容需要用到 Kubernetes 與 Docker，因此須完成以下需求與條件: 一座 Kubernetes v1.10+ 叢集。透過 Minikube 建立即可 minikube start --kubernetes-version=v1.15.4。 一個 Docker 環境，可以直接 Minikube 執行eval $(minikube docker-env)來取的 Docker 參數，並遠端操作。 Implementation本部分將建立這些檔案，並執行所需的操作。首先由於要將控制器部署到 Kubernetes 中，因此必須將控制器程式建構成容器映像檔，這樣才能透過 Pod 的形式來部署。而建構映像檔可以透過 Dockerfile 來達到，如以下內容: FROM kairen/golang-dep:1.12-alpine AS buildENV GOPATH \"/go\"ENV PROJECT_PATH \"$GOPATH/src/github.com/cloud-native-taiwan/controller101\"ENV GO111MODULE \"on\"COPY . $PROJECT_PATHRUN cd $PROJECT_PATH &amp;&amp; \\ make &amp;&amp; mv out/controller /tmp/controllerFROM alpine:3.7COPY --from=build /tmp/controller /bin/controllerENTRYPOINT [\"controller\"] 完成後，即可透過 Docker 指令來建構與推送到公有的 Container registry 上以便後續部署使用: $ eval $(minikube docker-env)$ docker build -t &lt;owner&gt;/controller101:v0.1.0 .$ docker push &lt;owner&gt;/controller101:v0.1.0 接著我們將新增以下檔案用於部署控制器到 Kubernetes 上執行。 deploy/crd.yml用於以 CRD API 來新增自定義資源的檔案。在這檔案中，通常會定義一個或多個自定義資源內容。 apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: virtualmachines.cloudnative.twspec: group: cloudnative.tw version: v1alpha1 names: kind: VirtualMachine singular: virtualmachine plural: virtualmachines shortNames: - vm - vms scope: Namespaced additionalPrinterColumns: - name: Status type: string JSONPath: .status.phase - name: CPU type: number JSONPath: .status.server.usage.cpu - name: Memory type: number JSONPath: .status.server.usage.memory - name: Age type: date JSONPath: .metadata.creationTimestamp 有些自定義控制器會直接透過 apiextensions-apiserver 在程式啟動時，將自定義資源自動新增到當前 Kubernetes 叢集中。 deploy/sa.yml該檔案會定義一個 Service Account 用於提供給控制器程式在 Pod 中與 API Server 溝通的帳戶，這個帳戶會透過 RBAC 賦予特定的權限，以便控制器程式獲取操作所需 API 資源的權限。 apiVersion: v1kind: ServiceAccountmetadata: name: controller101 namespace: kube-system deploy/rbac.yml該檔案會賦予指定 Service Account 相對應的 API 權限，由於這個自定義控制器需要對 VirtualMachines 資源做各種操作(如 Create、Update、Delete、Get 與 Watch 等等)，因此需要設定相對應的 API Group 與 Resources 來確保控制器程式能獲取權限。 kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: controller101-rolerules:- apiGroups: - cloudnative.tw resources: - \"virtualmachines\" verbs: - \"*\"---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: controller101-rolebindingroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: controller101-rolesubjects:- kind: ServiceAccount namespace: kube-system name: controller101 如果控制器需要存取其他 API 時，就必須在rules欄位額外新增。請務必確保給予最小可運作的權限。 deploy/deployment.yml該檔案定義自定義控制器的容器如何在 Kubernetes 部署，我們會在以 Deployment 方式進行部署，因為能利用 Deployment 機制來確保控制器在叢集中的可靠性。 apiVersion: apps/v1kind: Deploymentmetadata: name: controller101 namespace: kube-systemspec: replicas: 1 selector: matchLabels: k8s-app: controller101 strategy: type: Recreate template: metadata: labels: k8s-app: controller101 spec: priorityClassName: system-cluster-critical # 由於控制器有可能是重要元件，因此要確保節點資源不足時，不會優先被驅逐 tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master serviceAccountName: controller101 containers: - name: controller image: kairen/controller101:v0.1.0 env: - name: DOCKER_HOST value: \"tcp://192.168.99.159:2376\" - name: DOCKER_TLS_VERIFY value: \"1\" - name: DOCKER_CERT_PATH value: \"/etc/docker-certs\" args: - --v=2 - --logtostderr=true - --vm-driver=docker - --leader-elect=false volumeMounts: - name: docker-certs mountPath: \"/etc/docker-certs\" readOnly: true volumes: - name: docker-certs secret: secretName: docker-certs 其中env部分，需要依據環境差異來改變。 若有多個副本時，需要透過--leader-elect來啟用 Leader Election 機制。 Deployment當檔案都建立好後，就可以透過 kubectl 來部署到 Kubernetes 叢集。首先由於範例使用 Docker Driver 來測試，因此需要在控制器程式啟動時載入 Docker Certs，故這邊要先將這些檔案以 Secrets 方式存到 Kubernetes 中，之後再提供給 Deployment 掛載使用。 $ minikube docker-envexport DOCKER_TLS_VERIFY=\"1\"export DOCKER_HOST=\"tcp://192.168.99.159:2376\"export DOCKER_CERT_PATH=\"/Users/test/.minikube/certs\"$ kubectl -n kube-system create secret generic docker-certs \\ --from-file=$HOME/.minikube/certs/ca.pem \\ --from-file=$HOME/.minikube/certs/cert.pem \\ --from-file=$HOME/.minikube/certs/key.pem 建立好 Docker certs 後，就可以執行以下指令進行部署: $ kubectl apply -f deploy/customresourcedefinition.apiextensions.k8s.io/virtualmachines.cloudnative.tw createddeployment.apps/controller101 createdclusterrole.rbac.authorization.k8s.io/controller101-role createdclusterrolebinding.rbac.authorization.k8s.io/controller101-rolebinding createdserviceaccount/controller101 created$ kubectl -n kube-system logs -f controller101-7858db7484-5bjvfI1015 11:14:45.159556 1 controller.go:77] Starting the controllerI1015 11:14:45.159640 1 controller.go:78] Waiting for the informer caches to syncI1015 11:14:45.262042 1 controller.go:86] Started workers 若 Minikube 的 Docker envs 資訊不同時，需要修改deploy/deployment.yml裡面 envs。 接著嘗試建立一個 VirtualMachine 資源實例，並用 kubectl get 與 docker ps 查看狀態: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cloudnative.tw/v1alpha1kind: VirtualMachinemetadata: name: test-vmspec: resource: cpu: 2 memory: 4GEOFvirtualmachine.cloudnative.tw/test-vm created$ kubectl get vmsNAME STATUS CPU MEMORY AGEtest-vm Active 0 0.11359797976548552 22s$ docker ps --filter \"name=test-vm\"CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5ac009171998 nginx:1.17.4 \"nginx -g 'daemon of…\" 46 seconds ago Up 45 seconds 80/tcp test-vm 結語從今天實作中，可以了解到部署自定義控制器到 Kubernetes 叢集並非難事，一但控制器容器化後，並以 Kubernetes API 資源形式部署時，就能夠增加控制器的可攜帶性與維護性，往後有版本更新時，也可以利用 Kubernetes 的一些機制(如 Rolling Upgrade)來安全地更新控制器程式。 最後由於鐵人賽文章不夠用，因此關於 CKA/CKAD 認證、 Admission Controller 等文章，都會在 KaiRen’s Blog 上持續新增。 Reference https://kubernetes.io/docs/reference/access-authn-authz/rbac/ https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/ https://kubernetes.io/docs/concepts/workloads/controllers/deployment/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"動手實作 Kubernetes 自定義控制器 Part5","slug":"ironman2020/day29","date":"2019-10-13T16:00:00.000Z","updated":"2019-12-02T01:49:42.391Z","comments":true,"path":"2019/10/14/ironman2020/day29/","link":"","permalink":"https://k2r2bai.com/2019/10/14/ironman2020/day29/","excerpt":"前言在動手實作 Kubernetes 自定義控制器 Part4 文章結語部分，我有提到\b目前實作的自定義控制器還存在著問題(如下圖)，其中就是自定義資源 VirtualMachine 的實例被刪除前，未正確透過 VM Driver 刪除實際管理的虛擬機，這樣情況下的虛擬機都會變成失去控制器管理的殭屍(或孤兒)。基於此問題，今天將說明該如何修改程式以解決這樣問題。","text":"前言在動手實作 Kubernetes 自定義控制器 Part4 文章結語部分，我有提到\b目前實作的自定義控制器還存在著問題(如下圖)，其中就是自定義資源 VirtualMachine 的實例被刪除前，未正確透過 VM Driver 刪除實際管理的虛擬機，這樣情況下的虛擬機都會變成失去控制器管理的殭屍(或孤兒)。基於此問題，今天將說明該如何修改程式以解決這樣問題。 在開始實作前，我們先來探討一些概念。在接觸 Kubernetes 時，相信大家都玩過 Deployment、Job 與 DaemonSet 等功能，這些功能有個共同點，那就是都管理著一個或多個 Pod 的生命週期，這表示當一個實例(比如 Deployment)被執行刪除時，其相關聯的 Pod 都會接著被刪除。而這種機制正是 Kubernetes 垃圾收集器，在 v1.6+ 開始，Kubernetes 會自動對一些 API 資源物件(如 Deployment、ReplicaSet)引入ownerReferences欄位，這個欄位用來標示相依 API 資源物件的 Owner 是誰，而自己則為 Owner 的 Dependent，因此當 Owner 被刪除時，所有關聯的 API 資源物件就會被垃圾收集器回收(從叢集中刪除)，而這過程又稱級聯刪除(Cascading deletion)。 雖然命名為垃圾收集器，但實際上它也是以控制器模式(Controller Pattern)的形式實作。 $ kubectl run nginx --image nginx --port 80$ kubectl get po nginx-7c45b84548-gj999 -o yamlapiVersion: v1kind: Podmetadata: creationTimestamp: \"2019-10-14T14:43:07Z\" generateName: nginx-7c45b84548- labels: pod-template-hash: 7c45b84548 run: nginx name: nginx-7c45b84548-gj999 namespace: default ownerReferences: # Deployment 管理著 ReplicaSet，而 ReplicaSet 管理著 Pod - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: nginx-7c45b84548 uid: 426961f6-b94d-49d2-a110-db17b3c50008 值得一提的是，ownerReferences是可以手動設定與修改的。比如說上述指令建立了一個 NGINX Pod，這時我們用 kubectl 刪除 Pod 的ownerReferences，就可以讓 Kubernetes 垃圾收集器無法處理到該 Pod。只是這個 Pod 也變成殭屍(或孤兒)，不會因為 Deployment 刪除而被殺掉，因此必須手動殺掉才能。 那麼這樣的方式是否可以用來解決我們遇到的問題呢? 不幸的是，Kubernetes 垃圾收集器僅能用於刪除 Kubernetes API 資源，因此無法讓我們達到 VirtualMachine 資源實例被刪除前，確保所關聯的虛擬機已被刪除。那這樣該如何實現呢? 事實上 Kubernetes 也考慮到這樣問題，因此對於 API 資源的級聯刪除提供了兩種模式: Background:在這模式下，Kubernetes 會直接刪除 Owner 資源物件，然後再由垃圾收集器在後台刪除相關的 API 資源物件。 Foreground:在這模式下，Owner 資源物件會透過設定metadta.deletionTimestamp欄位來表示『正在刪除中』。這時 Owner 資源物件依然存在於叢集中，並且能透過 REST API 查看到相關資訊。該資源被刪除條件是當移除了metadata.finalizers欄位後，才會真正的從叢集中移除。這樣機制形成了預刪除掛鉤(Pre-delete hook)，因此我們能在正在刪除的期間，開始回收相關的資源(如虛擬機或其他 Kubernetes API 資源等等)，當回收完後，再將該物件刪除。 其中 Foreground 模式能透過 Kubernetes Finalizers 機制與 OwnerReferences 機制完成。不過 OwnerReferences 只能用於內部 API 資源物件，當想要處理外部資源時，就必須利用 Finalizers 來達成。 而 Finalizers 機制只需要在 API 資源物件中的metadata.finalizers欄位塞入一個字串值即可，比如說以下範例: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cloudnative.tw/v1alpha1kind: VirtualMachinemetadata: name: test-vm-finalizer finalizers: - finalizer.cloudnative.twspec: resource: cpu: 2 memory: 4GEOFvirtualmachine.cloudnative.tw/test-vm-finalizer created 當建立時，接著透過 kubectl 來刪除這個資源實例: $ kubectl delete vms test-vm-finalizervirtualmachine.cloudnative.tw \"test-vm-finalizer\" deleted 這時會發現 kubectl 卡在刪除指令，且不管怎麼執行都無法刪除。因為這樣情況，我們開啟另一個 Terminal 查看後，發現資源物件依然存在，但metadata.deletionTimestamp被下了時間，這表示該資源已經處於預刪除階段: $ kubectl get vms test-vm-finalizer -o yamlapiVersion: cloudnative.tw/v1alpha1kind: VirtualMachinemetadata: ... deletionGracePeriodSeconds: 0 deletionTimestamp: \"2019-10-14T16:28:58Z\" finalizers: - finalizer.cloudnative.tw name: test-vm-finalizer 那麼該怎麼讓這個資源物件被刪除呢? 我們只要透過kubectl edit把metadata.finalizers欄位拔掉即可。不過這樣做法都是透過 kubectl 來達成，那麼我們該如何在自定義控制器程式中實現呢? 接下來將針對這部份進行說明。 使用 Finalizer本部分將修改controller.go程式，以加入 Finalizers 機制來確保虛擬機被正確刪除。 環境準備由於使用這個功能需要用到 Kubernetes 與 Go 語言，因此需要透過以下來完成條件: 一座 Kubernetes v1.10+ 叢集。透過 Minikube 建立即可 minikube start --kubernetes-version=v1.15.4。 一個 Docker 環境，可以直接 Minikube 執行eval $(minikube docker-env)來取的 Docker 參數，並遠端操作。 安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 Go Getting Started。 Implementation從前言的過程中，可以發現 Finalizers 能在metadata.finalizers欄位手動加入來實現預刪除掛鉤。而在程式的實作中，要加入 Finalizers 機制以避免虛擬機變成殭屍(或孤兒)並不難，只要在 controller.go 的createServer()函式中，對 VirtualMachine 物件設定metadata.finalizers即可，只不過加入前提需要確保虛擬機已被正確建立，且 VirtualMachine 一定會進入 Active 狀態下進行。 func (c *Controller) createServer(vm *v1alpha1.VirtualMachine) error &#123; vmCopy := vm.DeepCopy() ok, _ := c.vm.IsServerExist(vm.Name) if !ok &#123; ... addFinalizer(&amp;vmCopy.ObjectMeta, finalizerName) if err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineActive, nil); err != nil &#123; return err &#125; &#125; return nil&#125; ... 表示不更改內容。完整程式請參考 controller.go L187-L214 其中finalizerName被定義在成 const 變數，其內容為finalizer.cloudnative.tw。 而addFinalizer()函式而在 util.go 中被實作。基本上就是傳入 API 物件的 ObjectMeta(metadata) 與 Finalizer 名稱來設定。 修改完成後，當控制器依據 VirtualMachine 資源實例正確建立虛擬機時，就會自動塞入 Finalizers。而當擁有 Finalizers 的資源實例被執行刪除時，Kubernetes API Server 會透過 Update 操作修改metadata.deletionTimestamp欄位，但不會執行 Delete 操作，因此 Informer 實際上收到會是 Update 事件。基於此改變，我們必須在 controller.go 中修改一些流程與內容。 func (c *Controller) syncHandler(key string) error &#123; ... switch vm.Status.Phase &#123; ... case v1alpha1.VirtualMachineActive: if !vm.ObjectMeta.DeletionTimestamp.IsZero() &#123; if err := c.makeTerminatingPhase(vm); err != nil &#123; return err &#125; return nil &#125; if err := c.updateUsage(vm); err != nil &#123; return err &#125; case v1alpha1.VirtualMachineTerminating: if err := c.deleteServer(vm); err != nil &#123; return err &#125; &#125;&#125; ... 表示不更改內容。完整程式請參考 controller.go。 當 VirtualMachine 實例 Active，且被執行刪除時，可以透過判斷metadata.deletionTimestamp來確認是否進入預刪除階段，若是的話，則將 VirtualMachine 資源實例更新成 Terminating 階段，若不是的話，則持續更新狀態。其中makeTerminatingPhase()的程式實現如下: func (c *Controller) makeTerminatingPhase(vm *v1alpha1.VirtualMachine) error &#123; vmCopy := vm.DeepCopy() return c.updateStatus(vmCopy, v1alpha1.VirtualMachineTerminating, nil)&#125; 接著當控制器接收到一個處於Terminating的資源物件時，就會執行deleteServer()來刪除虛擬機，並且直到刪除成功後，才將 Finalizers 從資源實例中移除。一但被 Finalizers 時，Kubernetes 就會在經過deletionGracePeriodSeconds設定的秒數後，將該資源實例從叢集中刪除。deleteServer()的程式實現如下: func (c *Controller) deleteServer(vm *v1alpha1.VirtualMachine) error &#123; vmCopy := vm.DeepCopy() if err := c.vm.DeleteServer(vmCopy.Name); err != nil &#123; // Requeuing object to workqueue for retrying return err &#125; removeFinalizer(&amp;vmCopy.ObjectMeta, finalizerName) if err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineTerminating, nil); err != nil &#123; return err &#125; return nil&#125; 最後由於採用 Finalizers 機制，因此 Informer 並不會觸發DeleteFunc對應的deleteObject()函式，因此我們可以在 Controller 的建構子中註解掉。 func New(clientset cloudnative.Interface, informer cloudnativeinformer.SharedInformerFactory, vm driver.Interface) *Controller &#123; ... vmInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.enqueue, UpdateFunc: func(old, new interface&#123;&#125;) &#123; controller.enqueue(new) &#125;, // DeleteFunc: controller.deleteObject, &#125;) return controller&#125; ... 表示不更改內容。完整程式請參考 controller.go Running當上述功能實現後，且已有新增完 VirtualMachine CRD 的 Kubernetes 環境時，就能執行以下指令啟動控制器: $ eval $(minikube docker-env)$ go run cmd/main.go --kubeconfig=$HOME/.kube/config \\ -v=3 --logtostderr \\ --leader-elect=false \\ --vm-driver=docker...I1015 16:02:57.180484 62884 controller.go:77] Starting the controllerI1015 16:02:57.180665 62884 controller.go:78] Waiting for the informer caches to syncI1015 16:02:57.285693 62884 controller.go:86] Started workers 接著開啟另一個 Terminal 來建立 VirtualMachine 資源實例。當建立時，會發現控制器更新了 test-vm 資源實例，這時可以利用 kubectl 與 docker 查看狀態: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cloudnative.tw/v1alpha1kind: VirtualMachinemetadata: name: test-vmspec: resource: cpu: 2 memory: 4GEOFvirtualmachine.cloudnative.tw/test-vm created$ kubectl get vmsNAME STATUS CPU MEMORY AGEtest-vm Active 0 0.10977787071142493 44s$ docker ps --filter \"name=test-vm\"CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES347f8626f36a nginx:1.17.4 \"nginx -g 'daemon of…\" 4 seconds ago Up 3 seconds 80/tcp test-vm 接著我們利用 kubectl 觀察 test-vm 的metadata變化: $ kubectl get vms test-vm -o=jsonpath='&#123;.metadata.finalizers&#125;'[finalizer.cloudnative.tw] 而當執行kubectl delete vm test-vm時，就會發現這樣變化: $ kubectl get vms -wNAME STATUS CPU MEMORY AGEtest-vm Active 0 0.10595776165736437 4m4stest-vm Terminating 0 0.10595776165736437 4m6s 這時查看 API 資源與 Container 時，都會被正確移除。另外也可以嘗試把控制器暫時關閉，並執行刪除一個 VirtualMachine 資源實例的操作，這時會看到該操作卡在刪除指令下，並且資源實例還存在於叢集中。而當重新啟動控制器時，才會停止這樣狀況。 結語今天簡單認識 Kubernetes 的垃圾收集器與 Finalizers 的機制，並且在自定義控制器實作 Finalizers 來確保外部資源能夠在 Kubernetes 內部關聯的 API 資源被刪除時，優先被回收。 到這邊一個自定義控制器大致上已完成，而接下來我們將說明如何讓控制器部署到 Kubernetes 叢集中，並且能夠實現哪些功能來加強這個控制器(Expose Metrics、Admission Controller 與 Fake client testing)。 由於鐵人賽文章不夠用，因此之後都會以 KaiRen’s Blog 來新增這些內容。 Reference https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/ https://book.kubebuilder.io/reference/using-finalizers.html https://draveness.me/kubernetes-garbage-collector https://blog.openshift.com/garbage-collection-custom-resources-available-kubernetes-1-8/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"動手實作 Kubernetes 自定義控制器 Part4","slug":"ironman2020/day28","date":"2019-10-12T16:00:00.000Z","updated":"2019-12-02T01:49:42.391Z","comments":true,"path":"2019/10/13/ironman2020/day28/","link":"","permalink":"https://k2r2bai.com/2019/10/13/ironman2020/day28/","excerpt":"前言在動手實作 Kubernetes 自定義控制器 Part3 文章中，了解如何實現自定義控制器的高可靠架構，而今天將延續之前位完成的部分，會簡單以 Docker 實作一個虛擬機驅動來提供給自定義控制器使用，控制器會依據自定義資源VirtualMachine的內容，來協調完成預期結果的事情。如下架構圖所示。","text":"前言在動手實作 Kubernetes 自定義控制器 Part3 文章中，了解如何實現自定義控制器的高可靠架構，而今天將延續之前位完成的部分，會簡單以 Docker 實作一個虛擬機驅動來提供給自定義控制器使用，控制器會依據自定義資源VirtualMachine的內容，來協調完成預期結果的事情。如下架構圖所示。 由於為了方便大家在 Minikube 上執行這個自定義控制器，因此這邊實作了一個 VM Driver 的 Golang 介面，並以該介面實現一個 Docker Driver 來作為使用。這個 Docker Driver 會以 Docker 預設的系統環境變數來載入 Endpoint、Certs 等等 Docker client 需要的資訊，接著透過這些資訊建立一個 client 與 Docker API 溝通進行各種操作。這邊介面只簡單實現以下函式來完成範例: type Interface interface &#123; CreateServer(*CreateRequest) (*CreateReponse, error) DeleteServer(name string) error IsServerExist(name string) (bool, error) GetServerStatus(name string) (*GetStatusReponse, error)&#125; 當控制器收到 VirtualMachine 的實例建立時，控制器會在syncHandler()函式依據接受到的資源物件資訊來呼叫 VM Driver 進行處理相關事情(如建立虛擬機環境、更新虛擬機使用率等等)，當處理完成後，再依據回應的內容更新到 VirtualMachine 資源實例的.status內容。而當控制器收到有個 VirtualMachine 實例被刪除時，就會呼叫 Informer 的DeleteFunc來進行處理實際虛擬機移除的事情。 由於這只是為了說明如何開發控制器，因此該範例使用的 Docker Driver 在建立容器時，只會以 NGINX 映像檔為基礎來建立。 原本規劃 Fake Driver 與 KVM 來模擬，但因為時間關析，只能之後再補上。 協調資源本部分將修改controller.go程式，以實現自定義資源 VirtualMachine 管理虛擬機的機制。 環境準備由於使用這個功能需要用到 Kubernetes 與 Go 語言，因此需要透過以下來完成條件: 一座 Kubernetes v1.10+ 叢集。透過 Minikube 建立即可 minikube start --kubernetes-version=v1.15.4。 一個 Docker 環境，可以直接 Minikube 執行eval $(minikube docker-env)來取的 Docker 參數，並遠端操作。 安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 Go Getting Started。 管理虛擬機邏輯實現\b前幾天我們在實作控制器時，有提到主要處理 API 資源實例的函式是syncHandler()，因此大部分邏輯會在這邊實現。但由於該控制器需要透過一些方法管理實際的虛擬機，因此這邊以 VM Driver 實現 Docker Driver 方式進行模擬。這時要修改控制器結構與建構子如下。 pkg/controller/controller.gotype Controller struct &#123; ... vm driver.Interface // 管理實際虛擬機的驅動程式&#125;func New(clientset cloudnative.Interface, informer cloudnativeinformer.SharedInformerFactory, vm driver.Interface) *Controller &#123; ... controller := &amp;Controller&#123; ... vm: vm, &#125; ... return controller&#125; ... 表示不更改內容。完整程式請參考 controller.go L44-L72 完成後，就可以透過上面物件，在這個控制器結構的函式操作虛擬機。接著在syncHandler()實現協調循環的邏輯: func (c *Controller) syncHandler(key string) error &#123; namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; utilruntime.HandleError(fmt.Errorf(\"invalid resource key: %s\", key)) return err &#125; vm, err := c.lister.VirtualMachines(namespace).Get(name) if err != nil &#123; if errors.IsNotFound(err) &#123; utilruntime.HandleError(fmt.Errorf(\"virtualmachine '%s' in work queue no longer exists\", key)) return err &#125; return err &#125; switch vm.Status.Phase &#123; case v1alpha1.VirtualMachineNone: if err := c.makeCreatingPhase(vm); err != nil &#123; return err &#125; case v1alpha1.VirtualMachinePending, v1alpha1.VirtualMachineFailed: if err := c.createServer(vm); err != nil &#123; return err &#125; case v1alpha1.VirtualMachineActive: if err := c.updateUsage(vm); err != nil &#123; return err &#125; &#125; return nil&#125;// 用於更新 VirtualMachine 資源狀態的通用函式func (c *Controller) updateStatus(vm *v1alpha1.VirtualMachine, phase v1alpha1.VirtualMachinePhase, reason error) error &#123; vm.Status.Reason = \"\" if reason != nil &#123; vm.Status.Reason = reason.Error() &#125; vm.Status.Phase = phase vm.Status.LastUpdateTime = metav1.NewTime(time.Now()) _, err := c.clientset.CloudnativeV1alpha1().VirtualMachines(vm.Namespace).Update(vm) return err&#125;// 用於將虛擬機狀態新增到 VirtualMachine 資源的通用函式func (c *Controller) appendServerStatus(vm *v1alpha1.VirtualMachine) error &#123; status, err := c.vm.GetServerStatus(vm.Name) if err != nil &#123; return err &#125; vm.Status.Server.Usage.CPU = status.CPUPercentage vm.Status.Server.Usage.Memory = status.MemoryPercentage vm.Status.Server.State = status.State return nil&#125; 當收到 Informer 的 Add/Update 事件時，會將資源實例的物件放到 Workqueue，然後控制器的 Workers 會呼叫processNextWorkItem()函式來持續消化 Workqueue 中的物件，並在取出物件的 Key 後，將其丟到syncHandler()函式處理。而syncHandler()函式會透過 Lister 從本地快取中獲取資源實例的內容，這時我們就能透過內容的狀態來處理對應事情。以上面程式為例，我們分成以下幾個狀態來處理。 這邊使用不同狀態來處理不同過程，其目的是確保控制器不會因為實例的狀態更新，而一直觸發 Update 事件導致無限循環，因此以狀態來做收斂的點。 VirtualMachineNone: 由於 VirtualMachine 資源實例被建立時，並不會有任何資源狀態，因此該狀態可用於判斷是否是第一次建立，若是的話則將狀態更新為 Creating，這樣可以讓該資源被標示為即將建立虛擬機。程式內容如下: func (c *Controller) makeCreatingPhase(vm *v1alpha1.VirtualMachine) error &#123; vmCopy := vm.DeepCopy() return c.updateStatus(vmCopy, v1alpha1.VirtualMachineCreating, nil)&#125; VirtualMachineCreating: 當處於 Creating 時，控制器會呼叫 VM Driver 以建立虛擬機，若成功的話，則更新 VirtualMachine 資源的.status為 Active 狀態;若失敗的話，則標示為 Failed，狀態，並提供失敗原因的訊息。程式內容如下: func (c *Controller) createServer(vm *v1alpha1.VirtualMachine) error &#123; vmCopy := vm.DeepCopy() ok, _ := c.vm.IsServerExist(vm.Name) if !ok &#123; req := &amp;driver.CreateRequest&#123; Name: vm.Name, CPU: vm.Spec.Resource.Cpu().Value(), Memory: vm.Spec.Resource.Memory().Value(), &#125; resp, err := c.vm.CreateServer(req) if err != nil &#123; if err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineFailed, err); err != nil &#123; return err &#125; return err &#125; vmCopy.Status.Server.ID = resp.ID if err := c.appendServerStatus(vmCopy); err != nil &#123; return err &#125; if err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineActive, nil); err != nil &#123; return err &#125; &#125; return nil&#125; VirtualMachineFailed: 類似 Creating 狀態，當嘗試透過 VM Driver 建立虛擬機失敗時，會讓資源 Requeuing 到 Workqueue 中，並繼續在協調循環中重新嘗試建立虛擬機，直到建立成功或 VirtualMachine 資源被刪除。程式內容同 Creating 狀態。 VirtualMachineActive: 當虛擬機被正確建立，並且能夠取得狀態後，就會進入 Active 狀態。而當資源一直處於 Active 時，就能夠持續透過 VM Driver 獲取當前虛擬機狀態，並更新到 API 資源上。程式內容如下: func (c *Controller) updateUsage(vm *v1alpha1.VirtualMachine) error &#123; vmCopy := vm.DeepCopy() t := subtractTime(vmCopy.Status.LastUpdateTime.Time) if t.Seconds() &gt; periodSec &#123; if err := c.appendServerStatus(vmCopy); err != nil &#123; return err &#125; if err := c.updateStatus(vmCopy, v1alpha1.VirtualMachineActive, nil); err != nil &#123; return err &#125; &#125; return nil&#125; 這邊subtractTime()用於避免控制器一直執行updateStatus()，而導致無限循環。 而當 API 資源物件被刪除時，Informer 會呼叫DeleteFunc的對應函式deleteObject()來刪除虛擬機。程式內容如下: func (c *Controller) deleteObject(obj interface&#123;&#125;) &#123; vm := obj.(*v1alpha1.VirtualMachine) if err := c.vm.DeleteServer(vm.Name); err != nil &#123; klog.Errorf(\"Failed to delete the '%s' server: %v\", vm.Name, err) &#125;&#125; cmd/main.go當 Controller 與 VM Driver 程式都完成後，就可以修改主程式來反映功能改變: var ( ... driverName string)func parseFlags() &#123; ... flag.StringVarP(&amp;driverName, \"vm-driver\", \"\", \"\", \"Driver is one of: [fake docker].\") ...&#125;func main() &#123; ... var vmDriver driver.Interface switch driverName &#123; case \"docker\": docker, err := driver.NewDockerDriver() if err != nil &#123; klog.Fatalf(\"Error to create docker driver: %s\", err.Error()) &#125; vmDriver = docker default: klog.Fatalf(\"The driver '%s' is not supported.\", driverName) &#125; ... controller := controller.New(clientset, informer, vmDriver) ...&#125; ... 表示不更改內容。完整程式請參考 main.co 執行當上述功能實現後，且已有新增完 VirtualMachine CRD 的 Kubernetes 環境時，就可以執行以下指令來啟動控制器: $ eval $(minikube docker-env)$ go run cmd/main.go --kubeconfig=$HOME/.kube/config \\ -v=3 --logtostderr \\ --leader-elect=false \\ --vm-driver=docker...I1015 16:02:57.180484 62884 controller.go:77] Starting the controllerI1015 16:02:57.180665 62884 controller.go:78] Waiting for the informer caches to syncI1015 16:02:57.285693 62884 controller.go:86] Started workers 接著開啟另一個 Terminal 來建立 VirtualMachine 資源實例。當建立時，會發現控制器更新了 test-vm 資源實例，這時可以利用 kubectl 查看狀態: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cloudnative.tw/v1alpha1kind: VirtualMachinemetadata: name: test-vmspec: resource: cpu: 2 memory: 4GEOFvirtualmachine.cloudnative.tw/test-vm created$ kubectl get vmsNAME STATUS CPU MEMORY AGEtest-vm Active 0 0.10977787071142493 44s 由於本範例使用 Docker 作為虛擬機驅動程式，因此該資源實際上是建立一個容器。我們可以利用 docker 指令來查看: $ docker ps --filter \"name=test-vm\"CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb0d7f2be48e5 nginx:1.17.4 \"nginx -g 'daemon of…\" 8 seconds ago Up 6 seconds 80/tcp test-vm$ docker inspect test-vm -f \"&#123;&#123;.HostConfig.Memory&#125;&#125;\"4000000000$ docker inspect test-vm -f \"&#123;&#123;.HostConfig.NanoCpus&#125;&#125;\"2 接著我們來增加這個 NIGNX 的工作負載，以查看 CPU 變化: $ IP=$(docker inspect test-vm -f \"&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;\")$ docker run --rm -it busybox /bin/sh -c \"while :; do wget -O- $&#123;IP&#125;; done\" 開啟新 Terminal 以 kubectl 指令來查看: $ kubectl get vms -wNAME STATUS CPU MEMORY AGEtest-vm Active 0 0.11279374628042013 5m30stest-vm Active 15.637706179775282 0.11279374628042013 5m43stest-vm Active 15.688157325581395 0.11299480465168647 6m13stest-vm Active 15.55665426966292 0.11279374628042013 6m43s 由於控制器設計關析，CPU 與 Memory 只會每 30s 同步一次。 結語今天將控制器管理 VirtualMachine 資源實例的邏輯完成。一但完成，就能利用 Kubernetes-like API 來管理虛擬機的生命週期，或取得虛擬機狀態等等事情。然而今天實作部分，事實上還有一些問題存在，比如說我們先把自定義控制器暫時關閉，然後執行kubectl delete vm test-vm指令來將該資源實例從 Kubernetes 中刪除，這時查看虛擬機列表(因為 VM Driver 為 Docker，因此對應查看為 docker ps)時，就會發現被管理的虛擬機依然存在，並且當重新啟動控制器時，也會因為該資源實例已經被刪除，因此無法讓控制器來協助刪除，這樣就會形成殭屍虛擬機問題。如下圖所示。 從 Controller 程式碼中也可以從deleteObject()看出問題，因為這邊若發生刪除錯誤，就會造成外部資源變成殭屍(或孤兒)。 那麼當遇到這個問題時，該怎麼解決呢?明天我們將針對這部份來實作，以確保 API 資源實例一定要先刪除所管理的虛擬機後，才能從 Kubernetes 叢集中刪除。 Reference https://godoc.org/github.com/docker/docker/client https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"動手實作 Kubernetes 自定義控制器 Part3","slug":"ironman2020/day27","date":"2019-10-11T16:00:00.000Z","updated":"2019-12-02T01:49:42.391Z","comments":true,"path":"2019/10/12/ironman2020/day27/","link":"","permalink":"https://k2r2bai.com/2019/10/12/ironman2020/day27/","excerpt":"前言在動手實作 Kubernetes 自定義控制器 Part2 文章中，我們利用 client-go 與產生的 Client 函式庫實作了一個控制器功能。而今天想在控制器實現協調預期狀態之前，探討一下 Kubernetes 自定義控制器的高可靠(Highly Available，HA)如何實現。 在 Kubernetes 中，許多系統相關元件都是以 Controller Pattern 方式實現，比如說: Scheduler 與 Controller Manager。這些元件通常負責 Kubernetes 中的某一環核心功能，像是 Scheduler 負責 Pod 的節點分配， Controller Manager 提供許多 Kubernetes API 資源的功能協調與關聯。那麼如果這些元件發生故障了，就可能造成某部分功能無法正常運行，進而影響到整個叢集的健康，這樣該如何解決呢?","text":"前言在動手實作 Kubernetes 自定義控制器 Part2 文章中，我們利用 client-go 與產生的 Client 函式庫實作了一個控制器功能。而今天想在控制器實現協調預期狀態之前，探討一下 Kubernetes 自定義控制器的高可靠(Highly Available，HA)如何實現。 在 Kubernetes 中，許多系統相關元件都是以 Controller Pattern 方式實現，比如說: Scheduler 與 Controller Manager。這些元件通常負責 Kubernetes 中的某一環核心功能，像是 Scheduler 負責 Pod 的節點分配， Controller Manager 提供許多 Kubernetes API 資源的功能協調與關聯。那麼如果這些元件發生故障了，就可能造成某部分功能無法正常運行，進而影響到整個叢集的健康，這樣該如何解決呢? 如果有閱讀過淺談 Kubernetes 高可靠架構與實現 Kubernetes 高可靠架構部署文章的人，可以從中知道 Kubernetes 的核心元件都支援 HA 的部署實現。而其中的兩個重要控制器 Scheduler 與 Controller Manager 是以 Lease 機制實現 Active-Passive 架構。這表示一個環境中，有多個相同元件運作時，只會有一個作為 Leader 負責程式的功能，而其餘則會等待 Leader 發生錯時，才接手工作。 而這機制的實踐方式有很多種，比如基於 Redis、Zookeeper、Consul、etcd，或是資料庫的分散式鎖(Distributed Lock)。而 Kubernetes 則是是採用資源鎖(Resource Lock)概念來實現，基本上就是建立 Kubernetes API 資源 ConfigMap、 Endpoint 或 Lease 來維護分散式鎖的狀態。 Kubernetes 從 v1.15 版本開始推薦使用 Lease 資源實現，而 ConfigMap、 Endpoint 已經被棄用。 \b分散式鎖最常見的實現方式就是搶資源的擁有權，搶到的人就是 Leader，接著 Leader 開始定期更新鎖狀態，以表示自己處於活躍狀態，以確保其他人沒辦法搶走擁有權。而 Kubernetes 也類似這樣概念，基本上就是搶 API 上的某個資源，當搶到時，就在該資源中標示自己是擁有者，並持續更新時間來表示自己還處於活躍狀態;而其他則持續取得資源鎖中的更新時間進行比對，以確認原擁有者是否已經死亡，若是的話，則更新資源鎖來標示自己為擁有者。 這邊看一下 Kubernetes Controller Manager 實際運作狀況，當 Controller Manager 被啟動時，預設會透過--leader-elect=true來開啟 HA 功能。當正確啟動後，在 kube-system 底下，就會看到被新增了一個用於維護分散式鎖狀態的 Endpoint 資源: $ kubectl -n kube-system get ep kube-controller-manager -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;\"holderIdentity\":\"k8s-m3_9f51cc32-679e-4cc3-951e-c35f8688bbc3\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-09-25T05:39:17Z\",\"renewTime\":\"2019-10-09T09:37:48Z\",\"leaderTransitions\":6&#125;' creationTimestamp: \"2019-09-20T14:00:55Z\" name: kube-controller-manager namespace: kube-system 然後可以在該資源的metadata.annotations看到用於儲存狀態的control-plane.alpha.kubernetes.io/leader欄位。其中holderIdentity用於表示當前擁有者，acquireTime為擁有者取得持有權的時間，renewTime為當前擁有者上一次活躍時間。而更換 Leader 條件是當 renewTime 與自己當下時間計算超過leaseDurationSeconds時進行。 當確認 Leader 後，即可透過 kubectl logs 來查看元件執行結果: # Leader$ kubectl -n kube-system logs -f kube-controller-manager-k8s-m3I0923 14:02:27.809016 1 serving.go:319] Generated self-signed cert in-memoryI0923 14:02:28.214820 1 controllermanager.go:161] Version: v1.16.0I0923 14:02:28.215142 1 secure_serving.go:123] Serving securely on 127.0.0.1:10257I0923 14:02:28.215415 1 deprecated_insecure_serving.go:53] Serving insecurely on [::]:10252I0923 14:02:28.215453 1 leaderelection.go:241] attempting to acquire leader lease kube-system/kube-controller-manager...I0925 05:39:17.506983 1 leaderelection.go:251] successfully acquired lease kube-system/kube-controller-managerI0925 05:39:17.507091 1 event.go:255] Event(v1.ObjectReference&#123;Kind:\"Endpoints\", Namespace:\"kube-system\", Name:\"kube-controller-manager\", UID:\"b6627d30-c879-449f-99ea-f94d536f2516\", APIVersion:\"v1\", ResourceVersion:\"794261\", FieldPath:\"\"&#125;): type: 'Normal' reason: 'LeaderElection' k8s-m3_9f51cc32-679e-4cc3-951e-c35f8688bbc3 became leaderI0925 05:39:17.766322 1 plugins.go:100] No cloud provider specified.I0925 05:39:17.767107 1 shared_informer.go:197] Waiting for caches to sync for tokensI0925 05:39:17.778474 1 controllermanager.go:534] Started \"daemonset\"I0925 05:39:17.778485 1 daemon_controller.go:267] Starting daemon sets controllerI0925 05:39:17.778505 1 shared_informer.go:197] Waiting for caches to sync for daemon sets...# Not leader$ kubectl -n kube-system logs -f kube-controller-manager-k8s-m1I0925 05:39:06.784042 1 serving.go:319] Generated self-signed cert in-memoryI0925 05:39:07.932147 1 controllermanager.go:161] Version: v1.16.0I0925 05:39:07.932782 1 secure_serving.go:123] Serving securely on 127.0.0.1:10257I0925 05:39:07.933364 1 deprecated_insecure_serving.go:53] Serving insecurely on [::]:10252I0925 05:39:07.933418 1 leaderelection.go:241] attempting to acquire leader lease kube-system/kube-controller-manager... 講了這麼多，那究竟該如何在自己的控制器實現同樣功能呢? 事實上，Kubernetes client-go 提供了 Leader Election 功能，因此我們能夠透過這個 Package 輕易實作。 Use Leader Election Package在 client-go 中，以提供了 Leader Election Example 讓大家可以了解如何實現。因此可以下載 client-go 來進行測試，或是依據範例在控制器中實作。 環境準備由於使用這個功能需要用到 Kubernetes 與 Go 語言，因此需要透過以下來完成條件: 一座 Kubernetes v1.10+ 叢集。透過 Minikube 建立即可 minikube start --kubernetes-version=v1.15.4。 安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 Go Getting Started。 在控制器實作要在自定義控制器中，應用 Leader Election 機制其實不難，只要參考 client-go 的範例，在OnStartedLeading()函式中執行控制器程式實例的啟動函式即可，而當觸發OnStoppedLeading()時，就關閉控制器程式的運作。如以下程式，我們修改 Controller101 的 main.go。 package mainimport ( \"context\" goflag \"flag\" \"fmt\" \"os\" \"os/signal\" \"syscall\" \"time\" \"github.com/cloud-native-taiwan/controller101/pkg/controller\" cloudnative \"github.com/cloud-native-taiwan/controller101/pkg/generated/clientset/versioned\" cloudnativeinformer \"github.com/cloud-native-taiwan/controller101/pkg/generated/informers/externalversions\" \"github.com/cloud-native-taiwan/controller101/pkg/version\" flag \"github.com/spf13/pflag\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" clientset \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/client-go/tools/leaderelection\" \"k8s.io/client-go/tools/leaderelection/resourcelock\" \"k8s.io/klog\")const defaultSyncTime = time.Second * 30var ( kubeconfig string showVersion bool threads int leaderElect bool id string leaseLockName string leaseLockNamespace string)func parseFlags() &#123; flag.StringVarP(&amp;kubeconfig, \"kubeconfig\", \"\", \"\", \"Absolute path to the kubeconfig file.\") flag.IntVarP(&amp;threads, \"threads\", \"\", 2, \"Number of worker threads used by the controller.\") flag.StringVarP(&amp;id, \"holder-identity\", \"\", os.Getenv(\"POD_NAME\"), \"the holder identity name\") flag.BoolVarP(&amp;leaderElect, \"leader-elect\", \"\", true, \"Start a leader election client and gain leadership before executing the main loop. \") flag.StringVar(&amp;leaseLockName, \"lease-lock-name\", \"controller101\", \"the lease lock resource name\") flag.StringVar(&amp;leaseLockNamespace, \"lease-lock-namespace\", \"\", \"the lease lock resource namespace\") flag.BoolVarP(&amp;showVersion, \"version\", \"\", false, \"Display the version.\") flag.CommandLine.AddGoFlagSet(goflag.CommandLine) flag.Parse()&#125;func restConfig(kubeconfig string) (*rest.Config, error) &#123; if kubeconfig != \"\" &#123; cfg, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfig) if err != nil &#123; return nil, err &#125; return cfg, nil &#125; cfg, err := rest.InClusterConfig() if err != nil &#123; return nil, err &#125; return cfg, nil&#125;func main() &#123; parseFlags() if showVersion &#123; fmt.Fprintf(os.Stdout, \"%s\\n\", version.GetVersion()) os.Exit(0) &#125; k8scfg, err := restConfig(kubeconfig) if err != nil &#123; klog.Fatalf(\"Error to build rest config: %s\", err.Error()) &#125; k8sclientset := clientset.NewForConfigOrDie(k8scfg) clientset, err := cloudnative.NewForConfig(k8scfg) if err != nil &#123; klog.Fatalf(\"Error to build cloudnative clientset: %s\", err.Error()) &#125; informer := cloudnativeinformer.NewSharedInformerFactory(clientset, defaultSyncTime) controller := controller.New(clientset, informer) ctx, cancel := context.WithCancel(context.Background()) signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM) if leaderElect &#123; lock := &amp;resourcelock.LeaseLock&#123; LeaseMeta: metav1.ObjectMeta&#123; Name: leaseLockName, Namespace: leaseLockNamespace, &#125;, Client: k8sclientset.CoordinationV1(), LockConfig: resourcelock.ResourceLockConfig&#123; Identity: id, &#125;, &#125; go leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig&#123; Lock: lock, ReleaseOnCancel: true, LeaseDuration: 60 * time.Second, RenewDeadline: 15 * time.Second, RetryPeriod: 5 * time.Second, Callbacks: leaderelection.LeaderCallbacks&#123; OnStartedLeading: func(ctx context.Context) &#123; if err := controller.Run(ctx, threads); err != nil &#123; klog.Fatalf(\"Error to run the controller instance: %s.\", err) &#125; klog.Infof(\"%s: leading\", id) &#125;, OnStoppedLeading: func() &#123; controller.Stop() klog.Infof(\"%s: lost\", id) &#125;, &#125;, &#125;) &#125; else &#123; if err := controller.Run(ctx, threads); err != nil &#123; klog.Fatalf(\"Error to run the controller instance: %s.\", err) &#125; &#125; &lt;-signalChan cancel() controller.Stop()&#125; 執行當程式開發完成後，就可以開啟三個單獨的 Terminal 來測試，其中每個 Terminal 會輸入一個唯一的 POD Name 來驗證: # first terminal $ POD_NAME=test1 go run cmd/main.go --kubeconfig=$HOME/.kube/config -v=3 --logtostderr --lease-lock-namespace=default# second terminal $ POD_NAME=test2 go run cmd/main.go --kubeconfig=$HOME/.kube/config -v=3 --logtostderr --lease-lock-namespace=default# third terminal$ POD_NAME=test1 go run cmd/main.go --kubeconfig=$HOME/.kube/config -v=3 --logtostderr --lease-lock-namespace=default 當三個控制器都啟動後，就會看到其中一個行程被選擇 Leader，這時如果停止該控制器，並經過一段時間後，就會發現新的 Leader 已經由其他行程接手。 結語今天主要透過 client-go 為自定義控制器實現高可靠機制，以確保控制器在發生問題時，能由其他節點上的控制器接手處理，這樣功能很適合以 Static Pod 部署的控制器。 自定義控制器其實也可以用 Kubernetes Deployment 來達到高可靠，但在一些場景下並不適用，且若 Deployment 因為一些原因同時有多個副本在執行時，有可能會發生多個控制器寫入同一個 API 資源，造成資訊不一致問題。 明天我們將回到 VM 控制器程式，深入了解如何實現核心功能，以讓我們透過 API 資源管理虛擬機。 Reference https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/ https://tunein.engineering/implementing-leader-election-for-kubernetes-pods-2477deef8f13 https://medium.com/michaelbi-22303/deep-dive-into-kubernetes-simple-leader-election-3712a8be3a99 http://liubin.org/blog/2018/04/28/how-to-build-controller-manager-high-available/ https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/ https://mathspanda.github.io/2017/05/11/k8s-leader-election/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"動手實作 Kubernetes 自定義控制器 Part2","slug":"ironman2020/day26","date":"2019-10-10T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/11/ironman2020/day26/","link":"","permalink":"https://k2r2bai.com/2019/10/11/ironman2020/day26/","excerpt":"前言在動手實作 Kubernetes 自定義控制器 Part1文章中，我們透過定義 API 資源結構，以及使用 code-generator 產生了用於開發自定義控制器的程式函式庫。今天將延續範例，利用昨天產生的函式庫(apis, clientsets)建立一個控制器程式，以監聽自定義資源VirtualMachine的 API 事件。","text":"前言在動手實作 Kubernetes 自定義控制器 Part1文章中，我們透過定義 API 資源結構，以及使用 code-generator 產生了用於開發自定義控制器的程式函式庫。今天將延續範例，利用昨天產生的函式庫(apis, clientsets)建立一個控制器程式，以監聽自定義資源VirtualMachine的 API 事件。 實現控制器程式當有了自定義資源的 api 與 client 的函式庫後，我們就能利用這些來撰寫控制器程式。延續 Controller101，我們將新增一些檔案來完成，如下所示: ├── cmd│ └── main.go├── example│ └── test-vm.yml └── pkg ├── controller │ └── controller.go └── version └── version.go cmd/main.go: 為控制器的主程式。 example/test-vm.yml: 用於測試控制器的 VirtualMachine 資源的範例檔。(optional) pkg/controller/controller.go: VirtualMachine 控制器核心程式。 pkg/version/version.go: 用於 Go build 時加入版本號。(optional) 目前 GitHub 範例已經新增這些程式，若不想看這累死人沒排版文章，可以直接透過 git 抓下來跑。 pkg/controller/controller.go該檔案會利用 Kubernetes client-go 函式庫，以及 code-generator 產生的程式函式庫來實現控制器核心功能。通常撰寫一個控制器時，會建立一個 Controller struct，並包含以下元素: Clientset: 擁有 VirtualMachine 的客戶端介面，讓控制器與 Kubernetes API Server 進行互動，以操作 VirtualMachine 資源。 Informer: 控制器的 SharedInformer，用於接收 API 事件，並呼叫回呼函式。 InformerSynced: 確認 SharedInformer 的儲存是否以獲得至少一次完整 LIST 通知。 Lister: 用於列出或獲取快取中的 VirtualMachine 資源。 Workqueue: 控制器的資源處理佇列，都 Informer 收到事件時，會將物件推到這個佇列，並在協調程式取出處理。當發生錯誤時，可以用於 Requeue 當前物件。 package controllerimport ( \"context\" \"encoding/json\" \"fmt\" \"time\" cloudnative \"github.com/cloud-native-taiwan/controller101/pkg/generated/clientset/versioned\" cloudnativeinformer \"github.com/cloud-native-taiwan/controller101/pkg/generated/informers/externalversions\" listerv1alpha1 \"github.com/cloud-native-taiwan/controller101/pkg/generated/listers/cloudnative/v1alpha1\" \"github.com/golang/glog\" \"k8s.io/apimachinery/pkg/api/errors\" utilruntime \"k8s.io/apimachinery/pkg/util/runtime\" \"k8s.io/apimachinery/pkg/util/wait\" \"k8s.io/client-go/tools/cache\" \"k8s.io/client-go/util/workqueue\" \"k8s.io/klog\")const ( resouceName = \"VirtualMachine\")type Controller struct &#123; clientset cloudnative.Interface informer cloudnativeinformer.SharedInformerFactory lister listerv1alpha1.VirtualMachineLister synced cache.InformerSynced queue workqueue.RateLimitingInterface&#125;func New(clientset cloudnative.Interface, informer cloudnativeinformer.SharedInformerFactory) *Controller &#123; vmInformer := informer.Cloudnative().V1alpha1().VirtualMachines() controller := &amp;Controller&#123; clientset: clientset, informer: informer, lister: vmInformer.Lister(), synced: vmInformer.Informer().HasSynced, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), resouceName), &#125; vmInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.enqueue, UpdateFunc: func(old, new interface&#123;&#125;) &#123; controller.enqueue(new) &#125;, &#125;) return controller&#125;func (c *Controller) Run(ctx context.Context, threadiness int) error &#123; go c.informer.Start(ctx.Done()) klog.Info(\"Starting the controller\") klog.Info(\"Waiting for the informer caches to sync\") if ok := cache.WaitForCacheSync(ctx.Done(), c.synced); !ok &#123; return fmt.Errorf(\"failed to wait for caches to sync\") &#125; for i := 0; i &lt; threadiness; i++ &#123; go wait.Until(c.runWorker, time.Second, ctx.Done()) &#125; klog.Info(\"Started workers\") return nil&#125;func (c *Controller) Stop() &#123; glog.Info(\"Stopping the controller\") c.queue.ShutDown()&#125;func (c *Controller) runWorker() &#123; defer utilruntime.HandleCrash() for c.processNextWorkItem() &#123; &#125;&#125;func (c *Controller) processNextWorkItem() bool &#123; obj, shutdown := c.queue.Get() if shutdown &#123; return false &#125; err := func(obj interface&#123;&#125;) error &#123; defer c.queue.Done(obj) key, ok := obj.(string) if !ok &#123; c.queue.Forget(obj) utilruntime.HandleError(fmt.Errorf(\"Controller expected string in workqueue but got %#v\", obj)) return nil &#125; if err := c.syncHandler(key); err != nil &#123; c.queue.AddRateLimited(key) return fmt.Errorf(\"Controller error syncing '%s': %s, requeuing\", key, err.Error()) &#125; c.queue.Forget(obj) glog.Infof(\"Controller successfully synced '%s'\", key) return nil &#125;(obj) if err != nil &#123; utilruntime.HandleError(err) return true &#125; return true&#125;func (c *Controller) enqueue(obj interface&#123;&#125;) &#123; key, err := cache.MetaNamespaceKeyFunc(obj) if err != nil &#123; utilruntime.HandleError(err) return &#125; c.queue.Add(key)&#125;func (c *Controller) syncHandler(key string) error &#123; namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; utilruntime.HandleError(fmt.Errorf(\"invalid resource key: %s\", key)) return err &#125; vm, err := c.lister.VirtualMachines(namespace).Get(name) if err != nil &#123; if errors.IsNotFound(err) &#123; utilruntime.HandleError(fmt.Errorf(\"virtualmachine '%s' in work queue no longer exists\", key)) return err &#125; return err &#125; data, err := json.Marshal(vm) if err != nil &#123; return err &#125; klog.Infof(\"Controller get %s/%s object: %s\", namespace, name, string(data)) return nil&#125; cmd/main.go該檔案為控制器主程式，主要提供 Flags 來設定控制器參數、初始化所有必要的程式功能(如 REST Client、K8s Clientset、K8s Informer 等等)，以及執行控制器核心程式。 package mainimport ( \"context\" goflag \"flag\" \"fmt\" \"os\" \"os/signal\" \"syscall\" \"time\" \"github.com/cloud-native-taiwan/controller101/pkg/controller\" cloudnative \"github.com/cloud-native-taiwan/controller101/pkg/generated/clientset/versioned\" cloudnativeinformer \"github.com/cloud-native-taiwan/controller101/pkg/generated/informers/externalversions\" \"github.com/cloud-native-taiwan/controller101/pkg/version\" flag \"github.com/spf13/pflag\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/klog\")const defaultSyncTime = time.Second * 30var ( kubeconfig string threads int)func parseFlags() &#123; flag.StringVarP(&amp;kubeconfig, \"kubeconfig\", \"\", \"\", \"Absolute path to the kubeconfig file.\") flag.IntVarP(&amp;threads, \"threads\", \"\", 2, \"Number of worker threads used by the controller.\") flag.BoolVarP(&amp;showVersion, \"version\", \"\", false, \"Display the version.\") flag.CommandLine.AddGoFlagSet(goflag.CommandLine) flag.Parse()&#125;func restConfig(kubeconfig string) (*rest.Config, error) &#123; if kubeconfig != \"\" &#123; cfg, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfig) if err != nil &#123; return nil, err &#125; return cfg, nil &#125; cfg, err := rest.InClusterConfig() if err != nil &#123; return nil, err &#125; return cfg, nil&#125;func main() &#123; parseFlags() k8scfg, err := restConfig(kubeconfig) if err != nil &#123; klog.Fatalf(\"Error to build rest config: %s\", err.Error()) &#125; clientset, err := cloudnative.NewForConfig(k8scfg) if err != nil &#123; klog.Fatalf(\"Error to build cloudnative clientset: %s\", err.Error()) &#125; informer := cloudnativeinformer.NewSharedInformerFactory(clientset, defaultSyncTime) controller := controller.New(clientset, informer) ctx, cancel := context.WithCancel(context.Background()) signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM) if err := controller.Run(ctx, threads); err != nil &#123; klog.Fatalf(\"Error to run the controller instance: %s.\", err) &#125; &lt;-signalChan cancel() controller.Stop()&#125; 其中restConfig()函式用於建立 RESTClient Config，如果有指定 Kubeconfig 檔案時，會透過client-go/tools/clientcmd解析 Kubeconfig 內容以產生 Config 內容;若沒有的話，則表示該控制器可能被透過 Pod 部署在 Kubernetes 中，因此使用 InClusterConfig 方式建立 Config。 執行當控制器程式實現完成，且已經擁有一座安裝好 VirtualMachine CRD 的 Kubernetes 時，就能透過以下指令來執行: $ go run cmd/main.go --kubeconfig=$HOME/.kube/config -v=2 --logtostderrI1008 15:38:30.350446 52017 controller.go:68] Starting the controllerI1008 15:38:30.350543 52017 controller.go:69] Waiting for the informer caches to syncI1008 15:38:30.454799 52017 controller.go:77] Started workers 接著開啟另一個 Terminal 來建立 VirtualMachine 實例: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cloudnative.tw/v1alpha1kind: VirtualMachinemetadata: name: test-vmspec: resource: cpu: 2 memory: 4GEOFvirtualmachine.cloudnative.tw/test-vm created 這時觀察控制器，會看到以下資訊: $ go run cmd/main.go --kubeconfig=$HOME/.kube/config -v=3 --logtostderr...I1008 17:28:18.775656 56945 controller.go:156] Controller get default/test-vm object: &#123;\"metadata\":&#123;\"name\":\"test-vm\",\"namespace\":\"default\",\"selfLink\":\"/apis/cloudnative.tw/v1alpha1/namespaces/default/virtualmachines/test-vm\",\"uid\":\"a1acb111-c71e-4d2b-a2f4-62605e616dfc\",\"resourceVersion\":\"52295\",\"generation\":1,\"creationTimestamp\":\"2019-10-08T09:28:18Z\",\"annotations\":&#123;\"kubectl.kubernetes.io/last-applied-configuration\":\"&#123;\\\"apiVersion\\\":\\\"cloudnative.tw/v1alpha1\\\",\\\"kind\\\":\\\"VirtualMachine\\\",\\\"metadata\\\":&#123;\\\"annotations\\\":&#123;&#125;,\\\"name\\\":\\\"test-vm\\\",\\\"namespace\\\":\\\"default\\\"&#125;,\\\"spec\\\":&#123;\\\"action\\\":\\\"active\\\",\\\"resource\\\":&#123;\\\"cpu\\\":2,\\\"memory\\\":\\\"4G\\\",\\\"rootDisk\\\":\\\"40G\\\"&#125;&#125;&#125;\\n\"&#125;&#125;,\"spec\":&#123;\"action\":\"active\",\"resource\":&#123;\"cpu\":\"2\",\"memory\":\"4G\",\"rootDisk\":\"40G\"&#125;&#125;,\"status\":&#123;\"phase\":\"\",\"server\":&#123;\"state\":\"\",\"usage\":&#123;\"cpu\":0,\"memory\":0&#125;&#125;,\"lastUpdateTime\":null&#125;&#125;I1008 17:28:18.775687 56945 controller.go:115] Controller successfully synced 'default/test-vm' 結語透過今天的實作，可以發現使用 code-generator 產生的相關程式碼操作自定義資源，就如同 Kubernetes client-go 的原生 API clientsets 一樣簡單，只要根據 sample-controller 內容做些調整，就能實現特定 API 資源的控制器程式。 Reference https://github.com/kubernetes/sample-controller https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"動手實作 Kubernetes 自定義控制器 Part1","slug":"ironman2020/day25","date":"2019-10-09T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/10/ironman2020/day25/","link":"","permalink":"https://k2r2bai.com/2019/10/10/ironman2020/day25/","excerpt":"前言昨天了解到 Kubernetes 官方的 Sample Controller，是如何讓一個自定義 API 資源被自定義控制器管理。雖然這個範例僅僅只是管理一個 Deployment 資源，但可以讓人認識到一個自定義控制器是如何運作的。而接下來的文章，我將每天撰寫一小部分程式內容，來重頭慢慢實作一個管理自定義資源VirtualMachine的控制器，並隨時間推移新增更多功能(如: LeaseLock、Metrics、Fake client 與 Finalizer、Admission Controller 等等)來完善這個控制器範例。","text":"前言昨天了解到 Kubernetes 官方的 Sample Controller，是如何讓一個自定義 API 資源被自定義控制器管理。雖然這個範例僅僅只是管理一個 Deployment 資源，但可以讓人認識到一個自定義控制器是如何運作的。而接下來的文章，我將每天撰寫一小部分程式內容，來重頭慢慢實作一個管理自定義資源VirtualMachine的控制器，並隨時間推移新增更多功能(如: LeaseLock、Metrics、Fake client 與 Finalizer、Admission Controller 等等)來完善這個控制器範例。 本文章的自定義控制器，會實作監聽自定義資源 VirtualMachine 的狀態，並依據 VirtualMachine 的.spec內容來操作私有雲中的虛擬機。另外該控制器也會持續同步私有雲虛擬機的狀態，並更新至子資源.status中。架構如下圖所示。 而今天文章先把重點放在自定義 API 資源的資料結構，以及如何透過 code-generator 產生控制器所需的程式碼。 開發環境設置由於開發中，會使用到 Kubernetes 與 Go 語言，因此需要安裝這些到開發環境中。 一座 Kubernetes v1.10+ 叢集。透過 Minikube 建立即可 minikube start --kubernetes-version=v1.15.4。 安裝 kubectl v1.10+ 工具，安裝請參考 Install and Set Up kubectl 安裝 Go 語言 v1.11+ 開發環境，由於開發中會使用到 Go mod 來管理第三方套件，因此必須符合支援版本。安裝請參考 Go Getting Started。 產生自定義資源程式碼在開始定義 API 資源結構與使用 code-generator 前，需要先初始化存放控制器程式的目錄，其結構如下所示: controller101├── LICENSE├── README.md├── cmd # Controller 主程式 main.go 檔├── deploy # 部署 Controller 的相關檔案，如 Deployment、CRD、RBAC。├── go.mod # Go mod package 檔案├── go.sum # Go mod package 檔案├── hack # 存放一些常使用到的腳本└── pkg # 控制器相關程式碼 目錄結構完成後，就能開始撰寫自定義資源，以及 code-generator 所需的內容。而要達到這個目的，我們必須涉及兩個步驟: 定義 CRD 內容與資源類型程式結構。 透過 code-generator 腳本產生自定義資源的 Clientset、Informers、Listers 程式碼。 Defining types and scripts如前言所述，本範例希望透過新增一個自定義 API 資源，用於提供給自定義控制器實現功能。故我們必須在開始撰寫控制器前，先依據 Kubernetes-style API types 的規範，來定義 API 群組(Group)與資源類型(Type)，並透過 CRD API 來建立自定義資源。所以假設控制器是所屬組織Cloud Native Taiwan想開發，那麼 API 群組就能定義為cloudnative.tw，而 API 資源則為VirtualMachine，因此 CRD 就會形成如下所示: # File name: deploy/crd.ymlapiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: virtualmachines.cloudnative.twspec: group: cloudnative.tw version: v1alpha1 names: kind: VirtualMachine singular: virtualmachine plural: virtualmachines shortNames: - vm scope: Namespaced 當 CRD 定義完後，就能透過 CRD API 來新增自定義資源: $ kubectl apply -f deploy/crd.ymlcustomresourcedefinition.apiextensions.k8s.io/virtualmachines.cloudnative.tw created$ kubectl get crdNAME CREATED ATvirtualmachines.cloudnative.tw 2019-10-07T14:50:47Z$ kubectl get vmNo resources found 到這裡，會發現我們只是新增 API 而已，這個 VirtualMachine 資源完全沒有規範內容，這使我們無法描述一個 VirtualMachine 的預期內容，以提供給控制器處理。基於此，必須依據實作的功能假設自定義資源結構，如本範例想用 VirtualMachine 來管理虛擬機器，那麼就需要定義如下: apiVersion: cloudnative.tw/v1alpha1kind: VirtualMachine metadata: name: test-vmspec: action: active resource: cpu: 2 memory: 4G rootDisk: 40Gstatus: phase: synchronized server: state: active usage: cpu: 13.3 memory: 40.1 lastUpdateTime: 2019-10-07T14:50:47Z 雖然這個 YAML 能夠透過 kubectl 進行各種 API 操作，但如果想在自定義控制器操作的話，那該怎麼辦呢? 有接觸過 Kubernetes clinet-go 的人，可能會想到用 Dynamic client 來解決。但這不是好做法，因為以下原因: Dynamic client 無法很方便實現 API 資源類型的操作、轉換與驗證等等事情。 Dynamic client 不像原生 API 資源類型，提供了很方便的 Typed client 可以使用。 因此過去版本(v1.7 或更舊版本)的控制器管理自定義資源較為複雜，但這問題在 v1.8 版中被解決，Kubernetes 官方引入 code-generator 專案，用於產生如同原生 API 資源類型一樣功能的 Typed client 程式碼，這樣當我們在自定義控制器使用時，就如同使用 client-go 一樣。而要達到這樣事情，必須在專案建立產生程式碼所需的所有檔案，其檔案結構如下所示: ├── hack │ └── k8s # Code-generator 腳本│ ├── boilerplate.go.txt│ ├── tools.go│ ├── update-generated.sh│ └── verify-codegen.sh│ └── pkg └── apis # APIs 定義 └── cloudnative # 提供該 Package 的 API Group Name。 ├── register.go └── v1alpha1 # API 各版本結構定義。Kubernetes API 是支援多版本的。 ├── doc.go ├── register.go └── types.go v1alpha1/doc.go該檔案用於定義 code-generator 的 Global tags。可標示當前版本 Package 中的每個類型，想要透過 code-generator 產生哪些程式碼(如 Deepcopy, Client)。 // +k8s:deepcopy-gen=package// +groupName=cloudnative.tw// Package v1alpha1 is the v1alpha1 version of the API.package v1alpha1 // import \"github.com/cloud-native-taiwan/controller101/pkg/apis/cloudnative/v1alpha1\" 如上述內容，透過+k8s:deepcopy-gen=package標示這個 package 要建立 Deepcopy 方法(Method)。另外+groupName=cloudnative.tw定義整個 API Group 名稱，以確保在 code-generator 發生錯誤時，能產生可識別的錯誤代號。 v1alpha1/types.go該檔案用於定義資源類型的資料結構，以及定義 code-generator 的 Local tags。可標示哪些資源類型想透過 code-generator 產生 Client 程式碼。 package v1alpha1import ( corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\")// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Objecttype VirtualMachine struct &#123; metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec VirtualMachineSpec `json:\"spec\"` Status VirtualMachineStatus `json:\"status\"`&#125;type VirtualMachineSpec struct &#123; Resource corev1.ResourceList `json:\"resource\"`&#125;type VirtualMachinePhase stringconst ( VirtualMachineNone VirtualMachinePhase = \"\" VirtualMachineCreating VirtualMachinePhase = \"Creating\" VirtualMachineActive VirtualMachinePhase = \"Active\" VirtualMachineFailed VirtualMachinePhase = \"Failed\" VirtualMachineTerminating VirtualMachinePhase = \"Terminating\" VirtualMachineUnknown VirtualMachinePhase = \"Unknown\")type ResourceUsage struct &#123; CPU float64 `json:\"cpu\"` Memory float64 `json:\"memory\"`&#125;type ServerStatus struct &#123; ID string `json:\"id\"` State string `json:\"state\"` Usage ResourceUsage `json:\"usage\"`&#125;type VirtualMachineStatus struct &#123; Phase VirtualMachinePhase `json:\"phase\"` Reason string `json:\"reason,omitempty\"` Server ServerStatus `json:\"server,omitempty\"` LastUpdateTime metav1.Time `json:\"lastUpdateTime\"`&#125;// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Objecttype VirtualMachineList struct &#123; metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata\"` Items []VirtualMachine `json:\"items\"`&#125; 這邊的+genclient tag 是表示在執行 code-generator 時，會對這個類型建立 Client 程式碼。而+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object則表示產生的 Deepcopy 使用 runtime.Object 介面實現。 v1alpha1/register.go用於將剛建立的新 API 版本與新資源類型註冊到 API Group Schema 中，以便 API Server 能夠識別。 Scheme: 用於 API 資源群組之間的序列化、反序列化與版本轉換。 package v1alpha1import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" \"github.com/cloud-native-taiwan/controller101/pkg/apis/cloudnative\")var SchemeGroupVersion = schema.GroupVersion&#123;Group: cloudnative.GroupName, Version: \"v1alpha1\"&#125;func Kind(kind string) schema.GroupKind &#123; return SchemeGroupVersion.WithKind(kind).GroupKind()&#125;func Resource(resource string) schema.GroupResource &#123; return SchemeGroupVersion.WithResource(resource).GroupResource()&#125;var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme)func addKnownTypes(scheme *runtime.Scheme) error &#123; scheme.AddKnownTypes(SchemeGroupVersion, &amp;VirtualMachine&#123;&#125;, &amp;VirtualMachineList&#123;&#125;, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil&#125; hack/k8s/boilerplate.go.txtcode-generator 自動將 boilerplate.go.txt 的文字，新增到產生的程式碼檔案內容的最上層。通常為 License 樣板。如以下範例: /*Copyright © 2019 The controller101 Authors.Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.*/ hack/k8s/tools.go確保go mod能夠 code-generator 視為相依套件。 // +build tools// This package imports things required by build scripts, to force `go mod` to see them as dependencies// See https://github.com/golang/go/issues/25922package toolsimport ( _ \"k8s.io/code-generator\") hack/k8s/update-generated.sh用於執行 code-generator 腳本，以產生自定義資源的 Deepcopy、Client、Informer 與 Lister 程式碼。 #!/usr/bin/env bashset -o errexitset -o nounsetset -o pipefailSCRIPT_ROOT=$(dirname \"$&#123;BASH_SOURCE[0]&#125;\")/../..CODEGEN_PKG=$&#123;CODEGEN_PKG:-$(cd \"$&#123;SCRIPT_ROOT&#125;\"; ls -d -1 ./vendor/k8s.io/code-generator 2&gt;/dev/null || echo ../code-generator)&#125;bash \"$&#123;CODEGEN_PKG&#125;\"/generate-groups.sh \"deepcopy,client,informer,lister\" \\ github.com/cloud-native-taiwan/controller101/pkg/generated \\ github.com/cloud-native-taiwan/controller101/pkg/apis \\ \"cloudnative:v1alpha1\" \\ --output-base \"$(dirname $&#123;BASH_SOURCE&#125;)/../../../../../\" \\ --go-header-file $&#123;SCRIPT_ROOT&#125;/hack/k8s/boilerplate.go.txt hack/verify-codegen.sh透過 diff 檢查當前的程式碼是否已經依據 apis 定義的內容產生。 #!/usr/bin/env bashset -o errexitset -o nounsetset -o pipefailSCRIPT_ROOT=$(dirname \"$&#123;BASH_SOURCE[0]&#125;\")/../..DIFFROOT=\"$&#123;SCRIPT_ROOT&#125;/pkg\"TMP_DIFFROOT=\"$&#123;SCRIPT_ROOT&#125;/_tmp/pkg\"_tmp=\"$&#123;SCRIPT_ROOT&#125;/_tmp\"cleanup() &#123; rm -rf \"$&#123;_tmp&#125;\"&#125;trap \"cleanup\" EXIT SIGINTcleanupmkdir -p \"$&#123;TMP_DIFFROOT&#125;\"cp -a \"$&#123;DIFFROOT&#125;\"/* \"$&#123;TMP_DIFFROOT&#125;\"\"$&#123;SCRIPT_ROOT&#125;/hack/k8s/update-generated.sh\"echo \"diffing $&#123;DIFFROOT&#125; against freshly generated codegen\"ret=0diff -Naupr \"$&#123;DIFFROOT&#125;\" \"$&#123;TMP_DIFFROOT&#125;\" || ret=$?cp -a \"$&#123;TMP_DIFFROOT&#125;\"/* \"$&#123;DIFFROOT&#125;\"if [[ $ret -eq 0 ]]then echo \"$&#123;DIFFROOT&#125; up to date.\"else echo \"$&#123;DIFFROOT&#125; is out of date. Please run hack/k8s/update-generated.sh\" exit 1fi Generate codes當所有用於產生程式碼的檔案都建立後，就能透過 code-generator 依據定義的內容，來產生相關程式碼。由於開發使用 Go mod 管理套件，因此需要執行以下指令來完成程式碼產生: $ go mod vendor$ ./hack/k8s/update-generated.shGenerating deepcopy funcsGenerating clientset for cloudnative:v1alpha1 at github.com/cloud-native-taiwan/controller101/pkg/generated/clientsetGenerating listers for cloudnative:v1alpha1 at github.com/cloud-native-taiwan/controller101/pkg/generated/listersGenerating informers for cloudnative:v1alpha1 at github.com/cloud-native-taiwan/controller101/pkg/generated/informers 完成後，即可看到以下檔案: pkg/generated├── clientset│ └── versioned│ ├── fake│ ├── scheme│ └── typed│ └── cloudnative│ └── v1alpha1│ └── fake├── informers│ └── externalversions│ ├── cloudnative│ │ └── v1alpha1│ └── internalinterfaces└── listers └── cloudnative └── v1alpha1 這樣就能夠在自定義控制器中，透過 Client 程式碼直接操作 VirtualMachine API 了。 結語今天主要了解如何自己定義 API 資源類型，並利用 code-generator 產生相關程式碼，以利我們在自定義控制器中使用。明天將透過這些定義與建立的程式碼，實際進行操作自定義 API 資源，並撰寫控制器程式。 Reference https://github.com/kubernetes/sample-controller https://github.com/kubernetes/code-generator https://itnext.io/how-to-generate-client-codes-for-kubernetes-custom-resource-definitions-crd-b4b9907769ba https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/ http://blog.xbblfz.site/2018/09/19/k8s%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E7%9A%84%E8%A7%A3%E6%9E%90/ https://rancher.com/blog/2018/2018-07-09-rancher-management-plane-architecture/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"探討 Kubernetes 自定義控制器是如何運作 Part2","slug":"ironman2020/day24","date":"2019-10-08T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/09/ironman2020/day24/","link":"","permalink":"https://k2r2bai.com/2019/10/09/ironman2020/day24/","excerpt":"前言在前幾天文章中，認識了開發 Kubernetes 自定義控制器的知識與概念，如: API 函式庫、client-go 函式庫、CRD 與自定義控制器本身等等。但講了這麼多，卻都沒有實際執行一個自定義控制器，因此今天將以 Kubernetes 社區提供的 sample-controller 範例為主，來說明如何運行與實作。","text":"前言在前幾天文章中，認識了開發 Kubernetes 自定義控制器的知識與概念，如: API 函式庫、client-go 函式庫、CRD 與自定義控制器本身等等。但講了這麼多，卻都沒有實際執行一個自定義控制器，因此今天將以 Kubernetes 社區提供的 sample-controller 範例為主，來說明如何運行與實作。 Sample Controller 架構與運作流程Sample Controller 是 Kubernetes 官方提供的範例，該範例實現了針對Foo自定義資源的控制器，當建立自定義資源 Foo 物件時，該控制器將使用 nginx:latest 容器映像檔與指定的副本數建立 Deployment，換句話說，控制器會確保每個 Foo 資源都有一個對應的 Deployment，其中 Foo 資源的.spec內容會與 Deployment 關聯，控制器會在協調循環中依據 Foo 資源的.spec內容處理預期的結果。另外該範例也提供了 CRD 的一些功能展示，如: Validation 與 Sub Resources。 這個控制器流程大致如下: Sample Controller 使用 client-go 與 Foo clientset 函式庫來與 Kubernetes API Server 溝通，並建立一個控制循環(Control Loop)。 在控制循環中，Sample Controller 使用 Reflector(實現這功能的是ListAndWatch()) 監視 Kubernetes API 中的 Foo 與 Deployment 資源類型(Kind)，以確保兩者持續同步。 當 Reflector 透過 Watch API，收到有關新Foo與Deployment資源實例存在的事件通知時，它將使用 List API 取得新建立的 API 資源物件，並將其放入watchHandler函式內的 DeltaFIFO 佇列中。 接著 Sample Controller 使用 SharedInformer(DeploymentInformer 與 FooInformer) 從 DeltaFIFO 佇列中取出 API 資源物件。這邊，SharedInformer 提供了AddEventHandler()函式，用於註冊事件處理程式，並將 API 資源物件新增到 Workqueue 中。這邊為 Foo 與 Deployment 傳遞了不同的資源處理函式(L116-L141)，以處理 API 資源在add、update與delete的事件。如果需要進行某些處理時，這些函式會負責將 Foo 資源物件的鍵(Key)排入 Workqueue。 在本範例中，Workqueue 採用RateLimitQueue來進行 API 物件的處理速率限制。 當事件函式被呼叫時，控制器透過MetaNamespaceKeyFunc()函式，將當前處理的 API 資源物件轉換為namespace/name或name(如果沒有 Namespace 的話)的格式作為 Key，然後將這些 Key 添加到 Workqueue。或者透過DeletionHandlingMetaNamespaceKeyFunc()來處理刪除事件的 API 資源物件。 這時，Sample Controller 會運行幾個 Worker(完成這個操作的函式是runWorker())，它們會透過不斷的呼叫processNextWorkItem()來消耗 Workqueue 中要被處理的 API 物件。這時會進入syncHandler()以協調 Foo 當前狀態至預期狀態。而在syncHandler()中，控制器會將 Key 恢復成 Namespace 與 Name，並用Lister來取得 API 物件內容。 在syncHandler()中，部分功能會使用 Indexer 引用或者是一個 Listing 封裝器(Wrapper)來檢索對應 Key 的 API 資源物件內容。 在呼叫syncHandler()時，控制器會使用指定名稱(Foo 的.spec.deploymentName)與副本數(Foo 的.spec.replicas)建立一個 Deployment，並同步 Foo 資源的.status狀態。 在這些過程中，控制器會建立一個 EventRecorder 來紀錄事件的變化過程到 Kubernetes API 中。 而 Sample Controller 整個目錄結構用意如下所示: sample-controller├── Godeps ├── artifacts│ └── examples # 存放 Foo 範例，以及 CRD 檔案。├── docs│ └── images├── hack # 存放└── pkg ├── apis │ └── samplecontroller │ └── v1alpha1 # 自定義資源 Foo 資料結構，會用於 code-generator 產生 client libraries。 ├── generated # 透過 code-generator 產生的 client libraries。用於跟 API Server 溝通操作 Foo 資源。 │ ├── clientset │ │ └── versioned │ │ ├── fake │ │ ├── scheme │ │ └── typed │ │ └── samplecontroller │ │ └── v1alpha1 │ │ └── fake │ ├── informers │ │ └── externalversions │ │ ├── internalinterfaces │ │ └── samplecontroller │ │ └── v1alpha1 │ └── listers │ └── samplecontroller │ └── v1alpha1 └── signals # 實現 Windows 與 POSIX 的 OS shutdown signal 測試環境部署由於本次文章將實際執行自定義控制器範例，因此需要建立測試用環境來觀察，這邊請依據需求來完成。 需要一座 Kubernetes 叢集。透過 Minikube 建立即可 minikube start --kubernetes-version=v1.15.4。 安裝 kubectl 工具，請參考 Install and Set Up kubectl 安裝 Go 語言 v1.11+ 開發環境，請參考 Go Getting Started。 安裝 Git 工具，請參考 Git。 設定 Sample Controller首先透過 Git(或 Go) 取得 sample-controller 原始碼: $ git clone https://github.com/kubernetes/sample-controller.git$ cd sample-controller 透過 kubectl 建立該自定義控制器的 CRD 到當前叢集中: $ kubectl apply -f artifacts/examples/crd.yamlcustomresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created$ kubectl get crdNAME CREATED ATfoos.samplecontroller.k8s.io 2019-10-06T12:55:18Z 執行 Sample Controller當 CRD 建立好後，就可以透過 Go 指令直接執行這個控制器。首先透過 go mod 下載相依函示庫: $ export GO111MODULE=on$ go mod download 載完後，即可透過以下指令來執行控制器: $ go run $(ls -1 *.go | grep -v _test.go) -kubeconfig=$HOME/.kube/config -v=3 -logtostderrI1006 21:14:32.364765 73322 controller.go:114] Setting up event handlersI1006 21:14:32.364892 73322 controller.go:155] Starting Foo controllerI1006 21:14:32.364905 73322 controller.go:158] Waiting for informer caches to syncI1006 21:14:32.365031 73322 reflector.go:150] Starting reflector *v1.Deployment (30s) from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105I1006 21:14:32.365724 73322 reflector.go:185] Listing and watching *v1.Deployment from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105I1006 21:14:32.365032 73322 reflector.go:150] Starting reflector *v1alpha1.Foo (30s) from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105I1006 21:14:32.365796 73322 reflector.go:185] Listing and watching *v1alpha1.Foo from pkg/mod/k8s.io/client-go@v0.0.0-20191005115821-b1fd78950135/tools/cache/reflector.go:105I1006 21:14:32.467616 73322 controller.go:163] Starting workersI1006 21:14:32.467660 73322 controller.go:169] Started workers 建立 Foo 資源實例當執行了控制器後，就可以開一個新 Terminal 來建立 Foo 實例，以觀察控制器執行的結果: $ cd sample-controller$ kubectl apply -f artifacts/examples/example-foo.yamlfoo.samplecontroller.k8s.io/example-foo created$ kubectl get fooNAME AGEexample-foo 48s$ kubectl get deploy,poNAME READY UP-TO-DATE AVAILABLE AGEdeployment.extensions/example-foo 1/1 1 1 5m22sNAME READY STATUS RESTARTS AGEpod/example-foo-d75d8587c-wlqsz 1/1 Running 0 5m22s 從範例中，可以發現 Foo 實際功能是管理著一個 Deployment 的建立，因此會看到上述的結果。當嘗試修改 Foo 中的.spec.replicas時，會發現 Foo 管理的 Deployment 會跟著變動副本數。 結語Sample Controller 雖然只是一個非常簡單的自定義控制器範例，但從中卻可以學習到很多觀念，從最基本的 API 資源結構定義、透過 code-generator 產生客戶端函式庫、管理原有的 Kubernetes API 資源等等。雖然範例交代了很多基礎觀念，但是在實際應用上，還是缺乏了一些元素，比如說:多個相同控制器的 HA 實現、如何取得控制器 Metrics、怎麼使用 Finalizer 實作垃圾資源回收、實際部署方式與 RBAC 設定等等問題與功能。 在接下來章節中，我將針對這部份一一說明，我會透過每天實作一小部分來完成一個自定義控制器，在過程中再把一些觀念進一步釐清。 Reference https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://engineering.bitnami.com/articles/kubewatch-an-example-of-kubernetes-custom-controller.html https://itnext.io/building-an-operator-for-kubernetes-with-the-sample-controller-b4204be9ad56 https://github.com/kubernetes/sample-controller","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"探討 Kubernetes 自定義控制器是如何運作 Part1","slug":"ironman2020/day23","date":"2019-10-07T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/08/ironman2020/day23/","link":"","permalink":"https://k2r2bai.com/2019/10/08/ironman2020/day23/","excerpt":"前言Kubernetes 控制器主要目的是協調(Reconcile)某個 API 資源的狀態，從實際狀態轉換為期望狀態，換句話說，在 Kubernetes API 資源上的說法，就是讓 API 資源的.status狀態，達到.spec所定義內容。而為了達到這點，控制器會透過 client-go 一直監視這兩種狀態的變化，並在發現變化時，觸發協調邏輯的循環，以更改當前狀態所需的任何操作，並使其往資源的預期狀態進展。Kubernetes 為了實現這樣機制，在 API 提供了一組 List 與 Watch 方法，用於監視任何 API 資源的事件。而自定義控制器就是以 client-go 操作這些 API 方法，監視其主要的自定義資源與其他任何相關的資源。","text":"前言Kubernetes 控制器主要目的是協調(Reconcile)某個 API 資源的狀態，從實際狀態轉換為期望狀態，換句話說，在 Kubernetes API 資源上的說法，就是讓 API 資源的.status狀態，達到.spec所定義內容。而為了達到這點，控制器會透過 client-go 一直監視這兩種狀態的變化，並在發現變化時，觸發協調邏輯的循環，以更改當前狀態所需的任何操作，並使其往資源的預期狀態進展。Kubernetes 為了實現這樣機制，在 API 提供了一組 List 與 Watch 方法，用於監視任何 API 資源的事件。而自定義控制器就是以 client-go 操作這些 API 方法，監視其主要的自定義資源與其他任何相關的資源。 舉例，想要實現管理 TensorFlow 分散式訓練的控制器。這個控制器不僅要監視 TensorFlowJob(這是自定義資源) 物件的變化，而必須響應 Pod 事件，以確保 Pod 能再發生狀況時，處理對應狀況。當然這種追蹤 API 資源之間關聯的機制，也能利用 Kubernetes 的 Owner references 機制達成。這機制允許控制器在任何 API 資源上，設定資源的父子關析(如 Deployment 與 Pod 關聯這樣)，而當子資源事件發生時，就能反應給控制器，以知道哪個 TensorFlowJob 物件已受到影響，這時再由控制器的檢查與協調循環，來解決狀態的變化。 但講這麼多，一個控制器內部究竟是如何運作呢?今天就是要來聊聊這個內容。 控制器如何運作?這部分將解釋 Kubernetes 自定義控制器運作流程，如下圖所示。一個自定義控制器是由 client-go 中的幾個主要功能實現，並結合自己實現的邏輯來達成。因此在開發前，必須先理解這些名詞與功能是做什麼用的。 (圖片擷取自：Kubernetes sample-controller) 從架構圖來看，主要的 client-go 會有以下三者處理: Reflector: 會透過 List/Watch API 監視著 Kubernetes 中指定的資源類型(Kind)，而這些資源可以是既有的資源(如 Pod、Deployment)，也可以是自定義資源。當 Reflector 透過 Watch API 收到新資源實例建立通知時，會將透過該資源的 List API 取得新建立的物件，並將物件放到 Delta Fifo 佇列中。 Informer: 是控制器機制的基礎，它會在協調循環中，從 Delta Fifo 佇列取出 API 資源，將其儲存成物件提供給 Indexer 快取，並提供物件事件的處理介面(Add、Update 與 Delete)。 Indexer: 為 API 資源物件提供檢索功能。利用執行緒安全的資料儲存中，將物件儲存成鍵(Key)/值(Value)形式，其 Key 的格式為 namespace/name。 而除了上面 client-go 元件外，在開發一個自定義控制器時，還會有以下幾個元件會被使用到: Informer reference: 在自定義控制器使用的 Informer 引用。在開發自定義控制器時，通常會宣告一個 Informer 實例用於多個 API 資源監聽使用，但自定義控制器有可能會需要監聽不同 API 資源，因此可能有多個子控制器，這時就可以用工廠模式(Factory Pattern)傳遞進去，並設定該控制器監聽的 API 資源。 Indexer reference: 同 Informer 概念。用於處理不同 API 資源的檢索。 Resource Event Handlers: 這些 Informer 的回呼函式(callback functions)，分別有onAdd()、onUpdate()與onDelete()。當 Informer 收到 API 資源物件事件時，就會呼叫這些函式，這時自定義控制器就能在函式中處理接下來事情。 Work queue: 當 Resource Event Handlers 被呼叫時，會將 Key 寫到這個佇列中，以確保 API 資源物件能被依序處理。在 client-go 支援不同的 Workqueue 類型，而這邊通常會以 RateLimiting 為主。 Process Item: 從 Workqueue 中取出物件 Key\b 的過程。 Handle Object: 處理物件的實際邏輯，在開發自定義控制器時，通常會在這邊撰寫功能邏輯。一般來說會利用一個 Indexer reference 從取得的 Key 來檢索指定 API 資源物件的內容，並依據內容當前狀態與預期狀態處理。 TODO: 補充更多細節 結語今天理解了自定義控制器使用到的元件，以及其運作方式，從中可以發現 Kubernetes client-go 幾乎是整個控制器的核心，許多功能實現都是圍繞著該函式庫。明天將部署 sample-controller 到 Minikube 上，以釐清一些功能運作流程，這些知識與觀念將用於後續範例開發中。 雖然 Kubernetes 控制器看起來似乎很複雜，事實上簡化來看，大概也就長這個樣子: Reference https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc https://github.com/kubernetes/sample-controller https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/ http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/ https://kubernetes.io/docs/reference/using-api/client-libraries/ https://speakerdeck.com/chanyilin/k8s-metacontroller https://toutiao.io/posts/4rnwh6/preview https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources https://www.oreilly.com/library/view/cloud-native-infrastructure/9781491984291/ch04.html https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://github.com/opsnull/kubernetes-dev-docs/tree/master/client-go https://blog.csdn.net/weixin_42663840/article/details/81482553","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"開發自定義控制器前，需要先了解的東西 Part3","slug":"ironman2020/day22","date":"2019-10-06T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/07/ironman2020/day22/","link":"","permalink":"https://k2r2bai.com/2019/10/07/ironman2020/day22/","excerpt":"前言前面文章中，了解到 Kubernetes 的架構一直以擴展性與靈活性為主，從 Extension Points 可以看到上至 API 層，下至基礎建設層都有各種擴展的介面、標準與 API，其中 API 的擴展，我們在介紹自定義資源(Custom Resource)時，有提及能透過CustomResourceDefinition(CRD)與API Aggregation來達成，一方面在開發自定義控制器時，很多情況下，會透過增加新的 API 資源來讓控制器實現功能，因此我們必須先瞭解這兩者擴展 API 的作法與選擇。","text":"前言前面文章中，了解到 Kubernetes 的架構一直以擴展性與靈活性為主，從 Extension Points 可以看到上至 API 層，下至基礎建設層都有各種擴展的介面、標準與 API，其中 API 的擴展，我們在介紹自定義資源(Custom Resource)時，有提及能透過CustomResourceDefinition(CRD)與API Aggregation來達成，一方面在開發自定義控制器時，很多情況下，會透過增加新的 API 資源來讓控制器實現功能，因此我們必須先瞭解這兩者擴展 API 的作法與選擇。 在 Kubernetes API 擴展中，不管是使用 CRD 或 API Aggregation，旨都是希望在不修改 Kubernetes 核心程式碼的情況下，讓新的 API 能夠被註冊或新增到 Kubernetes 叢集中。但兩者在用意上有一點小差異: API Aggregation: 用於將第三方服務資源註冊到 Kubernetes API 中，以統一透過 Kubernetes API 來存取外部服務。 CRD: 在當前叢集中新增 API 資源，並沿用 Kubernetes 原有的 API(如 Pod) 操作方式來管理這些新 API 資源。 從描述中，可以看到差別在於『把既有註冊』跟『直接增加新的』。而這兩者又是如何運作呢? CRD(CustomResourceDefinitions)CRD 是 Kubernetes 在 v1.7 版本新增的 API 資源，被用於新增自定義 API 資源上，一但 CRD 物件建立時，Kubernetes API Server 就會幫你處理自定義資源的 API 請求、狀態儲存等。而這過程中，完全不需要撰寫任何程式碼。 事實上，在 CRD 之前還有個 ThirdPartyResources(TPR) 被用於擴展 API，但因為一些限制關析，TPR 被 CRD 取代了，並在 v1.8 就被棄用。 不用撰寫任何程式碼，就能輕易擴展新 API。 能夠沿用熟悉的 Kubernetes UX 工具來管理。如 kubectl。 支援 SubResources、Multiple versions、Defaulting、Additional Properties 與 Validation 等等功能。 擴展 API 方式簡單，但相對靈活性差。 那要如何新增呢?在跑一個範例前，先來看一下 CRD 是如何定義 API URL。假設想實現在 Kubernetes 上管理 KVM 虛擬機時，我們需要先定義一個用於管理虛擬機的 API 類型 - VM，並且這個 API 是 kairen (或公司與組織)開發，這時 CRD 會透過資源類型名稱+域名來定義，如:域名為kairen.io，那麼 CRD 名稱就會是vms.kairen.io，而完整 API 資源路徑則是/apis/kairen.io/&lt;version&gt;/namespaces/&lt;namespace&gt;/vms/..。 基於上述，我們會這樣定義 CRD 內容。當這個範例被建立時，Kubernetes API 伺服器會增加新的 API 資源端點/apis/kairen.io/v1alpha1/namespaces/&lt;namespace&gt;/vms/..提供給客戶端存取。而當我們新增一個 VM 實例時，Kubernetes API 伺服器就會幫你管理整個 VMs API 的資源狀態儲存。 apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: vms.kairen.iospec: group: kairen.io version: v1alpha1 names: kind: VM plural: vms scope: Namespaced additionalPrinterColumns: - name: Status type: string description: The VM Phase JSONPath: .status.phase - name: CPU type: integer description: CPU Usage JSONPath: .status.cpuUtilization - name: MEMORY type: integer description: Memory Usage JSONPath: .status.memoryUtilization - name: Age type: date JSONPath: .metadata.creationTimestamp validation: openAPIV3Schema: properties: spec: properties: vmName: type: string pattern: '^[-_a-zA-Z0-9]+$' cpu: type: integer minimum: 1 memory: type: integer minimum: 128 diskSize: type: integer minimum: 1 scope: 分為 Cluster 與 Namespaced。前者為叢集面資源(如 PV)，這種 API 通常只有管理員才能操作。 additionalPrinterColumns*: 在 kubectl get 時，額外顯示的資訊。能夠以 Json Path 來取得 API 資源內容。 validation*: 在呼叫 API 時，能基於 OpenAPI v3 schema 來驗證 VM 這個物件內容是否符合要求。 當我們利用這個範例在 Kubernetes 建立時，會如下所示: $ kubectl get crdNAME CREATED ATvms.kairen.io 2019-10-03T12:30:55Z$ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: kairen.io/v1alpha1kind: VMmetadata: name: test-1spec: cpu: 1 memory: 2048 # MB diskSize: 10 # GiBEOF$ kubectl get vmsNAME STATUS CPU MEMORY AGEtest-1 2m31s 這邊會看到 Status 跟 CPU/Memory 使用率都沒更新，因為 CRD 只幫你管理與儲存 API 狀態，若背後沒有一個機制去處理這個 API 時，就不會有實際作用。當然要模擬也是可以，利用 kubectl edit 嘗試修改.status內容即可。 API Aggregation在 Kubernetes 架構下，每個資源都是由 API 伺服器處理 REST 操作請求，然後管理每個資源物件的狀態儲存。但有些情況下，擴展 API 的開發者，希望自行實現處理 REST API 的所有請求時，就無法透過 CRD 機制來達成。在這種需求下，就要用 API Aggregation 來解決，因為 API Aggregation 能利用一些機制，讓 Kubernetes API 伺服器知道如何委託自定義 API 的請求給第三方 API 伺服器處理。 雖然這種方式能處理更多 API 相關的事情，但相對的程式開發要求較高。那怎麼開發呢?我們能會利用 k8s.io/apiserver 函式庫來實現。 需要程式開發能力，通常建構在 k8s.io/apiserver 函式庫之上。 客製化程度非常高。如新增 HTTP verb、實現 Hooks。 能完成 CRD 所有能做到的事情。 支援 protobuf 與 OpenAPI schema 等等。 TODO: 補充更多細節 CRD vs API Aggregation CRD API Aggregation 不用寫程式 要用 Go 語言來開發 API 伺服器 不需要額外服務處理 API 請求與儲存，但還是需要一個控制器來實現資源功能邏輯 需要獨立的第三方服務處理 API 各種事情 任何問題都是由 Kubernetes 社區處理與修復 需要同步 Kubernetes 社區問題修復方法，並重新建構 API 伺服器 無需額外處理 API 多版本機制 需要自行處理 API 多版本機制 詳細英文描述，可以參考 Comparing ease of use。 更多詳細的功能支援比較，可以參考 Advanced features and flexibility。 看完這邊大家會發現 CRD 能達到的功能，API Aggregation 也都能達到。但 CRD 上手簡單又快速，API Aggregation 相對要處理事情更多。因此後續開發我會以 CRD 為主。 TODO: 補充更多比較 結語Kubernetes 提供了非常彈性的方式來擴展 API 功能，開發者能夠依據需求自行選擇擴展方式。也因為這些機制的完善，漸漸地越來越多在 Kubernetes 上新增自定義資源，並開發自家的控制器來管理這些資源，以實現各種在 Kubernetes 的新功能。 到這邊大致上了解一些開發自定義控制器前相關的知識，明天將說明一個 Kubernetes 自定義控制器是如何運作。 Reference https://hackmd.io/@onyiny-ang/HyxVWsS6z?type=view https://programming-kubernetes.info/ https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"開發自定義控制器前，需要先了解的東西 Part2","slug":"ironman2020/day21","date":"2019-10-05T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/06/ironman2020/day21/","link":"","permalink":"https://k2r2bai.com/2019/10/06/ironman2020/day21/","excerpt":"前言昨天提到 Kubernetes GitHub 組織上，有許多豐富的程式函式庫可以使用，除了昨天介紹的一些關於 API 的函式庫外，還有用於跟 Kubernetes API 伺服器溝通的客戶端函式庫，如: client-go，而這些客戶端函式庫在開發 Kubernetes 自定義控制器時，是幾乎避免不了，甚至整個 Kubernetes 控制器架構都是圍繞這些函式庫上實現，而今天就是要針對這些客戶端函式庫做初步認識。","text":"前言昨天提到 Kubernetes GitHub 組織上，有許多豐富的程式函式庫可以使用，除了昨天介紹的一些關於 API 的函式庫外，還有用於跟 Kubernetes API 伺服器溝通的客戶端函式庫，如: client-go，而這些客戶端函式庫在開發 Kubernetes 自定義控制器時，是幾乎避免不了，甚至整個 Kubernetes 控制器架構都是圍繞這些函式庫上實現，而今天就是要針對這些客戶端函式庫做初步認識。 client-go由於 Kubernetes 是以 Go 語言打造的專案，因此相關函式庫都是以 Go 語言來提供使用，因此將 Kubernetes 客戶端函式庫稱之為 client-go。而 client-go 是一套典型的 Web service 函式庫，它支援了所有 Kubernetes 正式的 API 資源類型，並對它進行以下 REST 操作: Create Get List Update Delete Patch 這些 REST 操作動詞都是基於 API 伺服器的 HTTP 介面實現。另外 client-go 也支援了 Watch API，這是透過 HTTP Streaming 機制來監聽資源在叢集中的變化事件。透過 client-go 我們能在程式做到什麼事情呢?這邊列舉幾個: 允許操作資源狀態(如:新增 Pod、修改 ConfigMap 或刪除 Persistent Volume)。 列出所有資源。 獲取有關當前資源狀態的詳細訊息。 開發自定義控制器。 而 Kubernetes 的 client-go 背後會使用到上述提到的 api、api-machinery 等等函式庫，因此在導入時，需要注意一下版本相容性，如下圖。 client-go 是官方最主要的 API client 函式庫，它在許多地方被使用，如 kubectl、kubeadm 與各種控制器等等。而目前 client-go 整體以目錄分成以下功能: kubernetes: 提供原生 Kubernetes API 的資源 REST 操作方法與結構，如 Pod。 informers: 提供原生 Kubernetes API 的資源 List/Watch API 機制與功能。經常被用於實現控制器中。 listers: 提供原生 Kubernetes API 的資源從 Local cache 取得 API 等功能。經常被用於實現控制器中。 discovery: 用於發現 Kubernetes API 伺服器支援哪些的 API 群組、版本與資源方法。 dynamic: 提供一個動態的客戶端，能用於操作任何 Kubernetes 上的 API 資源。功能與kubernetes套件類似，差別在於kubernetes是針對每種 API 資源提供自己的操作方法。 transport: 提供 TCP 授權/連接、Stream(如: exec、logs 與 portforward 等)、Websocket 等等功能。如果沒有明確選擇協定的話，預設會使用 HTTP2 進行溝通。其中 Stream 部分，若不支援 HTTP2 的話，則採用 SPDY 實現。 rest: 提供 REST 客戶端的介面與實現，為 client-go 的 kubernetes package 基礎。 plugin: 提供雲端供應商(Cloud Provider)的身份認證插件。 tools: 提供各種方便使用的功能與工具，如 Cache、LeaseLock、Metrics 等等。 scale: 提供 Auto Scaling 相關的客戶端。 util: 提供各種方便使用的程式功能，如 Workqueue、Flow control、Certificate 等等。 examples: 提供各種範例，如 Workqueue、Fake Client 等等操作。 TODO: 補範例 apiextensions-apiserverapiextensions-apiserver 類似 client-go 功能，但主要為 CRD(CustomResourceDefinitions) API 的資源結構，以及用於操作該資源的 Client 函式庫。如下面範例。 type customResource struct &#123; Name string Kind string Group string Plural string Version string Scope apiextensionsv1beta1.ResourceScope ShortNames []string&#125;func createCRD(clientset apiextensionsclientset.Interface, resource customResource) error &#123; crdName := fmt.Sprintf(\"%s.%s\", resource.Plural, resource.Group) crd := &amp;apiextensionsv1beta1.CustomResourceDefinition&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: crdName, &#125;, Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec&#123; Group: resource.Group, Version: resource.Version, Scope: resource.Scope, Names: apiextensionsv1beta1.CustomResourceDefinitionNames&#123; Singular: resource.Name, Plural: resource.Plural, Kind: resource.Kind, ShortNames: resource.ShortNames, &#125;, &#125;, &#125; _, err := clientset.ApiextensionsV1beta1().CustomResourceDefinitions().Create(crd) if err != nil &#123; if !errors.IsAlreadyExists(err) &#123; return fmt.Errorf(\"failed to create %s CRD. %+v\", resource.Name, err) &#125; &#125; return nil&#125;func main() &#123; res := customResource&#123; &#123; Name: \"security\", Plural: \"securities\", Kind: reflect.TypeOf(blendedv1.Security&#123;&#125;).Name(), Group: blendedv1.CustomResourceGroup, Version: blendedv1.Version, Scope: apiextensionsv1beta1.NamespaceScoped, &#125;, &#125; createCRD(extensionsClient, res)&#125; 範例並未提供完整內容，僅擷取部分資訊用以說明。 結語client-go 與 apiextensions-apiserver 是開發一個原生 Kubernetes 控制器的主要函式庫，整個控制器的工作流程與功能，都會利用這兩個客戶端函式庫來完成。而 client-go 除了提供各種 API 資源物件以外，也有各種方便的介面(Interface)、功能(Function)與方法(Method)，如: LeaseLock、Metrics 等等，能讓我們使用。 有了這些函式庫後，就能夠以程式實現各種操作功能，比如說 Websocket Pod Exec 這個範例，就是利用 client-go transport 實作。 今天主要分享有關客戶端的函式庫，明天將認識擴展 API 時，需要知道的兩個方法。 Reference https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/ http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/ https://kubernetes.io/docs/reference/using-api/client-libraries/ https://github.com/kubernetes-client/gen https://speakerdeck.com/chanyilin/k8s-metacontroller https://toutiao.io/posts/4rnwh6/preview","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"開發自定義控制器前，需要先了解的東西 Part1","slug":"ironman2020/day20","date":"2019-10-04T16:00:00.000Z","updated":"2019-12-02T01:49:42.390Z","comments":true,"path":"2019/10/05/ironman2020/day20/","link":"","permalink":"https://k2r2bai.com/2019/10/05/ironman2020/day20/","excerpt":"前言由於 Kubernetes 控制器是主動調和(Reconciliation)資源過程的程式，它會透過與 API 伺服器溝通，以監視叢集的資源狀態，並依據 API 物件的當前狀態，嘗試將其推向預期狀態。而本系列文章是說明如何採用官方 API client 函式庫來編寫 Kubernetes 自定義控制器。因此需要在開發之前，先了解會使用到的函式庫與工具等等。 Kubernetes 組織在 GitHub 上，維護了許多可以使用的程式函式庫，如: api、client 與 api-machinery 等等都被用於不同的功能實現。而要使用這些函式庫只需要以k8s.io/..方式，在 Go 語言的專案下導入即可。在接下來個小部分中，我將介紹一些會用於開發自定義控制器的 API 相關函式庫。","text":"前言由於 Kubernetes 控制器是主動調和(Reconciliation)資源過程的程式，它會透過與 API 伺服器溝通，以監視叢集的資源狀態，並依據 API 物件的當前狀態，嘗試將其推向預期狀態。而本系列文章是說明如何採用官方 API client 函式庫來編寫 Kubernetes 自定義控制器。因此需要在開發之前，先了解會使用到的函式庫與工具等等。 Kubernetes 組織在 GitHub 上，維護了許多可以使用的程式函式庫，如: api、client 與 api-machinery 等等都被用於不同的功能實現。而要使用這些函式庫只需要以k8s.io/..方式，在 Go 語言的專案下導入即可。在接下來個小部分中，我將介紹一些會用於開發自定義控制器的 API 相關函式庫。 這部分包含以下: API Machinery API gengo code-generator apimachineryAPI Machinery 是定義 API 級別的 Scheme、類型(Typing)、編碼(Encoding)、解碼(Decoding)、驗證(Validate)、類型轉換與相關工具等等功能。當我們要實現一個新的 API 資源時，就必須透過 API Machinery 來註冊 Scheme，另外 API Machinery 也定義了 TypeMeta、ObjectMeta、ListMeta、 Labels 與 Selector 等等物件，而這些物件幾乎在每個 Kubernetes API 資源中都會使用到，比如下面 YAML 所示。 apiVersion: v1 # TypeMetakind: Pod # TypeMetametadata: # ObjectMeta name: memory-demo namespace: mem-example labels: # Labels tt: xx spec: containers: - name: memory-demo-ctr image: polinux/stress command: [\"stress\"] args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] apiAPI 主要提供 Kubernetes 原生的 API 資源類型的 Scheme，這包含 Namespace、Pod 等等。該函式庫也提供了每個 API 資源類型，當前所支援的版本，如:v1、v1beta1。而每種 API 資源都依功能取向被群組化，如下圖所示。 gengogengo 主要用於透過 Go 語言檔案產生各種系統與 API 所需的文件，比如說 Protobuf。而該專案也包含了 Set、Deep-copy、Defaulter 等等產生器(Generator)，這些會被用於產生客製化 Client 函式庫。 大家在看 Kubernetes 源碼時，一定會看到這樣一段註解// Code generated by xxx. DO NOT EDIT.。事實上 Kubernetes 有許多程式碼是基於該專案產生出來的，因為 Kubernetes 有很多 API 資源類型，若每一種都寫套維護的話，會非常複雜，因此 Kubernetes 定義了一套標準(Interface 與 Scheme 等等)來維護，並透過 Generator 來產生一些程式碼。 code-generatorCode Generator 是基於 gengo 開發的程式碼產生器，主要用來實現產生 Kubernetes-style API types 的 Client、Deep-copy、Informer、Lister 等等功能的程式碼。這是因為 Go 語言中沒有泛型(Generic)概念，因此不同的 API 資源類型，若都要寫一次上述這些功能的話，會有大量重複的程式碼，因此 Kubernetes 採用定義好類型結構後，再透過該專案提供的工具產生相關程式碼。下面舉個例子。 其他語言的 Generator 可以參考 gen。 假設要實作一個 LINE Bot 的 API 資源，並產生 Client 程式時，我們必需先定義結構在 Go 檔案中。然後接著用註解方式，在程式碼標示物件結構要產生程式碼。比範例會產生 Bot 物件的 client 程式碼跟 Deep-copy 方法: package v1alpha1import ( \"github.com/line/line-bot-sdk-go/linebot\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\")// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Objecttype Bot struct &#123; metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata\"` Spec BotSpec `json:\"spec\"` Status BotStatus `json:\"status,omitempty\"`&#125;type BotExposeType stringconst ( NgrokExpose BotExposeType = \"Ngrok\" IngressExpose BotExposeType = \"Ingress\" LoadBalancerExpose BotExposeType = \"LoadBalancer\")type BotExpose struct &#123; Type BotExposeType `json:\"type\"` DomainName string `json:\"domainName\"` LoadBalanceIPs []string `json:\"loadBalanceIPs,omitempty\"` NgrokToken string `json:\"ngrokToken\"`&#125;type BotSpec struct &#123; Selector *metav1.LabelSelector `json:\"selector\"` ChannelSecretName string `json:\"channelSecretName\"` Expose BotExpose `json:\"expose\"` Version string `json:\"version\"` LogLevel int `json:\"logLevel\"`&#125;type BotPhase stringconst ( BotPending BotPhase = \"Pending\" BotActive BotPhase = \"Active\" BotFailed BotPhase = \"Failed\" BotTerminating BotPhase = \"Terminating\")type BotStatus struct &#123; Phase BotPhase `json:\"phase\"` Reason string `json:\"reason,omitempty\"` LastUpdateTime metav1.Time `json:\"lastUpdateTime\"`&#125;// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Objecttype BotList struct &#123; metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata\"` Items []Bot `json:\"items\"`&#125; 當完成定義與註解描述後，我們會以這樣的目錄方式放在開發專案中，其中types.go就是上述檔案。 pkg/apis└── line ├── register.go └── v1alpha1 ├── doc.go ├── register.go ├── types.go └── zz_generated.deepcopy.go 其他檔案會在後續開發時，詳細說明。 接著利用 code-generator 工具來指向 API 物件結構位置，以讓 code-generator 解析，並產生對應的程式碼。下面是執行腳本範例: #!/bin/bashset -o errexitset -o nounsetset -o pipefailSCRIPT_ROOT=$(dirname \"$&#123;BASH_SOURCE[0]&#125;\")/..CODEGEN_PKG=$&#123;CODEGEN_PKG:-$(cd \"$&#123;SCRIPT_ROOT&#125;\"; ls -d -1 ./vendor/k8s.io/code-generator 2&gt;/dev/null || echo ../code-generator)&#125;bash \"$&#123;CODEGEN_PKG&#125;\"/generate-groups.sh \"deepcopy,client,informer,lister\" \\ github.com/kairen/line-bot-operator/pkg/generated \\ github.com/kairen/line-bot-operator/pkg/apis \\ \"line:v1alpha1\" \\ --output-base \"$(dirname $&#123;BASH_SOURCE&#125;)/../../../../\" \\ --go-header-file $&#123;SCRIPT_ROOT&#125;/hack/boilerplate.go.txt 這邊boilerplate.go.txt為 Go 檔案的 License 內容。用於在產生程式碼時，自動塞在檔案內容的頭。 當完成後，我們會在指定輸出的目錄看到產生的程式碼，如 pkg/generated├── clientset│ └── versioned│ ├── clientset.go│ ├── doc.go│ ├── fake│ │ ├── clientset_generated.go│ │ ├── doc.go│ │ └── register.go│ ├── scheme│ │ ├── doc.go│ │ └── register.go│ └── typed│ └── line│ └── v1alpha1│ ├── bot.go│ ├── doc.go│ ├── event.go│ ├── eventbinding.go│ ├── fake│ │ ├── doc.go│ │ ├── fake_bot.go│ │ ├── fake_event.go│ │ ├── fake_eventbinding.go│ │ └── fake_line_client.go│ ├── generated_expansion.go│ └── line_client.go├── informers│ └── externalversions│ ├── factory.go│ ├── generic.go│ ├── internalinterfaces│ │ └── factory_interfaces.go│ └── line│ ├── interface.go│ └── v1alpha1│ ├── bot.go│ ├── event.go│ ├── eventbinding.go│ └── interface.go└── listers └── line └── v1alpha1 ├── bot.go ├── event.go ├── eventbinding.go └── expansion_generated.go 如此一來，我們就能在開發時，使用程式碼來操作自定義資源的 CRUD。 TODO: 補全部 Generator 細節與設定 結語今天主要初步了解 Kubernetes GitHub 組織上關於 API 的函式庫，在開發 Kubernetes 自定義控制器時，有可能因為跟原本 Kubernetes 的功能整合，因此會很頻繁地使用到這些函式庫。然而對這些函式庫有出不了的話，對於後續在自定義資源實作時，也能比較清楚 Kubernetes 的一些設計架構。 Reference https://learning.oreilly.com/library/view/programming-kubernetes/9781492047094/ http://www.edwardesire.com/2019/05/14/kubernetesbian-controller-pattern/ https://kubernetes.io/docs/reference/using-api/client-libraries/ https://github.com/kubernetes-client/gen https://speakerdeck.com/chanyilin/k8s-metacontroller https://toutiao.io/posts/4rnwh6/preview","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"淺談 Kubernetes 自定義資源(Custom Resource)與自定義控制器(Custom Controller)","slug":"ironman2020/day19","date":"2019-10-03T16:00:00.000Z","updated":"2019-12-02T01:49:42.389Z","comments":true,"path":"2019/10/04/ironman2020/day19/","link":"","permalink":"https://k2r2bai.com/2019/10/04/ironman2020/day19/","excerpt":"前言使用 Kubernetes 時，大家都能感受到其容器編配能力，當有一個容器發生異常時，Kubernetes 會透過自身機制幫你把容器遷移或重新啟動，或者能利用副本機制讓容器同時存在於叢集的不同節點上，甚至提供滾動升級(Rolling Update)容器機制。這些酷炫的功能，大家肯定都知道如何去使用，因為 Kubernetes 透過一些方式，將複雜的功能進行了抽象化與封裝，因此使用者只需要了解如何操作 API 物件，就能完成需要的功能，比如:Deployment 修改參數就會進行滾動升級。然而這些『抽象化』與『封裝』的過程究竟是如何實現呢?今天文章就是要針對這個部分進行探討。","text":"前言使用 Kubernetes 時，大家都能感受到其容器編配能力，當有一個容器發生異常時，Kubernetes 會透過自身機制幫你把容器遷移或重新啟動，或者能利用副本機制讓容器同時存在於叢集的不同節點上，甚至提供滾動升級(Rolling Update)容器機制。這些酷炫的功能，大家肯定都知道如何去使用，因為 Kubernetes 透過一些方式，將複雜的功能進行了抽象化與封裝，因此使用者只需要了解如何操作 API 物件，就能完成需要的功能，比如:Deployment 修改參數就會進行滾動升級。然而這些『抽象化』與『封裝』的過程究竟是如何實現呢?今天文章就是要針對這個部分進行探討。 Kubernetes 是個非常容易擴展的系統。Kubernetes 提供了多種方法讓我們能夠自定義 API，或擴展功能，比如: Cloud providers: 提供自定義雲平台整合的控制器。比如說跟 IaaS 進行整合，當建立一個 LoadBalancer Service 時，自動呼叫 IaaS 負載平衡 API 進行建立。 過去 Cloud providers 屬於 kube-controller-manager 的部分控制器，現在已從核心程式碼移出。 Admission control webhooks: 提供 kube-apiserver 存取擴展的 Webhook API。 kubelet plugins: 提供容器 Runtime(CRI)、網路(CNI)、儲存(CSI)與裝置(Device Plugins)等等介面。 kubectl plugins: 提供擴展 kubectl。如 krew。 Custom resources 與 Custom controllers: 提供自定義 API 資源(物件)，以及執行這些客製化 API 資源(物件)的邏輯程式。 Custom API servers: 提供透過 apiserver 函式庫開發用於 API Aggregation 的 API servers。 Custom schedulers: \b提供能實現自定義的排程演算法與機制，並在 Pod 指定使用。 Authentication webhooks: 提供擴展 Kubernetes 身份認證機制，可以與外部系統整合。如: LDAP、OCID。 而今天我們重點就是要放在探討自定義資源(Custom Resource)與自定義控制器(Custom Controller)上。 自定義資源(Custom Resource)在 Kubernetes API 中，一個端點(Endpoint)就是一個資源，這個資源是被用於儲存某個類型的 API 物件的集合。比如說 Pod 有 /api/v1/pods API 端點。 一個 API 端點的組成如下所示: API Group: 是邏輯上相關的種類集合，如 Job 與 CronJob 都屬於批次處理功能相關。 Version: 每個 API Group 存在多個版本，這些版本區分不同穩定度層級，一般功能會從 v1alpha1 升級到 v1beta1，然後在 v1 成為穩定版本。 Resource: 資源是透過 HTTP 發送與檢索的 API 物件實體，其以 JSON 來表示。可以是單一或者多個資源。 其中每個 API 都可能存在著不同版本，其意味著不同層級穩定度與支援度: Alpha Level: 在預設下是大多情況禁止使用狀態，這些功能有可能隨時在下一版本被遺棄，因此只適用於測試用，如: v1alpha1。 Beta Level: 在這級別一般預設會啟用，這表示該功能已經過很好的測試項目，但是物件內容可能會在後續版本或穩定版本發生變化。如: v1beta2。 Stable Level: 在這級別表示該功能已經穩定，會很長的時間一直存在。如: v1。 而自定義資源就是預設不存在於 Kubernetes 原生的額外 API 資源，這包含了從當前叢集擴展新的資源物件(如:原本沒有 DaemonJob，我透過一些機制新增了)，以及其他系統元件本身使用的(如: KubeadmConfig)。目前 Kubernetes 提供了兩種方式來新增自定義資源: CRD(CustomResourceDefinitions) API Aggregation TODO: 補充更多細節 自定義控制器(Controller)當利用 kubectl 建立一個 Pod 時，客戶端會透過 kubectl 與 kube-apiserver 進行溝通呼叫 Pod APIs，這時 API 物件經過驗證後，被成功建立到 Kubernetes 上，最後儲存到 etcd 中，然後過不久後，就會發現這個 Pod 在叢集中的某個節點上被執行了。到這邊一定會疑惑中間的過程，是怎麼判定建立到哪個節點上的，又是怎麼在該節點建立的呢?實際上，這需要由 Kubernetes 的kube-scheduler與kubelet元件完成的，其流程如下圖所示。 使用者透過客戶端工具與 kube-apiserver 以 REST API 方式進行溝通建立 Pod 物件，然後 kube-apiserver 進行各種驗證通過後，將其寫入 etcd 中。 這時 kube-scheduler 會透過監聽 kube-apiserver 的 Pod 物件變化事件，獲取到 Pod 物件的內容，而當觀察到該 Pod 的.spec.nodeName欄位沒有被分配節點名稱時，kube-scheduler就會透過過濾(Filter)與排名(Rank)演算法來計算所有節點的權重，並從中找出一個最佳的節點，接著在 Pod 的.spec.nodeName更新被選取的名稱，然後該狀態會被儲存到 etcd 中。 這時 kubelet 也一直監聽著 kube-apiserver 的 Pod 物件變化事件，當發現有一個 Pod 的.spec.nodeName欄位是這個節點時，kubelet 就會呼叫容器 Runtime 來啟動這個 Pod 所定義的相關功能，如:掛載儲存、透過 system call 寫入環境變數等等。 一方面 kubelet 會監控容器 Runtime 執行的 Pod 狀態。並隨著情況的變化，同步將內容更新到 Pod API 物件上，以讓使用者能夠了解當前狀態。 從這點了解 kube-scheduler 與 kubelet，才是實際上負責執行 Pod 邏輯的角色之一，而這些邏輯就是所謂的控制器(Controller)。因此儘管 Kubernetes 原生提供了許多的 API 資源可以使用(如下圖)，如果當前 Kubernetes 叢集並沒有啟動或安裝相關的控制器，以執行實際的邏輯的話，這些 API 資源就形同空殼般存在於叢集中。這邊再舉幾個例子，\b比如:實現 Service 功能的就是 kube-proxy 與 kube-controller-manager 的 Endpoint 控制器(或 Endpoint Slice 控制器)，這兩者分別監聽 Service 設定 NAT rules 與同步綁定 Service 與 Pod 的 IP。 那麼自定義控制器又跟自定義資源有什麼關析呢?就如同上面提到範例的 API 資源一樣，自定義資源本身只提供儲存與檢索結構化內容，因此當擴展時，並不會有實際功能，而這時就需要結合自定義控制器來完成功能的邏輯事情，並持續同步更新自定義資源。一般來說自定義控制器會有一個 Control Loop 邏輯，會持續監聽自定義資源在 API server 的事件變化(Create、Update 與 Delete)，一但收到變化後，取出 API 物件內容，並執行預期結果。 (圖片擷取自：Programming Kubernetes) TODO: 補充更多細節 結語在 Kubernetes 生態中，幾乎所有 API 物件功能都是以這樣形式來完成，讓使用者以宣告式 API(Declarative API)方式先定義該物件預期執行的需求，最後再由控制器想辦法執行到預期的結果。而在過去，這種模式並沒有盛行於開發者上，是直到 v1.7 版本 CRD(CustomResourceDefinitions) 的出現(當然 Kubernetes 成功也是原因)，才出現越來越多基於此概念的各種控制器出現，甚至出現了新的名詞『Operator』。 自定義控制器除了能夠讓開發人員擴展與添加新功能以外，事實上也能替換現有的功能來優化(如利用 kbue-router 取代 kube-proxy)。當然也能用於執行一些自動化管理任務。接下來我將用一系列文章說明如何實作自定義控制器，並了解一些技巧。 Reference https://github.com/kubernetes/kubernetes/tree/master/pkg/controller https://kubernetes.io/docs/concepts/extend-kubernetes/ https://speakerdeck.com/thockin/kubernetes-what-is-reconciliation https://medium.com/speechmatics/how-to-write-kubernetes-custom-controllers-in-go-8014c4a04235 https://itnext.io/how-to-create-a-kubernetes-custom-controller-using-client-go-f36a7a7536cc https://github.com/kubeflow/tf-operator/issues/300 https://admiralty.io/blog/kubernetes-custom-resource-controller-and-operator-development-tools/ https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/ https://zhuanlan.zhihu.com/p/59660536","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"利用 Device Plugins 提供硬體加速","slug":"ironman2020/day16","date":"2019-09-30T16:00:00.000Z","updated":"2019-12-02T01:49:42.389Z","comments":true,"path":"2019/10/01/ironman2020/day16/","link":"","permalink":"https://k2r2bai.com/2019/10/01/ironman2020/day16/","excerpt":"前言Device Plugins 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 Device Plugins 介面來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。 P.S. 傳統的alpha.kubernetes.io/nvidia-gpu將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。","text":"前言Device Plugins 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 Device Plugins 介面來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。 P.S. 傳統的alpha.kubernetes.io/nvidia-gpu將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。 Device Plugins 原理Device Plugins 主要提供了一個 gRPC 介面來給廠商實現ListAndWatch()與Allocate()等 gRPC 方法，並監聽節點的/var/lib/kubelet/device-plugins/目錄中的 gRPC Server Unix Socket，這邊可以參考官方文件 Device Plugins。一旦啟動 Device Plugins 時，透過 Kubelet Unix Socket 註冊，並提供該 plugin 的 Unix Socket 名稱、API 版本號與插件資源名稱(vendor-domain/resource，例如 nvidia.com/gpu)，接著 Kubelet 會將這些曝露到 Node 狀態以便 Scheduler 使用。 Unix Socket 範例： $ ls /var/lib/kubelet/device-plugins/kubelet_internal_checkpoint kubelet.sock nvidia.sock 一些 Device Plugins 列表： NVIDIA GPU RDMA Kubevirt SFC Intel Device Plugins SR-IOV 節點資訊部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為Ubuntu 18.04+: IP Address Hostname CPU Memory Role Extra Device 172.22.132.11 k8s-m1 4 16G Master None 172.22.132.12 k8s-m2 4 16G Master None 172.22.132.13 k8s-m3 4 16G Master None 172.22.132.21 k8s-n1 4 16G Node None 172.22.132.22 k8s-n2 4 16G Node None 172.22.132.32 k8s-g2 4 16G Node GTX 1060 3G *2 事前準備安裝 Device Plugin 前，需要確保以下條件達成： 所有節點需要安裝 Docker。 $ curl -fsSL \"https://get.docker.com/\" | sh GPU 節點需正確安裝指定版本的 NVIDIA Driver 與 CUDA。 $ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update $ sudo apt-get install -y linux-headers-$(uname -r)$ sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install -y cuda-10-0 cuda-drivers GPU 節點需正確安裝指定版本的 NVIDIA Docker 2。 $ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2$ sudo systemctl restart docker 部署一座 Kubernetes v1.10+ 叢集。請參考 kubeadm 部署 Kubernetes 叢集。 安裝 NVIDIA Device Plugin若上述要求以符合，再開始前需要在每台 GPU worker 節點修改/lib/systemd/system/docker.service檔案，將 Docker default runtime 改成 nvidia，依照以下內容來修改: ...ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia... 這邊也可以修改/etc/docker/daemon.json檔案，請參考 Configure and troubleshoot the Docker daemon。 完成後儲存，並重新啟動 Docker： $ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker 確認上述完成，接著在主節點透過 kubectl 來部署 NVIDIA Device Plugins: $ kubectl create -f https://gist.githubusercontent.com/kairen/bf967d566d35edda381edb9ba8659f7b/raw/ccc18711bf016d5b836280226785c1ad0282c035/nvidia-device-plugin.ymldaemonset \"nvidia-device-plugin-daemonset\" created$ kubectl -n kube-system get po -o wideNAME READY STATUS RESTARTS AGE IP NODE...nvidia-device-plugin-daemonset-nwx2s 1/1 Running 0 49s 10.244.255.80 k8s-g2 &lt;none&gt; &lt;none&gt; 由於目前 NVIDIA Device Plugin 的 beta3 有問題，因此以 beat1 為主。 測試 GPU首先執行以下指令確認是否可被分配資源: $ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"NAME GPUk8s-g2 2... 當 NVIDIA Device Plugins 部署完成後，即可建立一個簡單範例來進行測試: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: gpu-podspec: restartPolicy: Never containers: - image: nvidia/cuda name: cuda command: [\"nvidia-smi\"] resources: limits: nvidia.com/gpu: 1EOFpod \"gpu-pod\" created$ kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESgpu-pod 0/1 Completed 0 21m 10.244.255.81 k8s-g2 &lt;none&gt; &lt;none&gt;$ kubectl logs gpu-podSat Oct 01 15:28:38 2019+-----------------------------------------------------------------------------+| NVIDIA-SMI 418.87.01 Driver Version: 418.87.01 CUDA Version: 10.1 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:05:00.0 Off | N/A || 0% 38C P8 6W / 120W | 0MiB / 3019MiB | 1% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 從上面結果可以看到 Kubernetes Pod 正確的使用到 NVIDIA GPU。 結語Kubernetes 提供了 Device Plugin Interface 來讓硬體供應商實現自家硬體與 Kubernetes 整合的功能，這使 Kubernetes 社區不在需要維護各種廠商的硬體整合程式，以減少核心程式碼的複雜性，一方面能更加專注在規範 Device Plugin 標準的事情。 Reference https://medium.com/@maniac.tw/ubuntu-18-04-%E5%AE%89%E8%A3%9D-nvidia-driver-418-cuda-10-tensorflow-1-13-a4f1c71dd8e5 https://www.mvps.net/docs/install-nvidia-drivers-ubuntu-18-04-lts-bionic-beaver-linux/ https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"自建私有容器儲存庫(Container Registry)與實現內容信任(Content Trust)","slug":"ironman2020/day15","date":"2019-09-29T16:00:00.000Z","updated":"2019-12-02T01:49:42.389Z","comments":true,"path":"2019/09/30/ironman2020/day15/","link":"","permalink":"https://k2r2bai.com/2019/09/30/ironman2020/day15/","excerpt":"前言在地端的環境中，有許多原本能透過網路取的資源(如: 系統套件、容器映像檔等等)，有可能會基於公司一些考量(如:安全、網路等)，而將這些資源建立在本地端，然後再提供給叢集使用。其中容器儲存庫(Container Registry)是最常見的需求，因為有些團隊會要求公司測試與服務的容器映像檔，都必須從公司內部取得，這時自建一套私有容器儲存庫就非常重要。尤其是基於安全考量，還需要對映像檔進行安全掃描，或對映像檔內容進行加密等等。 而今天將說明如何自建一套容器儲存庫，並實現映像檔內容信任功能，以確保叢集使用的映像檔處於安全受信任的。在開始前，先來了解一下今天要使用到的開源軟體吧。","text":"前言在地端的環境中，有許多原本能透過網路取的資源(如: 系統套件、容器映像檔等等)，有可能會基於公司一些考量(如:安全、網路等)，而將這些資源建立在本地端，然後再提供給叢集使用。其中容器儲存庫(Container Registry)是最常見的需求，因為有些團隊會要求公司測試與服務的容器映像檔，都必須從公司內部取得，這時自建一套私有容器儲存庫就非常重要。尤其是基於安全考量，還需要對映像檔進行安全掃描，或對映像檔內容進行加密等等。 而今天將說明如何自建一套容器儲存庫，並實現映像檔內容信任功能，以確保叢集使用的映像檔處於安全受信任的。在開始前，先來了解一下今天要使用到的開源軟體吧。 HarborHarbor 是 CNCF Incubating 專案，該專案是基於 Docker Distribution 擴展功能的 Container Registry，提供映像檔儲存、簽署、漏洞掃描等功能。另外增加了安全、身份認證與 Web-based 管理介面等功能。 整合 LDAP/Active Directory、OIDC 進行使用者認證 整合 Clair 以實現容器映像檔安全掃描 整合 Notary 以實現容器映像檔簽署(Content trust) 支援 S3、Cloud Storage 等儲存後端 支援映像檔副本機制 提供使用者管理(User managment)UI 提供基於角色存取控制(Role-based access control)和活動稽核(Activity auditing)機制 ClairClair 是 CoreOS 開源的容器映像檔安全掃描專案，其提供 API 式的分析服務，透過比對公開漏洞資料庫 CVE（Common Vulnerabilities and Exposures）的漏洞資料，並發送關於容器潛藏漏洞的有用和可操作資訊給管理者。 NotaryNotary 是 CNCF Incubating 專案，該專案是 Docker 對安全模組重構時，抽離的獨立專案。Notary 是用於建立內容信任的平台，目標是確保 Server 與 Client 之間交互使用已經相互信任的連線，並保證在 Internet 上的內容發佈安全性，該專案在容器應用時，能夠對映像檔、映像檔完整性等安全需求提供內容信任支援。 TODO: 補 Portieris。 部署環境本部分將說明如何部署 Harbor，並設定啟用 Clair 與 Notary。 節點資訊部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為Ubuntu 18.04+: IP Address Hostname CPU Memory Role 172.22.132.11 k8s-m1 4 16G Master 172.22.132.12 k8s-m2 4 16G Master 172.22.132.13 k8s-m3 4 16G Master 172.22.132.21 k8s-n1 4 16G Node 172.22.132.22 k8s-n2 4 16G Node 172.22.132.31 k8s-g1 4 16G Node 172.22.132.32 k8s-g2 4 16G Node 172.22.132.253 deploy-node 4 16G Harbor k8s 節點不需要這麼多，這邊只是沿用。 事前準備在開始部署時，請確保滿足以下條件: deploy-node節點需要安裝容器引擎: $ curl -fsSL \"https://get.docker.com/\" | sh deploy-node節點需要安裝 docker-compose 工具: $ curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose$ chmod +x /usr/local/bin/docker-compose 部署一座 Kubernetes v1.10+ 叢集。可參考用 kubeadm 部署 Kubernetes 叢集。 Harbor 部署Harbor 會由多個容器部署而成，因此我們需要在部署前取得安裝檔案。這邊可以利用 wget 下載 Offline 安裝的檔案: $ wget https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz$ tar xvf harbor-offline-installer-v1.9.0.tgz &amp;&amp; \\ rm harbor-offline-installer-v1.9.0.tgz$ cd harbor 這邊安裝透過 Offline 進行，因此在部署前，需要透過 Docker 載入映像檔: $ docker load &lt; harbor.v1.9.0.tar.gz# 完成後，透過 images 指令查看$ docker images 接著由於部署的 Harbor 使用 HTTPS，因此需要提供憑證。這邊由於測試用，因此以自簽(Self signed)來處理: # 產生 CA crt $ openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout ca.key \\ -x509 -days 365 -out ca.crt \\ -subj \"/C=TW/ST=New Taipei/L=New Taipei/O=test_company/OU=IT/CN=test\"# 產生 Harbor csr $ openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout harbor-registry.key \\ -out harbor-registry.csr \\ -subj \"/C=TW/ST=New Taipei/L=New Taipei/O=test_company/OU=IT/CN=172.22.132.253\"$ cat &gt; v3.ext &lt;&lt;-EOFauthorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentextendedKeyUsage = serverAuth subjectAltName = @alt_names[alt_names]DNS.1=harbor-serverIP.1=172.22.132.253EOF$ openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in harbor-registry.csr \\ -out harbor-registry.crt 複製檔案至/data/cert/目錄底下: $ mkdir -p /data/cert/$ cp -rp ./&#123;ca.key,ca.crt,harbor-registry.key,harbor-registry.crt&#125; /data/cert/ 修改 Harbor 組態檔案harbor.yml內容，如以下所示: hostname: 172.22.132.253https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/cert/harbor-registry.crt private_key: /data/cert/harbor-registry.keyharbor_admin_password: p@ssw0rd 這邊僅修改需要欄位，其餘則保持不變。 完成後，即可執行腳本進行部署 Harbor: $ ./prepare...Clean up the input dir$ ./install.sh --with-notary --with-clair 在 Docker 存取映像檔首先複製 ca 憑證到 Docker certs 目錄，以確保 HTTPs 能夠授權: $ mkdir -p /etc/docker/certs.d/172.22.132.253$ cp /data/cert/ca.crt /etc/docker/certs.d/172.22.132.253/ 取得測試用映像檔，並推送映像檔到 Harbor 中: $ docker pull alpine:3.7# 輸入帳密登入$ docker login 172.22.132.253$ docker tag alpine:3.7 172.22.132.253/library/alpine:3.7$ docker push 172.22.132.253/library/alpine:3.7 完成後，可以在 UI 上查看，如同下圖所示: 在 Kubernetes 上存取映像檔首先在所有 K8s 節點上，複製 ca 憑證到 Docker certs 目錄，以確保 HTTPS 能夠授權: $ mkdir -p /etc/docker/certs.d/172.22.132.253$ scp /data/cert/ca.crt &lt;HOST&gt;:/etc/docker/certs.d/172.22.132.253/ 這邊建議用 Ansible 這種工具複製。 在任一能操作叢集的節點上，執行以下指令建立 Pull Secret: $ kubectl create secret docker-registry regcred \\ --docker-server=&quot;172.22.132.253&quot; \\ --docker-username=admin \\ --docker-password=p@ssw0rd \\ --docker-email=admin@example.com$ kubectl apply -f /vagrant/harbor$ kubectl get po 接著建立一個測試用 Pod: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: testspec: imagePullSecrets: - name: regcred containers: - name: alpine image: 172.22.132.253/library/alpine:3.7 command: [\"/bin/sh\", \"-c\"] args: - \"while :; do sleep 1; done\"EOF$ kubectl get poNAME READY STATUS RESTARTS AGEtest 1/1 Running 0 8s 映像檔 Content trust首先透過 UI 建立新的 Project 來測試內容信任功能。 在 Harbor 節點上，複製 ca 憑證到 Notary certs 目錄，以確保 HTTPS 能夠授權: $ mkdir -p $HOME/.docker/tls/172.22.132.253:4443/$ cp /data/cert/ca.crt $HOME/.docker/tls/172.22.132.253:4443/ 在 Docker 客戶端啟用 Content Trust，並推送一個簽署的映像檔到 Harbor: $ export DOCKER_CONTENT_TRUST=1$ export DOCKER_CONTENT_TRUST_SERVER=https://172.22.132.253:4443$ docker tag alpine:3.7 172.22.132.253/trust/alpine:3.7# 這邊會需要輸入密碼短語資訊$ docker push 172.22.132.253/trust/alpine:3.7...Enter passphrase for new root key with ID 93f1593:Repeat passphrase for new root key with ID 93f1593:Enter passphrase for new repository key with ID 224d9cd:Repeat passphrase for new repository key with ID 224d9cd:Finished initializing \"172.22.132.253/trust/alpine\"Successfully signed 172.22.132.253/trust/alpine:3.7 上傳完成後，即可以查看 UI。結果如下圖所示: 當 Docker 啟用 Content Trust 時，也可測試 pull 未簽署的映像檔: $ docker rmi 172.22.132.253/library/alpine:3.7$ docker pull 172.22.132.253/library/alpine:3.7Error: remote trust data does not exist for 172.22.132.253/library/alpine: 172.22.132.253:4443 does not have trust data for 172.22.132.253/library/alpine 結語今天簡單部署 Harbor 作為私有容器儲存庫使用，可以看到 Harbor 整合了許多有用的系統與工具，如:掃描映像檔 CVE、提供 Notary 映像檔內容信任、Web-based UI 等等功能。且 Harbor 也提供身份認證系統與後端儲存的整合，這讓我們擁有 Enterpise 級的功能。 Reference https://docs.docker.com/notary/getting_started/ https://goharbor.io/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實作 Kubernetes 外部認證系統整合: 以 LDAP 為例","slug":"ironman2020/day14","date":"2019-09-28T16:00:00.000Z","updated":"2019-12-02T01:49:42.389Z","comments":true,"path":"2019/09/29/ironman2020/day14/","link":"","permalink":"https://k2r2bai.com/2019/09/29/ironman2020/day14/","excerpt":"前言在一座 Kubernetes 叢集中，通常都會透過不同的使用者來給予不同的存取權限，因為若讓任何人擁有叢集最高權限的話，有可能帶來一些風險。而在 Kubernetes 中都會有兩種類型的使用者: 由 Kubernetes 管理的服務帳號(Service Account)。 普通使用者。 假設普通使用者是由外部獨立系統進行管理(如 LDAP)，那麼管理員分散私鑰、儲存使用者資訊等等功能，都必須由外部系統處理，因為在這方面，Kubernetes 並沒有普通使用者的 API 物件可以使用，因此無法透過 API 將普通使用者資訊添加到叢集中。","text":"前言在一座 Kubernetes 叢集中，通常都會透過不同的使用者來給予不同的存取權限，因為若讓任何人擁有叢集最高權限的話，有可能帶來一些風險。而在 Kubernetes 中都會有兩種類型的使用者: 由 Kubernetes 管理的服務帳號(Service Account)。 普通使用者。 假設普通使用者是由外部獨立系統進行管理(如 LDAP)，那麼管理員分散私鑰、儲存使用者資訊等等功能，都必須由外部系統處理，因為在這方面，Kubernetes 並沒有普通使用者的 API 物件可以使用，因此無法透過 API 將普通使用者資訊添加到叢集中。 但在 Kubernetes 生產環境中，管理普通使用者需求是很常見的需求，假設公司又希望讓管理使用者事情，由既有的帳戶系統管理的話，就會面臨問題。好在 Kubernetes 在這方面也都考慮到了，Kubernetes 提供了 Webhook Token Authentication 與 Authenticating Proxy 機制讓我們可以跟既有系統整合。 TODO: 補 Webhook 細節。 以 LDAP 作為 Kubernetes 身份認證本節以 LDAP 為例來實現身份認證整合。由於 Kubernetes 官方並沒有針對 LDAP/AD 的整合，因此需要藉由 Webhook Token 方式來達成。這邊概念上會開發一個 HTTP Server 提供認證 APIs，當 Kubernetes API Server 收到認證請求時，會轉發至認證用的 HTTP Server 上，這時 HTTP Server 會利用 LDAP client 檢索符合認證的 User 資訊，並將該 User 的 Group 回傳給 API Server，最後 API Server 以該資訊來進行認證授權。 節點資訊部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為Ubuntu 18.04+: IP Address Hostname CPU Memory Role 172.22.132.11 k8s-m1 4 16G Master 172.22.132.12 k8s-m2 4 16G Master 172.22.132.13 k8s-m3 4 16G Master 172.22.132.21 k8s-n1 4 16G Node 172.22.132.22 k8s-n2 4 16G Node 172.22.132.31 k8s-g1 4 16G Node 172.22.132.32 k8s-g2 4 16G Node 172.22.132.150 deploy-node 4 16G LDAP Server 節點不需要這麼多，這邊只是沿用。 事前準備在開始部署時，請確保滿足以下條件: LDAP Server節點需要安裝 Docker 容器引擎: $ curl -fsSL \"https://get.docker.com/\" | sh 部署一座 Kubernetes v1.10+ 叢集。可參考用 kubeadm 部署 Kubernetes 叢集。 OpenLDAP 與 phpLDAPadmin 部署本部分說明如何部署、設定與操作 OpenLDAP。首先進入ldap-server節點，接著利用容器部署 OpenLDAP 與 phpLDAPadmin: $ docker run -d \\ -p 389:389 -p 636:636 \\ --env LDAP_ORGANISATION=\"Kubernetes LDAP\" \\ --env LDAP_DOMAIN=\"k8s.com\" \\ --env LDAP_ADMIN_PASSWORD=\"password\" \\ --env LDAP_CONFIG_PASSWORD=\"password\" \\ --name openldap-server \\ osixia/openldap:1.2.0$ docker run -d \\ -p 443:443 \\ --env PHPLDAPADMIN_LDAP_HOSTS=172.22.132.150 \\ --name phpldapadmin \\ osixia/phpldapadmin:0.7.1 這邊的cn=admin,dc=k8s,dc=com為admin DN，而cn=admin,cn=config為config DN。 另外這邊僅做測試用，故沒有使用 Persistent Volumes，若需要的話，可以參考 Docker OpenLDAP 來設定。 執行完成後，就可以透過瀏覽器來 phpLDAPadmin。這邊點選Login輸入 DN 與 Password。成功登入後畫面，就可以自行新增其他資訊。 雖然可以直接利用 phpLDAPadmin 來新增跟 Kubernetes 整合的資訊，但為了操作快速，這邊以指令方式進行。 建立 Kubenretes Token Schema在ldap-server節點透過 Docker 進入openldap-server容器，然後執行以下指令建立 Kubernetes token schema 設定: $ docker exec -ti openldap-server sh$ mkdir ~/kubernetes_tokens$ cat &lt;&lt;EOF &gt; ~/kubernetes_tokens/kubernetesToken.schemaattributeType ( 1.3.6.1.4.1.18171.2.1.8 NAME 'kubernetesToken' DESC 'Kubernetes authentication token' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )objectClass ( 1.3.6.1.4.1.18171.2.3 NAME 'kubernetesAuthenticationObject' DESC 'Object that may authenticate to a Kubernetes cluster' AUXILIARY MUST kubernetesToken )EOF$ echo \"include /root/kubernetes_tokens/kubernetesToken.schema\" &gt; ~/kubernetes_tokens/schema_convert.conf$ slaptest -f ~/kubernetes_tokens/schema_convert.conf -F ~/kubernetes_tokensconfig file testing succeeded 然後執行以下指令來修改內容: $ vim ~/kubernetes_tokens/cn=config/cn=schema/cn\\=\\&#123;0\\&#125;kubernetestoken.ldif# AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.# CRC32 e502306edn: cn=kubernetestoken,cn=schema,cn=configobjectClass: olcSchemaConfigcn: kubernetestokenolcAttributeTypes: &#123;0&#125;( 1.3.6.1.4.1.18171.2.1.8 NAME 'kubernetesToken' DESC 'Kubernetes authentication token' EQUALITY caseExactIA5Match SUBSTR caseExa ctIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )olcObjectClasses: &#123;0&#125;( 1.3.6.1.4.1.18171.2.3 NAME 'kubernetesAuthenticationO bject' DESC 'Object that may authenticate to a Kubernetes cluster' AUXILIAR Y MUST kubernetesToken ) 接著利用 ldapadd 指令將 Kubernetes token schema 物件新增到當前 LDAP 伺服器中: $ cd ~/kubernetes_tokens/cn=config/cn=schema$ ldapadd -c -Y EXTERNAL -H ldapi:/// -f cn\\=\\&#123;0\\&#125;kubernetestoken.ldifSASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry \"cn=kubernetestoken,cn=schema,cn=config\" 完成後，透過 ldapsearch 指令查詢是否有正確新增 Entry: $ ldapsearch -x -H ldap:/// -LLL -D \"cn=admin,cn=config\" -w password -b \"cn=schema,cn=config\" \"(objectClass=olcSchemaConfig)\" dn -ZEnter LDAP Password:dn: cn=schema,cn=config...dn: cn=&#123;14&#125;kubernetestoken,cn=schema,cn=config 新增測試用 LDAP Groups 與 Users一但 Kubernetes token schema 建立完成後，就能夠新增一些測試用 Groups 來模擬。這邊一樣在openldap-server容器中執行: $ cat &lt;&lt;EOF &gt; groups.ldifdn: ou=People,dc=k8s,dc=comou: PeopleobjectClass: topobjectClass: organizationalUnitdescription: Parent object of all UNIX accountsdn: ou=Groups,dc=k8s,dc=comou: GroupsobjectClass: topobjectClass: organizationalUnitdescription: Parent object of all UNIX groupsdn: cn=kubernetes,ou=Groups,dc=k8s,dc=comcn: kubernetesgidnumber: 100memberuid: user1memberuid: user2objectclass: posixGroupobjectclass: topEOF$ ldapmodify -x -a -H ldap:// -D \"cn=admin,dc=k8s,dc=com\" -w password -f groups.ldifadding new entry \"ou=People,dc=k8s,dc=com\"adding new entry \"ou=Groups,dc=k8s,dc=com\"adding new entry \"cn=kubernetes,ou=Groups,dc=k8s,dc=com\" 當 Group 建立完成後，再接著建立 Users 資訊: $ cat &lt;&lt;EOF &gt; users.ldifdn: uid=user1,ou=People,dc=k8s,dc=comcn: user1gidnumber: 100givenname: user1homedirectory: /home/users/user1loginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersonsn: user1uid: user1uidnumber: 1000userpassword: user1dn: uid=user2,ou=People,dc=k8s,dc=comhomedirectory: /home/users/user2loginshell: /bin/shobjectclass: inetOrgPersonobjectclass: posixAccountobjectclass: topobjectClass: shadowAccountobjectClass: organizationalPersoncn: user2givenname: user2sn: user2uid: user2uidnumber: 1001gidnumber: 100userpassword: user2EOF$ ldapmodify -x -a -H ldap:// -D \"cn=admin,dc=k8s,dc=com\" -w password -f users.ldifadding new entry \"uid=user1,ou=People,dc=k8s,dc=com\"adding new entry \"uid=user2,ou=People,dc=k8s,dc=com\" 完成後，就可以透過指令或是登入 phpLDAPadmin 頁面查看資訊。如下圖所示。 新增 Kubernetes Token 至 Users當 Users 建立完成後，就可以透過執行以下指令來新增每個 User 的 Kubernetes Token: $ cat &lt;&lt;EOF &gt; users.txtdn: uid=user1,ou=People,dc=k8s,dc=comdn: uid=user2,ou=People,dc=k8s,dc=comEOF# 新增 token 腳本指令\b$ while read -r user; dofname=$(echo $user | grep -E -o \"uid=[a-z0-9]+\" | cut -d\"=\" -f2)token=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d \"=+/\" | dd bs=32 count=1 2&gt;/dev/null)cat &lt;&lt; EOF &gt; \"$&#123;fname&#125;.ldif\"$userchangetype: modifyadd: objectClassobjectclass: kubernetesAuthenticationObject-add: kubernetesTokenkubernetesToken: $tokenEOFldapmodify -a -H ldapi:/// -D \"cn=admin,dc=k8s,dc=com\" -w password -f \"$&#123;fname&#125;.ldif\"done &lt; users.txt# outputEnter LDAP Password:modifying entry \"uid=user1,ou=Users,dc=k8s,dc=com\"Enter LDAP Password:modifying entry \"uid=user2,ou=Users,dc=k8s,dc=com\" 部署 LDAP Webhook當 OpenLDAP 都完成，且 Kubernetes 叢集也建立完成後，就可以進入任一 Kubernetes 主節點部署 LDAP Webhook，這邊透過 Git 取得: $ git clone https://github.com/kairen/kube-ldap-authn.git$ cd kube-ldap-authn Golang 版本可以參考 kube-ldap-webhook。 新增一個config.py檔案，並設定查詢時需要的相關內容： LDAP_URL='ldap://172.22.132.150/ ldap://172.22.132.150'LDAP_START_TLS = FalseLDAP_BIND_DN = 'cn=admin,dc=k8s,dc=com'LDAP_BIND_PASSWORD = 'password'LDAP_USER_NAME_ATTRIBUTE = 'uid'LDAP_USER_UID_ATTRIBUTE = 'uidNumber'LDAP_USER_SEARCH_BASE = 'ou=People,dc=k8s,dc=com'LDAP_USER_SEARCH_FILTER = \"(&amp;(kubernetesToken=&#123;token&#125;))\"LDAP_GROUP_NAME_ATTRIBUTE = 'cn'LDAP_GROUP_SEARCH_BASE = 'ou=Groups,dc=k8s,dc=com'LDAP_GROUP_SEARCH_FILTER = '(|(&amp;(objectClass=posixGroup)(memberUid=&#123;username&#125;))(&amp;(member=&#123;dn&#125;)(objectClass=groupOfNames)))' 可以參考 Config example 查看詳細變數說明。 接著將上述的設定檔以 Secret 方式上傳至 Kubernetes 叢集中，然後部署 LDAP webhook 的 DaemonSet 到所有主節點上: $ kubectl -n kube-system create secret generic ldap-authn-config --from-file=config.py=config.py$ kubectl create -f daemonset.yaml$ kubectl -n kube-system get po -l app=kube-ldap-authn -o wideNAME READY STATUS RESTARTS AGE IP NODEkube-ldap-authn-sx994 1/1 Running 0 13s 192.16.35.11 k8s-m1... 部署到所有主節點是在 HA 架構中進行，因為呼叫 API 時，有可能會因為負載平衡關析，而導到不同節點上，這時若沒有在每個節點設定 Webhook 的話，就會認證失敗。 部署成功後，就可以透過 cURL 工具來測試: $ curl -X POST -H \"Content-Type: application/json\" \\ -d '&#123;\"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"spec\": &#123;\"token\": \"&lt;LDAP_K8S_TOKEN&gt;\"&#125;&#125;' \\ http://localhost:8087/authn# output&#123; \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"status\": &#123; \"authenticated\": true, \"user\": &#123; \"groups\": [ \"kubernetes\" ], \"uid\": \"1000\", \"username\": \"user1\" &#125; &#125;&#125; 確認沒問題後，接著在所有主節點上，新增/srv/kubernetes/webhook-authn檔案，並加入以下內容: $ mkdir /srv/kubernetes$ cat &lt;&lt;EOF &gt; /srv/kubernetes/webhook-authnclusters: - name: ldap-authn cluster: server: http://localhost:8087/authnusers: - name: apiservercurrent-context: webhookcontexts:- context: cluster: ldap-authn user: apiserver name: webhookEOF 完成後，修改所有主節點的/etc/kubernetes/manifests目錄底下的kube-apiserver.yaml檔案，其內容修改成如下: ...spec: containers: - command: ... - --runtime-config=authentication.k8s.io/v1beta1=true - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-authn - --authentication-token-webhook-cache-ttl=5m volumeMounts: ... - mountPath: /srv/kubernetes/webhook-authn name: webhook-authn readOnly: true volumes: ... - hostPath: path: /srv/kubernetes/webhook-authn type: File name: webhook-authn 這邊...表示已存在的內容，請不要刪除與變更。 測試功能都完成部署後，就可以進入任一主節點進行測試。這邊建立一個綁定在 user1 Namespace 的 Role 與 RoleBinding 來提供權限測試: $ kubectl create ns user1# 建立 Role$ cat &lt;&lt;EOF | kubectl create -f -kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: readonly-role namespace: user1rules:- apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"]EOF# 建立 RoleBinding$ cat &lt;&lt;EOF | kubectl create -f -kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: readonly-role-binding namespace: user1subjects:- kind: Group name: kubernetes apiGroup: \"\"roleRef: kind: Role name: readonly-role apiGroup: \"\"EOF 在 RoleBinding 中的 subjects 需要對應於 LDAP 中的 Group 資訊。 接著在任一台操作端設定 Kubeconfig 來以 user1 使用者，存取叢集: $ cd$ kubectl config set-credentials user1 --kubeconfig=.kube/config --token=&lt;user-ldap-token&gt;$ kubectl config set-context user1-context \\ --kubeconfig=.kube/config \\ --cluster=kubernetes \\ --namespace=user1 --user=user1 透過 kubectl 來測試權限是否正確設定: $ kubectl --context=user1-context get poNo resources found$ kubectl --context=user1-context run nginx --image nginx --port 80Error from server (Forbidden): deployments.extensions is forbidden: User \"user1\" cannot create deployments.extensions in the namespace \"user1\"$ kubectl --context=user1-context get po -n defaultError from server (Forbidden): pods is forbidden: User \"user1\" cannot list pods in the namespace \"default\" 結語今天簡單實作了 Kubernetes 整合外部認證系統的功能，讓我們能夠以 LDAP 方式來管理 Kubernetes 的普通使用者。可以發現 Kubernetes 在各種方面都考慮了許多擴充方式，不只是網路、儲存等等，在認證與授權部分也提供了一些 API 與機制來實現。現在也有很多開源專案實作了 Auth Webhook 來整合認證，如以下: Dex Kubehook OpenStack Keystone Guard Reference https://github.com/osixia/docker-openldap https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/ https://github.com/torchbox/kube-ldap-authn https://superuser.openstack.org/articles/strengthening-open-infrastructure-integrating-openstack-and-kubernetes/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實作 Kubernetes 裸機 Load Balancer Part3","slug":"ironman2020/day13","date":"2019-09-27T16:00:00.000Z","updated":"2019-12-02T01:49:42.389Z","comments":true,"path":"2019/09/28/ironman2020/day13/","link":"","permalink":"https://k2r2bai.com/2019/09/28/ironman2020/day13/","excerpt":"前言在公有雲環境中，負載平衡器建立與外部 IP 位址分配都能由雲平台完成，且 Kubernetes 也能輕易地用 Cloud Provider 來進行整合。但在地端(或裸機)環境中，原生 Kubernetes 就無法達到這樣功能，必須額外開發系統才能達到目的。而慶幸的是前 Google 工程師也看到這樣問題，因此開發了 MetalLB 來協助非雲平台 Kubernetes 能實現網路負載平衡的提供。且 MetalLB 以 Kubernetes 原生方式，直接在 Kubernetes Service 描述LoadBalancer 類型來要求分配負載平衡器 IP 位址。雖然 MetalLB 確實帶來了好處，但它使用起來沒問題嗎?另外它究竟是怎麼實作的?會不會影響目前叢集網路環境呢? 基於這些問題，今天想透過深入了解 MetalLB 功能與實作原理，以確保發生問題時，能夠快速解決。","text":"前言在公有雲環境中，負載平衡器建立與外部 IP 位址分配都能由雲平台完成，且 Kubernetes 也能輕易地用 Cloud Provider 來進行整合。但在地端(或裸機)環境中，原生 Kubernetes 就無法達到這樣功能，必須額外開發系統才能達到目的。而慶幸的是前 Google 工程師也看到這樣問題，因此開發了 MetalLB 來協助非雲平台 Kubernetes 能實現網路負載平衡的提供。且 MetalLB 以 Kubernetes 原生方式，直接在 Kubernetes Service 描述LoadBalancer 類型來要求分配負載平衡器 IP 位址。雖然 MetalLB 確實帶來了好處，但它使用起來沒問題嗎?另外它究竟是怎麼實作的?會不會影響目前叢集網路環境呢? 基於這些問題，今天想透過深入了解 MetalLB 功能與實作原理，以確保發生問題時，能夠快速解決。 架構MetalLB 是基於標準路由協定實作的 Kubernetes 叢集負載平衡專案。該專案主要以兩個元件實現裸機負載平衡功能，分別為: Controller:是叢集內的 MetalLB 控制器，主要負責分配 IP 給 Kubernetes Service 資源。該元件會監聽 Kubernetes Service 資源的事件，一但叢集有 LoadBalancer 類型的 Service 被新增時，就依據內容從一個 IP 位址池分配負載平衡 IP 給 Service 使用。 Speaker:利用網路協定(L2: ARP/NDP, L3: BGP)告知負載平衡 IP 的目的位址在何處，並且如何路由。Speaker 是一個被安裝在所有節點上的 Controller，而這些叢集上的 Speaker 只會有一個負責處理事情。 TODO: 需補架構、流程圖跟程式細節說明。 在 MetalLB 中，實現了 L2(ARP/NDP) 與 L3(BGP) 的模式，使用者可以透過在 MetalLB 組態檔設定。而這種模式差異在哪邊呢? Layer 2在 L2 模式下，MetalLB Speaker 會在叢集中，選出一個節點以標準地址發現協定(IPv4 用 ARP、IPv6 用 NDP)讓已分配的負載平衡 IP，透過 ARP/NDP 讓本地網路能夠得知目的位址。 TODO: 需補架構、流程圖跟程式細節說明。 這種模式好處在於簡單，且不需要外部硬體或配置。但受限於 L2 網路協定。 BGP在這種模式下，叢集節點的 MetalLB Speaker 會與外部路由器建立 BGP 對等互連，並告訴路由器如何將流量轉發到Service IP，然後藉由 BGP 策略機制，在多個節點之間實現負載平衡，以及細粒度的流量控制。 TODO: 需補架構、流程圖與程式細節說明。 這種模式適合生產環境，但需要更多的外部硬體與配置來達成。但要確保實作 BGP 的 CNI 不會衝突。 結語從了解 MetalLB 原理後，可以更清楚知道一個 Kubernetes 裸機負載平衡該如何實現。但是除了 MetalLB 以外，還有其他方法可以實現嗎?當然有!大家可以參考我之前在社群分享的投影片 How to impletement Kubernetes Bare metal Load Balancer。 TODO: 需加入 IPVS 實現架構 Reference https://metallb.universe.tf/ https://github.com/danderson/metallb https://blog.cybozu.io/entry/2019/03/25/093000 https://www.objectif-libre.com/en/blog/2019/06/11/metallb/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實作 Kubernetes 裸機 Load Balancer Part2","slug":"ironman2020/day12","date":"2019-09-26T16:00:00.000Z","updated":"2019-12-02T01:49:42.388Z","comments":true,"path":"2019/09/27/ironman2020/day12/","link":"","permalink":"https://k2r2bai.com/2019/09/27/ironman2020/day12/","excerpt":"前言昨天文章中，我們提到想要讓同一個叢集能夠支援兩個同樣的 TCP/UDP 曝露給外部存取，雖然能夠利用 Service LoadBalancer 或 NodePort 類型來達到需求，但是這兩者依然存在著限制，比如說 NodePort 使用叢集節點 IP:Port 方式來提供存取，這存在著單點故障問題，且建立一個 Port 就會在所有節點綁定;而 LoadBalancer 則不支援地端分配負載平衡 IP 的機制，只能透過手動在externalIPs欄位指定，若沒指定的話，其功能只是繼承 NodePort 機制，多了個 Target Port 能夠直接存取而已，而且儘管能夠在externalIPs指定 IP，但這些 IP 又該從哪邊來呢?又怎麼分配呢?那該怎麼解決呢? 很慶幸的是有人開發了一個開源專案 MetalLB 來幫助我們解決這些問題，而今天就是要來探討這個專案如何使用。","text":"前言昨天文章中，我們提到想要讓同一個叢集能夠支援兩個同樣的 TCP/UDP 曝露給外部存取，雖然能夠利用 Service LoadBalancer 或 NodePort 類型來達到需求，但是這兩者依然存在著限制，比如說 NodePort 使用叢集節點 IP:Port 方式來提供存取，這存在著單點故障問題，且建立一個 Port 就會在所有節點綁定;而 LoadBalancer 則不支援地端分配負載平衡 IP 的機制，只能透過手動在externalIPs欄位指定，若沒指定的話，其功能只是繼承 NodePort 機制，多了個 Target Port 能夠直接存取而已，而且儘管能夠在externalIPs指定 IP，但這些 IP 又該從哪邊來呢?又怎麼分配呢?那該怎麼解決呢? 很慶幸的是有人開發了一個開源專案 MetalLB 來幫助我們解決這些問題，而今天就是要來探討這個專案如何使用。 環境部署本部分將說明如何部署與使用 MetalLB，並用於後續架構分使用。 節點資訊部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為Ubuntu 18.04+: IP Address Hostname CPU Memory Role 172.22.132.11 k8s-m1 4 16G Master 172.22.132.12 k8s-m2 4 16G Master 172.22.132.13 k8s-m3 4 16G Master 172.22.132.21 k8s-n1 4 16G Node 172.22.132.22 k8s-n2 4 16G Node 172.22.132.31 k8s-g1 4 16G Node 172.22.132.32 k8s-g2 4 16G Node 另外所有 Master 節點將透過 Keepalived 提供一個 Virtual IP 172.22.132.10 作為使用。 事前準備在開始部署時，請確保滿足以下條件: 確保擁有一座版本為 v1.13.0+ 的 Kubernetes 叢集。 使用能夠與 MetalLB 共存的 Network Plugins。 準備一些用於 IPv4 的 IP 位址。必須確保 L2/L3 網路能夠通。這邊將使用172.22.132.150-172.22.132.200。 若使用到 L3 功能，則還需要 BGP 路由。 MetalLB 安裝MetalLB 提供了以容器方式部署到 Kubernetes，且官方也有提供 YAML 讓我們執行: $ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yamlnamespace/metallb-system createdpodsecuritypolicy.policy/speaker createdserviceaccount/controller createdserviceaccount/speaker createdclusterrole.rbac.authorization.k8s.io/metallb-system:controller createdclusterrole.rbac.authorization.k8s.io/metallb-system:speaker createdrole.rbac.authorization.k8s.io/config-watcher createdclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller createdclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker createdrolebinding.rbac.authorization.k8s.io/config-watcher createddaemonset.apps/speaker createddeployment.apps/controller created 使用 Helm 也能參考這邊 Installation With Helm 安裝 只要執行上面指令後，即可完成部署。這時可以透過 kubectl 來查看metallb-system的 Namespace: $ kubectl -n metallb-system get poNAME READY STATUS RESTARTS AGEcontroller-6bcfdfd677-q9fzp 1/1 Running 0 5mspeaker-8648w 1/1 Running 0 5mspeaker-8h4gs 1/1 Running 0 5mspeaker-f9zh4 1/1 Running 0 5mspeaker-dc134 1/1 Running 0 5mspeaker-xnkt5 1/1 Running 0 5mspeaker-zczp5 1/1 Running 0 5mspeaker-zzn5v 1/1 Running 0 5m 若這邊沒問題，就表示已經完成 MetalLB 安裝。接著需要新增 IP Pools 設定，以讓 MetalLB 能夠自動分配 IP 給 Service 的 LoadBalancer 類型使用，下面為一個 L2 的範例: $ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: ConfigMapmetadata: namespace: metallb-system name: configdata: config: | address-pools: - name: default protocol: layer2 auto-assign: true addresses: - 172.22.132.150-172.22.132.200 # - name: production # auto-assign: false # avoid-buggy-ips: true # addresses: # - 172.22.131.0/24EOF 這邊也可以用 CIDR 來表示。更多的設定可以參考 MetalLB Configuration 另外由於測試環境限制，僅以 L2 範例為主。 功能驗證當安裝與設定完成後，即可新增一個 Service 來驗證功能: $ kubectl run nginx --image nginx --port 80deployment.apps/nginx created$ kubectl expose deploy nginx --port 8080 --target-port 80 --type LoadBalancerservice/nginx exposed 建立好後，透過 kubectl 來查看 Service: $ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx LoadBalancer 10.111.63.50 172.22.132.153 8080:31827/TCP 59s 這時會看到，MetalLB 在 Service 為 LoadBalancer 時，會自動從前面設定的default Pool 中，分配一個 IP 給 Service 使用。當有 IP 時，可以嘗試利用 cURL 來存取看看: $ curl 172.22.132.153:8080&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;... 接著若再新建一個不同 Port 的 Service 會怎樣呢? $ kubectl expose deploy nginx --name nginx-80 --port 80 --target-port 80 --type LoadBalancer$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx LoadBalancer 10.111.63.50 172.22.132.153 8080:31827/TCP 5m16snginx-80 LoadBalancer 10.97.188.162 172.22.132.154 80:30218/TCP 4s 大家會發現 MetalLB 又分配了另一個 IP 來使用，這時肯定會覺得這樣是不是每個 IP 只能使用一個 Port，事實上 MetalLB 能夠在 Service 的 Annotation 中，新增metallb.universe.tf/allow-shared-ip欄位來達到 IP Sharing 功能，這邊可以參考 IP Address Sharing。 結語今天利用 MetalLB 達成了裸機負載平衡功能，而明天我將針對該專案進行原理分析。 最近都沒啥時間好好寫，所以一些缺少內容後續會再慢慢補齊。只能說一天寫一篇真的不簡單… Reference https://metallb.universe.tf/ https://medium.com/@JockDaRock/metalloadbalancer-kubernetes-on-prem-baremetal-loadbalancing-101455c3ed48","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實作 Kubernetes 裸機 Load Balancer Part1","slug":"ironman2020/day11","date":"2019-09-25T16:00:00.000Z","updated":"2019-12-02T01:49:42.388Z","comments":true,"path":"2019/09/26/ironman2020/day11/","link":"","permalink":"https://k2r2bai.com/2019/09/26/ironman2020/day11/","excerpt":"前言在生產環境中，通常都是以 Ingress 方式來曝露 HTTP/HTTPS 存取服務，而前幾天分享如何透過 NGINX Ingress、ExternalDNS 與 CoreDNS 等，就是在自建 Kubernetes 上實現這樣功能，讓我們以 L7 網路協定功能來達到服務存取目的。但在實際應用中，還是有很多需要以 TCP/UDP 方式存取或連接服務，這樣該如何進行呢?相信有研究過 Kubernetes 朋友都知道 NGINX Ingress 也支援了 TCP/UDP 的反向代理，這表示 Ingress 也能支援 TCP/UDP。但另一個問題來了，如果叢集有兩個服務需要用到同一個 TCP/UDP Port 時，這該怎麼辦呢?這時 Ingress 就無法很好地達到該需求，那我們該怎麼做呢?事實上，Kubernetes Service 的 LoadBalancer 類型就能達到，但是僅限於公有雲服務上才能完成，在地端的 Kubernetes 存在著一些限制，使得無法滿足需求。而這次主題就是要針對該議題進行說明與嘗試實作。","text":"前言在生產環境中，通常都是以 Ingress 方式來曝露 HTTP/HTTPS 存取服務，而前幾天分享如何透過 NGINX Ingress、ExternalDNS 與 CoreDNS 等，就是在自建 Kubernetes 上實現這樣功能，讓我們以 L7 網路協定功能來達到服務存取目的。但在實際應用中，還是有很多需要以 TCP/UDP 方式存取或連接服務，這樣該如何進行呢?相信有研究過 Kubernetes 朋友都知道 NGINX Ingress 也支援了 TCP/UDP 的反向代理，這表示 Ingress 也能支援 TCP/UDP。但另一個問題來了，如果叢集有兩個服務需要用到同一個 TCP/UDP Port 時，這該怎麼辦呢?這時 Ingress 就無法很好地達到該需求，那我們該怎麼做呢?事實上，Kubernetes Service 的 LoadBalancer 類型就能達到，但是僅限於公有雲服務上才能完成，在地端的 Kubernetes 存在著一些限制，使得無法滿足需求。而這次主題就是要針對該議題進行說明與嘗試實作。 在開始實作前，我們先來聊聊目前 Kubernetes 支援的存取方式吧。 Host Network在生產環境中，有些容器應用程式會需要用到主機層面的網路資源，這時我們能夠在 Pod 設定hostNetwork: true來讓該 Pod 能夠使用主機的 Network namespace。而這種方法有幾個優點: 沒有 NAT 轉換，因此效能佳。 故障排除較簡單。 設計單純。 雖然這在某些需求上使用很合理，，但用於 Pod 曝露給外部存取的話，就會面臨一些問題，比如說:由於 Pod 相依於主機，因此 Pod 使用的埠口(Port)會佔用主機、每次啟動會被排程道不同節點上，因此導致存取 IP 改變\b。如下圖建立了兩個應用程式，並分別使用 Host Network 功能。 從上圖中，可以看到使用 Host Network 的 Pod 能夠以主機 IP 位址來存取容器中的應用。假設有個 A 服務寫死要跟這模式的 Pod 溝通的 IP 位址，然後突然發生 HostA 節點故障狀況，這時 A 服務的 Pod 可能被搬移至其他節點上，因此存取 Pod 端點也跟著改變了，這時 A 服務就會發生連不到的狀況。 從上圖中，可以看到這種模式無法透過一個虛擬 IP 來提供外部存取，因此該節點故障時，就會影響到該 Pod。 Host Port除了 Host network 以外，Kubernetes 也支援在 Pod 指定hostPort與containerPort欄位來建立 Port 映射功能，讓某個 Pod 的 Port 能夠以主機的IP:Port來存取服務。該模式與 Host network 好處差不多，但相對 Host network 來的安全一點，因為不需使用到主機的 Network namespace，但要注意此功能的支援，在目前版本中需要依賴 CNI 來完成。 另外使用 Host port 的容器只能被排程到沒有指定 Port 衝突的節點上，而這種方式也不適合提供給應用程式級的外部存取使用，比較適用於系統級背景服務。 圖片待補。 ServiceKubernetes Service 是一種抽象資源，目的是希望把外部存取 Pod 機制進行分層處理，因為 Kubernetes Pod 監聽的 IP 位址不能作為外部存取端點來使用，這是由於 Pod 在叢集中，會隨著一些狀況而動態改變，或者重新建立，這使得原本的 Pod IP 位址也跟著變更，那如果有個服務寫死存取某個 IP 時，就會發生問題。因此 Kubernetes 利用 Service、EndPoint 與 Pod 三者關析來達到 Pod IP 改變時，也能透過統一的 IP 端點存取到 Pod。 預設情況下，Kubernetes Service 使用 ClusterIP 類型讓使用者可以在叢集內部存取 Pod。當將 Service 設定為 ClusterIP 時，Kubernetes 會自動從設定的網段分配一個名為 Cluster IP 的虛擬 IP 給 Service，接著利用 Linux 網路技術將指定的 Cluster IP 以 NAT 方式，轉發到 Pod 上來達成連接，因此大家會發現 Cluster IP 並不會真的存在實體網卡上。而這背後實現者就是 kube-proxy 這個元件，它利用 IPTables 或 IPVS 等技術，來實現 Service 運作機制，如下圖所示。 而使用過 Kubernetes ClusterIP 類型的朋友會發現，該類型僅能在 Kubernetes 叢集內部存取，那如果要曝露服務給外部存取的話呢?這就要用到另兩個 Kubernetes Service 的類型了。下面我們也簡單的介紹一下。 NodePortService 的 NodePort 是一種很常用到的方式。由於 Kubernetes Service 預設都是使用 Cluster IP 來提供存取，但是 Cluster IP 只能在叢集內部存取用，當要給外部時，就無法達到需求，這時就能利用 NodePort 來達成。一但設定成 NodePort 時，叢集中裝有 kube-proxy 的節點，就會幫你把一個亂數 Port 綁定到主機上，接著利用 IPtables/IPVS 設定 NAT 轉發到關聯的 Pod 上，這樣只要存取任一個 Kubernetes 節點的 IP:Port 就能夠存取到 Pod。 TODO: 內容待補。 LoadBalancerLoadBalancer 是提供給公有雲使用的類型，如果一個 Service 設定為這個類型時，就會利用叢集中的 Cloud Provider 元件去呼叫公有雲的服務 API 來建立負載平衡器，接著跟這個 Pod 進行綁定。但在地端部署時，該類型會實現類似 NodePort 機制，但可以額外設定 externalIPs 來直接存取指定的 Port。 TODO: 內容待補。 IngressIngress 雖然提供了許多負載平衡器的特性，如 HTTP/HTTPs 路由、SSL Termination、TCP/UDP 負載平衡等等。 TODO: 內容待補。 結語今天了解了目前 Kubernetes 支援的外部存取服務方式。從中，我們知道在地端 Kubernetes 叢集中，並不支援網路負載平衡器的實現(Service 類型為 LoadBalancer)，因為 Kubernetes 內建的 LB 功能，幾乎都用於公有雲(GCP、AWS、Azure 等)服務上。但在自建(地端)叢集中，若沒有 IaaS 平台或環境能夠使用的話，就會在建立 LoadBalancer Service 時，一直處於pending狀態。 那麼我們該如何解決呢?下一篇將分享一些使用過的專案與做法。 今天由於一些原因沒有太多時間寫完整，這部分會在之後慢慢調整上來。 Reference https://www.hwchiu.com/kubernetes-service-i.html https://www.hwchiu.com/kubernetes-service-ii.html https://www.hwchiu.com/kubernetes-service-iii.html https://medium.com/@tao_66792/how-does-the-kubernetes-networking-work-part-1-5e2da2696701 https://medium.com/practo-engineering/networking-with-kubernetes-1-3db116ad3c98 https://collabnix.com/3-node-kubernetes-cluster-on-bare-metal-system-in-5-minutes/ https://medium.com/@maniankara/kubernetes-tcp-load-balancer-service-on-premise-non-cloud-f85c9fd8f43c https://jimmysong.io/posts/accessing-kubernetes-pods-from-outside-of-the-cluster/ https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies https://thinkit.co.jp/article/13739","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實現 Kubernetes Service/Ingress 同步設定 DNS 資源紀錄 Part2","slug":"ironman2020/day10","date":"2019-09-24T16:00:00.000Z","updated":"2019-12-02T01:49:42.388Z","comments":true,"path":"2019/09/25/ironman2020/day10/","link":"","permalink":"https://k2r2bai.com/2019/09/25/ironman2020/day10/","excerpt":"前言有時地端 Kubernetes 會需要提供給內部團隊使用，而團隊人員若希望以域名方式直接存取 Kubernetes 上的服務時，就必須建立一套機制。但這樣需求中，也增加了維運人員的負擔，因為若沒有自動化機制，就要在 Kubernetes Ingress/Service 變動時，手動處理 DNS 資源紀錄，以確保內部團隊能夠解析到位址。 基於此原因，今天延續在實現 Kubernetes Service/Ingress 同步設定 DNS 資源紀錄 Part1的文章提到的架構，實際將該架構部署到一座地端 Kubernetes 叢集上測試，並透過實作過程來了解其功能是如何運作的。","text":"前言有時地端 Kubernetes 會需要提供給內部團隊使用，而團隊人員若希望以域名方式直接存取 Kubernetes 上的服務時，就必須建立一套機制。但這樣需求中，也增加了維運人員的負擔，因為若沒有自動化機制，就要在 Kubernetes Ingress/Service 變動時，手動處理 DNS 資源紀錄，以確保內部團隊能夠解析到位址。 基於此原因，今天延續在實現 Kubernetes Service/Ingress 同步設定 DNS 資源紀錄 Part1的文章提到的架構，實際將該架構部署到一座地端 Kubernetes 叢集上測試，並透過實作過程來了解其功能是如何運作的。 環境建置本部分將說明如何建立昨天文章提到的架構。 節點資訊部署沿用之前文章建置的 HA 環境進行測試，全部都採用裸機部署，作業系統為Ubuntu 18.04+: IP Address Hostname CPU Memory Role 172.22.132.11 k8s-m1 4 16G Master 172.22.132.12 k8s-m2 4 16G Master 172.22.132.13 k8s-m3 4 16G Master 172.22.132.21 k8s-n1 4 16G Node 172.22.132.22 k8s-n2 4 16G Node 172.22.132.31 k8s-g1 4 16G Node 172.22.132.32 k8s-g2 4 16G Node 另外所有 Master 節點將透過 Keepalived 提供一個 Virtual IP 172.22.132.10 作為使用。 部署 DNS 系統首先取得 Kubernetes 部署檔案的 Git 存放庫，並進入 addons 目錄: $ git clone https://github.com/cloud-native-taiwan/kourse.git$ cd kourse/addons 接著利用 sed(or perl) 工具修改等下要被部署的檔案(範例原本是 Workshop 使用，內容 IP 打死): $ export VIP=\"172.22.132.10\"$ export DN=\"k8s.kairen.tw\"$ sed -i \"s/192.16.35.12/$&#123;VIP&#125;/g\" ingress-controller/service.yml$ sed -i \"s/192.16.35.12/$&#123;VIP&#125;/g\" dns/coredns/service-tcp.yml$ sed -i \"s/192.16.35.12/$&#123;VIP&#125;/g\" dns/coredns/service-udp.yml$ sed -i \"s/k8s.local/$&#123;DN&#125;/g\" dns/coredns/configmap.yml VIP 請依據自己環境部署為主。DN由於當初是寫死用於教學用，這邊可以自行修改。 NGINX Ingress Controller當取得到檔案，且修改需要的內容後，就可以透過 kubectl 來部署元件到 Kubernetes 叢集中。首先由於測試會用到 Ingress，因此需要一個 Ingress Controller，可以透過以下指令進行: $ kubectl apply -f ingress-controller/namespace/ingress-nginx createdconfigmap/nginx-configuration createdconfigmap/tcp-services createdconfigmap/udp-services createdserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createddeployment.apps/nginx-ingress-controller createdservice/ingress-nginx created 若環境不同，請修改addons/ingress-controller/底下 YAML 檔案。 完成後，查看 ingress-nginx Namespace 是否有正確啟動 Ingress controller: $ kubectl -n ingress-nginx get po,svcNAME READY STATUS RESTARTS AGEpod/nginx-ingress-controller-85b6f5f57d-cs4pw 1/1 Running 0 101sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/ingress-nginx LoadBalancer 10.109.121.69 172.22.132.10 80:30297/TCP,443:32686/TCP 101s 沒問題後，利用 cURL 工具存取服務來驗證: $ curl 172.22.132.10&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.15.6&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 也可以以瀏覽器開啟External-IP:80頁面來查看。 CoreDNS + etcd接著要部署一套 DNS 來提供 FQDN 查詢使用。首先建立一個 Namespace 用來管理這些部署的元件，以確保不會跟 Kubernetes 叢集中的其他服務混肴: $ kubectl apply -f dns/namespace/ddns created Namespace 建立後，即可部署 etcd 與 CoreDNS: $ kubectl apply -f dns/etcd/ -f dns/coredns/deployment.apps/coredns-etcd createdservice/coredns-etcd createdconfigmap/coredns createddeployment.apps/coredns createdservice/coredns-tcp createdservice/coredns-udp created$ kubectl -n ddns get po,svcNAME READY STATUS RESTARTS AGEpod/coredns-7cc8dcc778-9xght 1/1 Running 0 2m47spod/coredns-etcd-675b96b65-2kmdb 1/1 Running 0 2m47sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/coredns-etcd ClusterIP 10.107.159.40 &lt;none&gt; 2379/TCP,2380/TCP 2m47sservice/coredns-tcp LoadBalancer 10.105.156.110 172.22.132.10 53:32627/TCP,9153:31482/TCP 2m46sservice/coredns-udp LoadBalancer 10.105.63.16 172.22.132.10 53:30388/UDP 2m46s 若環境不同，請修改dns/coredns/底下 YAML 檔案。 完成後，利用 dig 工具來查看 DNS SOA(Start Of Authority) 是否正常: $ dig @172.22.132.10 SOA k8s.kairen.tw +noall +answer; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @172.22.132.10 SOA k8s.kairen.tw +noall +answer; (1 server found);; global options: +cmdk8s.kairen.tw. 30 IN SOA ns.dns.k8s.kairen.tw. hostmaster.k8s.kairen.tw. 1569412779 7200 1800 86400 30 確認沒問題後，即可在測試機器設定 DNS Nameserver，如下圖。 ExternalDNS當用於查詢的 CoreDNS 與儲存 DNS 資源紀錄的 etcd 完成後，即可部署 ExternalDNS 來提供同步 Kubernetes Ingress/Service 的 DNS 資源紀錄: $ kubectl apply -f dns/externaldns/deployment.apps/external-dns createdclusterrole.rbac.authorization.k8s.io/external-dns createdclusterrolebinding.rbac.authorization.k8s.io/external-dns-viewer createdserviceaccount/external-dns created$ kubectl -n ddns get po -l k8s-app=external-dnsNAME READY STATUS RESTARTS AGEexternal-dns-d674c579f-r5xp6 1/1 Running 0 99s 若環境不同，請修改addons/dns/external-dns/底下 YAML 檔案。 完成後，檢查是否正確執行: $ kubectl -n ddns logs -f external-dns-d674c579f-r5xp6...time=\"2019-09-25T12:05:57Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx\"time=\"2019-09-25T12:05:57Z\" level=debug msg=\"No endpoints could be generated from service ddns/coredns-etcd\"time=\"2019-09-25T12:05:57Z\" level=debug msg=\"No endpoints could be generated from service ddns/coredns-tcp\" 到這邊就完成所有元件部署了，接下來就能實際測試功能囉。 功能驗證一但該系統在 Kubernetes 建立完成後，就能夠執行一些簡單範例進行驗證。這邊我們使用名為 cheese 的範例來驗證，這個範例會建立三個不同網頁，並利用 Ingress 來導向指定頁面: stilton.k8s.kairen.tw 將導到斯蒂爾頓起司頁面。 cheddar.k8s.kairen.tw 將導到切達起司頁面。 wensleydale.k8s.kairen.tw 將導到文斯勒德起司起司頁面。 在開始前，先用 dig 工具查看一下 A record 是否能夠解析到: $ dig @172.22.132.10 A stilton.k8s.kairen.tw +noall +answer; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @172.22.132.10 A stilton.k8s.kairen.tw +noall +answer; (1 server found);; global options: +cmd 若沒有的話，就可以執行以下指令來部署 cheese 範例: $ export DN=\"k8s.kairen.tw\"$ cd kourse/practical-k8s/practical-apps/$ sed -i \"s/example.k8s.local/$&#123;DN&#125;/g\" lab6-cheese/cheese-ing.yml$ kubectl apply -f lab6-cheese/deployment.apps/stilton createddeployment.apps/cheddar createddeployment.apps/wensleydale createdingress.networking.k8s.io/cheese createdservice/stilton createdservice/cheddar createdservice/wensleydale created$ kubectl get po,svc,ingNAME READY STATUS RESTARTS AGEpod/cheddar-59666cdbc4-mzlsl 1/1 Running 0 74spod/stilton-d9485c498-g9xp4 1/1 Running 0 74spod/wensleydale-79f5fc4c5d-pv9cg 1/1 Running 0 74sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/cheddar ClusterIP 10.108.201.33 &lt;none&gt; 80/TCP 74sservice/stilton ClusterIP 10.105.135.218 &lt;none&gt; 80/TCP 74sservice/wensleydale ClusterIP 10.109.92.103 &lt;none&gt; 80/TCP 73sNAME HOSTS ADDRESS PORTS AGEingress.extensions/cheese stilton.k8s.kairen.tw,cheddar.k8s.kairen.tw,wensleydale.k8s.kairen.tw 172.22.132.10 80 74s 建立完成後，當 ExternalDNS 輪詢時，就會將 Ingress/Service 產生成 DNS 資源紀錄，並儲存到 etcd 中。我們可以利用 kubectl logs 來查看: $ kubectl -n ddns logs -f external-dns-d674c579f-r5xp6...time=\"2019-09-25T12:37:07Z\" level=debug msg=\"Endpoints generated from ingress: default/cheese: [stilton.k8s.kairen.tw 0 IN A 172.22.132.10 [] cheddar.k8s.kairen.tw 0 IN A 172.22.132.10 [] wensleydale.k8s.kairen.tw 0 IN A 172.22.132.10 []]\"... 都沒問題後，透過 dig 工具解析看看 A record 是否正常: $ dig @172.22.132.10 A stilton.k8s.kairen.tw +noall +answer; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @172.22.132.10 A stilton.k8s.kairen.tw +noall +answer; (1 server found);; global options: +cmdstilton.k8s.kairen.tw. 300 IN A 172.22.132.10 這時也能透過瀏覽器查看stilton.k8s.kairen.tw、cheddar.k8s.kairen.tw與wensleydale.k8s.kairen.tw頁面。如下圖所示 結語今天實作了自動化同步 Ingress/Service 的 DNS 資源紀錄的功能。實作中，涉及了 L7 網路協定功能(HTTP, HTTPS, DNS)在地端 Kubernetes 上的實現。我們可以發現 Kubernetes 社區的生態圈，在各種需求上，已經有很多完善的工具或元件可以使用，像是 ExternalDNS 就是很好例子，讓人可以不用手動設定 DNS 資源紀錄，自動從 Kubernetes Ingress/Service 產生。 不過今天做法是地端(或內部)部署情境使用，若公司網域是由供應商(如 CloudFlare 或 AWS Route53)提供的話，就必須調整 ExternalDNS Providers 來支援。 Reference https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/cloudflare.md https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/ https://zhengyinyong.com/coredns-basis.html https://www.hwchiu.com/ingress-1.html https://kubernetes.github.io/ingress-nginx/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實現 Kubernetes Service/Ingress 同步設定 DNS 資源紀錄 Part1","slug":"ironman2020/day09","date":"2019-09-23T16:00:00.000Z","updated":"2019-12-02T01:49:42.388Z","comments":true,"path":"2019/09/24/ironman2020/day09/","link":"","permalink":"https://k2r2bai.com/2019/09/24/ironman2020/day09/","excerpt":"前言前幾天都在分享關於叢集部署與升級事情，今天來聊聊在地端 Kubernetes 常見的需求功能吧。在生產環境中，我們會將網站或系統放到 Kubernetes 上執行與管理，再利用 Service 機制把服務暴露給外部存取使用，但 Service 在預設情況下僅能支援第四層網路協定(L4，TCP/UDP)功能，故無法設定完整網域名稱(Fully Qualified Domain Name，FQDN)來存取服務，這時大家肯定會想到 Kubernetes 的 Ingress 功能，因為 Ingress 能夠實現第七層網路協定(L7)功能，並以域名(Domain name)形式來對應到 Service 的 Pod 端點。 但是地端環境不像公有雲有各種基礎建設服務(Infrastructure as a Service，IaaS)可以輕易使用與整合(比如:透過 Cloud Provider 讓 Service 整合負載平衡服務、利用 DNS 服務來對應到 Service 的負載平衡 IP 等等)，這時如果想要實現自動同步 Kubernetes Service/Ingress 資源，來設定 FQDN 的話，就會需要建立一套網域名稱系統(Domain Name System，DNS)，並實作同步 Kubernetes 物件設定 DNS 資源紀錄(DNS record)的控制器。慶幸的是，Kubernetes 社區已經有相關元件可以協助我們實踐這些機制，今天就將針對這部分來說明與實現。","text":"前言前幾天都在分享關於叢集部署與升級事情，今天來聊聊在地端 Kubernetes 常見的需求功能吧。在生產環境中，我們會將網站或系統放到 Kubernetes 上執行與管理，再利用 Service 機制把服務暴露給外部存取使用，但 Service 在預設情況下僅能支援第四層網路協定(L4，TCP/UDP)功能，故無法設定完整網域名稱(Fully Qualified Domain Name，FQDN)來存取服務，這時大家肯定會想到 Kubernetes 的 Ingress 功能，因為 Ingress 能夠實現第七層網路協定(L7)功能，並以域名(Domain name)形式來對應到 Service 的 Pod 端點。 但是地端環境不像公有雲有各種基礎建設服務(Infrastructure as a Service，IaaS)可以輕易使用與整合(比如:透過 Cloud Provider 讓 Service 整合負載平衡服務、利用 DNS 服務來對應到 Service 的負載平衡 IP 等等)，這時如果想要實現自動同步 Kubernetes Service/Ingress 資源，來設定 FQDN 的話，就會需要建立一套網域名稱系統(Domain Name System，DNS)，並實作同步 Kubernetes 物件設定 DNS 資源紀錄(DNS record)的控制器。慶幸的是，Kubernetes 社區已經有相關元件可以協助我們實踐這些機制，今天就將針對這部分來說明與實現。 架構與元件介紹在開始實現前，我們先來簡單了解一下會使用到的元件，並說明將會運用在什麼方面。 CoreDNSCoreDNS 是經過 CNCF 孵化畢業的開源 DNS 專案，該專案是基於 Caddy 的一部分開發而來，由於傳統的 DNS 無法很彈性地加入插件，因此非常不靈活，但 CoreDNS 實作了一套中介軟體(Middleware)介面，因此我們能夠很輕易實現插件來完成客制的功能(如 Log、Cache 等等)，也因為這關析，CoreDNS 能夠將資源紀錄儲存至 Redis、etcd 這種 Key/Value 儲存系統上。值得再提的是，CoreDNS 在 v1.11 版本正式取代 KubeDNS，我們能夠透過開始 Kubernetes plugin 來達到叢集服務發現功能。 在實作中，我們將用來給 Kubernetes 同步設定 DNS 資源紀錄使用，並提供解析 Kubernetes Service/Ingress 的 DNS 資源紀錄。 etcdetcd 是一套分散式鍵值(Key/Value)儲存系統，類似 ZooKeeper 與 Consul，而 etcd 共識機制上採用 Raft 演算法來處理多節點共識問題，另外 etcd 支援了 REST API、JSON 格式與 SSL 等等功能。 在這邊主要提供儲存 DNS 資源紀錄，主要作為 CoreDNS 與 ExternalDNS 溝通的中介。 etcd 也被用於 Kubernetes 與 Cloud Foundry 專案中。 ExternalDNSExternalDNS 是 Kubernetes 社區的孵化專案，目的是協助同步 Kubernetes Service/Ingress 的資源，並將內容轉成 DNS 資源紀錄設定的 DNS 供應商與服務上。這邊將利用 ExternalDNS 定期同步 Kubernetes 資源來轉換成 DNS 資源紀錄，並將轉換的紀錄存儲到 etcd 上，接著利用 CoreDNS 的 etcd plugin 來完成資源紀錄查詢功能。 ExternalDNS 除了支援 CoreDNS 以外，也可以設定各種 DNS 服務。 NGINX Ingress ControllerNGINX Ingress 是以 NGINX 引擎為基礎開發的 Kubernetes 控制器與代理系統，主要讓 Kubernetes 能夠透過 L7 協定功能來提供外部存取容器。NGINX Ingress 會監聽 Kubernetes Ingress 資源，並依據內容產生 NGINX 設定，然後熱更新給 NGINX 使用，因此當存取 NGINX Ingress 時，就能依據設定檔的內容轉送給對應的 Kubernetes Service。 Ingress 控制器除了社區提供的專案外，也能夠使用 Traefik、Kong、HAProxy 等等。 執行流程本節說明架構上的運作流程。這邊會分成兩種方式實現，分別為 Service 與 Ingress。 Ingress: 當建立一個 Ingress 到叢集時，NGINX Ingress 會接收到 API 資源的事件更新，並在新增與更新事件中，取得 Ingress 資訊來設定 NGINX。而當 Ingress 資源順利完成功能後，ExternalDNS 會以輪詢方式取得所有 Namespace(或指定的)中的 Ingress 資源(Ingress 必須被分派 Host IP 才能進行)，並從 Ingress 資源的spec.rules取出 host 資訊，以產生 DNS 資源紀錄(如: A record)，接著將產生的紀錄透過 etcd 儲存，這樣當 CoreDNS 收到 FQDN 查詢請求時，就可以利用 etcd 作為 DNS 資源紀錄後端來來辨識導向。 Service: 當建立一個有metadata.annotations.external-dns.alpha.kubernetes.io/hostname 的 Service 時，ExternalDNS 會在輪詢期間取得該欄位的值來產生 DNS 資源紀錄，然後同樣利用 etcd 作為儲存，並在對 CoreDNS 發起 FQDN 查詢請求時，能夠到 etcd 查詢 DNS 資源紀錄以解析結果返回給客戶端。 如上圖所示，我們簡單拆解不同步驟來說明。 建立一個有以下範例 Annotations 的 Service/Ingress。 apiVersion: v1kind: Servicemetadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nginx.k8s.local # 將被自動註冊的 domain name.spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-ingressspec: rules: - host: nginx.k8s.local # 將被自動註冊的 domain name. http: paths: - backend: serviceName: nginx servicePort: 80 當使用 Service 時，需要設定成 Load Balancer 或 NodePort。 當使用 Ingress 時，不需要在 Service 塞入external-dns.alpha.kubernetes.io/hostname欄位，且不需要使用 NodePort 與 LoadBalancer。 ExternalDNS 透過輪詢取得 Service/Ingress 資源，取出將被用來產生 DNS 資源紀錄的資訊，接著在產生完成後，利用 etcd 進行儲存記錄。 當客戶端存取 nginx.k8s.local 時，將對 CoreDNS 發起 FQDN 查詢請求，這時 CoreDNS 會到 etcd 查找 DNS 資源紀錄，以解析指定的 IP 回應給客戶端， 這時客戶端會接受到解析結果，被正確地導向到解析的 IP 位址。 若使用 Service 時，因為不是走 HTTP/HTTPS 協定，因此要輸入 TCP/UPD 的 Port。用 Ingress 則以域名存取即可，因為 NGINX Ingress 提供了一個 NGINX 代理後端，它會幫你轉發至 Kubernetes 內部 Service。 結語今天說明如何在地端實現自動更新 Kubernetes Service/Ingress 到 DNS 中的架構與流程，過程中大家可以了解到 Kubernetes 社區利用 Controller Pattern 機制來擴展功能，以讓供應商整合自家功能與服物到 Kubernetes 中，像今天提到的 ExternalDNS 就是實現自動同步 DNS 資源紀錄到各種 DNS 服務的 Kubernetes 控制器。 今天只是說明元件架構與流程，明天我們將實際的實現該功能。 Reference https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/cloudflare.md https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/ https://zhengyinyong.com/coredns-basis.html https://www.hwchiu.com/ingress-1.html","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"分析 Kubeadm 叢集更新流程","slug":"ironman2020/day08","date":"2019-09-22T16:00:00.000Z","updated":"2019-12-02T01:49:42.388Z","comments":true,"path":"2019/09/23/ironman2020/day08/","link":"","permalink":"https://k2r2bai.com/2019/09/23/ironman2020/day08/","excerpt":"前言昨天分享了如何使用 Kubeadm 工具來升級既有叢集，過程中可以發現 Kubeadm 升級時，將許多複雜的工作做了許多簡化，因此讓不熟悉 Kubernetes 的人也能輕易達成叢集元件更新事情。雖然工具協助我們更簡單的達成某些功能是很好的事，但不是所有狀況都能透過 Kubeadm，假設在升級時，發生問題的話，要該怎麼辦呢?這時可能會因為不熟悉整個升級過程，而不知如何下手來解決問題。基於上述原因，今天就是要來分析 Kubeadm 的叢集更新是如何實作的，以讓我們能夠在發生問題時，更快的解決。","text":"前言昨天分享了如何使用 Kubeadm 工具來升級既有叢集，過程中可以發現 Kubeadm 升級時，將許多複雜的工作做了許多簡化，因此讓不熟悉 Kubernetes 的人也能輕易達成叢集元件更新事情。雖然工具協助我們更簡單的達成某些功能是很好的事，但不是所有狀況都能透過 Kubeadm，假設在升級時，發生問題的話，要該怎麼辦呢?這時可能會因為不熟悉整個升級過程，而不知如何下手來解決問題。基於上述原因，今天就是要來分析 Kubeadm 的叢集更新是如何實作的，以讓我們能夠在發生問題時，更快的解決。 Kubeadm upgrade 流程從昨天操作的流程中，我們了解到透過 Kubeadm 更新叢集的指令流程: \b圖中，我們會發現 kubeadm 最少用一個指令就完成更新的過程，但是事實上這一個指令背後做了許多事情，接下來各小節將針對圖中的指令進行解析。 kubeadm upgrade plan該指令主要檢查是否可以升級目前叢集到指定的 Kubernetes 版本，這過程會進行驗證以下幾件事情: 檢查當前叢集中的 Kubeadm 組態檔。 檢查當前叢集是否處於健康狀態。 取得指定版本是否可以使用。若位指定版本的話，則以當前釋出的最新穩定版本。 確認是否符合 Version Skew Policies。 若上面執行都沒問題的話，會顯示當前叢集元件版本，以及目標叢集元件版本。 這邊做的事情很簡單，主要都是檢查當前與目標版本狀態，並顯示資訊讓使用者知道狀況。 kubeadm upgrade apply一但檢查都沒問題後，我們就可以執行kubeadm upgrade apply來進行主節點的更新，這時 kubeadm 會開始進行以下流程: 檢查組態參數是否有效，並載入預設參數與使用者輸入的參數。另外也會檢查當前是否為 root。 kubeadm 會透過 admin user 來與 API server 溝通，並檢查當前叢集是否處於健康狀態，以及所有節點是否處於 Ready 狀態。當檢查都完成後，會讀取叢集中的 kubeadm InitConfiguration 組態檔案，並將內容調整成新版的資訊後，更新至當前叢集中，以在後續執行更新時使用。另外該階段也會再次檢查是否符合 Version Skew Policies。 當上述確認後，kubeadm 會與 API server 溝通，以建立一個 DaemonSet 來取得指定版本的控制平面元件容器映像檔。這邊原理是利用 Kubernetes 機制來下載映像檔，當 Pod 被建立時，容器 Runtime 就會先下載映像檔才啟動，因此可以確保指定容器映像檔被載入到節點上。這也是為何前面需要確保節點處於 Ready 狀態原因。另外 Kubeadm 不實作直接從容器 Runtime 拉取映像檔，是因為現在有太多容器 Runtime 被使用，因此這麼做的話，會增加程式與維護的困難與複雜性。 一但映像檔載入完成後，kubeadm 就會執行元件升級。首先會備份 etcd 的 mainifest 檔案，並更新成新版本內容，接著等待 etcd 啟動。接著會開始對控制元件進行更新，過程中跟 etcd 類似，會先寫入新版本 YAML 到 /tmp 底下，接著將新版本檔案放到 mainifest 目錄，然後將舊的 mainifest 檔案進行備份，最後等待監聽 kubelet 重新啟動 Static Pod 的變動。另外過程中，如果 kubeadm 檢查發現相關憑證將在 180 天過期的話，kubeadm 會自動更新相關 TLS 憑證，以確保叢集不會因為過期而出問題。 若控制平面元件都升級完成的話，會開始進行以下步驟: 將新版本的 kubeadm ClusterStatus 與 kubelet 組態檔更新至叢集中，並建立相對應的 RBAC 權限，以利其他節點使用。 從叢集取得最新版本的 kubelet 內容，並覆寫到/var/lib/kubelet/config.yaml中。 更新當前節點的 /var/lib/kubelet/kubeadm-flags.env 檔案，以確保使用新版本的參數。 設定主節點的 Annotations(CRISocket)。 更新 CoreDNS 與 kube-proxy 的 DaemonSet 內榮，以透過 Rolling upgrade 方式更新至新版本。 kubeadm upgrade node在更新叢集中，kubeadm 會分為第一主節點、其他主節點與工作節點來進行，而想要更新到新版本，必須先在任一台主節點上執行kubeadm upgrade apply指令，來確保新版本的組態檔被新增到 Kubernetes 叢集中。一但有了新版本組態資訊後，就能在其他節點執行kubeadm upgrade node進行更新，而執行該指令時，又會依據節點的角色分成主節點與工作節點進行，這兩者流程如下所示: 取得叢集中的 kubeadm ClusterConfiguration 資訊，並識別該節點為什麼角色。若是主節點的話，則會進行控制平面元件的更新(流程同 kubeadm upgrade apply)。 從叢集取得最新版本的 kubelet 內容，並覆寫到/var/lib/kubelet/config.yaml中。 更新當前節點的 /var/lib/kubelet/kubeadm-flags.env 檔案，以確保使用新版本的參數。 結語今天簡單分析 kubeadm 更新流程，以學習 kubeadm 叢集更新的實踐方式，透過瞭解其流程也能確保發生問題時，能更快知道問題點。且實際了解後，才能知道如何針對調整過參數的叢集進行更新，因為執行預設值時，kubeadm 會將過去的設定覆蓋掉，並且以升級版本的最佳實踐組態來設定，因此可能會造成原本舊版本叢集有開這功能，但新版本則沒有問題。 最近時間被一些事情影響到，沒辦法在短時間內，寫出更多詳細與完整內容，這邊深感抱歉… 我這之後會再利用時間繼續將這挑戰文章優化與調整，以讓大家可以看到更多東西。 Reference https://static.sched.com/hosted_files/kccna18/cf/KubeCon_2018_NA-kubeadm-deep-dive.pdf https://v1-15.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/ https://github.com/kubernetes/kubernetes/tree/master/cmd/kubeadm","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"動手嘗試 Kubernetes 叢集更新吧","slug":"ironman2020/day07","date":"2019-09-21T16:00:00.000Z","updated":"2019-12-02T01:49:42.388Z","comments":true,"path":"2019/09/22/ironman2020/day07/","link":"","permalink":"https://k2r2bai.com/2019/09/22/ironman2020/day07/","excerpt":"前言許多人都知道，讓應用程式保持較新的狀態，以優化安全性與效能是一種好習慣，這點套用在 Kubernetes 元件升級上也適用，因為 Kubernetes 每三個月左右就會有新的功能發布或改變，且隨著 Kubernetes 盛行，越來越多安全漏洞被揭露後，升級叢集慢慢變成一件重要議題。但 Kubernetes 並不像升級叢集中的應用程式那麼簡單，因為需要考慮各層面問題，就如昨天提到的內容一樣，要針對每一種可能發生狀況先有個認知，以盡可能在發生狀況時，能夠化險為夷。很慶幸的是，這方面越來越受到重視，有許多 Kubernetes 部署工具開始試圖引入一些機制與流程來簡化更新叢集的過程，雖然這並不能解決所有會發生問題，但至少我們可以善用工具來增加更新的正確性與成功率。","text":"前言許多人都知道，讓應用程式保持較新的狀態，以優化安全性與效能是一種好習慣，這點套用在 Kubernetes 元件升級上也適用，因為 Kubernetes 每三個月左右就會有新的功能發布或改變，且隨著 Kubernetes 盛行，越來越多安全漏洞被揭露後，升級叢集慢慢變成一件重要議題。但 Kubernetes 並不像升級叢集中的應用程式那麼簡單，因為需要考慮各層面問題，就如昨天提到的內容一樣，要針對每一種可能發生狀況先有個認知，以盡可能在發生狀況時，能夠化險為夷。很慶幸的是，這方面越來越受到重視，有許多 Kubernetes 部署工具開始試圖引入一些機制與流程來簡化更新叢集的過程，雖然這並不能解決所有會發生問題，但至少我們可以善用工具來增加更新的正確性與成功率。 今天我們將實際利用 Kubeadm 工具來更新叢集，這邊沿用之前分享時，部署的 HA 叢集來進行 v1.15.4 更新至 v1.16.0。 利用 kubeadm 更新叢集本部分將依據以下步驟進行，並透過 Kubeadm 升級既有 Kubernetes 叢集至新版本: 主節點(Masters) 更新 kube-apiserver, controller manager, scheduler 與 etcd。 更新 Addons。如: kube-proxy, CoreDNS。 更新 kubelet binary file 與組態檔案。 (optional)更新 Node bootstrap tokens 的 RBAC 規則。 工作節點(Nodes) 在主節點使用 kubectl drain 來驅趕 Pods 到其他節點，並進入維運模式。另外建議使用 PodDisruptionBudget 確保應用程式在 Kubernetes 叢集的可用與不可用數。 更新 kubelet 二進制檔與組態檔案 在主節點使用 kubectl uncordon 讓節點能夠被排程。 事前準備在開始更新叢集前，請確保以下條件已達成: 用 kubeadm 建立一座 Kubernetes 叢集。可以參考實現 Kubernetes 高可靠架構部署文章進行。 確保叢集的所有節點處於 Ready 狀態。 確保應用程式利用進階的 Kubernetes API 建立，如 Deployment。並利用多副本機制來避免服務中斷。 以下步驟請一台一台來更新，這是為了確保 Kubernetes 功能不會因為更新而中斷。 更新主節點(Masters)依據步驟建議，我們需要先升級主節點，再接著進行工作節點升級。首先需要更新所有主節點的 kubeadm 工具版本: $ apt-mark unhold kubeadm$ apt-get update &amp;&amp; apt-get install -y kubeadm=1.16.0-00 &amp;&amp; \\ apt-mark hold kubeadm$ kubeadm versionkubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:34:01Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125; 安裝完成後，在第一台主節點執行以下指令來更新控制平面元件: $ kubeadm upgrade plan v1.16.0...COMPONENT CURRENT AVAILABLEAPI Server v1.15.4 v1.16.0Controller Manager v1.15.4 v1.16.0Scheduler v1.15.4 v1.16.0Kube Proxy v1.15.4 v1.16.0CoreDNS 1.3.1 1.6.2Etcd 3.3.10 3.3.15-0You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.16.0# 看到上面訊息後，即可執行$ kubeadm upgrade apply v1.16.0...[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.16.0\". Enjoy![upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. 接著在其他主節點執行以下指令來更新控制平面元件: $ kubeadm upgrade node...[kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.16\" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"[upgrade] The configuration for this node was successfully updated![upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 當一個主節點完成後，接著需要更新 kubelet 與 kubectl 元件: $ apt-mark unhold kubelet kubectl$ apt-get update &amp;&amp; apt-get install -y kubelet=1.16.0-00 kubectl=1.16.0-00 &amp;&amp; \\ apt-mark hold kubelet kubectl 重新啟動 kubelet 來更新叢集資訊: $ systemctl restart kubelet 最後利用 kubectl 來檢查叢集狀態: $ kubectl versionClient Version: version.Info&#123;Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:36:53Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;Server Version: version.Info&#123;Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:27:17Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 2d23h v1.15.4k8s-g2 Ready &lt;none&gt; 2d23h v1.15.4k8s-m1 Ready master 3d v1.16.0k8s-m2 Ready master 2d23h v1.16.0k8s-m3 Ready master 2d23h v1.16.0k8s-n1 Ready &lt;none&gt; 2d23h v1.15.4k8s-n2 Ready &lt;none&gt; 2d23h v1.15.4 更新節點(Nodes)當所有主節點都更新完成後，即可進行更新 Nodes。如同主節點一樣，首先需要更新 kubeadm 工具版本: $ apt-mark unhold kubeadm$ apt-get update &amp;&amp; apt-get install -y kubeadm=1.16.0-00 &amp;&amp; \\ apt-mark hold kubeadm$ kubeadm versionkubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:34:01Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125; 接著在任一台主節點上執行 kubectl drain 指令，將要更新節點的 Pod 轉移到其他節點，並進入 Unschedulable 狀態: $ kubectl drain &lt;node&gt; --ignore-daemonsetsWARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-clcq6, kube-system/kube-proxy-sgcl2evicting pod \"coredns-bf7759867-jkhf4\"pod/coredns-bf7759867-jkhf4 evictednode/k8s-n1 evicted 在更新元件之前，先更新相關組態檔: $ kubeadm upgrade node...[upgrade] The configuration for this node was successfully updated![upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. 一但新版本組態檔案正確更新後，即可更新 kubelet: $ apt-mark unhold kubelet$ apt-get update &amp;&amp; apt-get install -y kubelet=1.16.0-00 &amp;&amp; \\ apt-mark hold kubelet 重新啟動 kubelet 來更新叢集資訊: $ systemctl restart kubelet 完成後，進入任一台主節點執行以下指令來恢復節點至可排程狀態: $ kubectl uncordon &lt;node&gt; 驗證最後主節點與節點完成後，即可進入任一台主節點透過 kubectl 來查看狀態: $ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 2d23h v1.16.0k8s-g2 Ready &lt;none&gt; 2d23h v1.16.0k8s-m1 Ready master 3d v1.16.0k8s-m2 Ready master 2d23h v1.16.0k8s-m3 Ready master 2d23h v1.16.0k8s-n1 Ready &lt;none&gt; 2d23h v1.16.0k8s-n2 Ready &lt;none&gt; 2d23h v1.16.0$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;\"health\":\"true\"&#125; 結語今天簡單透過 kubeadm 來實現更新叢集元件。過程中，可以發現 kubeadm 幫助我們簡化了許多流程，讓我們不需要再手動完成太多事情，但是為了徹底知道怎麼運作過程，明天我們將對 kubeadm 的更新步驟進行分析與說明。 Reference https://kubernetes.io/docs/setup/release/version-skew-policy/ https://v1-15.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/ https://medium.com/@fairwinds/the-reactiveops-bestest-kubernetes-cluster-upgrade-f7a7589b21fb","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"淺談 Kubernetes 叢集元件更新","slug":"ironman2020/day06","date":"2019-09-20T16:00:00.000Z","updated":"2019-12-02T01:49:42.387Z","comments":true,"path":"2019/09/21/ironman2020/day06/","link":"","permalink":"https://k2r2bai.com/2019/09/21/ironman2020/day06/","excerpt":"前言在 Kubernetes 的快速發展下，每隔三個月左右，我們就會看到新版本的推出，以及舊版本進入 EOL(End of Life) 狀態，這時一但想要嘗試新功能與特性，或者是由於安全漏洞關析需要進行補丁時，就必須面對升級 Kubernetes 叢集元件問題。但是若在生產環境中，隨意升級 Kubernetes 叢集元件會不會發生什麼問題? 是的，若對於 Kubernetes 不熟悉的話，很容易掉入陷阱，比如說:已被棄用的 API、不相容的 Add-ons 版本、已棄用的元件參數、不正確的升級方式等等，這些都是有可能在升級後，導致 Kubernetes 叢集功能無法正常工作的原因。","text":"前言在 Kubernetes 的快速發展下，每隔三個月左右，我們就會看到新版本的推出，以及舊版本進入 EOL(End of Life) 狀態，這時一但想要嘗試新功能與特性，或者是由於安全漏洞關析需要進行補丁時，就必須面對升級 Kubernetes 叢集元件問題。但是若在生產環境中，隨意升級 Kubernetes 叢集元件會不會發生什麼問題? 是的，若對於 Kubernetes 不熟悉的話，很容易掉入陷阱，比如說:已被棄用的 API、不相容的 Add-ons 版本、已棄用的元件參數、不正確的升級方式等等，這些都是有可能在升級後，導致 Kubernetes 叢集功能無法正常工作的原因。 而今天就是要來跟大家分享一下，如何更安全地升級 Kubernetes 叢集，並且遇到問題時，該如何避免與解決。在開始前，我們先來了解一座 Kubernetes 叢集要更新時，需要關注哪些元件。 Kubernetes 主節點元件: kube-apiserver, kube-scheduler, kube-controller-manager 等等。 節點: kubelet。 Add-ons: kube-proxy、CoreDNS 等等。 容器 Runtime: Docker、CRI-O 等等。 叢集資料儲存: etcd。 叢集網路(CNI plugins): Ｃalico, Flannel 等等。 作業系統: 系統軟體與 Kernel 版本。 更新叢集前須知為了預防在更新 Kubernetes 叢集時發生問題，因此不需先注意幾件事情，以確保升級順利。 在更新叢集時，請務必備份 etcd 資料。因為 etcd 儲存 Kubernetes 叢集的狀態，若發生不一致時，將會影響叢集運行。這邊可以透過以下工具來完成。 etcd Operator 透過 etcdctl 的 snapshot + cron + restore 指令。 叢集更新時，務必以一個次要版本(Minor)為間隔進行。 Kubernetes 約每三個月會發布一個次要版本，其中每個次要版本號都會提供相容功能與 APIs 來轉移將被棄用的。 Good: My cluster is of v1.10, I want to upgrade to v1.11. Bad: My cluster is of v1.10, I want to upgrade to v.1.13. Good: My cluster is of v1.10, I upgrade to v1.11 and then upgrade to v1.12. 閱讀 Release Notes 來了解每個版本的變化。 了解已知問題 需要採取的措施 被棄用與移除的功能 善用工具或公有雲服務來完成叢集更新過程。 Kubeadm, Kops, Kubespray, Cluster API, …. GKE, EKS, AKS, …. 公有雲通常提供了更新機制，這也是公有雲 Kubernetes 服務的優勢之一，因為自建的環境往往很容易因為更新發生問題。 了解要更新的 Kubernetes 目標版本 API 變化。 API 會隨版本演進而改變，如 v1.16 要移除 extensions/v1beta1。 確保應用程式使用進階或穩定的 API 建置實例，如 Deployment。 確保應用程式有多個實例(Pod)支撐。 利用探針確保應用狀態，以攔阻流量的分發。 使用 Pod 的 PreStop hook 來加強生命週期管理。 更新 Node 以前，優先更新 Master。 TODO: 補充更多細節 可預見問題與解法 etcd 中過舊的資料。 不要刪除 API 版本(#52185)。 使用 Storage migration 系統1。 不要部署 EOL 版本 API2。 Clients 使用的版本已過舊(過時) 應用與服務使用的 API 依然是相依於 extensions/v1beta1。 需在叢集更新以前，優先將應用與服務的 API object 轉移到新版本 開發的 Custom Controller 涉及過舊版本的 API 與函式庫。 需要更新程式碼使用新版 API，並且將相依 Libraries 也更新至相容的新版本。 Policy breaks after upgrade(Webhook, RBAC) 有新版本 batch/v2 與 test_batch/v1 在叢集更新前，先更新 Policies 使用目前所有支援的版本。 TODO: 補充更多細節 不可預見問題與解法 在升級前盡可能地在實際環境測試要升級的版本。 使用舊版本的 Sonobuoy 來測試新版本叢集。 確保組態檔案保持一致，並確保設定是否因為版本改變而出現錯誤。 檢視 Addons 是否因為新版本的特性而崩潰。 利用多階段升級方式來確保 API 的相容。 TODO: 補充更多細節 結語今天簡單的分享了升級 Kubernetes 叢集時，需要優先了解的知識，目的是希望幫助正在煩惱升級的人，能夠更有信心的執行。這邊在總結一下更新叢集前、更新叢集中與更新叢集後需要瞭解的事情。 更新叢集前 備份 etcd 資料!!備份 etcd 資料!!備份 etcd 資料!! 閱讀新版本的 Release notes 更新客戶端相關軟體與程式，以支援新舊版本的相容 API 與函式庫。 更新相關的組態檔案，以支援新舊版本的相容 API 與函式庫。 更新叢集中 先更新主節點(Master node)，在更新工作節點(Node)。 若主節點為 HA 架構，請確保所有節點都更新完，在使用新的 APIs 與功能。 更新工作節點前，透過kubectl drain &lt;node&gt;將節點設定為維運模式。 確認 Add-ons 支援新版本的 Kubernetes 元件、APIs 與函式庫等等後，再進行更新。 網路插件同上。 更新叢集後 檢查 Kubernetes 叢集控制平面元件。 檢查 Add-ons 與網路插件。 確保所有節點處於 Ready 狀態。 平衡因為升級而集中的 Pod。 當有了基本的叢集更新知識後，明天我們將實際實踐更新一座叢集的過程。 Reference https://static.sched.com/hosted_files/kccncchina2018english/1c/Safely%20upgrading%20Kubernetes%20clusters.pdf https://static.sched.com/hosted_files/kccna18/8b/Highly%20Available%20Kubernetes%20Clusters%20-%20Best%20Practices%20-%20Kubecon%20NA%202018.pdf https://v1-15.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/ https://medium.com/@fairwinds/the-reactiveops-bestest-kubernetes-cluster-upgrade-f7a7589b21fb https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-upgrading-your-clusters-with-zero-downtime","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"實現 Kubernetes 高可靠架構部署","slug":"ironman2020/day05","date":"2019-09-19T16:00:00.000Z","updated":"2019-12-02T01:49:42.387Z","comments":true,"path":"2019/09/20/ironman2020/day05/","link":"","permalink":"https://k2r2bai.com/2019/09/20/ironman2020/day05/","excerpt":"前言隨著團隊越來越多地在生產環境使用 Kubernetes 管理雲原生應用程式，我們必須考量在各種故障下，Kubernetes 能正常運行的情況，比如說:在流量高峰期間，將工作負載分散到更多節點或轉往公有雲、跨多個 Availability Zones/Regions 部署、建構高可靠(Highly Available，HA)架構等等要求。其中高可靠架構在昨天的淺談 Kubernetes 高可靠架構文章中，簡單地複習了高可靠架構。而今天將說明如何實現與利用 kubeadm 建立一座架構大致如下圖所示的 HA 叢集。","text":"前言隨著團隊越來越多地在生產環境使用 Kubernetes 管理雲原生應用程式，我們必須考量在各種故障下，Kubernetes 能正常運行的情況，比如說:在流量高峰期間，將工作負載分散到更多節點或轉往公有雲、跨多個 Availability Zones/Regions 部署、建構高可靠(Highly Available，HA)架構等等要求。其中高可靠架構在昨天的淺談 Kubernetes 高可靠架構文章中，簡單地複習了高可靠架構。而今天將說明如何實現與利用 kubeadm 建立一座架構大致如下圖所示的 HA 叢集。 在開始建立前，我們先簡單瞭解每個主節點要執行元件，以及這些元件如何完成高可靠架構。 etcd: 透過多節點的 etcd 實例組成叢集，並利用 Raft 演算法，來選取一個領導者(Leader)處理需要叢集共識的所有客戶端的請求(Request)，如下圖所示。另外由於 Raft 演算法關析，還需要注意叢集的故障容許度(Failure Tolerance)。 Cluster Size Majority Failure Tolerance 1 1 0 2 2 0 3 2 1 4 3 1 5 3 2 6 4 2 7 4 3 計算故障容許節點數為(N/2)+1，其中 N 為叢集大小。 (圖片擷取自： Kubecon NA 2018 - Highly Available Kubernetes Clusters - Best Practices) API server:: 每個 API server 會與本地端的 etcd 溝通，並接收來至客戶端與其他元件的 API 請求。由於 API server 屬於 Active-Active 架構，因此每個 API server 在叢集中都處於可用狀態。 (圖片擷取自： Kubecon NA 2018 - Highly Available Kubernetes Clusters - Best Practices) controllers, scheduler: 這些元件採用 Lease 機制來從所有實例中選取一個作為領導者，並由領導者處理監聽對應的 API 資源來完成功能，因此整個叢集只會有一個擁有完整功能，除非原本領導的節點發生故障，才會尤其它接手。 (圖片擷取自： Kubecon NA 2018 - Highly Available Kubernetes Clusters - Best Practices) kubelet: 由於 kubelet 只能設定跟一個 API server 的端點(Endpoint)，但為了達到某個 API server 故障時，還能夠繼續正常執行的需求，我們需要提供一個虛擬 IP(VIP, Virtual IP)，以及負載平衡器(Load Balancer)來讓 kubelet 能夠存取多個 API server，一方面利用負載平衡器的機制來分散工作負載到所有 API server 上。 另外由於 API servers 需要提供 VIP 與負載平衡器，因此必須在所有主節點上額外安裝以下元件來達到需求。 Keepalived: 基於 VRRP 協定來實現高可靠架構，所有主節點會基於此元件舉出一個 VIP 來作為存取 API 的端點，這主要是確保其他元件連接 API 時，不會因為某個主節點中斷而無法存取。 HAProxy: 與 API server 一樣，為 Active-Active 架構，因此每個主節點都可以存取作為 Proxy 的 IP 與 Port。 簡單了解完實現方式後，下一小節將說明如何利用 kubeadm 來建構 Kubernetes HA 叢集。 選用 kubeadm 是因為方便手動做測試，且 kubeadm HA 功能在 v1.15 版本進入了 Beta 階段，因此值得大家嘗試看看。當然過程中，若節點數過多的話，建議搭配 Ansible(or Puppet, SaltStack) 這類工具進行。 Set up HA cluster using kubeadm本部分將透過 Kubeadm 來部署 Kubernetes v1.15 版本的 High Availability 叢集，而本安裝主要是參考官方文件中的 Creating Highly Available Clusters with kubeadm 內容來進行，這邊將透過 HAProxy 與 Keepalived 的結合來實現控制面的 Load Balancer 與 VIP。 Kubernetes 部署的版本資訊： kubeadm: v1.15.4 Kubernetes: v1.15.4 CNI: v0.7.5 etcd: v3.2.18 Docker CE: 19.03.2 Calico: v3.8 Kubernetes 部署的網路資訊： Cluster IP CIDR: 10.244.0.0/16 Service Cluster IP CIDR: 10.96.0.0/12 Service DNS IP: 10.96.0.10 DNS DN: cluster.local Kubernetes API Virtual IP: 172.22.132.10 節點資訊本文採用以下節點數進行裸機部署，作業系統採用Ubuntu 18.04+進行測試: IP Address Hostname CPU Memory Role 172.22.132.11 k8s-m1 4 16G Master 172.22.132.12 k8s-m2 4 16G Master 172.22.132.13 k8s-m3 4 16G Master 172.22.132.21 k8s-n1 4 16G Node 172.22.132.22 k8s-n2 4 16G Node 172.22.132.31 k8s-g1 4 16G Node 172.22.132.32 k8s-g2 4 16G Node 另外所有 Master 節點將透過 Keepalived 提供一個 Virtual IP 172.22.132.10 作為使用。 所有操作全部用root使用者進行，主要方便部署用。 事前準備開始部署叢集前需先確保以下條件已達成： 所有節點彼此網路互通，並且k8s-m1 SSH 登入其他節點為 passwdless，由於過程中很多會在某台節點(k8s-m1)上以 SSH 複製與操作其他節點。 確認所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0$ vim /etc/selinux/configSELINUX=disabled 關閉是為了方便安裝使用，若有需要防火牆可以參考 Required ports 來設定。 所有節點需要安裝 Docker CE 版本的容器引擎： $ curl -fsSL https://get.docker.com/ | sh 所有節點需要加入 APT Kubernetes package 來源： $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -$ echo 'deb http://apt.kubernetes.io/ kubernetes-xenial main' | tee /etc/apt/sources.list.d/kubernetes.list 所有節點需要設定以下系統參數。 $ cat &lt;&lt;EOF | tee /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl -p /etc/sysctl.d/k8s.conf 關於bridge-nf-call-iptables的啟用，主要取決於是否將容器連接到Linux bridge或使用其他一些機制(如 SDN vSwitch)。 Kubernetes v1.8+ 要求關閉系統 Swap，請在所有節點利用以下指令關閉： $ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0# 不同機器會有差異$ sed '/swap.img/d' -i /etc/fstab 記得/etc/fstab也要註解掉SWAP掛載。 關閉 Swap 是避免 Kubernetes Pod 不會因為使用到 Swap 而影響效能，另一方面可以讓 OOM Killer 正常運作。 Kubernetes Master 建立本節將說明如何部署與設定 Kubernetes Master 節點中的各元件。 在開始部署master節點元件前，請先安裝好 kubeadm、kubelet 等套件，並建立/etc/kubernetes/manifests/目錄存放 Static Pod 的 YAML 檔： $ export KUBE_VERSION=\"1.15.4\"$ apt-get update &amp;&amp; apt-get install -y kubelet=$&#123;KUBE_VERSION&#125;-00 kubeadm=$&#123;KUBE_VERSION&#125;-00 kubectl=$&#123;KUBE_VERSION&#125;-00$ apt-mark hold kubeadm kubectl kubelet$ mkdir -p /etc/kubernetes/manifests/ 完成後，依照下面小節完成部署。 HAProxy本節將說明如何建立 HAProxy 來提供 Kubernetes API Server 的負載平衡。在所有master節點的/etc/haproxy/目錄： $ mkdir -p /etc/haproxy/ 接著在所有master節點新增/etc/haproxy/haproxy.cfg設定檔，並加入以下內容： $ cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local0 log 127.0.0.1 local1 notice tune.ssl.default-dh-param 2048defaults log global mode http option dontlognull timeout connect 5000ms timeout client 600000ms timeout server 600000mslisten stats bind :9090 mode http balance stats uri /haproxy_stats stats auth admin:admin123 stats admin if TRUEfrontend kube-apiserver-https mode tcp bind :8443 default_backend kube-apiserver-backendbackend kube-apiserver-backend mode tcp balance roundrobin stick-table type ip size 200k expire 30m stick on src server apiserver1 172.22.132.11:6443 check server apiserver2 172.22.132.12:6443 check server apiserver3 172.22.132.13:6443 checkEOF 這邊會綁定8443作為 API Server 的 Proxy。 接著在新增一個路徑為/etc/kubernetes/manifests/haproxy.yaml的 YAML 檔來提供 HAProxy 的 Static Pod 部署，其內容如下： $ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/haproxy.yamlkind: PodapiVersion: v1metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: component: haproxy tier: control-plane name: kube-haproxy namespace: kube-systemspec: hostNetwork: true priorityClassName: system-cluster-critical containers: - name: kube-haproxy image: docker.io/haproxy:1.7-alpine resources: requests: cpu: 100m volumeMounts: - name: haproxy-cfg readOnly: true mountPath: /usr/local/etc/haproxy/haproxy.cfg volumes: - name: haproxy-cfg hostPath: path: /etc/haproxy/haproxy.cfg type: FileOrCreateEOF 接下來將新增另一個 YAML 來提供部署 Keepalived。 Keepalived本節將說明如何建立 Keepalived 來提供 Kubernetes API Server 的 VIP。在所有master節點新增一個路徑為/etc/kubernetes/manifests/keepalived.yaml的 YAML 檔來提供 HAProxy 的 Static Pod 部署，其內容如下： $ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/keepalived.yamlkind: PodapiVersion: v1metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: component: keepalived tier: control-plane name: kube-keepalived namespace: kube-systemspec: hostNetwork: true priorityClassName: system-cluster-critical containers: - name: kube-keepalived image: docker.io/osixia/keepalived:2.0.17 env: - name: KEEPALIVED_VIRTUAL_IPS value: 172.22.132.10 - name: KEEPALIVED_INTERFACE value: enp3s0 - name: KEEPALIVED_UNICAST_PEERS value: \"#PYTHON2BASH:['172.22.132.11', '172.22.132.12', '172.22.132.13']\" - name: KEEPALIVED_PASSWORD value: d0cker - name: KEEPALIVED_PRIORITY value: \"100\" - name: KEEPALIVED_ROUTER_ID value: \"51\" resources: requests: cpu: 100m securityContext: privileged: true capabilities: add: - NET_ADMINEOF KEEPALIVED_VIRTUAL_IPS：Keepalived 提供的 VIPs。 KEEPALIVED_INTERFACE：VIPs 綁定的網卡。 KEEPALIVED_UNICAST_PEERS：其他 Keepalived 節點的單點傳播 IP。 KEEPALIVED_PASSWORD： Keepalived auth_type 的 Password。 KEEPALIVED_PRIORITY：指定了備援發生時，接手的介面之順序，數字越小，優先順序越高。這邊k8s-m1設為 100，其餘為150。 KEEPALIVED_ROUTER_ID：一組 Keepalived instance 的數字識別子。 First control plane node首先在k8s-m1節點建立kubeadm-config.yaml的 Kubeadm Master Configuration 檔： $ cat &lt;&lt;EOF &gt; kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: v1.15.4controlPlaneEndpoint: \"172.22.132.10:8443\"networking: podSubnet: \"10.244.0.0/16\"EOF controlPlaneEndpoint填入 VIPs 與 bind port。 新增完後，透過 kubeadm 來初始化 control plane： $ kubeadm init --config=kubeadm-config.yaml --upload-certs...You can now join any number of the control-plane node running the following command on each as root: kubeadm join 172.22.132.10:8443 --token qawtjn.l0bpc3o12fef33t5 \\ --discovery-token-ca-cert-hash sha256:7310e2e34b47214eba2be7a44375ea588a1d59d3126ac11759853d59fa76fadc \\ --control-plane --certificate-key 6b9fbbac56a7af8576d8c7f98e44d5d78984c7331ca6d41a066d05c3d3795cc7Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 172.22.132.10:8443 --token qawtjn.l0bpc3o12fef33t5 \\ --discovery-token-ca-cert-hash sha256:7310e2e34b47214eba2be7a44375ea588a1d59d3126ac11759853d59fa76fadc 請記下來 join 節點資訊，方便後面使用。若忘記的話，可以用 kubeadm token 指令重新取得。 經過一段時間完成後，接著透過 netstat 檢查是否正常啟動服務： $ netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 172.22.132.10:8443 0.0.0.0:* LISTEN 11218/haproxytcp 0 0 0.0.0.0:9090 0.0.0.0:* LISTEN 11218/haproxytcp 0 0 127.0.0.1:44551 0.0.0.0:* LISTEN 9237/kubelettcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 9237/kubelettcp 0 0 127.0.0.1:10249 0.0.0.0:* LISTEN 11669/kube-proxytcp 0 0 172.22.132.11:2379 0.0.0.0:* LISTEN 10367/etcdtcp 0 0 172.22.132.11:2380 0.0.0.0:* LISTEN 10367/etcdtcp 0 0 127.0.0.1:10257 0.0.0.0:* LISTEN 10460/kube-controlltcp 0 0 127.0.0.1:10259 0.0.0.0:* LISTEN 10615/kube-schedule 經過一段時間完成後，執行以下指令來使用 kubeconfig： $ mkdir -p $HOME/.kube$ cp -rp /etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config 透過 kubectl 檢查 Kubernetes 叢集狀況： $ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-m1 NotReady master 31s v1.15.4$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Healthy &#123;\"health\":\"true\"&#125; 接著部署 Calico CNI plugin: $ wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml$ sed -i 's/192.168.0.0\\/16/10.244.0.0\\/16/g' calico.yaml$ kubectl apply -f calico.yaml 完成後，透過 kubectl 來查看 kube-system 的 Pod 建立狀況: $ kubectl -n kube-system get poNAME READY STATUS RESTARTS AGEcalico-kube-controllers-65b8787765-ckd7b 1/1 Running 0 8m8scalico-node-l9wh9 1/1 Running 0 8m8scoredns-5c98db65d4-89wq5 1/1 Running 0 9m14scoredns-5c98db65d4-lmvvn 1/1 Running 0 9m14setcd-k8s-m1 1/1 Running 0 8m24skube-apiserver-k8s-m1 1/1 Running 0 8m17skube-controller-manager-k8s-m1 1/1 Running 0 8m26skube-haproxy-k8s-m1 1/1 Running 0 9m30skube-keepalived-k8s-m1 1/1 Running 0 8m13skube-proxy-g7clj 1/1 Running 0 9m14skube-scheduler-k8s-m1 1/1 Running 0 8m41s 到這邊k8s-m1就完成部署了，接著我們要將k8s-m2與k8s-m3以控制平面節點加入現有叢集。 Other control plane nodes在 kubeadm v1.15 版本中，提供了自動配置 HA 的機制，因此只需要在其他主節點執行以下指令即可: $ kubeadm join 172.22.132.10:8443 --token qawtjn.l0bpc3o12fef33t5 \\ --discovery-token-ca-cert-hash sha256:7310e2e34b47214eba2be7a44375ea588a1d59d3126ac11759853d59fa76fadc \\ --control-plane \\ --certificate-key 6b9fbbac56a7af8576d8c7f98e44d5d78984c7331ca6d41a066d05c3d3795cc7 \\ --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests 經過一段時間完成後，執行以下指令來使用 kubeconfig： $ mkdir -p $HOME/.kube$ cp -rp /etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config 透過 kubectl 檢查 Kubernetes 叢集狀況： $ kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-m1 Ready master 10m v1.15.4k8s-m2 Ready master 3m v1.15.4k8s-m3 Ready master 74s v1.15.4 由於其他主節點加入方式一樣，所以k8s-m3節點請重複執行本節過程來加入。 Kubernetes Nodes 建立本節將說明如何部署與設定 Kubernetes Node 節點中。在開始部署node節點元件前，請先安裝好 kubeadm、kubelet 等套件： $ export KUBE_VERSION=\"1.15.4\"$ apt-get update &amp;&amp; apt-get install -y kubelet=$&#123;KUBE_VERSION&#125;-00 kubeadm=$&#123;KUBE_VERSION&#125;-00$ apt-mark hold kubeadm kubelet 安裝好後，在所有node節點透過 kubeadm 來加入節點： $ kubeadm join 172.22.132.10:8443 \\ --token qawtjn.l0bpc3o12fef33t5 \\ --discovery-token-ca-cert-hash sha256:7310e2e34b47214eba2be7a44375ea588a1d59d3126ac11759853d59fa76fadc...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 測試部署結果當節點都完成後，進入任一台master節點透過 kubectl 來檢查： $ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 92s v1.15.4k8s-g2 Ready &lt;none&gt; 3m33s v1.15.4k8s-m1 Ready master 20m v1.15.4k8s-m2 Ready master 13m v1.15.4k8s-m3 Ready master 8m32s v1.15.4k8s-n1 Ready &lt;none&gt; 3m30s v1.15.4k8s-n2 Ready &lt;none&gt; 3m35s v1.15.4$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Healthy &#123;\"health\": \"true\"&#125; kubeadm 的方式會讓狀態只顯示 etcd-0。 接著進入k8s-m1節點測試叢集 HA 功能，這邊先關閉該節點： $ sudo poweroff 接著進入到k8s-m2節點，透過 kubectl 來檢查叢集是否能夠正常執行： # 先檢查元件狀態$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;\"health\":\"true\"&#125;# 檢查 nodes 狀態$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 4m54s v1.15.4k8s-g2 Ready &lt;none&gt; 6m55s v1.15.4k8s-m1 NotReady master 55m v1.15.4k8s-m2 Ready master 33m v1.15.4k8s-m3 Ready master 11m v1.15.4k8s-n1 Ready &lt;none&gt; 6m52s v1.15.4k8s-n2 Ready &lt;none&gt; 6m57s v1.15.4# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl expose pod nginx --port 80 --type NodePort$ kubectl get po,svcNAME READY STATUS RESTARTS AGEpod/nginx 1/1 Running 0 27sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 56mservice/nginx NodePort 10.106.25.118 &lt;none&gt; 80:30461/TCP 21s 透過 cURL 檢查 NGINX 服務是否正常： $ curl 172.22.132.10:30461&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;... 結語今天簡單透過 kubeadm 來實現 Kubernetes HA 架構，但大家肯定會發現這樣手動操作非常累，若要建立大量裸機節點時，將會有很多重複指令需要執行，因此這邊推薦結合 Ansible 這類工具進行，或者直接使用 Kubespray 來自動化部署 Kubernetes HA 環境。 但是這樣就能確保服務執行在 Kubernetes 上後，就都不會出事了嗎? 事實上，不光要針對 Kubernetes 做 HA 架構，我們還要對許多層面進行處理。 Reference https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/ https://medium.com/velotio-perspectives/demystifying-high-availability-in-kubernetes-using-kubeadm-3d83ed8c458b http://www.haproxy.org/ https://www.keepalived.org/ https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md https://kccna18.sched.com/event/GrWQ https://kubernetes.io/blog/2019/06/24/automated-high-availability-in-kubeadm-v1.15-batteries-included-but-swappable/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"淺談 Kubernetes 高可靠架構","slug":"ironman2020/day04","date":"2019-09-18T16:00:00.000Z","updated":"2019-12-02T01:49:42.387Z","comments":true,"path":"2019/09/19/ironman2020/day04/","link":"","permalink":"https://k2r2bai.com/2019/09/19/ironman2020/day04/","excerpt":"前言近幾年來容器的興起，重塑了我們對於開發、部署與維運軟體的方式。容器允許我們透過打包應用程式成容器映像檔，並透過容器引擎部署到一組虛擬或實體的機器上執行。也正因為這樣過程與需求，產生了所謂的容器編排系統(Container Orchestration)，以自動化、基於容器應用程式的方式部署、管理與擴展。其中 Kubernetes 就是近年來的一套容器編排系統標準，它能允許大規模部署與管理基於容器的應用程式，而 Kubernetes 的特性還能實現分散式的應用程式，以帶來更高的可靠性與穩定性。但是，當 Kubernetes 重要元件或是其主節點(Master Node)發生故障時，那 Kubernetes 會發生什麼事呢?又該如何確保 Kubernetes 本身保持正常運行呢?","text":"前言近幾年來容器的興起，重塑了我們對於開發、部署與維運軟體的方式。容器允許我們透過打包應用程式成容器映像檔，並透過容器引擎部署到一組虛擬或實體的機器上執行。也正因為這樣過程與需求，產生了所謂的容器編排系統(Container Orchestration)，以自動化、基於容器應用程式的方式部署、管理與擴展。其中 Kubernetes 就是近年來的一套容器編排系統標準，它能允許大規模部署與管理基於容器的應用程式，而 Kubernetes 的特性還能實現分散式的應用程式，以帶來更高的可靠性與穩定性。但是，當 Kubernetes 重要元件或是其主節點(Master Node)發生故障時，那 Kubernetes 會發生什麼事呢?又該如何確保 Kubernetes 本身保持正常運行呢? 前幾天分享部署工具章節，可以了解在生產環境中，部署一座高可靠(Highly Available，HA)架構的 Kubernetes 是非常重要的一件事。因為當服務在一座非高可靠的 Kubernetes 叢集上執行時，發生了節點故障的話，將會造成正在執行的服務受影響，嚴重甚至中斷整個服務的運行，除此之外還可能發生 Kubernetes 叢集狀態不一致、叢集功能全失效等等問題，既而造成維運人員負擔，以及造成公司損失。 也正因此，今天想針對 Kubernetes 高可靠架構進行探討，希望帶大家了解 Kubernetes 高可靠架構如何規劃與實現。 高可靠架構在開始談 Kubernetes 高可靠架構時，我們先來複習 Kubernetes 的節點角色，以及其元件: 主節點(Master node): 該節點主要運行 Kubernetes 的控制平面元件，如: kube-apiserver、kube-controller-manager、kube-scheduler、kubelet 與容器 runtime 等等元件，目的是用於維護整個 Kubernetes 的運作，負責接收 API 請求、容器排程、執行容器等等事情。另外根據架構不同還有可能運行 etcd 作為整個叢集狀態儲存用。 etcd 是一套分散式 key-value 儲存系統，在 Kubernetes 中，它被用於儲存叢集狀態，如: API resources。 在目前常見架構中，主節點也被視為工作節點，但預設會透過一些機制來確保容器不會被排程到這些節點。 節點(Node): 又稱 Worker node 或 Minion node，該節點主要運行 kubelet、容器 runtime 等等。主要被用於執行應用程式容器，會定期與主節點回報目前節點資訊。 而 Kubernetes 的高可靠旨在使用一種沒有單點故障(SPOF, Single Point of Failure)的方式，設定與建構 Kubernetes 元件與支援的元件(etcd)。當使用單一主節點叢集時，很容易因為節點或元件發生錯誤，導致叢集故障; 而多主節點叢集則可以利用每個主節點元件來存取相同的工作節點，因此能更提升叢集的穩定性與可靠性。 在單個主叢集中，kube-apiserver 與 kube-controller-manager 等等重要元件都僅在單一節點上，如果故障的話，則無法建立或執行 Kubernetes 的功能。但在 Kubernetes HA 架構中，這些重要元件會在多個節點各執行一組(通常最小為 3 個主節點)，因此有主節點故障時，就會有正常的主節點來確保叢集的運作。其架構如下圖所示: (圖片擷取自：kubernetes.io) 而在 Kubernetes HA 的架構實現中，etcd 會根據需求有不同的拓樸架構，分別為: Stacked etcd topology: 該拓樸是將 etcd 連同 Kubernetes 控制平面放同一個節點上。這時 etcd 可能會透過 kubelet 以 Static Pod 方式來管理，或者以作業系統的背景行程管理機制進行(如 Linux systemd)。雖然這種架構只需要最少三台就能達到 HA，但由於放一起，所以故障域不會分離。 (圖片擷取自：kubernetes.io) External etcd topology: 該拓樸是讓 kube-apiserver 存取外部的 etcd 叢集，這種架構將叢集資料儲存系統與叢集功能系統做分離，因此可靠性比 Stacked etcd 架構來的好些，不會因為一個節點故障，而同時影響到 Kubernetes 控制平面與 etcd。相反的，這種架構需要更多的機器來支撐。 (圖片擷取自：kubernetes.io) TODO: 補充更多細節 結語今天主要再次複習 Kubernetes HA 架構是如何實現的，因為在 On-Premise(Custom) Kubernetes 中，不像公有雲服務會幫你管理主節點的元件，因此適當地了解對於部署 On-Premise(Custom) Kubernetes 有很多幫助，也可以再發生問題時，更快的知道問題點在哪裡。 而除了今天提到的事情外，究竟建立 HA 架構還有什麼好處呢? 透過負載平衡器來分散 Kubernetes API server 的負載。 Kubernetes 狀態資料的故障轉移(etcd)。 在 Multi-zones 中執行，確保跨不同故障域(Failure domains)。 實現安全地更新 Kubernetes 叢集元件。 更新 Kubernetes 叢集元件部分，在之後文章我也會說明如何進行，並需要注意哪些事情，以確保更新版本時不會出太大錯誤。 最後明天我們將實際利用工具來建立一套 Kubernetes HA 架構的環境，並在過程中說明如何做最佳實踐，一但完成後，也將嘗試驗證功能。 Reference https://www.kubeclusters.com/docs/How-to-Deploy-a-Highly-Available-kubernetes-Cluster-with-Kubeadm-on-CentOS7 https://platform9.com/blog/create-highly-available-kubernetes-cluster/ https://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198 https://techbeacon.com/enterprise-it/6-best-practices-highly-available-kubernetes-clusters https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/ https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/ https://kccna18.sched.com/event/GrWQ","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"淺談 Kubernetes 的部署工具選擇 Part3","slug":"ironman2020/day03","date":"2019-09-17T16:00:00.000Z","updated":"2019-12-02T01:49:42.387Z","comments":true,"path":"2019/09/18/ironman2020/day03/","link":"","permalink":"https://k2r2bai.com/2019/09/18/ironman2020/day03/","excerpt":"前言在昨天文章中，我針對開發環境(Development)與測試環境(Testing)使用情境下，選擇 Kubernetes 工具的看法，而今天將延續 淺談 Kubernetes 的部署工具選擇 Part2 未講完的部分繼續分享預備環境(Staging)與生產環境(Production)看法。","text":"前言在昨天文章中，我針對開發環境(Development)與測試環境(Testing)使用情境下，選擇 Kubernetes 工具的看法，而今天將延續 淺談 Kubernetes 的部署工具選擇 Part2 未講完的部分繼續分享預備環境(Staging)與生產環境(Production)看法。 在開始談預備環境與生產環境上，我如何選擇部署工具前，先來回顧一下我在公司聽到關於這兩者的幾句經典話: 敏感與重要的系統與程式，就直接在生產環境測試就好。 先蓋好生產環境再去考慮預備環境，趕緊上線比較重要。 東西自己測完，就可以上生產環境了阿，不用等人拉。 然後這幾句話換來的結果就是。 回想過去參與某些專案時，經歷的幾個駭人狀況: 有人在生產環境開發 API，不小心把所有使用者的物件儲存 Buckets 砍掉了。 又或者不小心在開發時，誤刪生產環境的 DB 虛擬機。 有人被要求在生產環境上，開發與測試要整合的外部防火牆，結果一個不小心就把防火牆的 API Request 上限打爆了，導致整個卡住無法正常運作。 有人把測試用與正式用的資源都放在生產環境上，所以測試結束後，刪除時，發現只剩下測試用。 諸如此類問題，三不五時就會聽聞與遭遇，這也讓我對於這種情況越來越重視，或許不是每間公司都有資源可以分多個環境階段來預防服務上線前的問題，但適當地切分還是能有效避免一些不該發生問題。不過上面這些只是題外話，總之我們開始進入主題吧。 預備環境(Staging)當我們開發的程式經過測試環境的單元測試、煙霧測試、E2E 測試(或整合測試)，並將程式轉為發布階段時，就會利用 CI/CD 平台(工具)部署至預備環境(或者 QA 自行部署)，然後由內部 QA 或軟體測試人員進行各種上線時，可能會發生的情境或者模擬客戶使用的狀況。那在這樣的環境下，該如何選擇 Kubernetes 部署工具呢? 有時候我們會疑惑測試環境與預備環境的差異，因為這兩者都被用來做測試的環境。我自己的理解大概是這樣: Testing:是開發人員驗證程式的各種測試的環境，開發人員可以在程式轉到發行版本前，測試任何程式的變更或錯誤修復。 Staging:比較偏向專給 QA 或軟體測試人員對開發人員發布的程式使用。在這環境中，會盡可能去模擬與實踐 Production 環境的各種事情，以確保上線的品質。也可以想像預備環境就是生產環境的副本。 事實上，選擇條件類似先前一些提到的，但是針對某幾點特別再去思考: 支援高可靠(Highly Available)部署: 在生產環境中，高可靠架構是非常重要的一件事情，因為任何機器或 Kubernetes 節點在發生故障時，而使 Kubernetes 整個停止運作，繼而影響執行的應用程式。然而，由於預備環境必須從基礎建設到應用程式，都要盡可能模擬生產環境狀況，因此建立一座最小高可靠叢集是必須的。若部署工具對於高可靠架構支援程度太差的話，將有可能增加維運人員的負擔與風險。 支援更新叢集元件: 由於預備環境跟生產環境不可能隨便就打掉重練，大多數情況下，會以升級當前叢集方式來進行，但升級 Kubernetes 叢集需要考慮到很多層面問題。因此若工具本身不會幫忙檢查，並實踐安全更新機制的話，可能在未來新版本釋出，舊版本進入 EoL 時，會有一些風險存在(如當前版本有重大資安問題，但卻無法更新)。 方便新增/刪除節點: 同上類似狀況。但有時候節點發生異常時，可能需要將該節點轉成維運模式(或從叢集中移除)，並進行修復動作，直到確認正常後，再重新加入原本的 Kubernetes 叢集。當遇到這種情境時，若工具本身不支援這樣需求的話，就會發生如前面幾點一樣問題。 自動化: 由於生產環境有可能需要部署大量機器，所以使用純 kubeadm 這種方式的話，維運人員就可能會面臨執行重複指令的時間成本增長問題，也可能發生執行錯誤指令狀況，因此一個部署工具是否能夠對多節點進行操作，並盡可能保持一致性是很重要的。 可客製化: 在自建(或地端)部署時，有可能需要進行一些調整，或實現離線安裝需求，若工具客製化程度不高的話，可能會影響到部署的結果。 通過 CNCF Kubernetes Conformance: 同 Testing 環境，雖然通過不代表就完全沒問題，但至少能對該工具多一點信心。 最佳實踐(Best Practices): 能夠盡可能依據當前 Kubernetes 版本的功能做最佳實踐，確保不會在預設下開啟實驗中或已棄用的功能。 那麼從上面這些條件中過濾，有哪些可以選擇呢?我個人大概是使用以下這些: Kubespray Kops Rancher Breeze KubeOne Puppetlabs Kubernetes 其實還有很多可以選擇，但這邊比較偏向以自己使用過為主，若有興趣的人可以到 CNCF Landscape 查看經過一致性認證的部署工具。 \b不過這麼多究竟哪個是最好呢? 事實上，沒有所謂最好最合適的部署工具，因為我通常還是依據情況來選擇，例如: 若環境都是裸機節點時，就會利用 Kubespray 以 Ansible 機制來自動化安裝多節點。且 Kubespray 可客製化程度很高，學習門檻也相對比一個用程式碼撰寫的工具來得低。 在裸機部署時，往往需要連同作業系統要一起安裝，但若叢集的節點過多的話，就會增加維運負擔，因此我還會結合 PXE/iPXE 以 Network boot 方式安裝作業系統，因為裸機自建環境不像公有雲提供的服務一樣，點一點就能部署起來。 若環境是在公有雲上自建 Kubernetes 時，就會偏向使用 Kops 或 KubeOne 工具來達成，因為這些工具整合了公有雲服務，可以方便幫你快速進行公有雲最佳實踐。 若希望有管理 UI 的話，那選擇 Rancher 或 Breeze 會是一個不錯方案，\b尤其是 Rancher 提供了非常完善的 Web-based 管理與監控介面，且能對多個 Kubernetes 叢集進行管理，並整合許多原生 Kubernetes 沒有的功能。 生產環境(Production)當預備環境上的服務經過 QA 與軟體測試人員嚴厲的測試後，就會進入部署生產環境的階段，這時在生產環境上的工具該如何選擇呢? 在上一小節有提到預備環境\b可以看作是生產環境的副本，因此所有預備環境\b的條件都要去考量。但要額外注意的是升級叢集時，要確保使用工具一次執行的節點數是否正確，因為要是一次太多節點升級發生故障，且沒有多餘節點支撐服務運作的話，將會帶來嚴重的損失。 在這部分，我較多情況下是使用 Kubespray 與 Rancher 結合使用。部署 Kubernetes 叢集會以 Kubespray 為基礎來自行調整與修改，而 Rancher 則作為管理介面來管理 Kubespray 部署的 Kubernetes 叢集。 結語這幾天都是分享自己選擇部署工具的看法與經驗，但老實說沒有哪個一個工具是最好的，終究還是依據公司跟團隊的需求來找出最適合的。不過若公司真的想要搞自建(或地端)部署的話，尤其是要幫客戶提供部署服務，我蠻建議手動去嘗試 Kubernetes The Hard Way 的過程，至少可以讓自己對於部署 Kubernetes 的過程有更近一步的認知，這樣才不會遇到部署有問題時無從下手。 而若團隊在部署方面有很多經驗的話，那基於 Kubernetes The Hard Way 與 kubeadm 來開發一套公司專用的工具也是一個選擇。像是我們也有這樣做，透過分析 kubeadm 與手動部署的流程來了解每個部署過程，並以 Ansible 或 Puppet 方式來實現自動化。只是若開發都是由某一個人貢獻的話，那麼他離開時，就會面臨沒人維護問題。 Reference https://chrislema.com/staging-environment/ https://dzone.com/articles/13-reasons-why-staging-environment-is-failing-for-1 https://www.quora.com/What-is-difference-between-testing-environment-and-staging https://kubernetes.io/docs/setup/best-practices/ https://techbeacon.com/enterprise-it/6-best-practices-highly-available-kubernetes-clusters https://blog.sqreen.com/kubernetes-security-best-practices/ https://www.cio.com/article/3411994/kubernetes-security-best-practices-for-enterprise-deployment.html","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"淺談 Kubernetes 的部署工具選擇 Part2","slug":"ironman2020/day02","date":"2019-09-16T16:00:00.000Z","updated":"2019-12-02T01:49:42.387Z","comments":true,"path":"2019/09/17/ironman2020/day02/","link":"","permalink":"https://k2r2bai.com/2019/09/17/ironman2020/day02/","excerpt":"前言上一篇 淺談 Kubernetes 的部署工具選擇 Part1 提到選擇部署工具時，需要思考的幾件事情，並簡單提了一下目前 Kubernetes SIG cluster lifecycle 的發展。而今天我將依據不同環境與情境來說明自己怎麼選用 Kubernetes 部署工具。 原本這篇是要放在第一天一起講，但是發現一天超過 6000 字似乎有點壓力… 感覺三天後我就放推了，但因為是團隊報名，所以不能放棄!!!只好拆成三篇來分享。","text":"前言上一篇 淺談 Kubernetes 的部署工具選擇 Part1 提到選擇部署工具時，需要思考的幾件事情，並簡單提了一下目前 Kubernetes SIG cluster lifecycle 的發展。而今天我將依據不同環境與情境來說明自己怎麼選用 Kubernetes 部署工具。 原本這篇是要放在第一天一起講，但是發現一天超過 6000 字似乎有點壓力… 感覺三天後我就放推了，但因為是團隊報名，所以不能放棄!!!只好拆成三篇來分享。 接下來的內容，我將分成以下幾個用途來分享自己想法與如何選擇: 開發環境(Development) 測試環境(Testing) 預備環境(Staging) 生產環境(Production) 因為都只是自己想法，沒什麼參考價值，大家看看就好。 開發環境(Development)在公司時，我比較常負責開發各種 Kubernetes 相關整合元件(比如說 Controller/Operator、CNI/CSI/Device Plugin、Authentication webhook 等等)，以及提供客戶想要容器化的應用程式，並放到 Kubernetes 中。而其中開發 Kubernetes 整合元件部分又以 Controller 居多，因為客戶希望能夠透過 Kubernetes 機制來實現一些新功能或是管理硬體資源(如透過 Kubernetes resource 來管理防火牆設備等等)，或者希望能夠跟 Kubernetes 既有的 API resources 直接整合，如當建立 Service 時，自動設定對應防火牆的規則。而諸如此類的工作事情，往往需要一座實際的 Kubernetes 叢集來測試與驗證開發的功能，這時該怎麼選擇呢?在開始選擇前，我們先來看看被用於開發與測試的 Kubernetes 部署工具有哪些，下面簡單列幾個專案與方案: kubeadm minikube kind microk8s Kubernetes-powered Docker CE kube-spawn virtuakube Simplekube 除了這些，大家也可以看看 awesome-kubernetes 中的列表來查看其他工具。 大家會發現有這麼工具多可以使用，那是不是只要自己用的爽，或是反正團隊想用什麼就用什麼，這樣就好了? 答案是『Yes』，但我還是會進一步去考慮一些條件，比如說: 由Kubernetes 社區或某公司維護: 選擇社區與公司維護的原因在於，往往一個專案若沒有組織維護時，可能會因為本身專案不夠大(或者自身工作太忙碌)，而願意投入的開發者相對少，甚至只有自己時，就會無法持續跟進與改進，這樣時間久了自然就不會在維護。而這點尤其選擇Kubernetes 社區維護為佳。 可客製化程度: 能夠客製化部署的叢集好處在於能夠對一些特別的功能進行驗證，因為 Kubernetes 有些功能在不同版本不是都預設開啟，而在這種情況下，若使用的工具本身難以調整時，就會帶來一些麻煩。另外不同 Kubernetes 版本的支援，\b是希望確保開發的程式，能夠在同一機器驗證不同版本的執行結果，以降低不必要的時間浪費。 是否支援多節點(Multi-node)環境: 雖然開發環境大多情況下用單節點 Kubernetes 就能夠完成，但如果遇到是開發 Custom Scheduler 或者特殊應用時，能夠模擬多節點的 Kubernetes 叢集就極為重要。 是否會污染開發機器的環境: 大多情況下，我不太喜歡因為開發某個程式，就安裝一堆東西來污染整個環境，因為有些工具會安裝過多相依元件，這時可能會造成開發環境的元件難以清理。老實說這是奇摩子問題。 是否跨平台或作業系統: 最主要是確保團隊成員不管是使用哪個平台(或作業系統)進行開發，都能夠用同一套工具來驗證開發的結果，以保證團隊驗證環境的一致性。 能 3 - 5 行搞定: 使用工具本來就是為了降低繁雜的部署過程，因此能夠用最少的指令來完成那對於開發人員是最好不過，因為開發人員理論上不應該太專注於 Kubernetes 部署本身，而是要開發的功能。 考慮了這些條件後，我會快速地先過濾掉一些非社區或公司維護的工具，比如說上面列出來的 virtuakube、Simplekube 專案。相信大家點進去看過這些專案後，也會發現大多數個人維護的工具，都會在經過一段時間後，就慢慢沒有那麼頻繁維護，最後 EoL 並 Archived 整個程式碼專案。而當過濾掉這些後，就會進一步依據剩下條件來找出最佳的工具。 比如說用於測試開發程式或上手 Kubernetes 時，個人就特別推薦 minikube，因為它以下幾個好處: Kubernetes 社區維護，目前核心人員積極在改進功能。 能夠透過執行參數來改變 kube-apiserver、kube-controller-manager 與 kube-scheduler 等等元件的設定。 支援不同的平台(需要自己編譯)與作業系統。 提供不同 cluster bootstrapper。 minikube 的 cluster bootstrapper 目前雖然只有 kubeadm，但是 Kubernetes 社區有在規劃 Free-VM(kind)的部署。 支援不同虛擬機驅動(VM driver)。也支援 None VM 部署，但僅限於 Linux。 但 minikube 最可惜之處，在於無法模擬多節點部署的 Kubernetes 叢集，因此沒辦法驗證一些開發的東西。 事實上也不是不能，畢竟是開源專案，只要稍微改程式就可以了，大家可以參考 利用 Minikube 快速建立測試用多節點 Kubernetes 叢集 這篇文章。另一方面 minikube 團隊也把多節點部署視為 2019 年希望支援的功能，也看到會議上有持續在討論這個內容，大家可以參考 Add multi-node support 。 如果遇到多節點支援問題時，我就會選擇 kind 或 microk8s 作為替代方案，尤其是 kind 是由 Kubernetes 社區維護，其背後也是基於 kubeadm 的函式庫實現而成，因此能夠確保叢集是當前版本的最佳實踐。 這時有人會問說，為啥不直接用 kubeadm?上面很多工具都是基於它開發而成的啊。事情是這樣的，kubeadm 並不像其他工具能幫你建立執行的虛擬環境(如 VM 或 Container)，因此執行 kubeadm 的環境依然需要自己處理，而這過程會增加開發人員不必要的時間浪費，並且對開發這件事失焦。 當然喜歡用 Vagrant 的人，也是可以自己寫個 Vagrantfile + Shell/BASH 腳本來部署。 那 Docker 內建的 Kubernetes 呢?Docker 能夠簡單又快速地建立開發與測試用的 Kubernetes 環境，也是知名公司維護阿。確實上是如此沒錯，但我沒有特別推薦用於開發是因為 Kubernetes 是 Docker 工具附加的功能，所有版本跟設定幾乎都是綁死於特定的 Docker 版本，因此我們很難依據個人需求，去調整部署的 Kubernetes 叢集參數與版本。 測試環境(Testing)而當開發完功能後，往往會再額外撰寫單元測試(Unit tests)、煙霧測試(Smoke tests)與 E2E 測試(或整合測試)的程式，並使用持續性整合(Continuous Integration)平台自動化地在程式碼推送至 Git 時，執行團隊要求的測試步驟。比如說當單元測試完成後，會檢查測試覆蓋率與錯誤率是否符合要求，若符合的話，則接著進行 E2E 測試(或整合測試)，這時也需要實際部署一座用於測試的 Kubernetes 叢集來驗證功能。那我會怎麼做呢? 要不要把單元測試跟 E2E 測試(或整合測試)放一個階段執行要看團隊，後者往往需要部署一座 Kubernetes 叢集提供測試用，所以為了避免不必要的資源浪費，我比較偏向先跑完單元測試後，在判斷是否執行 E2E 測試(或整合測試)。 這部分我會依據以下幾件事來選擇使用的部署工具: \b\b確認使用的 CI 平台(工具): 由於開發的專案放置在不同 Git 儲存庫時，我會依據情況使用不同的 CI 來執行測試，比如說 GitHub 上就會用 Travis CI 來完成，但 Travis CI 無法讓我們透過 minikube 以虛擬機方式，建立一座測試用 Kubernetes 叢集，且使用 None driver 也可能影響到 Travis CI 提供的環境。這時我就會選用 kind 這種 Free-VM，但又有隔離性的工具來完成。當然若是自建的 CI 就另當別論，像是我比較喜歡在專案塞個 Vagrantfile 來完成。 儘可能與開發環境使用的工具一致: 確保開發環境若有對 Kubernetes 叢集做什麼更動時，測試環境也能透過同樣指令與設定來建立相同叢集。 能夠保留/刪除舊叢集，以及快速建立新叢集: 有時候我會利用上一次測試過的環境來執行新的 Commit，當測試完成後再刪除，並建立新叢集進行第二次測試。這是因為預備環境(Staging)或生產環境(Production)通常不會頻繁更動 Kubernetes 叢集，尤其是生產環境。 通過 CNCF Kubernetes Conformance: CNCF 的一致性測試會基於 Sonobuoy 工具對 Kubernetes 叢集進行各種測試。若使用的 Kubernetes 部署工具通過測試的話，理論上能在被認證的其他方案與專案上轉移。 當然這前提是不考慮太多網路與儲存狀況。 事實上，在測試環境中，我使用的工具大多是 minikube、kind 與 kubeadm with vagrant。但測試功能需要一座小規模節點數的裸機叢集時，我就會透過結合 Ansible 這種工具來搭配 kubeadm。 結語今天先分享自己在開發環境跟測試環境上的選擇，明天再進一步分享剩下的預備環境(Staging)與生產環境(Production)。老實說也沒有真的哪個方式是最好，只要能達成目標用什麼確實都可以。當然這是不考慮後續維護的前提下，若團隊長期會持續在 Kubernetes 上進行開發的話，我還是建議以 Kubernetes 社區維護的工具為主，至少能確保支援當前的 Kubernetes 的版本與最佳實踐。當然公司團隊有餘力的話，以 Kubernetes 社區的工具為基礎打造一套公司專用的 Kubernetes 部署工具也是不錯選擇。 其實原本想要今天也把預備環境(Staging)與生產環境(Production)寫完，但是發現好像只利用晚上時間有點困難… Reference http://k2r2bai.com https://www.hwchiu.com/travisci-k8s.html https://github.com/ramitsurana/awesome-kubernetes#installers https://blog.docker.com/2018/01/docker-mac-kubernetes/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"淺談 Kubernetes 的部署工具選擇 Part1","slug":"ironman2020/day01","date":"2019-09-15T16:00:00.000Z","updated":"2019-12-02T01:49:42.387Z","comments":true,"path":"2019/09/16/ironman2020/day01/","link":"","permalink":"https://k2r2bai.com/2019/09/16/ironman2020/day01/","excerpt":"前言我們都知道 Kubernetes 有很多特性可以確保系統與應用程式能夠更加穩定、部署更加流暢與管理更加簡單，而且使用起來也相對容易，但在自建(或地端)部署時，卻與想像的不同，有很多需要關注的問題與事情，例如:高可靠性(Highly Available，HA)、如何安全地更新叢集、裸機負載平衡器(Bare-metal Load Balancer)、動態 DNS 更新、硬體設備整合、分散式儲存整合與離線安裝(Offline Installation)等等，除了上述問題外，還要為了客戶能夠對部署的 Kubernetes 更有信心，因此必須進行壓力測試、E2E 測試、一致性測試與叢集安全掃描等等。從上述簡短的描述中，可以了解到自己蓋一套可以用的 Kubernetes 叢集，並且自己維運是多麼麻煩的事，這對公司來說都是成本支出。也正因此大多數人會偏向使用公有雲的 Kubernetes 服務，如 EKS、GKE 與 AKS 等等來減少維運成本的支出。下圖顯示不同部署的方式需要管理的部分。","text":"前言我們都知道 Kubernetes 有很多特性可以確保系統與應用程式能夠更加穩定、部署更加流暢與管理更加簡單，而且使用起來也相對容易，但在自建(或地端)部署時，卻與想像的不同，有很多需要關注的問題與事情，例如:高可靠性(Highly Available，HA)、如何安全地更新叢集、裸機負載平衡器(Bare-metal Load Balancer)、動態 DNS 更新、硬體設備整合、分散式儲存整合與離線安裝(Offline Installation)等等，除了上述問題外，還要為了客戶能夠對部署的 Kubernetes 更有信心，因此必須進行壓力測試、E2E 測試、一致性測試與叢集安全掃描等等。從上述簡短的描述中，可以了解到自己蓋一套可以用的 Kubernetes 叢集，並且自己維運是多麼麻煩的事，這對公司來說都是成本支出。也正因此大多數人會偏向使用公有雲的 Kubernetes 服務，如 EKS、GKE 與 AKS 等等來減少維運成本的支出。下圖顯示不同部署的方式需要管理的部分。 當然不是所有系統跟應用程式跑在 Kubernetes 就能活得好好的，整個好棒棒。 (圖片擷取自：kubernetes.io) 而在研替剛進去那一年，我因為在學期間有接觸過 Kubernetes 手動安裝，因此\b在還沒進入狀況下，被公司安排處理所有 Kubernetes 相關案子，這也使我從一開始對 Kubernetes 懵懵懂懂，到現在才覺得對 Kubernetes 有一點點認知(老實說還是一知半解XDD)。由於研替是在一家人員有限的公司，再加上大多數人對於 Kubernetes 不是那麼熟悉，作為一位軟體工程師的我，因此還需要兼顧不同角色，來解決幫助客戶自建的 Kubernetes 問題，因此在過程中學習到了一點點東西，而剛好有幸參加本次鐵人賽，想透過研替最後的 30 天來盡可能把經驗分享給大家。 而本系列文章將分為三大部分來分享經驗，分別為: On-Premise(Custom) Container 與 Kubernetes 經驗 Controller 與 Operator 開發經驗 Kubernetes Certification 經驗 On-Premise(Custom) Container 與 Kubernetes 經驗本部分將說明自己在 On-Premise(Custom) 部署的經驗，就如同前言提到的那些內容，我將說明自己在協助客戶自建 Kubernetes 時，使用到與學習到的東西，並帶大家了解如何實踐。 Controller 與 Operator 開發經驗本部分將說明如何開發 Kubernetes Controller/Operator，過程中將實現幾個範例來帶大家了解 Controller/Operator 原理，並教導如何正確使用 Finalizer、Leader election 等等機制與功能。 Kubernetes Certification 經驗本部分將說明如何準備與通過 Certified Kubernetes Administrator、Certified Kubernetes Application Developer、Kubernetes Certified Service Provider 與 Certified Kubernetes Conformance Program，這四項都是自己有經驗的部分。 由於認證題目與要求會隨著時間而改變，這邊只能盡可能提供一些 Tips 來幫助大家。 所以，Kubernetes 的部署工具該怎選擇?第一次接觸 Kubernetes 時，大多數人都會思考該用什麼工具部署比較好，因為現在市面上有太多 Kubernentes 部署方案與開源專案，因此選擇時，往往無從下手，比如說下圖內的工具，相信就夠大家花很多時間去研究與嘗試了。 而這麼多部署工具，究竟該怎麼選擇呢?哪一個才是最好的工具呢? 事實上，沒有一個部署工具與方案是適用於每一間公司的，因此在自建一座 Kubernetes 叢集前，我們必須依據需求，思考以下幾件事情: Kubernetes 支援的版本 HA(Highly Available) 能夠安全地更新叢集元件 叢集部署規模 是否會擴展節點(能輕易地新增與刪除節點) 自動化與客製化程度 部署叢集使用的情境與用途 支援的網路插件(CNI Plugin) 該工具是否有持續更新改進 團隊有沒有共識(這也很重要) 是否跨不同 CPU 指令集架構 是否跨不同作業系統 該工具是否通過一致性測試(CNCF Certified Kubernetes) 思考這幾點是因為部署完一座 Kubernetes 叢集後，接下來的維運才是真正的開始。當選擇了錯誤的工具與方案後，後續維運往往會受到影響，因此而增加困難性與複雜性，導致各種風險發生，比如說上述提到的 1 與 9 點，當一個工具因為無法跟上 Kubernetes 版本快速演進時，團隊就必須面臨更換的決策，而一但確定更換時，又必須面臨升級與遷移問題;又或者如 2 - 8 點沒考慮到時，隨便選擇一個工具就進行部署，結果該工具對於這些需求支援程度低，就會增加維運團隊的困難與時間成本，甚至面臨整組打掉重練的地步。 Kubernetes 採用 Semantic Versioning 方式來管理版本，其中以次要版本號(Minor version)與補丁版本號(Patch version)為主。通常一個新的次要版本大約3 個月釋出，並持續維護約9 個月就會進入 EoL(End of Life)，而補丁版本則大約2 - 3 週釋出，除非遇到重大錯誤。 而過去 Kubernetes 社區裡一直沒有足夠的重視這些問題，所以一直沒有一套完整的工具來降低 Kubernetes 部署與維運的困難。一直到 2016 年才在社區貢獻者的推動下，發起了由 Kubernetes 社區維護的部署工具 kubeadm。 kubeadm 是一位名為 Lucas Käldström 芬蘭人貢獻者發起，該貢獻者在當時僅是個高中生。我想這也是開源社區魅力所在，任何人都可以對社區進行貢獻，儘管貢獻的品質可能不是最好，但跟著成熟的開源社區中頂尖貢獻者持續地改善的話，終將會往好的方向發展。 而為何要提到 kubeadm 呢? 相信用過的人肯定會發現這工具真的很簡單，我們只需要幾個指令就可以部署一座 Kubernetes 叢集。kubeadm 設計非常精簡，又不失最佳實踐(Best Practices)理念，使大家會覺得很有『官方原生』的感覺，事實也是如此。既然官方有維護部署工具，那為什麼還有那麼多部署工具與方案呢?事情是這樣的，kubeadm 在過去版本雖然一直在實踐最佳的 Kubernetes 組態設定，但是卻缺乏了對生產環境重要的 HA 一環支援，因此許多人僅把這個工具當作測試用叢集部署使用。 然而經過 Kubernetes 社區持續的發展，kubeadm 在 2018 年 12 月的 v1.13 版本釋出時，正式進入 GA(Generally available) 階段，並被標示為 Production-Ready 的工具，且隨著後續版本的推進，對於 HA 的支援已經慢慢完善; 另一方面可以觀察到有些部署工具開始從自行實現部署方案，慢慢轉為以 kubeadm 為基礎進行部署與產生組態檔，比如說 kubespray、kops、KRIB、minikube、SUSE skuba 等等。 而為什麼以 kubeadm 為基礎最近部署與產生組態檔案比較好呢?我想這是因為 kubeadm 的架構設計能夠提供各種介面與方法來進行客製化 Kubernetes 從，如 phases 能讓使用者輕易地客製化每個部署階段，也正因為這樣的好處，所以上層工具開發者(商)只需要實現符合 kubeadm 的標準或 APIs 就能夠輕易改變叢集功能，此舉可以讓開發者(商)簡化複雜性，一來又能夠減少跟進新版的功能時間成本，直接藉由 Kubernetes 官方的最佳實踐，來保持與社區工具一致性。當然，這也是 kubeadm 維護團隊 SIG cluster lifecycle 團隊希望達到的，他們希望提供一系列工具鏈與 APIs 給大家開發自家的 Cluster provisioners，因此才會有越來越多工具往這方向發展，我想這樣發展是充滿信心的，畢竟自己在參與 Kubernetes 貢獻時，覺得 Kubernetes 是一個蠻成功的開源社區。 (圖片擷取自：KubeCon EU 2019) (圖片擷取自：KubeCon EU 2019) 今天僅是簡單說明選擇一個工具時，必須要思考幾件事情，並提一下目前社區的發展。或許有人看到這邊可能會想說:『現在開始選用 kubeadm 就對了?』，確實 kubeadm 能夠完成大多數事情，但當面臨不同規模的叢集時，還是要搭配不同工具與系統的結合，才能發揮最大效果，因為要考慮到部署與維運的時間成本，以及部署完節點一致性問題。比如說有大量節點需要部署作業系統，那該怎麼辦呢?因為自建叢集不像公有雲提供的服務一樣，能便利地建立所需的資源;又或者說節點太多，要如來確保節點部署的一致性呢?諸如此類問題是自建(地端)的 Kubernetes 叢集經常會遇到的，而下一篇我將依據不同情境來說明怎麼選擇工具，或如何利用 kubeadm 結合其他工具來盡可能解決，當然也會提一下自己實際上作法。 Reference https://kubernetes.io/docs/setup/ https://kccnceu19.sched.com/event/MPhI/intro-cluster-lifecycle-sig-lucas-kaldstrom-independent-tim-st-clair-vmware https://caylent.com/50-useful-kubernetes-tools https://github.com/ramitsurana/awesome-kubernetes#installers https://docs.google.com/spreadsheets/d/1LxSqBzjOxfGx3cmtZ4EbB_BGCxT_wlxW_xgHVVa23es/edit#gid=0 https://kubernetes.io/docs/setup/release/version-skew-policy/ https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#kubernetes-release-versioning https://kubernetes.io/blog/2019/06/24/automated-high-availability-in-kubeadm-v1.15-batteries-included-but-swappable/ https://kubernetes.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"},{"name":"IT Ironman","slug":"Kubernetes/IT-Ironman","permalink":"https://k2r2bai.com/categories/Kubernetes/IT-Ironman/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"KubeFed: Kubernetes Federation v2","slug":"kubernetes/federation-v2","date":"2019-04-19T16:00:00.000Z","updated":"2019-12-02T01:49:42.395Z","comments":true,"path":"2019/04/20/kubernetes/federation-v2/","link":"","permalink":"https://k2r2bai.com/2019/04/20/kubernetes/federation-v2/","excerpt":"Kubernetes Federation(聯邦) 一直是很有趣的議題，並被重視的功能，Federation 目的是希望實現單一叢集統一管理多個 Kubernetes 叢集的機制，這些叢集可能是跨地區(Region)，也可能是在不同公有雲供應商(Cloud Provider)上，亦或者是公司內部自行建立的叢集。一但叢集進行聯邦後，就可以利用 Federation API 資源來統一管理多個叢集的 Kubernetes API 資源，如定義 Deployment 如何部署到不同叢集上，其叢集所需的副本數等。 而 Kubernetes Federation 的誕生正是希望利用聯邦機制來解決一些問題，並達到一些好處，如以下： 簡化管理多個叢集的 Kubernetes 物件(如 Deployment, Service 等)。 在多個叢集之間分散工作負載(容器)，以提升應用程式(服務)的可靠性。 跨叢集的資源排程，依據排程策略在多個叢集進行應用程式(服務)部署。 在不同叢集中，能更快速更容易地遷移應用程式(服務)。 跨叢集的服務發現，服務可以提供給當地存取，以降低延遲。 實踐多雲(Multi-cloud)或混合雲(Hybird Cloud)的部署。","text":"Kubernetes Federation(聯邦) 一直是很有趣的議題，並被重視的功能，Federation 目的是希望實現單一叢集統一管理多個 Kubernetes 叢集的機制，這些叢集可能是跨地區(Region)，也可能是在不同公有雲供應商(Cloud Provider)上，亦或者是公司內部自行建立的叢集。一但叢集進行聯邦後，就可以利用 Federation API 資源來統一管理多個叢集的 Kubernetes API 資源，如定義 Deployment 如何部署到不同叢集上，其叢集所需的副本數等。 而 Kubernetes Federation 的誕生正是希望利用聯邦機制來解決一些問題，並達到一些好處，如以下： 簡化管理多個叢集的 Kubernetes 物件(如 Deployment, Service 等)。 在多個叢集之間分散工作負載(容器)，以提升應用程式(服務)的可靠性。 跨叢集的資源排程，依據排程策略在多個叢集進行應用程式(服務)部署。 在不同叢集中，能更快速更容易地遷移應用程式(服務)。 跨叢集的服務發現，服務可以提供給當地存取，以降低延遲。 實踐多雲(Multi-cloud)或混合雲(Hybird Cloud)的部署。 雖然 Kubernetes 社區很早之前就已實現 Federation(v1) 機制，但隨著 Kubernetes 成長，Federation v1 發展的越來越緩慢，並在之後版被棄用了\b\b，而 SIG Multi-Cluster 團隊也隨即提出新架構 Federation v2 來取代。 至於 v1 為何被棄用?而 v2 又帶來什麼變化呢?我將在接下來部分簡單說明。 為什麼 v1 被棄用？Federation v1 在 Kubernetes v1.\b3 左右時，就已經著手設計(Design Proposal)，並在後面幾個版本中釋出了相關的元件與命令列工具(kubefed)，來用於幫助使用者快速建立聯邦叢集，並在 Kubernetes v1.6 時，進入了 Beta 階段。 但 Federation v1 在進入 Beta 後，就沒有更進一步的發展，一直到 Kubernetes v1.11 左右正式被棄用。至於為什麼被棄用呢？這是因為開發團隊認為叢集聯邦的實踐比想象中還要困難，有許多問題是 v1 架構沒被考慮進去的，比如說： 控制平面元件會因為發生問題，而影響整體叢集效能。 無法相容新的 Kubernetes API 資源。 無法有效的在多個叢集管理權限，如不支援 RBAC。 聯邦層級的設定與策略依賴 API 資源的 Annotations 物件內容，這使得彈性不佳。 而這些問題基本上是當初設計架構所延伸而來。我們可以透過了解整體架構來更容易知道問題點。 從上圖架構中得知 Federation v1 的設計沿用類似 Kubernetes 控制平面架構，其主要元件有以下: federation-apiserver: 提供 Federation API 資源，只支援部分 Kubernetes API resources。 federation-controller-manager: 協調不同叢集之間的狀態，如同步 Federated 資源與策略，並建立 Kubernetes 物件至對應叢集上。 etcd: 儲存 Federation 的狀態。 Federation v1 提供了kubefed工具來簡化\b安裝 Federation 控制平面元件的過程，並提供新增/刪除叢集的操作。 從程式碼大致可以了解 Federation v1 的 API Server 在基礎上是透過 k8s.io/apiserver 套件開發，這種是採用 API Aggregation 方式來擴充 Kubernetes API。而 Federated API 資源則是實作 Adapter 來管理 Kubernetes API 資源，但這些 Adapter 都是寫死 API 版本的，比如說 Deployment 只支援extensions/v1beta1版本 API，所以當建立一個apps/v1的 Deployment，並且在 Annotations 設定聯邦策略時，就會無法正常運作。 因此，若想要支援其他資源(如 CronJob)或版本，就必須在 Federation types 新增對應的 Adapter，然後透過 Code Generator 產生 API 的 client-go 套件給 Controller Manager 操作 API 使用，最後重新建構一版本來更新 API Server 與 Controller Manager 以提供對其他資源與版本的支援。另外 Federation v1 在設計之初未考慮 RBAC 與 CRD(TPR) 功能，因此無法提供跨叢集的權限控管，以及客製化資源的擴展。從上述狀況中，就能感受到 Federation v1 在擴展性有很多挑戰。 也正因為在架構上無法很好解決這些問題，Federation v1 才會發展的越來越緩慢，並在最後被棄用。當然也是因為有這樣的發展過程與經驗，才能有 Federation v2 的誕生。 Federation v2Kubernetes Cluster Federation 又名 KubeFed 或 Federation v2，是 Kubernetes SIG Multi-Cluster 團隊新提出的叢集聯邦架構(Architecture Doc 與 Brainstorming Doc)，新架構在 Federation v1 基礎之上，簡化擴展 Federated API 過程，並加強跨叢集服務發現與排程的功能。另外 KubeFed 在設計之初，有兩個最重要核心理念是 KubeFed 希望實現的，分別為Modularization（模組化）與Customizable(可客制)，這兩個理念大概是希望 KubeFed 能夠跟隨著 Kubernetes 生態發展，並持續保持相容性與擴展性。 KubeFed 在元件上最大改變是將 API Server 移除，並透過 CRD 機制來完成 Federated Resources 的擴充。而 KubeFed Controller 則管理\b這些 CRD，並實作同步 Resources、跨叢集排程等功能。從架構圖中看到，KubeFed 提出了一些新概念來加強功能的擴展。 Concepts目前 KubeFed 透過 CRD 方式新增了四種 API 群組來實現聯邦機制的核心功能： API Group 用途 core.kubefed.k8s.io 叢集組態、聯邦資源組態、KubeFed Controller 設定檔等。 types.kubefed.k8s.io 被聯邦的 Kubernetes API 資源。 scheduling.kubefed.k8s.io 副本排程策略。 multiclusterdns.kubefed.k8s.io 跨叢集服務發現設定。 而在這些核心功能中，我們必須先瞭解一些 KebeFed 提出的基礎概念後，才能更清楚知道 KubeFed 是如何運作的。 Cluster Configuration用來定義哪些 Kubernetes 叢集要被聯邦。可透過kubefedctl join/unjoin來加入/刪除叢集，當成功加入時，會建立一個 KubeFedCluster 物件來儲存叢集相關資訊，如 API Endpoint、CA Bundle 等。這些資訊會被用在 KubeFed Controller 存取不同 Kubernetes 叢集上，以確保能夠建立 Kubernetes API 資源，示意圖如下所示。 在 Federation 中，會區分 Host 與 Member 兩種類型叢集。 Host: 用於提供 KubeFed API 與控制平面的叢集。 Member: 透過 KubeFed API 註冊的叢集，並提供相關身份憑證來讓 KubeFed Controller 能夠存取叢集。Host 叢集也可以作為 Member 被加入。 Type Configuration定義了哪些 Kubernetes API 資源要被用於聯邦管理。比如說想將 ConfigMap 資源透過聯邦機制建立在不同叢集上時，就必須先在 Federation Host 叢集中，透過 CRD 建立新資源 FederatedConfigMap，接著再建立名稱為 configmaps 的 Type configuration(FederatedTypeConfig) 資源，然後描述 ConfigMap 要被 FederatedConfigMap 所管理，這樣 KubeFed Controllers 才能知道如何建立 Federated 資源。以下為簡單範例： apiVersion: core.kubefed.k8s.io/v1beta1kind: FederatedTypeConfigmetadata: name: configmaps namespace: kube-federation-systemspec: federatedType: group: types.kubefed.k8s.io kind: FederatedConfigMap pluralName: federatedconfigmaps scope: Namespaced version: v1beta1 propagation: Enabled targetType: kind: ConfigMap pluralName: configmaps scope: Namespaced version: v1 若想新增 CRD 的 Federated API 的話，可透過 kubefedctl enable &lt;res&gt; 指令來建立，如下: $ kubefedctl enable etcdclusters$ kubectl api-resources | grep etcdetcdclusters etcd etcd.database.coreos.com true EtcdClusterfederatedetcdclusters fetcd types.kubefed.k8s.io true FederatedEtcdCluster$ kubectl -n kube-federation-system get federatedtypeconfigs | grep etcdetcdclusters.etcd.database.coreos.com 3m16s 而一個 Federated 資源一般都會具備三個主要功能，這些資訊能夠在 spec 中由使用者自行定義，如下範例： apiVersion: types.kubefed.k8s.io/v1beta1kind: FederatedDeploymentmetadata: name: test-deployment namespace: test-namespacespec: template: # 定義 Deployment 的所有內容，可理解成 Deployment 與 Pod 之間的關析。 metadata: labels: app: nginx spec: ... placement: # 定義哪些叢集要建立該 Federated 物件。 clusters: - name: cluster2 - name: cluster1 overrides: # 定義叢集的 spec.template 中哪個欄位要被覆寫。 - clusterName: cluster2 clusterOverrides: - path: spec.replicas value: 5 Template: 定義 Federated 資源所引用的(即 Type configuration 的目標資源)物件資訊。如 FederatedDeployment 中的spec.template會定義 Deployment 所有內容。 Placement: 定義 Federated 資源要分散到哪些叢集上，若沒有該物件，則不會分散到任何叢集中。如 FederatedDeployment 中的spec.placement定義了兩個叢集時，這些叢集將被同步建立相同的 Deployment。另外也支援用 spec.placement.clusterSelector 的方式來選擇要放置的叢集。 Override: 定義修改指定叢集的 Federated 資源中的spec.template內容。如部署FederatedDeployment 到不同公有雲上的叢集時，就能透過spec.overrides來調整 Volume 或副本數。 目前 Override 不支援List(Array)的欄位。比如說無法修改spec.template.spec.containers[0].image。 SchedulingKubeFed 提供了一種自動化機制來將工作負載實例分散到不同的叢集中，這能夠基於總副本數與叢集的放置策略來將 Deployment 或 ReplicaSet 資源進行排程。\b排程策略是透過建立 ReplicaSchedulingPreference(RSP) 物件，再由 KubeFed RSP Controller 監聽與擷取 RSP 內容來將工作負載實例建立到指定的叢集上。 以下為一個 RSP 範例。假設有三個叢集被聯邦，名稱分別為 ap-northeast、us-east 與 us-west。 apiVersion: scheduling.kubefed.k8s.io/v1alpha1kind: ReplicaSchedulingPreferencemetadata: name: test-deployment namespace: test-nsspec: targetKind: FederatedDeployment totalReplicas: 15 # 所有叢集 Pod 副本數總和 clusters: # 定義叢集排程策略 \"*\": weight: 2 maxReplicas: 12 ap-northeast: minReplicas: 1 maxReplicas: 3 weight: 1 當該範例建立後，RSP Controller 會收到資源，並匹配對應 namespace/name 的 FederatedDeployment 與 FederatedReplicaSet 是否存在，若存在的話，會根據設定的策略計算出每個叢集預期的副本數，之後覆寫 Federated 資源中的spec.overrides內容以修改每個叢集的副本數，最後再由 KubeFed Sync Controller 來同步至每個叢集的 Deployment。以上面為例，結果會是 ap-northeast 叢集會擁有 3 個 Pod，us-east 跟 us-west 則分別會有 6 個 Pod。 若spec.clusters未定義的話，則預設為{&quot;*&quot;:{Weight: 1}}。 若有定義 spec.replicas 的 overrides 時，副本會以 RSP 為優先考量。 分配的計算機制可以參考 kubefed/pkg/controller/util/planner/planner.go。 Multi-Cluster DNSKubeFed 提供了一組 API 資源，以及 Controllers 來實現跨叢集 Service/Ingress 的 DNS records 自動產生機制，並結合 ExternalDNS 來同步更新至 DNS 服務供應商。以下為簡單範例： apiVersion: multiclusterdns.kubefed.k8s.io/v1alpha1kind: Domainmetadata: name: test namespace: kube-federation-systemdomain: k8s.example.com---apiVersion: multiclusterdns.kubefed.k8s.io/v1alpha1kind: ServiceDNSRecordmetadata: name: nginx namespace: developmentspec: domainRef: test recordTTL: 300 首先假設已建立一個名稱為 nginx 的 FederatedDeployment，然後放到 development namespace 中，並且也建立了對應的 FederatedService 提供 LoadBalancer。這時當建立上述 Domain 與 ServiceDNSRecord 後，KubeFed 的 Service DNS Controller 會依據 ServiceDNSRecord 物件內容，去收集不同叢集的 Service 資訊，並將這些資訊更新至 ServiceDNSRecord 狀態中，接著 DNS Endpoint Controller 會依據該 ServiceDNSRecord 的狀態內容，建立一個 DNSEndpoint 物件，並產生 DNS records 資源，最後再由 ExternalDNS 來同步更新 DNS records 至 DNS 供應商。下圖是 Service DNS 建立的架構。 若是 Ingress 的話，會由 IngressDNSRecord 物件取代，並由 Ingress DNS Controller 收集資訊。 Setup Federation v2 on AWS本節將簡單說明如何在 AWS 建立跨不同地區的 Kubernetes 聯邦叢集，其架構如下圖所示。 KubeFed 官方文件也提供了 On-premises(minikube 與 kind)、GKE 與 IBM ICP 的教學，來讓使用者快速學習如何建置。 事前準備開始前，需要先安裝下列工具到操作機器上來提供使用： kubectl：用來操作部署完成的 Kubernetes 叢集。版本為 v1.14.1。 kops：用來部署與管理公有雲上的 Kubernetes 叢集。版本為 v1.14.0。 Mac OS X： $ brew update &amp;&amp; brew install kops Linux distro： $ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops helm: 用來部署 Federation v2 元件的工具。 kubefedctl：用來新增與加入叢集成為聯邦的工具。可以到連結中下載二進制檔，版本為 v0.1.0-rc1 AWS CLI：用來操作 AWS 服務的工具。 $ sudo pip install awscli$ aws --versionaws-cli/1.15.4 上述工具完成後，還要準備一下資訊： 申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。 一般來說只需開啟 S3、Route53、EC2、EBS、ELB 與 VPC 權限，但由於偷懶就全開。以下為各 AWS 服務在本次安裝中的用意： IAM: 提供身份認證與存取管理。 EC2: Kubernetes 叢集部署的虛擬機環境。 ELB: Kubernetes 元件與 Service 負載平衡。 Route53: 提供 Public domain 存取 Kubernetes 叢集環境與應用程式。 S3: 儲存 Kops 狀態。 VPC: 提供 Kubernetes 的 Host 與 CNI 網路環境。 擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。 設定腳本參數教學將使用撰寫好的腳本 aws-k8s-federation 進行部署。首先在操作的節點透過 Git 取得腳本： $ git clone https://github.com/kairen/aws-k8s-federation$ cd aws-k8s-federation$ cp .env.sample .env 編輯.env檔案並修改一下參數： # Kubernetes versionexport KUBERNETES_VERSION=\"1.14.1\"# Your domain name and envsexport DOMAIN_NAME=\"k8s.example.com\" # 你的 Domain Name(這邊為 &lt;hoste_dzone_name&gt;.&lt;domain_name&gt;) 建立 Route53 Hosted Zone首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey： $ aws configureAWS Access Key ID [****************QGEA]:AWS Secret Access Key [****************zJ+w]:Default region name [None]:Default output format [None]: 設定的 Keys 可以在~/.aws/credentials找到。 接著需要在 Route53 建立一個 Hosted Zone，並在 Domain Name 供應商上設定NameServers： $ ./0-create-hosted-domain.sh# output...&#123; \"HostedZone\": &#123; \"ResourceRecordSetCount\": 2, \"CallerReference\": \"2018-04-25-16:16\", \"Config\": &#123; \"PrivateZone\": false &#125;, \"Id\": \"/hostedzone/Z2JR49ADZ0P3WC\", \"Name\": \"k8s.example.com.\" &#125;, \"DelegationSet\": &#123; \"NameServers\": [ \"ns-1547.awsdns-01.co.uk\", \"ns-1052.awsdns-03.org\", \"ns-886.awsdns-46.net\", \"ns-164.awsdns-20.com\" ] &#125;, \"Location\": \"https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC\", \"ChangeInfo\": &#123; \"Status\": \"PENDING\", \"SubmittedAt\": \"2018-04-25T08:16:57.462Z\", \"Id\": \"/change/C3802PE0C1JVW2\" &#125;&#125; 之後將上述NameServers新增至自己的 Domain name 的 record 中，如 Godaddy： 部署叢集當上述流程都完成後，即可依照腳本順序來完成 Kubernetes 叢集與 Federation 叢集的建立，過程中也會包含操作 Federation v2 的功能。最終會以 Federated 物件建立一個 NGINX 應用程式，並部署到不同叢集中，然後透過 MultiClusterDNS + ExternalDNS 來自動建立 DNS records 到 AWS 上。 Summary雖然 KubeFed 還處於 Alpha 階段，但整體架構基於 Federation v1 進行了很多改善，而從 GitHub 近幾個月的貢獻來看，可以發現 Red Hat 在此專案投入許多的貢獻，這對於 KubeFed 來說是很好的發展。而現在 KubeFed 官方也有釋出一些文件，用來幫助使用者快速在 Minikube、kind(Kubernetes IN Docker)、GCP 上部署，這對於初次接觸的人是一項福利。 另外有趣的是從 OpenShift v4.0 Roadmap 中，也能看到 KubeFed 被放入其中，在 Operator Hub 跟 federation-v2-operator 也能看到 Red Hat 在這方面的佈局。我想之後 Kubernetes 叢集聯邦又會再次成為熱烈討論的議題，或許 Red Hat 也是希望透過 KubeFed 實現 OpenShift 的多雲(Multi-cloud)與混合雲(Hybird Cloud)部署吧(個人覺得拉)。 不過這不完全表示 KubeFed 在今後勢必會發展得很好，因為隨著 Kubernetes 發展，想必還會有更多的問題需要去發現、提出與解決的，比如說有狀態服務(Stateful Service)、Federation Host 叢集掛了怎麼辦等問題，這些問題若之後有相關社群的資訊，我也會更新上來。 Related Posts https://k2r2bai.com/2018/03/21/kubernetes/federation-v1/ https://k2r2bai.com/2018/04/21/kubernetes/aws-federation-v1/ https://speakerdeck.com/kairen/setup-kubernetes-federation-v2-on-aws https://speakerdeck.com/kairen/setup-eks-multi-cluster-using-federation-v2 References https://github.com/kubernetes-sigs/kubefed https://kubernetes.io/blog/2018/12/12/kubernetes-federation-evolution/ https://containerjournal.com/2019/04/09/kubernetes-and-the-challenge-of-federation/ https://blog.openshift.com/combining-federation-v2-and-istio-multicluster/ https://medium.com/condenastengineering/k8s-federation-v2-a-guide-on-how-to-get-started-ec9cc26b1fa7 https://podctl.com/multi-cluster-and-federation-v2/ https://www.youtube.com/watch?v=q27rbaX5Jis&amp;feature=youtu.be&amp;t=7m20s","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Federation","slug":"Federation","permalink":"https://k2r2bai.com/tags/Federation/"}]},{"title":"利用 Minikube 快速建立測試用多節點 Kubernetes 叢集","slug":"kubernetes/deploy/minikube-multi-node","date":"2019-01-22T09:08:54.000Z","updated":"2019-12-02T01:49:42.394Z","comments":true,"path":"2019/01/22/kubernetes/deploy/minikube-multi-node/","link":"","permalink":"https://k2r2bai.com/2019/01/22/kubernetes/deploy/minikube-multi-node/","excerpt":"本文將說明如何透過 Minikube 建立多節點 Kubernetes 叢集。一般來說 Minikube 僅提供單節點功能，即透過虛擬機建立僅有一個具備 Master/Node 節點的 Kubernetes 叢集，但由時候需要測試多節點功能，因此自己改了一下 Minikube 來支援最新版本(v1.13.2)的多節點部署，且 CNI Plugin 採用 Calico，以方便測試 Network Policy 功能。","text":"本文將說明如何透過 Minikube 建立多節點 Kubernetes 叢集。一般來說 Minikube 僅提供單節點功能，即透過虛擬機建立僅有一個具備 Master/Node 節點的 Kubernetes 叢集，但由時候需要測試多節點功能，因此自己改了一下 Minikube 來支援最新版本(v1.13.2)的多節點部署，且 CNI Plugin 採用 Calico，以方便測試 Network Policy 功能。 事前準備開始部署叢集前需先確保以下條件已達成： 在測試機器下載 Minikube 二進制執行檔： Linux Mac OS X Windows 如果上面連結掛了，可以透過以下方式安裝： $ git clone https://github.com/kairen/minikube.git -b multi-node $GOPATH/src/k8s.io/minikube$ cd $GOPATH/src/k8s.io/minikube$ make 在測試機器下載 Virtual Box 來提供給 Minikube 建立虛擬機。 IMPORTANT: 測試機器記得開啟 VT-x or AMD-v virtualization. 雖然建議用 VBox，但是討厭 Oracle 的人可以改用其他虛擬化工具(ex: KVM, xhyve)，理論上可以動。 下載所屬作業系統的 kubeclt。 目前已測試過 Ubuntu 16.04 Desktop、Mac OS X 與 Windows 10 作業系統。 Windows 使用者建議用 git bash 來操作。 建立叢集本節將說明如何建立 Master 與 Node 節點，並將這些節點組成一個叢集。 在開始前確認之前是否已經裝過 Minikube，若有的話，就把上面下載二進制檔放任意方便你執行的位置，或者直接取代之前的，然後再開始前請先刪除 Home 目錄的.minikube資料夾： $ rm -rf $HOME/.minikube Master 節點首先透過 Minikube 執行以下指令來啟動 Master 節點，並透過 kubectl 檢查： $ minikube --profile k8s-m1 start --network-plugin=cni...Everything looks great. Please enjoy minikube!$ kubectl -n kube-system get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScalico-node-8cbc6 2/2 Running 0 6m29s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;coredns-86c58d9df4-4nzlx 1/1 Running 0 6m32s 10.244.0.3 k8s-m1 &lt;none&gt; &lt;none&gt;coredns-86c58d9df4-9879v 1/1 Running 0 6m32s 10.244.0.2 k8s-m1 &lt;none&gt; &lt;none&gt;etcd-k8s-m1 1/1 Running 0 5m58s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-addon-manager-k8s-m1 1/1 Running 0 5m47s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-m1 1/1 Running 0 5m43s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-controller-manager-k8s-m1 1/1 Running 0 5m47s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-proxy-qnq25 1/1 Running 0 6m32s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-scheduler-k8s-m1 1/1 Running 0 5m59s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;storage-provisioner 1/1 Running 0 6m30s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt; --vm-driver 可以選擇使用其他 VM driver 來啟動虛擬機，如 xhyve、hyperv、hyperkit 與 kvm2 等等。 完成後，確認 k8s-m1 節點處於 Ready 狀態： $ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-m1 Ready master 2m8s v1.13.2 Node 節點確認 Master 完成後，這邊接著透過 Minikube 開啟新的節點來加入： $ minikube --profile k8s-n1 start --network-plugin=cni --node...Stopping extra container runtimes...# 接著取得 Master IP 與 Token$ minikube --profile k8s-m1 ssh \"ifconfig eth1\"$ minikube --profile k8s-m1 ssh \"sudo kubeadm token list\"# 執行以下指令進入 k8s-n1$ minikube --profile k8s-n1 ssh# 這邊為進入 k8s-n1 VM 內執行的指令$ sudo su -$ TOKEN=7rzqkm.1goumlnntalpxvw0$ MASTER_IP=192.168.99.100$ kubeadm join --token $&#123;TOKEN&#125; $&#123;MASTER_IP&#125;:8443 \\ --discovery-token-unsafe-skip-ca-verification \\ --ignore-preflight-errors=Swap \\ --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests# 看到以下結果後，即可以在 k8s-m1 context 來操作。...Run 'kubectl get nodes' on the master to see this node join the cluster. 另外上面的 IP 有可能會不同，請確認 Master 節點 IP。 其他節點以此類推。 完成後，透過 kubectl 檢查 Node 是否有加入叢集： $ kubectl config use-context k8s-m1Switched to context \"k8s-m1\".$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-m1 Ready master 3m44s v1.13.2k8s-n1 Ready &lt;none&gt; 80s v1.13.2$ kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-Ut1k5mLXpXVsyZwjn2z2-fpie9HHyTkMU7wnrjDnD3E 118s system:bootstrap:3qeeeu Approved,Issued$ kubectl -n kube-system get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScalico-node-qxkw5 2/2 Running 0 86s 192.168.99.101 k8s-n1 &lt;none&gt; &lt;none&gt;calico-node-srhlk 2/2 Running 0 3m24s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;coredns-86c58d9df4-826nz 1/1 Running 0 3m27s 10.244.0.3 k8s-m1 &lt;none&gt; &lt;none&gt;coredns-86c58d9df4-9z7mr 1/1 Running 0 3m27s 10.244.0.2 k8s-m1 &lt;none&gt; &lt;none&gt;etcd-k8s-m1 1/1 Running 0 2m40s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-addon-manager-k8s-m1 1/1 Running 0 3m48s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-addon-manager-k8s-n1 1/1 Running 0 86s 192.168.99.101 k8s-n1 &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-m1 1/1 Running 0 2m36s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-controller-manager-k8s-m1 1/1 Running 0 2m50s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-proxy-768w8 1/1 Running 0 86s 192.168.99.101 k8s-n1 &lt;none&gt; &lt;none&gt;kube-proxy-b7ndj 1/1 Running 0 3m27s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;kube-scheduler-k8s-m1 1/1 Running 0 2m46s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt;storage-provisioner 1/1 Running 0 3m17s 192.168.99.100 k8s-m1 &lt;none&gt; &lt;none&gt; 這樣一個 Kubernetes 叢集就完成了，速度快一點不到 10 分鐘就可以建立好了。 刪除虛擬機與檔案最後若想清除環境的話，直接刪除虛擬機即可： $ minikube --profile &lt;node_name&gt; delete 而檔案只要刪除 Home 目錄的.minikube資料夾，以及minikube執行檔即可。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Calico","slug":"Calico","permalink":"https://k2r2bai.com/tags/Calico/"}]},{"title":"透過原始碼建構 Docker 與 NVIDIA Docker","slug":"container/build-docker","date":"2018-10-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.380Z","comments":true,"path":"2018/10/17/container/build-docker/","link":"","permalink":"https://k2r2bai.com/2018/10/17/container/build-docker/","excerpt":"本文說明如何從 GitHub 上的原始碼專案，來建構 Docker 與 NVIDIA Docker Package。由於過程需要修改多個專案，因此開始前需要先了解一下將修改的 GitHub repos： Moby: 建構 Docker server side 的開源專案，前身為 Docker Engine 專案。 Docker CE: 建構 Docker CE 整套工具的專案，最上層沒下 License，屬於 Docker 官方產品，因此受到 EULA 限制，但該專案底下的 Engine 與 CLI 都是 Apache v2，故這邊只拿 CLI 來使用。 NVIDIA Docker: 用以建構 NVIDIA Docker 工具。 NVIDIA Container Runtime: 提供 NVIDIA 的 Container runtime。 Libnvidia Container: Container runtime 的底層函式庫。","text":"本文說明如何從 GitHub 上的原始碼專案，來建構 Docker 與 NVIDIA Docker Package。由於過程需要修改多個專案，因此開始前需要先了解一下將修改的 GitHub repos： Moby: 建構 Docker server side 的開源專案，前身為 Docker Engine 專案。 Docker CE: 建構 Docker CE 整套工具的專案，最上層沒下 License，屬於 Docker 官方產品，因此受到 EULA 限制，但該專案底下的 Engine 與 CLI 都是 Apache v2，故這邊只拿 CLI 來使用。 NVIDIA Docker: 用以建構 NVIDIA Docker 工具。 NVIDIA Container Runtime: 提供 NVIDIA 的 Container runtime。 Libnvidia Container: Container runtime 的底層函式庫。 事前準備開始建構前需要確保以下條件已達成： Host節點需要安裝 Docker 容器引擎： $ curl -fsSL \"https://get.docker.com/\" | sh Ubuntu建構 Docker Server 與 Client本節將分別透過 Moby 與 Docker CE 建構 Server 與 Client 二進制執行檔，並透過簡單 dpkg 來封裝成 deb packages。 Moby首先透過 Git 取得 Moby 最新版本專案，並透過 Make tool 編譯： $ cd ~/$ git clone https://github.com/moby/moby$ cd moby$ VERSION=1.0.0-terran make binary...Created binary: bundles/binary-daemon/dockerd-1.0.0-terranCopying nested executables into bundles/binary-daemon 當完成後，可以在bundles/binary-daemon看到建構的檔案： $ rm -rf bundles/binary-daemon/*.sha256 bundles/binary-daemon/*.md5 bundles/binary-daemon/dockerd$ mv bundles/binary-daemon/dockerd-1.0.0-terran bundles/binary-daemon/dockerd$ ls bundles/binary-daemondocker-containerd docker-containerd-ctr docker-containerd-shim docker-init docker-proxy docker-runc dockerd Docker CE首先透過 Git 取得 Docker CE 最新版本專案： $ cd ~/$ git clone https://github.com/docker/docker-ce.git$ cd docker-ce$ echo \"1.0.0-terran\" &gt; VERSION$ make static...make[2]: Leaving directory '/root/docker-ce/components/packaging/static'make[1]: Leaving directory '/root/docker-ce/components/packaging' 當完成後，可查看components/packaging/static/build/linux/docker底下檔案： $ ls components/packaging/static/build/linux/dockerdocker docker-containerd docker-containerd-ctr docker-containerd-shim docker-init docker-proxy docker-runc dockerd 這邊 dockerd 等檔案如 Moby 一樣，但為了避免被哭哭所以只拿 docker client 來用。 建構 deb 檔案首先建立一個資料夾terran-docker，並建立下列目錄結構： $ cd ~/$ mkdir terran-docker$ mkdir -p terran-docker/DEBIAN \\ terran-docker/lib/systemd/system \\ terran-docker/usr/local/bin 結果如下所示： $ tree terran-docker/terran-docker/|-- DEBIAN|-- lib| `-- systemd| `-- system`-- usr `-- local `-- bin 在terran-docker/DEBIAN新增檔案control來描述 deb packages： Package: dockerVersion: 1.0.0-terranArchitecture: amd64Description: This is the terran project docker packages.Maintainer: Kyle Bai &lt;kyle.b@inwinstack.com&gt;Depends: iptables, init-system-helpers (&gt;= 1.18~), lsb-base (&gt;= 4.1+Debian11ubuntu7), libc6 (&gt;= 2.17), libdevmapper1.02.1 (&gt;= 2:1.02.97), libltdl7 (&gt;= 2.4.6), libseccomp2 (&gt;= 2.3.0), libsystemd0Recommends: aufs-tools, ca-certificates, cgroupfs-mount | cgroup-lite, git, pigz, xz-utils, apparmor 接著在terran-docker/DEBIAN新增檔案postinst，加入以下內容： #!/bin/sh## Preinstall scripts# Release May 15, 2018. Kyle Bai &lt;kyle.b@inwinstack.com&gt;set -ecase \"$1\" in configure) if [ -z \"$2\" ]; then if ! getent group docker &gt; /dev/null; then groupadd --system docker fi fi systemctl enable docker.socket systemctl enable docker.service ;; abort-*) # How'd we get here?? exit 1 ;; *) ;;esac#DEBHELPER# 修改terran-docker/DEBAIN/postinst腳本權限： $ chmod 755 terran-docker/DEBIAN/postinst 在terran-docker/lib/systemd/system 新增檔案docker.socket，並加入以下內容： [Unit]Description=Terran Docker Socket for the APIPartOf=docker.service[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.target 在terran-docker/lib/systemd/system 新增檔案docker.service，並加入以下內容： [Unit]Description=Terran Docker Container EngineAfter=network-online.target docker.socket firewalld.serviceWants=network-online.targetRequires=docker.socket[Service]Type=notifyExecStart=/usr/local/bin/dockerd -H fd://ExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=1048576LimitNPROC=infinityLimitCORE=infinityTasksMax=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target 複製 Binaries 檔案到usr/local/bin，如下指令： $ cp -rp moby/bundles/binary-daemon/* terran-docker/usr/local/bin$ cp -rp docker-ce/components/packaging/static/build/linux/docker/docker terran-docker/usr/local/bin 確認檔案如以下結構： $ tree terran-docker/terran-docker/|-- DEBIAN| |-- control| `-- postinst|-- lib| `-- systemd| `-- system| |-- docker.service| `-- docker.socket`-- usr `-- local `-- bin |-- docker |-- docker-containerd |-- docker-containerd-ctr |-- docker-containerd-shim |-- dockerd |-- docker-init |-- docker-proxy `-- docker-runc 完成後透過 dpkg 指令來建置 Debain package 檔： $ dpkg -b terran-dockerdpkg-deb: building package 'terran-docker' in 'terran-docker.deb'.$ ls *.debterran-docker.deb$ dpkg -f terran-docker.deb 建構 NVIDIA Docker 工具本步驟將需要分別建構三個專案來才能完整的使用 NVIDIA Docker。 Libnvidia Container首先透過 Git 取得 Libnvidia 最新版本專案原始碼，並進行建構： $ git clone https://github.com/NVIDIA/libnvidia-container.git$ cd libnvidia-container$ make docker-ubuntu:16.04 TAG=rc.1 完成後，可以查看dist/ubuntu16.04/底下的檔案： $ ls dist/ubuntu16.04/libnvidia-container-dev_1.0.0~rc.1-1_amd64.deb libnvidia-container1_1.0.0~rc.1-1_amd64.deb libnvidia-container-tools_1.0.0~rc.1-1_amd64.deb libnvidia-container1-dbg_1.0.0~rc.1-1_amd64.deb NVIDIA Container Runtime首先透過 Git 取得 Container Runtime 最新版本專案原始碼： $ git clone https://github.com/NVIDIA/nvidia-container-runtime.git$ cd nvidia-container-runtime 編輯runtime/Makefile修改以下內容： # line 17ubuntu16.04: $(addsuffix -ubuntu16.04, 1.0.0-terran)# line 311.0.0-terran-%-runc: echo \"4fc53a81fb7c994640722ac585fa9ca548971871\" 接著執行以下指令建構 deb packages 檔案： $ make ubuntu16.04...dpkg-buildpackage: binary-only upload (no source included)make[1]: Leaving directory '/root/nvidia-container-runtime/hook' 完成後查看dist/ubuntu16.04/目錄底下檔案： $ ls dist/ubuntu16.04/nvidia-container-runtime-hook_1.3.0-1_amd64.deb nvidia-container-runtime_2.0.0+docker1.0.0-terran-1_amd64.deb NVIDIA Docker首先透過 Git 取得 NVIDIA Docker 最新版本專案原始碼： $ git clone https://github.com/NVIDIA/nvidia-docker.git$ cd nvidia-docker 編輯Makefile，並修改以下內容： ...# line 18ubuntu16.04: $(addsuffix -ubuntu16.04, 1.0.0-terran)# line 50%-ubuntu16.04: $(DOCKER) build --build-arg VERSION_ID=\"16.04\" \\ --build-arg RUNTIME_VERSION=\"$(RUNTIME_VERSION)+docker$*-1\" \\ --build-arg DOCKER_VERSION=\"docker (= $*)\" \\ --build-arg PKG_VERS=\"$(VERSION)+docker$*\" \\ --build-arg PKG_REV=\"$(PKG_REV)\" \\ -t \"nvidia/nvidia-docker2/ubuntu:16.04-docker$*\" -f Dockerfile.ubuntu . $(DOCKER) run --rm -v $(DIST_DIR)/ubuntu16.04:/dist:Z \"nvidia/nvidia-docker2/ubuntu:16.04-docker$*\" 完成後透過make來建構： $ make ubuntu16.04...dpkg-source -i --after-build nvidia-docker2-2.0.3+docker1.0.0-terrandpkg-buildpackage: binary-only upload (no source included) 查看dist/ubuntu16.04/目錄底下檔案： $ ls dist/ubuntu16.04/nvidia-docker2_2.0.3+docker1.0.0-terran-1_all.deb 集合 Debain Packages最後把所有建構的 Debain Packages 複製到一個目錄底下： $ cd $ mkdir nvidia-debs$ cp -rp libnvidia-container/dist/ubuntu16.04/*.deb nvidia-debs/$ cp -rp nvidia-container-runtime/dist/ubuntu16.04/*.deb nvidia-debs/$ cp -rp nvidia-docker/dist/ubuntu16.04/*.deb nvidia-debs/$ ls nvidia-debslibnvidia-container-dev_1.0.0~rc.1-1_amd64.deb libnvidia-container1_1.0.0~rc.1-1_amd64.deb nvidia-docker2_2.0.3+docker1.0.0-terran-1_all.deblibnvidia-container-tools_1.0.0~rc.1-1_amd64.deb nvidia-container-runtime-hook_1.3.0-1_amd64.deblibnvidia-container1-dbg_1.0.0~rc.1-1_amd64.deb nvidia-container-runtime_2.0.0+docker1.0.0-terran-1_amd64.deb 測試建構檔案找一台空的 Ubuntu 16.04 機器，然後複製檔案到該節點。然後先安裝 Terran Docker： $ sudo dpkg -i terran-docker.deb 若發生缺少 dep 檔案則下以下指令： $ sudo apt-get install -y -f 接著安裝 NVIDIA Docker 與相關套件： $ sudo dpkg -i nvidia-debs/*.deb Docker 測試結果： Kubernetes 測試結果： CentOS建構 Docker Server 與 Client本節將透過 Docker CE 建構 Server 與 Client 二進制執行檔。 Build from Docker CE首先透過 Git 取得 Docker CE 最新版本專案： $ git clone https://github.com/docker/docker-ce.git$ cd docker-ce/components/packaging/rpm$ VERSION=18.05.0-terran make centos...+ /usr/bin/rm -rf /root/rpmbuild/BUILDROOT/docker-ce-18.05.0.terran-3.el7.x86_64+ exit 0 當完成後，可查看rpmbuild/RPMS/x86_64底下檔案： $ ls rpmbuild/RPMS/x86_64docker-ce-18.05.0.terran-3.el7.x86_64.rpm docker-ce-debuginfo-18.05.0.terran-3.el7.x86_64.rpm 複製到測試用節點： $ ssh centos@$NODE_IP \"mkdir ~/docker\"$ scp rpmbuild/RPMS/x86_64/*.rpm centos@$NODE_IP:~/docker 建構 NVIDIA Docker 工具本步驟將需要分別建構三個專案來才能完整的使用 NVIDIA Docker。 Libnvidia Container首先透過 Git 取得 Libnvidia 最新版本專案原始碼，並進行建構： $ git clone https://github.com/NVIDIA/libnvidia-container.git$ cd libnvidia-container$ make docker-centos:7 TAG=rc.2 完成後，可以查看dist/centos7底下的檔案： $ ls dist/centos7/libnvidia-container_1.0.0-rc.2_x86_64.tar.xz libnvidia-container1-debuginfo-1.0.0-0.1.rc.2.x86_64.rpm libnvidia-container-static-1.0.0-0.1.rc.2.x86_64.rpmlibnvidia-container1-1.0.0-0.1.rc.2.x86_64.rpm libnvidia-container-devel-1.0.0-0.1.rc.2.x86_64.rpm libnvidia-container-tools-1.0.0-0.1.rc.2.x86_64.rpm 複製到測試用節點： $ ssh centos@$NODE_IP \"mkdir ~/libnvidia-container\"$ scp dist/centos7/*.rpm centos@$NODE_IP:~/libnvidia-container NVIDIA Container Runtime首先透過 Git 取得 Container Runtime 最新版本專案原始碼： $ git clone https://github.com/NVIDIA/nvidia-container-runtime.git$ cd nvidia-container-runtime 編輯runtime/Makefile修改以下內容： # line 25centos7: $(addsuffix -centos7, 18.05.0-terran)# Add new lines in line 31(runc v1.0-rc5)18.05.0-terran-%-runc: echo \"4fc53a81fb7c994640722ac585fa9ca548971871\" # Modify line 109%-terran-centos7: 接著執行以下指令建構 rpm packages 檔案： $ make centos7...+ exit 0make[1]: Leaving directory '/home/k2r2bai/Desktop/build-docker/nvidia-container-runtime/hook' 完成後查看dist/centos7/目錄底下檔案： $ ls dist/centos7/nvidia-container-runtime-2.0.0-1.docker18.05.0.x86_64.rpm nvidia-container-runtime-hook-1.3.0-1.x86_64.rpm 複製到測試用節點： $ ssh centos@$NODE_IP \"mkdir ~/nvidia-container-runtime\"$ scp dist/centos7/*.rpm centos@$NODE_IP:~/nvidia-container-runtime NVIDIA Docker首先透過 Git 取得 NVIDIA Docker 最新版本專案原始碼： $ git clone https://github.com/NVIDIA/nvidia-docker.git$ cd nvidia-docker 編輯Makefile，並修改以下內容： # line 26centos7: $(addsuffix -centos7, 18.05.0.terran)# Modify line 122%.terran-centos7: $(DOCKER) build --build-arg VERSION_ID=\"7\" \\ --build-arg RUNTIME_VERSION=\"$(RUNTIME_VERSION)-1.docker$*\" \\ --build-arg DOCKER_VERSION=\"docker-ce = $*.terran\" \\ --build-arg PKG_VERS=\"$(VERSION)\" \\ --build-arg PKG_REV=\"$(PKG_REV).docker$*.terran\" \\ -t \"nvidia/nvidia-docker2/centos:7-docker$*.terran\" -f Dockerfile.centos . $(DOCKER) run --rm -v $(DIST_DIR)/centos7:/dist:Z \"nvidia/nvidia-docker2/centos:7-docker$*.terran\" 完成後透過make來建構： $ make centos7...+ cd /tmp/nvidia-container-runtime-2.0.3/BUILD+ exit 0 查看dist/centos7/目錄底下檔案： $ ls dist/centos7nvidia-docker2-2.0.3-1.docker18.05.0.terran.noarch.rpm 複製到測試用節點： $ ssh centos@$NODE_IP \"mkdir ~/nvidia-docker\"$ scp dist/centos7/*.rpm centos@$NODE_IP:~/nvidia-docker 測試建構檔案首先在 CentOS 7 測試機器上安裝 NVIDIA Driver 與一些套件： $ echo -e \"blacklist nouveau\\noptions nouveau modeset=0\" | sudo tee /etc/modprobe.d/blacklist-nouveau.conf$ sudo yum -y install gcc kernel-devel \"kernel-devel-uname-r == $(uname -r)\" dkms$ sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org$ sudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm$ sudo yum -y install kmod-nvidia libtool-ltdl container-selinux$ sudo reboot 重新啟動後，該節點的 Home 目錄必須存在以下檔案: $ tree ~/ 確認沒問題後，透過 RPM 來進行安裝： $ sudo rpm -ivh docker/*.rpm$ sudo rpm -ivh libnvidia-container/*.rpm$ sudo rpm -ivh nvidia-container-runtime/*.rpm$ sudo rpm -ivh nvidia-docker/*.rpm 啟動 Dockerd，並設定開啟啟動 systemd： $ sudo systemctl enable docker &amp;&amp; sudo systemctl start docker$ sudo docker versionClient: Version: 18.05.0-terran API version: 1.38 Go version: go1.10.3 Git commit: ccbd518 Built: Wed Jun 13 03:19:43 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarmServer: Engine: Version: 18.05.0-terran API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: ccbd518 Built: Wed Jun 13 03:20:54 2018 OS/Arch: linux/amd64 Experimental: false 測試 NVIDIA Docker: $ sudo docker run --runtime=nvidia --rm nvidia/cuda nvidia-smiWed Jun 13 06:28:25 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 390.67 Driver Version: 390.67 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A || 0% 34C P5 26W / 120W | 0MiB / 3019MiB | 2% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ Kubernetes v1.11 測試結果：","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"GPU","slug":"GPU","permalink":"https://k2r2bai.com/tags/GPU/"}]},{"title":"開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集","slug":"kubernetes/deploy/kube-ansible","date":"2018-09-12T09:08:54.000Z","updated":"2019-12-02T01:49:42.392Z","comments":true,"path":"2018/09/12/kubernetes/deploy/kube-ansible/","link":"","permalink":"https://k2r2bai.com/2018/09/12/kubernetes/deploy/kube-ansible/","excerpt":"本篇將介紹如何透過 Ansible Playbooks 來快速部署多節點 Kubernetes，一般自建 Kubernetes 叢集時，很多初步入門都會透過 kubeadm 或腳本來部署，雖然 kubeadm 簡化了很多流程，但是還是需要很多手動操作過程，這使得當節點超過 5 - 8 台時就覺得很麻煩，因此許多人會撰寫腳本來解決這個問題，但是腳本的靈活性不佳，一旦設定過程過於龐大時也會造成其複雜性增加，因此這邊採用 Ansible 來完成許多重複的部署過程，並提供相關變數來調整叢集部署的元件、Container Runtime 等等。","text":"本篇將介紹如何透過 Ansible Playbooks 來快速部署多節點 Kubernetes，一般自建 Kubernetes 叢集時，很多初步入門都會透過 kubeadm 或腳本來部署，雖然 kubeadm 簡化了很多流程，但是還是需要很多手動操作過程，這使得當節點超過 5 - 8 台時就覺得很麻煩，因此許多人會撰寫腳本來解決這個問題，但是腳本的靈活性不佳，一旦設定過程過於龐大時也會造成其複雜性增加，因此這邊採用 Ansible 來完成許多重複的部署過程，並提供相關變數來調整叢集部署的元件、Container Runtime 等等。 這邊我將利用自己撰寫的 kube-ansible 來部署一組 Kubernetes HA 叢集，而該 Playbooks 的 HA 是透過 HAProxy + Keepalived 來完成，這邊也會將 docker 取代成 containerd 來提供更輕量的 container runtime，另外該 Ansible 會採用全二進制檔案(kube-apiserver 等除外)方式進行安裝。 本次 Kubernetes 安裝版本： Kubernetes v1.11.2 Etcd v3.2.9 containerd v1.1.2 節點資訊本次安裝作業系統採用Ubuntu 16+，測試環境為實體主機： IP Address Role CPU Memory 172.22.132.8 VIP 172.22.132.9 k8s-m1 4 16G 172.22.132.10 k8s-m2 4 16G 172.22.132.11 k8s-m3 4 16G 172.22.132.12 k8s-g1 4 16G 172.22.132.13 k8s-g2 4 16G 理論上CentOS 7.x或Debian 8都可以。 事前準備安裝前需要確認以下幾個項目： 所有節點的網路之間可以互相溝通。 部署節點對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝 Python。 所有節點需要設定/etc/host解析到所有主機。 部署節點需要安裝 Ansible。 Ubuntu 16.04 安裝 Ansible: $ sudo apt-get install -y software-properties-common git cowsay$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible CentOS 7 安裝 Ansible： $ sudo yum install -y epel-release$ sudo yum -y install ansible cowsay Mac OS X 安裝 Ansible: $ brew install ansible 透過 Ansible 部署 Kubernetes本節將說明如何使用 Ansible 來部署 Kubernetes HA 叢集，首先我們透過 Git 取得專案: $ git clone https://github.com/kairen/kube-ansible.git$ cd kube-ansible Kubernetes 叢集首先建立一個檔案inventory/hosts.ini來描述被部署的節點與群組關析： [etcds]k8s-m[1:3] ansible_user=ubuntu[masters]k8s-m[1:3] ansible_user=ubuntu[nodes]k8s-g1 ansible_user=ubuntuk8s-g2 ansible_user=ubuntu[kube-cluster:children]mastersnodes ansible_user為作業系統 SSH 的使用者名稱。 接著編輯group_vars/all.yml來根據需求設定功能，如以下範例： kube_version: 1.11.2container_runtime: containerdcni_enable: truecontainer_network: calicocni_iface: \"\" # CNI 網路綁定的網卡vip_interface: \"\" # VIP 綁定的網卡vip_address: 172.22.132.8 # VIP 位址etcd_iface: \"\" # etcd 綁定的網卡enable_ingress: trueenable_dashboard: trueenable_logging: trueenable_monitoring: trueenable_metric_server: truegrafana_user: \"admin\"grafana_password: \"p@ssw0rd\" 上面綁定網卡若沒有輸入，通常會使用節點預設網卡(一般來說是第一張網卡)。 完成設定group_vars/all.yml檔案後，就可以先透過 Ansible 來檢查叢集狀態： $ ansible -i inventory/hosts.ini all -m pingk8s-g1 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;... 當叢集確認沒有問題後，即可執行cluster.yml來部署 Kubernetes 叢集： $ ansible-playbook -i inventory/hosts.ini cluster.yml...PLAY RECAP ***********************************************************************************************************************k8s-g1 : ok=64 changed=32 unreachable=0 failed=0k8s-g2 : ok=62 changed=32 unreachable=0 failed=0k8s-m1 : ok=171 changed=85 unreachable=0 failed=0k8s-m2 : ok=144 changed=69 unreachable=0 failed=0k8s-m3 : ok=144 changed=69 unreachable=0 failed=0 確認都沒發生錯誤後，表示部署完成。 這邊選擇一台 master 節點(k8s-m1)來 SSH 進入測試叢集是否正常，透過 kubectl 指令來查看： # 查看元件狀態$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-1 Healthy &#123;\"health\": \"true\"&#125;etcd-2 Healthy &#123;\"health\": \"true\"&#125;etcd-0 Healthy &#123;\"health\": \"true\"&#125;# 查看節點狀態$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 3m v1.11.2k8s-g2 Ready &lt;none&gt; 3m v1.11.2k8s-m1 Ready master 5m v1.11.2k8s-m2 Ready master 5m v1.11.2k8s-m3 Ready master 5m v1.11.2 Addons 部署確認節點沒問題後，就可以透過addons.yml來部署 Kubernetes extra addons： $ ansible-playbook -i inventory/hosts.ini addons.yml...PLAY RECAP ***********************************************************************************************************************k8s-m1 : ok=27 changed=22 unreachable=0 failed=0k8s-m2 : ok=10 changed=5 unreachable=0 failed=0k8s-m3 : ok=10 changed=5 unreachable=0 failed=0 完成後即可透過 kubectl 來檢查服務，如 kubernetes-dashboard： $ kubectl get po,svc -n kube-system -l k8s-app=kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/kubernetes-dashboard-6948bdb78-bkqbr 1/1 Running 0 32mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes-dashboard ClusterIP 10.105.199.72 &lt;none&gt; 443/TCP 32m 完成後，即可透過 API Server 的 Proxy 來存取 https://172.22.132.8:8443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/。 測試是否有 HA首先透過 etcdctl 來檢查狀態： $ export PKI=\"/etc/kubernetes/pki/etcd\"$ ETCDCTL_API=3 etcdctl \\ --cacert=$&#123;PKI&#125;/etcd-ca.pem \\ --cert=$&#123;PKI&#125;/etcd.pem \\ --key=$&#123;PKI&#125;/etcd-key.pem \\ --endpoints=\"https://172.22.132.9:2379\" \\ member listc9c9f1e905ce83ae, started, k8s-m1, https://172.22.132.9:2380, https://172.22.132.9:2379cb81b1446a3a689f, started, k8s-m3, https://172.22.132.11:2380, https://172.22.132.11:2379db0b2674ebb24f80, started, k8s-m2, https://172.22.132.10:2380, https://172.22.132.10:2379 接著進入k8s-m1節點測試叢集 HA 功能，這邊先關閉該節點： $ sudo poweroff 接著進入到k8s-m2節點，透過 kubectl 來檢查叢集是否能夠正常執行： # 先檢查元件狀態$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-2 Healthy &#123;\"health\": \"true\"&#125;etcd-1 Healthy &#123;\"health\": \"true\"&#125;etcd-0 Unhealthy Get https://172.22.132.9:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)# 檢查 nodes 狀態$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 10m v1.11.2k8s-g2 Ready &lt;none&gt; 10m v1.11.2k8s-m1 NotReady master 12m v1.11.2k8s-m2 Ready master 12m v1.11.2k8s-m3 Ready master 12m v1.11.2# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl expose pod nginx --port 80 --type NodePort$ kubectl get po,svcNAME READY STATUS RESTARTS AGEpod/nginx 1/1 Running 0 1mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3hservice/nginx NodePort 10.102.191.102 &lt;none&gt; 80:31780/TCP 6s 透過 cURL 檢查 NGINX 服務是否正常： $ curl 172.22.132.8:31780&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;... 重置叢集最後若想要重新部署叢集的話，可以透過reset-cluster.yml來清除叢集： $ ansible-playbook -i inventory/hosts.ini reset-cluster.yml","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"}]},{"title":"[Knative] 初探基本功能與概念","slug":"kubernetes/knative/quick-start","date":"2018-08-24T09:08:54.000Z","updated":"2019-12-02T01:49:42.396Z","comments":true,"path":"2018/08/24/kubernetes/knative/quick-start/","link":"","permalink":"https://k2r2bai.com/2018/08/24/kubernetes/knative/quick-start/","excerpt":"Knative 是基於 Kubernetes 平台建構、部署與管理現代 Serverless 工作負載的開源專案，其目標是要幫助雲端供應商與企業平臺營運商替任何雲端環境的開發者、操作者等提供 Serverless 服務體驗。Knative 採用了 Kubernetes 概念來建構函式與應用程式，並以 Istio 實現了叢集內的網路路由，以及進入服務的外部連接，這讓開發者在部署或執行變得更加簡單。而目前 Knative 元件焦距在解決許多平凡但困難的事情，例如以下：","text":"Knative 是基於 Kubernetes 平台建構、部署與管理現代 Serverless 工作負載的開源專案，其目標是要幫助雲端供應商與企業平臺營運商替任何雲端環境的開發者、操作者等提供 Serverless 服務體驗。Knative 採用了 Kubernetes 概念來建構函式與應用程式，並以 Istio 實現了叢集內的網路路由，以及進入服務的外部連接，這讓開發者在部署或執行變得更加簡單。而目前 Knative 元件焦距在解決許多平凡但困難的事情，例如以下： 部署一個容器。 在 Kubernetes 上編排 Source-to-URL 的工作流程。 使用 Blue/Green 部署來路由與管理流量。 按需自動擴展與調整工作負載的大小。 將運行服務(Running services)綁定到事件生態系統(Eventing ecosystems)。 利用原始碼建構應用程式與函式。 讓應用程式能夠零停機升級。 自動增減應用程式與函式實例。 透過 HTTP request 觸發函式的呼叫。 為函式、應用程式與容器建立事件。 而 Knative 的設計考慮了不同的工作角色使用情境： 然而 Knative 不只使用 Kubernetes 與 Istio 的功能，也自行開發了三個元件以提供更完整的 Serverless 平台。而下節將針對這三個元件進行說明。 Knative 元件與概念目前 Knative 提供了以下幾個元件來處理不同的功能需求，本節我們將針對這些元件進行說明。 BuildBuild 是 Knative 中的自定義資源，並提供了 Build API object 來處理從原始碼(Sources)建構容器的可插拔(Pluggable)模型，這是基於 Google 的容器建構服務(Container Build Service) 而來，這允許開發者定義容器的來源來打包，例如 Git、Registery(ex: Docker hub)，另外也能將 Buildpacks 當作一種建構的插件來使用，這使 Knative 在建構功能上有更靈活的擴展。 除了 Buildpacks 外，也能夠將 Google Container Builder、Bazel、Kaniko 與 Jib 等等當作建構插件使用。 而一個 Knative builds 的主要特性如下： 一個Build可以包含多個 step，其中每個 step 會指定一個Builder。 一個Builder是一種容器映像檔，可以建立該映像檔來完成任何任務，如流程中的單一 step，或是整個流程本身。 Build 中的 steps 可以推送(push)到一個儲存庫(repository)。 一個BuildTemplate可用在定義重用的模板。 可以定義Build中的source來將檔案或專案掛載到 Kubernetes Volume(會掛載成/workspace)。目前支援： Git 儲存庫 Google Cloud Storage 任意的容器映像檔 利用 Kubernetes Secrets 結合 ServiceAccount 進行身份認證 這邊的step可以看作是 Kubernetes 的 init-container。 以下是一個提供使用者身份認證的 Build 範例，該範例包含多個 step 與 Git repo： apiVersion: build.knative.dev/v1alpha1kind: Buildmetadata: name: example-buildspec: serviceAccountName: build-auth-example source: git: url: https://github.com/example/build-example.git revision: master steps: - name: ubuntu-example image: ubuntu args: [\"ubuntu-build-example\", \"SECRETS-example.md\"] steps: - image: gcr.io/example-builders/build-example args: ['echo', 'hello-example', 'build'] ServingServing 以 Kubernetes 與 Istio 為基礎，實現了中介軟體原語(Middleware Primitives)來達到自動化從容器到函式執行的整個流程，另外也支援了快速部署容器並進行伸縮的功能，甚至能根據請求來讓容器實例降到 0，而 Serving 也會利用 Istio 在修訂版本之間路由流量，或是將流量傳送到同一個應用程式的多個修訂版本中，除了上述功能外， Serving 也能實現了不停機更新、Bule/Green 部署、部分負載測試，以及程式碼回滾等功能。 從上圖，可以得知 Serving 利用了 Kubernetes CRD 新增一組 API 來定義與控制在 Kubernetes 上的 Serverless 的行為，其分別為以下： Service：該資源用來自動管理整個工作負載的生命週期，並提供單點控制。它控制了其他物件的建立，以確保應用程式與函式具備每次 Service 更新的 Route、Configuration 與 Revision，而 Service 也可以定義流量路由到最新 Revision 或固定的 Revision。 apiVersion: serving.knative.dev/v1alpha1kind: Servicemetadata: name: service-examplespec: runLatest: configuration: revisionTemplate: spec: container: image: gcr.io/knative-samples/helloworld-go env: - name: TARGET value: \"Go Sample v1\" Route：該資源將網路端點映射到一個或多個 revision，並且能透過多種方式來管理流量，如部分的流量(fractional traffic)、命名路由(named routes)。 apiVersion: serving.knative.dev/v1alpha1kind: Routemetadata: name: route-examplespec: traffic: - configurationName: stock-configuration-example percent: 100 Configuration：該資源維護部屬所需的狀態，它提供了程式碼與組態檔之間的分離，並遵循 Twelve-Factor App 方法，若修改 Configuration 會建立新 revision。 apiVersion: serving.knative.dev/v1alpha1kind: Configurationmetadata: name: configuration-examplespec: revisionTemplate: metadata: labels: knative.dev/type: container spec: container: image: github.com/knative/docs/serving/samples/rest-api-go env: - name: RESOURCE value: stock readinessProbe: httpGet: path: / initialDelaySeconds: 3 periodSeconds: 3 Revision：該資源是記錄每個工作負載修改的程式碼與組態的時間點快照，而 Revision 是不可變物件，並且只要它還有用處，就會被長時間保留。 apiVersion: serving.knative.dev/v1alpha1kind: Revisionmetadata: labels: serving.knative.dev/configuration: helloworld-go name: revision-example namespace: defaultspec: concurrencyModel: Multi container: env: - name: TARGET value: Go Sample v1 image: gcr.io/knative-samples/helloworld-go generation: 1 servingState: Active EventingEventing 提供用於 Consuming 以及 Producing 的事件建構區塊，並遵守著 CloudEvents 規範來實現，而該元件目標是對事件進行抽象處理，以讓開發者不需要關注後端相關具體細節，這樣開發者就不需要思考使用哪一套訊息佇列系統。 而 Knative Eventing 也透過 Kubernetes CRD 定義了一組新資源，這些資源被用在事件的 Producing 與 Consuming 上，而這類資源主要分成以下： Channels 這些是發布者(Publishers)向其發送訊息的 Pub/Sub Topics，因此 Channel 可視為獲取或放置事件的位置目錄。 Bus。Channels 的後端供應者，即支援事件的訊息服務平台，如 Google Cloud PubSub、Apache Kafka 與 NATS 等等。 apiVersion: channels.knative.dev/v1alpha1kind: Busmetadata: name: kafkaspec: dispatcher: args: - -logtostderr - -stderrthreshold - INFO env: - name: KAFKA_BROKERS valueFrom: configMapKeyRef: key: KAFKA_BROKERS name: kafka-bus-config image: gcr.io/knative-releases/github.com/knative/eventing/pkg/buses/kafka/dispatcher@sha256:d925663bb965001287b042c8d3ebdf8b4d3f0e7aa2a9e1528ed39dc78978bcdb name: dispatcher 為應用程式與函式指定 Knative Service，並指明 Channel 所要傳遞的具體訊息。為程式與函式的進入位址。 Feeds: 提供一個抽象層來讓外部可以提供資料來源，並將之路由到叢集中。會將事件來源中的單個事件類型附加到某一個行為。 EventSource 與 ClusterEventSource 是一個 Kubernetes 資源，被用來描述可能產生的 EventTypes 外部系統。 apiVersion: feeds.knative.dev/v1alpha1kind: EventSourcemetadata: name: github namespace: defaultspec: image: gcr.io/knative-releases/github.com/knative/eventing/pkg/sources/github@sha256:a5f6733797d934cd4ba83cf529f02ee83e42fa06fd0e7a9d868dd684056f5db0 source: github type: github EventType 與 ClusterEventType 同樣是 Kubernetes 資源，被用來表示不同 EventSource 支援的事件類型。 apiVersion: feeds.knative.dev/v1alpha1kind: EventTypemetadata: name: pullrequest namespace: defaultspec: description: notifications on pullrequests eventSource: github Flows: 該資源會將事件綁定到 Route(應用程式與函式端點)上，並選擇使用哪種事件路由的 Channel 與 Bus。 apiVersion: flows.knative.dev/v1alpha1kind: Flowmetadata: name: k8s-event-flow namespace: defaultspec: serviceAccountName: feed-sa trigger: eventType: dev.knative.k8s.event resource: k8sevents/dev.knative.k8s.event service: k8sevents parameters: namespace: default action: target: kind: Route apiVersion: serving.knative.dev/v1alpha1 name: read-k8s-events 以上是簡單介紹，接下來我們將透過 Minikube 來初步玩玩 Knative 功能。 透過 Minikube 初步入門本節將安裝 Minikube 來建立 Knative 環境，透過完成簡單範例來體驗。 事前準備 在測試機器安裝 Minikube 二進制執行檔，請至 Minikube Releases 下載。 在測試機器下載 Virtual Box 來提供給 Minikube 建立虛擬機。 IMPORTANT: 測試機器記得開啟 VT-x or AMD-v virtualization. 雖然建議用 vbox，但是討厭 Oracle 的人可以改用其他虛擬化工具(ex: kvm, xhyve)，理論上可以動。 下載 Kubernetes CLI 工具 kubeclt。 啟動 Minukube首天透過 Minikube 來啟動一台 VM 部署單節點 Kubernetes，由於這邊會用到很多系統服務，因此需要開多一點系統資源： $ minikube start --memory=8192 --cpus=4 \\ --kubernetes-version=v1.10.5 \\ --extra-config=apiserver.admission-control=\"LimitRanger,NamespaceExists,NamespaceLifecycle,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook\" 完成後，透過 kubectl 檢查： $ kubectl get noNAME STATUS ROLES AGE VERSIONminikube Ready master 1m v1.10.5 部署 Knative由於 Knative 是基於 Istio 所開發，因此需要先部署相關服務，這邊透過 kubectl 來建立： $ curl -L https://storage.googleapis.com/knative-releases/serving/latest/istio.yaml \\ | sed 's/LoadBalancer/NodePort/' \\ | kubectl apply -f -# 設定 inject namespace$ kubectl label namespace default istio-injection=enabled 這邊會需要一點時間下載映像檔，並啟動 Istio 服務，完成後會如下所示： $ kubectl -n istio-system get poNAME READY STATUS RESTARTS AGEistio-citadel-7bdc7775c7-jn2bw 1/1 Running 0 5mistio-cleanup-old-ca-msvkn 0/1 Completed 0 5mistio-egressgateway-795fc9b47-4nz7j 1/1 Running 0 6mistio-ingress-84659cf44c-pvqd5 1/1 Running 0 6mistio-ingressgateway-7d89dbf85f-tgm24 1/1 Running 0 6mistio-mixer-post-install-lvrjv 0/1 Completed 0 6mistio-pilot-66f4dd866c-zmbv5 2/2 Running 0 6mistio-policy-76c8896799-cqmdn 2/2 Running 0 6mistio-sidecar-injector-645c89bc64-9mdwx 1/1 Running 0 5mistio-statsd-prom-bridge-949999c4c-qhdgf 1/1 Running 0 6mistio-telemetry-6554768879-b6vss 2/2 Running 0 6m 接著部署 Knative 元件至 Kubernetes 叢集，官方提供了一個release-lite.yaml檔案來協助建立輕量的測試環境，因此可以直接透過 kubectl 來建立： $ curl -L https://storage.googleapis.com/knative-releases/serving/latest/release-lite.yaml \\ | sed 's/LoadBalancer/NodePort/' \\ | kubectl apply -f - 這邊會部署以 Prometheus 組成的 Monitoring 系統，以及 Knative Serving 與 Build。 若是其他環境上的 Kubernetes 可以參考 Knative Install。 這邊同樣需要一點時間來下載映像檔，並啟動相關服務，一但完成後會如下所示： # Monitoring$ kubectl -n monitoring get poNAME READY STATUS RESTARTS AGEgrafana-798cf569ff-m8w9c 1/1 Running 0 4mkube-state-metrics-77597b45f8-mxhxv 4/4 Running 0 1mnode-exporter-8wbxd 2/2 Running 0 4mprometheus-system-0 1/1 Running 0 4mprometheus-system-1 1/1 Running 0 4m# Knative build$ kubectl -n knative-build get poNAME READY STATUS RESTARTS AGEbuild-controller-5cb4f5cb67-bs94k 1/1 Running 0 6mbuild-webhook-6b4c65546b-fzffg 1/1 Running 0 6m# Knative serving$ kubectl -n knative-serving get poNAME READY STATUS RESTARTS AGEactivator-869d7d76c5-fngdm 2/2 Running 0 7mautoscaler-65855c89f6-pmzhr 2/2 Running 0 7mcontroller-5fbcf79dfb-q8cb8 1/1 Running 0 7mwebhook-c98c7c654-lpnjj 1/1 Running 0 7m 到這邊已完成部署 Knative 元件，接下來將透過一些範例來了解 Knative 功能。 部署 Knative 應用程式當上述元件部署完成後，就可以開始建立 Knative 應用程式與函式，這邊將利用簡單 HTTP Server + Slack 來實作一個簡單 Channel 訊息傳送，過程中將會使用到 Build、BuildTemplate 與 Knative Service 等資源。在開始前，先透過 Git 來取得範例專案，這邊主要是使用裡面的 Kubernetes 部署檔案： $ git clone https://github.com/kairen/knative-slack-app$ cd knative-slack-app 由於本範例會利用 Kaniko 來建構應用程式的容器映像檔，並將自動將建構好的映像檔上傳至 DockeHub，因此這邊為了確保能夠上傳到自己的 DockerHub，需要建立 Secert 與 Service Account 來提供 Docker ID 與 Passwrod 給 Knative serving 使用： $ export DOCKER_ID=$(echo -n \"username\" | base64)$ export DOCKER_PWD=$(echo -n \"password\" | base64)$ cat deploy/docker-secret.yml | \\ sed \"s/BASE64_ENCODED_USERNAME/$&#123;DOCKER_ID&#125;/\" | \\ sed \"s/BASE64_ENCODED_PASSWORD/$&#123;DOCKER_PWD&#125;/\" | \\ kubectl apply -f -$ kubectl apply -f deploy/kaniko-sa.yml 接著建立一個 Secret 來保存 Slack 的資訊以提供給 Slack App 使用，如 Token： $ export SLACK_TOKEN=$(echo -n \"slack-token\" | base64)$ export SLACK_CHANNEL_ID=$(echo -n \"slack-channel-id\" | base64)$ cat deploy/slack-secret.yml | \\ sed \"s/BASE64_ENCODED_SLACK_TOKEN/$&#123;SLACK_TOKEN&#125;/\" | \\ sed \"s/BASE64_ENCODED_SLACK_CHANNEL_ID/$&#123;SLACK_CHANNEL_ID&#125;/\" | \\ kubectl apply -f - 接著建立 Kaniko Build template 來提供給 Knative Service 建構使用： $ kubectl apply -f deploy/kaniko-buildtemplate.yml$ kubectl get buildtemplateNAME CREATED ATkaniko 7s 上面完成後，建立 Knative service 與 Istio HTTPS Service Entry 來提供應用程式，以及讓 Pod 能夠存取 Slack HTTPs API： $ kubectl apply -f deploy/slack-https-sn.yml$ kubectl apply -f deploy/slack-app-service.yml$ kubectl get po -wNAME READY STATUS RESTARTS AGEslack-app-00001-9htqm 0/1 Init:2/3 0 8sslack-app-00001-9htqm 0/1 Init:2/3 0 8sslack-app-00001-9htqm 0/1 PodInitializing 0 3mslack-app-00001-9htqm 0/1 Completed 0 4mslack-app-00001-deployment-75f7f8dd8c-tskq8 0/3 Pending 0 0sslack-app-00001-deployment-75f7f8dd8c-tskq8 0/3 Pending 0 0sslack-app-00001-deployment-75f7f8dd8c-tskq8 0/3 Init:0/1 0 0sslack-app-00001-deployment-75f7f8dd8c-tskq8 0/3 PodInitializing 0 7sslack-app-00001-deployment-75f7f8dd8c-tskq8 2/3 Running 0 33s 這邊第一次執行會比較慢，因為需要下載 knative build 相關映像檔。 經過一段時間完成後，透過以下指令來確認服務是否正常： $ export IP_ADDRESS=$(minikube ip):$(kubectl get svc knative-ingressgateway -n istio-system -o 'jsonpath=&#123;.spec.ports[?(@.port==80)].nodePort&#125;')$ export DOMAIN=$(kubectl get services.serving.knative.dev slack-app -o=jsonpath='&#123;.status.domain&#125;')# 透過 cURL 工具以 Get method 存取$ curl -X GET -H \"Host: $&#123;DOMAIN&#125;\" $&#123;IP_ADDRESS&#125;&lt;h1&gt;Hello slack app for Knative!!&lt;/h1&gt;# 透過 cURL 工具以 Post method 傳送 msg$ curl -X POST \\ -H 'Content-type: application/json' \\ -H \"Host: $&#123;DOMAIN&#125;\" \\ --data '&#123;\"msg\":\"Hello, World!\"&#125;' \\ $&#123;IP_ADDRESS&#125;success 若成功的話，可以查看 Slack channel 是否有傳送訊息： 最後由於 Knative Serving 是 Request-driven，因此經過長時間沒有任何 request 時，將會自動縮減至 0 副本，直到再次收到 request 才會再次啟動一個實例，然而 Knative 也將大多資訊以 Prometheus 進行監控，因此我們能透過 Prometheus 來觀察狀態變化。 首先透過 kubectl 取得 Grafana NodePort 資訊： $ kubectl -n monitoring get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE...grafana NodePort 10.96.197.116 &lt;none&gt; 30802:30326/TCP 1hprometheus-system-np NodePort 10.99.64.228 &lt;none&gt; 8080:32628/TCP 1h... 透過瀏覽器開啟 http://minikube_ip:port 來查看。 另外也可以查看 HTTP request 狀態。 由於 Knative 是蠻大的系統，這邊先暫時使用基本 Knative 的 Build 與 Serving，而 Eventing 將會在之後補充範例。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Knative","slug":"Knative","permalink":"https://k2r2bai.com/tags/Knative/"},{"name":"Serverless","slug":"Serverless","permalink":"https://k2r2bai.com/tags/Serverless/"},{"name":"Istio","slug":"Istio","permalink":"https://k2r2bai.com/tags/Istio/"}]},{"title":"Kubernetes v1.11.x HA 全手動苦工安裝教學(TL;DR)","slug":"kubernetes/deploy/manual-install","date":"2018-07-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.393Z","comments":true,"path":"2018/07/17/kubernetes/deploy/manual-install/","link":"","permalink":"https://k2r2bai.com/2018/07/17/kubernetes/deploy/manual-install/","excerpt":"本篇延續過往手動安裝方式來部署 Kubernetes v1.11.x 版本的 High Availability 叢集，而此次教學將直接透過裸機進行部署 Kubernetes 叢集。以手動安裝的目標是學習 Kubernetes 各元件關析、流程、設定與部署方式。若不想這麼累的話，可以參考 Picking the Right Solution 來選擇自己最喜歡的方式。","text":"本篇延續過往手動安裝方式來部署 Kubernetes v1.11.x 版本的 High Availability 叢集，而此次教學將直接透過裸機進行部署 Kubernetes 叢集。以手動安裝的目標是學習 Kubernetes 各元件關析、流程、設定與部署方式。若不想這麼累的話，可以參考 Picking the Right Solution 來選擇自己最喜歡的方式。 Kubernetes 部署資訊Kubernetes 部署的版本資訊： Kubernetes: v1.11.0 CNI: v0.7.1 Etcd: v3.3.8 Docker: v18.05.0-ce Calico: v3.1 Kubernetes 部署的網路資訊： Cluster IP CIDR: 10.244.0.0/16 Service Cluster IP CIDR: 10.96.0.0/12 Service DNS IP: 10.96.0.10 DNS DN: cluster.local Kubernetes API VIP: 172.22.132.9 Kubernetes Ingress VIP: 172.22.132.8 節點資訊本教學採用以下節點數與機器規格進行部署裸機(Bare-metal)，作業系統採用Ubuntu 16+(理論上 CentOS 7+ 也行)進行測試： IP Address Hostname CPU Memory Extra Device 172.22.132.10 k8s-m1 4 16G None 172.22.132.11 k8s-m2 4 16G None 172.22.132.12 k8s-m3 4 16G None 172.22.132.13 k8s-g1 4 16G GTX 1060 3G 172.22.132.14 k8s-g2 4 16G GTX 1060 3G 另外由所有 master 節點提供一組 VIP 172.22.132.9。 這邊m為 K8s Master 節點，g為 K8s Node 節點。 所有操作全部用root使用者進行，主要方便部署用。 事前準備開始部署叢集前需先確保以下條件已達成： 所有節點彼此網路互通，並且k8s-m1 SSH 登入其他節點為 passwdless，由於過程中很多會在某台節點(k8s-m1)上以 SSH 複製與操作其他節點。 確認所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0$ vim /etc/selinux/configSELINUX=disabled 關閉是為了方便安裝使用，若有需要防火牆可以參考 Required ports 來設定。 所有節點需要設定/etc/hosts解析到所有叢集主機。 ...172.22.132.10 k8s-m1172.22.132.11 k8s-m2172.22.132.12 k8s-m3172.22.132.13 k8s-g1172.22.132.14 k8s-g2 所有節點需要安裝 Docker CE 版本的容器引擎： $ curl -fsSL https://get.docker.com/ | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後，需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker 所有節點需要設定以下系統參數。 $ cat &lt;&lt;EOF | tee /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl -p /etc/sysctl.d/k8s.conf 關於bridge-nf-call-iptables的啟用取決於是否將容器連接到Linux bridge或使用其他一些機制(如 SDN vSwitch)。 Kubernetes v1.8+ 要求關閉系統 Swap，請在所有節點利用以下指令關閉： $ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0# 不同機器會有差異$ sed '/swap.img/d' -i /etc/fstab 記得/etc/fstab也要註解掉SWAP掛載。 在所有節點下載 Kubernetes 二進制執行檔： $ export KUBE_URL=https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/linux/amd64$ wget $&#123;KUBE_URL&#125;/kubelet -O /usr/local/bin/kubelet$ chmod +x /usr/local/bin/kubelet# Node 可忽略下載 kubectl$ wget $&#123;KUBE_URL&#125;/kubectl -O /usr/local/bin/kubectl$ chmod +x /usr/local/bin/kubectl 在所有節點下載 Kubernetes CNI 二進制執行檔： $ export CNI_URL=https://github.com/containernetworking/plugins/releases/download$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin$ wget -qO- --show-progress \"$&#123;CNI_URL&#125;/v0.7.1/cni-plugins-amd64-v0.7.1.tgz\" | tar -zx 在k8s-m1節點安裝cfssl工具，這將會用來建立 CA ，並產生 TLS 憑證。 $ export CFSSL_URL=https://pkg.cfssl.org/R1.2$ wget $&#123;CFSSL_URL&#125;/cfssl_linux-amd64 -O /usr/local/bin/cfssl$ wget $&#123;CFSSL_URL&#125;/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson 建立 CA 與產生 TLS 憑證本節將會透過 CFSSL 工具來產生不同元件的憑證，如 Etcd、Kubernetes API Server 等等，其中各元件都會有一個根數位憑證認證機構(Root Certificate Authority)被用在元件之間的認證。 要注意 CA JSON 檔中的CN(Common Name)與O(Organization)等內容是會影響 Kubernetes 元件認證的。 首先在k8s-m1透過 Git 取得部署用檔案： $ git clone https://github.com/kairen/k8s-manual-files.git ~/k8s-manual-files$ cd ~/k8s-manual-files/pki Etcd在k8s-m1建立/etc/etcd/ssl資料夾，並產生 Etcd CA： $ export DIR=/etc/etcd/ssl$ mkdir -p $&#123;DIR&#125;$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare $&#123;DIR&#125;/etcd-ca 接著產生 Etcd 憑證： $ cfssl gencert \\ -ca=$&#123;DIR&#125;/etcd-ca.pem \\ -ca-key=$&#123;DIR&#125;/etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,172.22.132.10,172.22.132.11,172.22.132.12 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare $&#123;DIR&#125;/etcd -hostname需修改成所有 masters 節點。 刪除不必要的檔案，並檢查/etc/etcd/ssl目錄是否成功建立以下檔案： $ rm -rf $&#123;DIR&#125;/*.csr$ ls /etc/etcd/ssletcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem 複製檔案至其他 Etcd 節點，這邊為所有master節點： $ for NODE in k8s-m2 k8s-m3; do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; \" mkdir -p /etc/etcd/ssl\" for FILE in etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem; do scp /etc/etcd/ssl/$&#123;FILE&#125; $&#123;NODE&#125;:/etc/etcd/ssl/$&#123;FILE&#125; done done Kubernetes 元件在k8s-m1建立/etc/kubernetes/pki資料夾，並依據下面指令來產生 CA： $ export K8S_DIR=/etc/kubernetes$ export PKI_DIR=$&#123;K8S_DIR&#125;/pki$ export KUBE_APISERVER=https://172.22.132.9:6443$ mkdir -p $&#123;PKI_DIR&#125;$ cfssl gencert -initca ca-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/ca$ ls $&#123;PKI_DIR&#125;/ca*.pem/etc/kubernetes/pki/ca-key.pem /etc/kubernetes/pki/ca.pem KUBE_APISERVER這邊設定為 VIP 位址。 接著依照以下小節來建立各元件的 TLS 憑證。 API Server此憑證將被用於 API Server 與 Kubelet Client 溝通使用。首先透過以下指令產生 Kubernetes API Server 憑證： $ cfssl gencert \\ -ca=$&#123;PKI_DIR&#125;/ca.pem \\ -ca-key=$&#123;PKI_DIR&#125;/ca-key.pem \\ -config=ca-config.json \\ -hostname=10.96.0.1,172.22.132.9,127.0.0.1,kubernetes.default \\ -profile=kubernetes \\ apiserver-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/apiserver$ ls $&#123;PKI_DIR&#125;/apiserver*.pem/etc/kubernetes/pki/apiserver-key.pem /etc/kubernetes/pki/apiserver.pem 這邊-hostname的10.96.0.1是 Cluster IP 的 Kubernetes 端點; 172.22.132.9為 VIP 位址; kubernetes.default為 Kubernetes 系統在 default namespace 自動建立的 API service domain name。 Front Proxy Client此憑證將被用於 Authenticating Proxy 的功能上，而該功能主要是提供 API Aggregation 的認證。首先透過以下指令產生 CA： $ cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/front-proxy-ca$ ls $&#123;PKI_DIR&#125;/front-proxy-ca*.pem/etc/kubernetes/pki/front-proxy-ca-key.pem /etc/kubernetes/pki/front-proxy-ca.pem 接著產生 Front proxy client 憑證： $ cfssl gencert \\ -ca=$&#123;PKI_DIR&#125;/front-proxy-ca.pem \\ -ca-key=$&#123;PKI_DIR&#125;/front-proxy-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ front-proxy-client-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/front-proxy-client$ ls $&#123;PKI_DIR&#125;/front-proxy-client*.pem/etc/kubernetes/pki/front-proxy-client-key.pem /etc/kubernetes/pki/front-proxy-client.pem Controller Manager憑證會建立system:kube-controller-manager的使用者(憑證 CN)，並被綁定在 RBAC Cluster Role 中的system:kube-controller-manager來讓 Controller Manager 元件能夠存取需要的 API object。這邊透過以下指令產生 Controller Manager 憑證： $ cfssl gencert \\ -ca=$&#123;PKI_DIR&#125;/ca.pem \\ -ca-key=$&#123;PKI_DIR&#125;/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/controller-manager$ ls $&#123;PKI_DIR&#125;/controller-manager*.pem/etc/kubernetes/pki/controller-manager-key.pem /etc/kubernetes/pki/controller-manager.pem 接著利用 kubectl 來產生 Controller Manager 的 kubeconfig 檔： $ kubectl config set-cluster kubernetes \\ --certificate-authority=$&#123;PKI_DIR&#125;/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=$&#123;K8S_DIR&#125;/controller-manager.conf$ kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=$&#123;PKI_DIR&#125;/controller-manager.pem \\ --client-key=$&#123;PKI_DIR&#125;/controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=$&#123;K8S_DIR&#125;/controller-manager.conf$ kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=$&#123;K8S_DIR&#125;/controller-manager.conf$ kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=$&#123;K8S_DIR&#125;/controller-manager.conf Scheduler憑證會建立system:kube-scheduler的使用者(憑證 CN)，並被綁定在 RBAC Cluster Role 中的system:kube-scheduler來讓 Scheduler 元件能夠存取需要的 API object。這邊透過以下指令產生 Scheduler 憑證： $ cfssl gencert \\ -ca=$&#123;PKI_DIR&#125;/ca.pem \\ -ca-key=$&#123;PKI_DIR&#125;/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/scheduler$ ls $&#123;PKI_DIR&#125;/scheduler*.pem/etc/kubernetes/pki/scheduler-key.pem /etc/kubernetes/pki/scheduler.pem 接著利用 kubectl 來產生 Scheduler 的 kubeconfig 檔： $ kubectl config set-cluster kubernetes \\ --certificate-authority=$&#123;PKI_DIR&#125;/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=$&#123;K8S_DIR&#125;/scheduler.conf$ kubectl config set-credentials system:kube-scheduler \\ --client-certificate=$&#123;PKI_DIR&#125;/scheduler.pem \\ --client-key=$&#123;PKI_DIR&#125;/scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=$&#123;K8S_DIR&#125;/scheduler.conf$ kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=$&#123;K8S_DIR&#125;/scheduler.conf$ kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=$&#123;K8S_DIR&#125;/scheduler.conf AdminAdmin 被用來綁定 RBAC Cluster Role 中 cluster-admin，當想要操作所有 Kubernetes 叢集功能時，就必須利用這邊產生的 kubeconfig 檔案。這邊透過以下指令產生 Kubernetes Admin 憑證： $ cfssl gencert \\ -ca=$&#123;PKI_DIR&#125;/ca.pem \\ -ca-key=$&#123;PKI_DIR&#125;/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/admin$ ls $&#123;PKI_DIR&#125;/admin*.pem/etc/kubernetes/pki/admin-key.pem /etc/kubernetes/pki/admin.pem 接著利用 kubectl 來產生 Admin 的 kubeconfig 檔： $ kubectl config set-cluster kubernetes \\ --certificate-authority=$&#123;PKI_DIR&#125;/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=$&#123;K8S_DIR&#125;/admin.conf$ kubectl config set-credentials kubernetes-admin \\ --client-certificate=$&#123;PKI_DIR&#125;/admin.pem \\ --client-key=$&#123;PKI_DIR&#125;/admin-key.pem \\ --embed-certs=true \\ --kubeconfig=$&#123;K8S_DIR&#125;/admin.conf$ kubectl config set-context kubernetes-admin@kubernetes \\ --cluster=kubernetes \\ --user=kubernetes-admin \\ --kubeconfig=$&#123;K8S_DIR&#125;/admin.conf$ kubectl config use-context kubernetes-admin@kubernetes \\ --kubeconfig=$&#123;K8S_DIR&#125;/admin.conf Masters Kubelet這邊使用 Node authorizer 來讓節點的 kubelet 能夠存取如 services、endpoints 等 API，而使用 Node authorizer 需定義 system:nodes 群組(憑證的 Organization)，並且包含system:node:&lt;nodeName&gt;的使用者名稱(憑證的 Common Name)。 首先在k8s-m1節點產生所有 master 節點的 kubelet 憑證，這邊透過下面腳本來產生： $ for NODE in k8s-m1 k8s-m2 k8s-m3; do echo \"--- $NODE ---\" cp kubelet-csr.json kubelet-$NODE-csr.json; sed -i \"s/\\$NODE/$NODE/g\" kubelet-$NODE-csr.json; cfssl gencert \\ -ca=$&#123;PKI_DIR&#125;/ca.pem \\ -ca-key=$&#123;PKI_DIR&#125;/ca-key.pem \\ -config=ca-config.json \\ -hostname=$NODE \\ -profile=kubernetes \\ kubelet-$NODE-csr.json | cfssljson -bare $&#123;PKI_DIR&#125;/kubelet-$NODE; rm kubelet-$NODE-csr.json done$ ls $&#123;PKI_DIR&#125;/kubelet*.pem/etc/kubernetes/pki/kubelet-k8s-m1-key.pem /etc/kubernetes/pki/kubelet-k8s-m2.pem/etc/kubernetes/pki/kubelet-k8s-m1.pem /etc/kubernetes/pki/kubelet-k8s-m3-key.pem/etc/kubernetes/pki/kubelet-k8s-m2-key.pem /etc/kubernetes/pki/kubelet-k8s-m3.pem 產生完成後，將 kubelet 憑證複製到所有master節點上： $ for NODE in k8s-m1 k8s-m2 k8s-m3; do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; \"mkdir -p $&#123;PKI_DIR&#125;\" scp $&#123;PKI_DIR&#125;/ca.pem $&#123;NODE&#125;:$&#123;PKI_DIR&#125;/ca.pem scp $&#123;PKI_DIR&#125;/kubelet-$NODE-key.pem $&#123;NODE&#125;:$&#123;PKI_DIR&#125;/kubelet-key.pem scp $&#123;PKI_DIR&#125;/kubelet-$NODE.pem $&#123;NODE&#125;:$&#123;PKI_DIR&#125;/kubelet.pem rm $&#123;PKI_DIR&#125;/kubelet-$NODE-key.pem $&#123;PKI_DIR&#125;/kubelet-$NODE.pem done 接著利用 kubectl 來產生 kubelet 的 kubeconfig 檔，這邊透過腳本來產生所有master節點的檔案： $ for NODE in k8s-m1 k8s-m2 k8s-m3; do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; \"cd $&#123;PKI_DIR&#125; &amp;&amp; \\ kubectl config set-cluster kubernetes \\ --certificate-authority=$&#123;PKI_DIR&#125;/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=$&#123;K8S_DIR&#125;/kubelet.conf &amp;&amp; \\ kubectl config set-credentials system:node:$&#123;NODE&#125; \\ --client-certificate=$&#123;PKI_DIR&#125;/kubelet.pem \\ --client-key=$&#123;PKI_DIR&#125;/kubelet-key.pem \\ --embed-certs=true \\ --kubeconfig=$&#123;K8S_DIR&#125;/kubelet.conf &amp;&amp; \\ kubectl config set-context system:node:$&#123;NODE&#125;@kubernetes \\ --cluster=kubernetes \\ --user=system:node:$&#123;NODE&#125; \\ --kubeconfig=$&#123;K8S_DIR&#125;/kubelet.conf &amp;&amp; \\ kubectl config use-context system:node:$&#123;NODE&#125;@kubernetes \\ --kubeconfig=$&#123;K8S_DIR&#125;/kubelet.conf\" done Service Account KeyKubernetes Controller Manager 利用 Key pair 來產生與簽署 Service Account 的 tokens，而這邊不透過 CA 做認證，而是建立一組公私鑰來讓 API Server 與 Controller Manager 使用： $ openssl genrsa -out $&#123;PKI_DIR&#125;/sa.key 2048$ openssl rsa -in $&#123;PKI_DIR&#125;/sa.key -pubout -out $&#123;PKI_DIR&#125;/sa.pub$ ls $&#123;PKI_DIR&#125;/sa.*/etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub 刪除不必要檔案當所有檔案建立與產生完成後，將一些不必要檔案刪除： $ rm -rf $&#123;PKI_DIR&#125;/*.csr \\ $&#123;PKI_DIR&#125;/scheduler*.pem \\ $&#123;PKI_DIR&#125;/controller-manager*.pem \\ $&#123;PKI_DIR&#125;/admin*.pem \\ $&#123;PKI_DIR&#125;/kubelet*.pem 複製檔案至其他節點將憑證複製到其他master節點： $ for NODE in k8s-m2 k8s-m3; do echo \"--- $NODE ---\" for FILE in $(ls $&#123;PKI_DIR&#125;); do scp $&#123;PKI_DIR&#125;/$&#123;FILE&#125; $&#123;NODE&#125;:$&#123;PKI_DIR&#125;/$&#123;FILE&#125; done done 複製各元件 kubeconfig 檔案至其他master節點： $ for NODE in k8s-m2 k8s-m3; do echo \"--- $NODE ---\" for FILE in admin.conf controller-manager.conf scheduler.conf; do scp $&#123;K8S_DIR&#125;/$&#123;FILE&#125; $&#123;NODE&#125;:$&#123;K8S_DIR&#125;/$&#123;FILE&#125; done done Kubernetes Masters本節將說明如何部署與設定 Kubernetes Master 角色中的各元件，在開始前先簡單了解一下各元件功能： kubelet：負責管理容器的生命週期，定期從 API Server 取得節點上的預期狀態(如網路、儲存等等配置)資源，並呼叫對應的容器介面(CRI、CNI 等)來達成這個狀態。任何 Kubernetes 節點都會擁有該元件。 kube-apiserver：以 REST APIs 提供 Kubernetes 資源的 CRUD，如授權、認證、存取控制與 API 註冊等機制。 kube-controller-manager：透過核心控制循環(Core Control Loop)監聽 Kubernetes API 的資源來維護叢集的狀態，這些資源會被不同的控制器所管理，如 Replication Controller、Namespace Controller 等等。而這些控制器會處理著自動擴展、滾動更新等等功能。 kube-scheduler：負責將一個(或多個)容器依據排程策略分配到對應節點上讓容器引擎(如 Docker)執行。而排程受到 QoS 要求、軟硬體約束、親和性(Affinity)等等規範影響。 Etcd：用來保存叢集所有狀態的 Key/Value 儲存系統，所有 Kubernetes 元件會透過 API Server 來跟 Etcd 進行溝通來保存或取得資源狀態。 HAProxy：提供多個 API Server 的負載平衡(Load Balance)。 Keepalived：建立一個虛擬 IP(VIP) 來作為 API Server 統一存取端點。 而上述元件除了 kubelet 外，其他將透過 kubelet 以 Static Pod 方式進行部署，這種方式可以減少管理 Systemd 的服務，並且能透過 kubectl 來觀察啟動的容器狀況。 部署與設定首先在k8s-m1節點進入k8s-manual-files目錄，並依序執行下述指令來完成部署： $ cd ~/k8s-manual-files 首先利用./hack/gen-configs.sh腳本在每台master節點產生組態檔： $ export NODES=\"k8s-m1 k8s-m2 k8s-m3\"$ ./hack/gen-configs.shk8s-m1 config generated...k8s-m2 config generated...k8s-m3 config generated... 完成後記得檢查/etc/etcd/config.yml與/etc/haproxy/haproxy.cfg是否設定正確。 這邊主要確認檔案中的${xxx}字串是否有被更改，並且符合環境。詳細內容可以查看k8s-manual-files。 接著利用./hack/gen-manifests.sh腳本在每台master節點產生 Static pod YAML 檔案，以及其他相關設定檔(如 EncryptionConfig)： $ export NODES=\"k8s-m1 k8s-m2 k8s-m3\"$ ./hack/gen-manifests.shk8s-m1 manifests generated...k8s-m2 manifests generated...k8s-m3 manifests generated... 完成後記得檢查/etc/kubernetes/manifests、/etc/kubernetes/encryption與/etc/kubernetes/audit目錄中的檔案是否是定正確。 這邊主要確認檔案中的${xxx}字串是否有被更改，並且符合環境需求。詳細內容可以查看k8s-manual-files。 確認上述兩個產生檔案步驟完成後，即可設定所有master節點的 kubelet systemd 來啟動 Kubernetes 元件。首先複製下列檔案到指定路徑： $ for NODE in k8s-m1 k8s-m2 k8s-m3; do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; \"mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d\" scp master/var/lib/kubelet/config.yml $&#123;NODE&#125;:/var/lib/kubelet/config.yml scp master/systemd/kubelet.service $&#123;NODE&#125;:/lib/systemd/system/kubelet.service scp master/systemd/10-kubelet.conf $&#123;NODE&#125;:/etc/systemd/system/kubelet.service.d/10-kubelet.conf done 接著在k8s-m1透過 SSH 啟動所有master節點的 kubelet： $ for NODE in k8s-m1 k8s-m2 k8s-m3; do ssh $&#123;NODE&#125; \"systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service\" done 完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看： $ watch netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.1:10251 0.0.0.0:* LISTEN 9407/kube-schedulertcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 9338/kube-controlletcp 0 0 127.0.0.1:38420 0.0.0.0:* LISTEN 8676/kubelettcp 0 0 0.0.0.0:8443 0.0.0.0:* LISTEN 9602/haproxytcp 0 0 0.0.0.0:9090 0.0.0.0:* LISTEN 9602/haproxytcp6 0 0 :::10250 :::* LISTEN 8676/kubelettcp6 0 0 :::2379 :::* LISTEN 9487/etcdtcp6 0 0 :::6443 :::* LISTEN 9133/kube-apiservertcp6 0 0 :::2380 :::* LISTEN 9487/etcd... 若看到以上資訊表示服務正常啟動，若發生問題可以用docker指令來查看。 接下來將建立 TLS Bootstrapping 來讓 Node 簽證並授權註冊到叢集。 建立 TLS Bootstrapping由於本教學採用 TLS 認證來確保 Kubernetes 叢集的安全性，因此每個節點的 kubelet 都需要透過 API Server 的 CA 進行身份驗證後，才能與 API Server 進行溝通，而這過程過去都是採用手動方式針對每台節點(master與node)單獨簽署憑證，再設定給 kubelet 使用，然而這種方式是一件繁瑣的事情，因為當節點擴展到一定程度時，將會非常費時，甚至延伸初管理不易問題。 而由於上述問題，Kubernetes 實現了 TLS Bootstrapping 來解決此問題，這種做法是先讓 kubelet 以一個低權限使用者(一個能存取 CSR API 的 Token)存取 API Server，接著對 API Server 提出申請憑證簽署請求，並在受理後由 API Server 動態簽署 kubelet 憑證提供給對應的node節點使用。具體作法請參考 TLS Bootstrapping 與 Authenticating with Bootstrap Tokens。 在k8s-m1建立 bootstrap 使用者的 kubeconfig 檔： $ export TOKEN_ID=$(openssl rand 3 -hex)$ export TOKEN_SECRET=$(openssl rand 8 -hex)$ export BOOTSTRAP_TOKEN=$&#123;TOKEN_ID&#125;.$&#123;TOKEN_SECRET&#125;$ export KUBE_APISERVER=\"https://172.22.132.9:6443\"$ kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf$ kubectl config set-credentials tls-bootstrap-token-user \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf$ kubectl config set-context tls-bootstrap-token-user@kubernetes \\ --cluster=kubernetes \\ --user=tls-bootstrap-token-user \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf$ kubectl config use-context tls-bootstrap-token-user@kubernetes \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf KUBE_APISERVER這邊設定為 VIP 位址。若想要用手動簽署憑證來進行授權的話，可以參考 Certificate。 接著在k8s-m1建立 TLS Bootstrap Secret 來提供自動簽證使用： $ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Secretmetadata: name: bootstrap-token-$&#123;TOKEN_ID&#125; namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData: token-id: \"$&#123;TOKEN_ID&#125;\" token-secret: \"$&#123;TOKEN_SECRET&#125;\" usage-bootstrap-authentication: \"true\" usage-bootstrap-signing: \"true\" auth-extra-groups: system:bootstrappers:default-node-tokenEOFsecret \"bootstrap-token-65a3a9\" created 然後建立 TLS Bootstrap Autoapprove RBAC 來提供自動受理 CSR： $ kubectl apply -f master/resources/kubelet-bootstrap-rbac.ymlclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap createdclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap createdclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created 驗證 Master 節點完成後，在任意一台master節點複製 Admin kubeconfig 檔案，並透過簡單指令驗證： $ cp /etc/kubernetes/admin.conf ~/.kube/config$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Healthy &#123;\"health\":\"true\"&#125;etcd-1 Healthy &#123;\"health\":\"true\"&#125;etcd-2 Healthy &#123;\"health\":\"true\"&#125;$ kubectl -n kube-system get poNAME READY STATUS RESTARTS AGEetcd-k8s-m1 1/1 Running 0 1hetcd-k8s-m2 1/1 Running 0 1hetcd-k8s-m3 1/1 Running 0 1hkube-apiserver-k8s-m1 1/1 Running 0 1hkube-apiserver-k8s-m2 1/1 Running 0 1hkube-apiserver-k8s-m3 1/1 Running 0 1h...$ kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-m1 NotReady master 38s v1.11.0k8s-m2 NotReady master 37s v1.11.0k8s-m3 NotReady master 36s v1.11.0 在這階段狀態處於NotReady是正常，往下進行就會了解為何。 透過 kubectl logs 來查看容器的日誌： $ kubectl -n kube-system logs -f kube-apiserver-k8s-m1Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-k8s-m1) 這邊會發現出現 403 Forbidden 問題，這是因為 kube-apiserver user 並沒有 nodes 的資源存取權限，屬於正常。 為了方便管理叢集，因此需要透過 kubectl logs 來查看，但由於 API 權限問題，故需要建立一個 RBAC Role 來獲取存取權限，這邊在k8s-m1節點執行以下指令建立： $ kubectl apply -f master/resources/apiserver-to-kubelet-rbac.ymlclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created 完成後，再次透過 kubectl logs 查看 Pod： $ kubectl -n kube-system logs -f kube-apiserver-k8s-m1I0708 15:22:33.906269 1 get.go:245] Starting watch for /api/v1/services, rv=2494 labels= fields= timeout=8m29sI0708 15:22:40.919638 1 get.go:245] Starting watch for /apis/certificates.k8s.io/v1beta1/certificatesigningrequests, rv=11084 labels= fields= timeout=7m29s... 接著設定 Taints and Tolerations 來讓一些特定 Pod 能夠排程到所有master節點上： $ kubectl taint nodes node-role.kubernetes.io/master=\"\":NoSchedule --allnode \"k8s-m1\" taintednode \"k8s-m2\" taintednode \"k8s-m3\" tainted 截至這邊已完成master節點部署，接下來將針對node的部署進行說明。 Kubernetes Nodes本節將說明如何建立與設定 Kubernetes Node 節點，Node 是主要執行容器實例(Pod)的工作節點。這過程只需要將 PKI、Bootstrap conf 等檔案複製到機器上，再用 kubelet 啟動即可。 在開始部署前，在k8-m1將需要用到的檔案複製到所有node節點上： $ for NODE in k8s-g1 k8s-g2; do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; \"mkdir -p /etc/kubernetes/pki/\" for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do scp /etc/kubernetes/$&#123;FILE&#125; $&#123;NODE&#125;:/etc/kubernetes/$&#123;FILE&#125; done done 部署與設定確認檔案都複製後，即可設定所有node節點的 kubelet systemd 來啟動 Kubernetes 元件。首先在k8s-m1複製下列檔案到指定路徑： $ cd ~/k8s-manual-files$ for NODE in k8s-g1 k8s-g2; do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; \"mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests\" scp node/var/lib/kubelet/config.yml $&#123;NODE&#125;:/var/lib/kubelet/config.yml scp node/systemd/kubelet.service $&#123;NODE&#125;:/lib/systemd/system/kubelet.service scp node/systemd/10-kubelet.conf $&#123;NODE&#125;:/etc/systemd/system/kubelet.service.d/10-kubelet.conf done 接著在k8s-m1透過 SSH 啟動所有node節點的 kubelet： $ for NODE in k8s-g1 k8s-g2; do ssh $&#123;NODE&#125; \"systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service\" done 驗證 Node 節點完成後，在任意一台master節點複製 Admin kubeconfig 檔案，並透過簡單指令驗證： $ kubectl get csrNAME AGE REQUESTOR CONDITIONcsr-99n76 1h system:node:k8s-m2 Approved,Issuedcsr-9n88h 1h system:node:k8s-m1 Approved,Issuedcsr-vdtqr 1h system:node:k8s-m3 Approved,Issuednode-csr-5VkCjWvb8tGVtO-d2gXiQrnst-G1xe_iA0AtQuYNEMI 2m system:bootstrap:872255 Approved,Issuednode-csr-Uwpss9OhJrAgOB18P4OIEH02VHJwpFrSoMOWkkrK-lo 2m system:bootstrap:872255 Approved,Issued$ kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-g1 NotReady &lt;none&gt; 8m v1.11.0k8s-g2 NotReady &lt;none&gt; 8m v1.11.0k8s-m1 NotReady master 20m v1.11.0k8s-m2 NotReady master 20m v1.11.0k8s-m3 NotReady master 20m v1.11.0 在這階段狀態處於NotReady是正常，往下進行就會了解為何。 到這邊就表示node節點部署已完成了，接下來章節將針對 Kubernetes Addons 安裝進行說明。 Kubernetes Core Addons 部署當完成master與node節點的部署，並組合成一個可運作叢集後，就可以開始透過 kubectl 部署 Addons，Kubernetes 官方提供了多種 Addons 來加強 Kubernetes 的各種功能，如叢集 DNS 解析的kube-dns(or CoreDNS)、外部存取服務的kube-proxy與 Web-based 管理介面的dashboard等等。而其中有些 Addons 是被 Kubernetes 認定為必要的，因此本節將說明如何部署這些 Addons。 首先在k8s-m1節點進入k8s-manual-files目錄，並依序執行下述指令來完成部署： $ cd ~/k8s-manual-files Kubernetes Proxykube-proxy 是實現 Kubernetes Service 資源功能的關鍵元件，這個元件會透過 DaemonSet 在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的事件，並依據資源預期狀態透過 iptables 或 ipvs 來實現網路轉發，而本次安裝採用 ipvs。 在k8s-m1透過 kubeclt 執行下面指令來建立，並檢查是否部署成功： $ export KUBE_APISERVER=https://172.22.132.9:6443$ sed -i \"s/\\$&#123;KUBE_APISERVER&#125;/$&#123;KUBE_APISERVER&#125;/g\" addons/kube-proxy/kube-proxy-cm.yml$ kubectl -f addons/kube-proxy/$ kubectl -n kube-system get po -l k8s-app=kube-proxyNAME READY STATUS RESTARTS AGEkube-proxy-dd2m7 1/1 Running 0 8mkube-proxy-fwgx8 1/1 Running 0 8mkube-proxy-kjn57 1/1 Running 0 8mkube-proxy-vp47w 1/1 Running 0 8mkube-proxy-xsncw 1/1 Running 0 8m# 檢查 log 是否使用 ipvs$ kubectl -n kube-system logs -f kube-proxy-fwgx8I0709 08:41:48.220815 1 feature_gate.go:230] feature gates: &amp;&#123;map[SupportIPVSProxyMode:true]&#125;I0709 08:41:48.231009 1 server_others.go:183] Using ipvs Proxier.... 若有安裝 ipvsadm 的話，可以透過以下指令查看 proxy 規則： $ ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.96.0.1:443 rr -&gt; 172.22.132.9:5443 Masq 1 0 0 CoreDNS本節將透過 CoreDNS 取代 Kube DNS 作為叢集服務發現元件，由於 Kubernetes 需要讓 Pod 與 Pod 之間能夠互相溝通，然而要能夠溝通需要知道彼此的 IP 才行，而這種做法通常是透過 Kubernetes API 來取得達到，但是 Pod IP 會因為生命週期變化而改變，因此這種做法無法彈性使用，且還會增加 API Server 負擔，基於此問題 Kubernetes 提供了 DNS 服務來作為查詢，讓 Pod 能夠以 Service 名稱作為域名來查詢 IP 位址，因此使用者就再不需要關切實際 Pod IP，而 DNS 也會根據 Pod 變化更新資源紀錄(Record resources)。 CoreDNS 是由 CNCF 維護的開源 DNS 專案，該專案前身是 SkyDNS，其採用了 Caddy 的一部分來開發伺服器框架，使其能夠建構一套快速靈活的 DNS，而 CoreDNS 每個功能都可以被實作成一個插件的中介軟體，如 Log、Cache、Kubernetes 等功能，甚至能夠將源紀錄儲存至 Redis、Etcd 中。 在k8s-m1透過 kubeclt 執行下面指令來建立，並檢查是否部署成功： $ kubectl create -f addons/coredns/$ kubectl -n kube-system get po -l k8s-app=kube-dnsNAME READY STATUS RESTARTS AGEcoredns-589dd74cb6-5mv5c 0/1 Pending 0 3mcoredns-589dd74cb6-d42ft 0/1 Pending 0 3m 這邊會發現 Pod 處於Pending狀態，這是由於 Kubernetes 的叢集網路沒有建立，因此所有節點會處於NotReady狀態，而這也導致 Kubernetes Scheduler 無法替 Pod 找到適合節點而處於Pending，為了解決這個問題，下節將說明與建立 Kubernetes 叢集網路。 若 Pod 是被 DaemonSet 管理，且設定使用hostNetwork的話，則不會處於Pending狀態。 Kubernetes 叢集網路Kubernetes 在預設情況下與 Docker 的網路有所不同。在 Kubernetes 中有四個問題是需要被解決的，分別為： 高耦合的容器到容器溝通：透過 Pods 與 Localhost 的溝通來解決。 Pod 到 Pod 的溝通：透過實現網路模型來解決。 Pod 到 Service 溝通：由 Service objects 結合 kube-proxy 解決。 外部到 Service 溝通：一樣由 Service objects 結合 kube-proxy 解決。 而 Kubernetes 對於任何網路的實現都需要滿足以下基本要求(除非是有意調整的網路分段策略)： 所有容器能夠在沒有 NAT 的情況下與其他容器溝通。 所有節點能夠在沒有 NAT 情況下與所有容器溝通(反之亦然)。 容器看到的 IP 與其他人看到的 IP 是一樣的。 慶幸的是 Kubernetes 已經有非常多種的網路模型以網路插件(Network Plugins)方式被實現，因此可以選用滿足自己需求的網路功能來使用。另外 Kubernetes 中的網路插件有以下兩種形式： CNI plugins：以 appc/CNI 標準規範所實現的網路，詳細可以閱讀 CNI Specification。 Kubenet plugin：使用 CNI plugins 的 bridge 與 host-local 來實現基本的 cbr0。這通常被用在公有雲服務上的 Kubernetes 叢集網路。 如果想了解如何選擇可以閱讀 Chris Love 的 Choosing a CNI Network Provider for Kubernetes 文章。 網路部署與設定從上述了解 Kubernetes 有多種網路能夠選擇，而本教學選擇了 Calico 作為叢集網路的使用。Calico 是一款純 Layer 3 的網路，其好處是它整合了各種雲原生平台(Docker、Mesos 與 OpenStack 等)，且 Calico 不採用 vSwitch，而是在每個 Kubernetes 節點使用 vRouter 功能，並透過 Linux Kernel 既有的 L3 forwarding 功能，而當資料中心複雜度增加時，Calico 也可以利用 BGP route reflector 來達成。 想了解 Calico 與傳統 overlay networks 的差異，可以閱讀 Difficulties with traditional overlay networks 文章。 由於 Calico 提供了 Kubernetes resources YAML 檔來快速以容器方式部署網路插件至所有節點上，因此只需要在k8s-m1透過 kubeclt 執行下面指令來建立： $ cd ~/k8s-manual-files$ sed -i 's/192.168.0.0\\/16/10.244.0.0\\/16/g' cni/calico/v3.1/calico.yaml$ kubectl -f cni/calico/v3.1/ 這邊要記得將CALICO_IPV4POOL_CIDR的網路修改 Cluster IP CIDR。 另外當節點超過 50 台，可以使用 Calico 的 Typha 模式來減少透過 Kubernetes datastore 造成 API Server 的負擔。 部署後透過 kubectl 檢查是否有啟動： $ kubectl -n kube-system get po -l k8s-app=calico-nodeNAME READY STATUS RESTARTS AGEcalico-node-27jwl 2/2 Running 0 59scalico-node-4fgv6 2/2 Running 0 59scalico-node-mvrt7 2/2 Running 0 59scalico-node-p2q9g 2/2 Running 0 59scalico-node-zchsz 2/2 Running 0 59s 確認 calico-node 都正常運作後，透過 kubectl exec 進入 calicoctl pod 來檢查功能是否正常： $ kubectl exec -ti -n kube-system calicoctl -- calicoctl get profiles -o wideNAME LABELSkns.default map[]kns.kube-public map[]kns.kube-system map[]$ kubectl exec -ti -n kube-system calicoctl -- calicoctl get node -o wideNAME ASN IPV4 IPV6k8s-g1 (unknown) 172.22.132.13/24k8s-g2 (unknown) 172.22.132.14/24k8s-m1 (unknown) 172.22.132.10/24k8s-m2 (unknown) 172.22.132.11/24k8s-m3 (unknown) 172.22.132.12/24 若沒問題，就可以將 kube-system 下的 calicoctl pod 刪除。 完成後，透過檢查節點是否不再是NotReady，以及 Pod 是否不再處於Pending： $ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-g1 Ready &lt;none&gt; 35m v1.11.0k8s-g2 Ready &lt;none&gt; 35m v1.11.0k8s-m1 Ready master 35m v1.11.0k8s-m2 Ready master 35m v1.11.0k8s-m3 Ready master 35m v1.11.0$ kubectl -n kube-system get po -l k8s-app=kube-dns -o wideNAME READY STATUS RESTARTS AGE IP NODEcoredns-589dd74cb6-5mv5c 1/1 Running 0 10m 10.244.4.2 k8s-g2coredns-589dd74cb6-d42ft 1/1 Running 0 10m 10.244.3.2 k8s-g1 當成功到這邊時，一個能運作的 Kubernetes 叢集基本上就完成了，接下來將介紹一些好用的 Addons 來幫助使用與管理 Kubernetes。 Kubernetes Extra Addons 部署本節說明如何部署一些官方常用的額外 Addons，如 Dashboard、Metrics Server 與 Ingress Controller 等等。 所有 Addons 部署檔案均存已放至k8s-manual-files中，因此在k8s-m1進入該目錄，並依序下小節建立： $ cd ~/k8s-manual-files Ingress ControllerIngress 是 Kubernetes 中的一個抽象資源，其功能是透過 Web Server 的 Virtual Host 概念以域名(Domain Name)方式轉發到內部 Service，這避免了使用 Service 中的 NodePort 與 LoadBalancer 類型所帶來的限制(如 Port 數量上限)，而實現 Ingress 功能則是透過 Ingress Controller 來達成，它會負責監聽 Kubernetes API 中的 Ingress 與 Service 資源物件，並在發生資源變化時，依據資源預期的結果來設定 Web Server。另外 Ingress Controller 有許多實現可以選擇： Ingress NGINX: Kubernetes 官方維護的專案，也是本次安裝使用的 Controller。 F5 BIG-IP Controller: F5 所開發的 Controller，它能夠讓管理員透過 CLI 或 API 從 Kubernetes 與 OpenShift 管理 F5 BIG-IP 設備。 Ingress Kong: 著名的開源 API Gateway 專案所維護的 Kubernetes Ingress Controller。 Træfik: 是一套開源的 HTTP 反向代理與負載平衡器，而它也支援了 Ingress。 Voyager: 一套以 HAProxy 為底的 Ingress Controller。 而 Ingress Controller 的實現不只這些專案，還有很多可以在網路上找到，未來自己也會寫一篇 Ingress Controller 的實作方式文章。 首先在k8s-m1執行下述指令來建立 Ingress Controller，並檢查是否部署正常： $ export INGRESS_VIP=172.22.132.8$ sed -i \"s/\\$&#123;INGRESS_VIP&#125;/$&#123;INGRESS_VIP&#125;/g\" addons/ingress-controller/ingress-controller-svc.yml$ kubectl create ns ingress-nginx$ kubectl apply -f addons/ingress-controller$ kubectl -n ingress-nginx get po,svcNAME READY STATUS RESTARTS AGEpod/default-http-backend-846b65fb5f-l5hrc 1/1 Running 0 2mpod/nginx-ingress-controller-5db8d65fb-z2lf9 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/default-http-backend ClusterIP 10.99.105.112 &lt;none&gt; 80/TCP 2mservice/ingress-nginx LoadBalancer 10.106.18.106 172.22.132.8 80:31197/TCP 2m 完成後透過瀏覽器存取 http://172.22.132.8:80 來查看是否能連線，若可以會如下圖結果。 當確認上面步驟都沒問題後，就可以透過 kubeclt 建立簡單 NGINX 來測試功能： $ kubectl apply -f apps/nginx/deployment.extensions/nginx createdingress.extensions/nginx-ingress createdservice/nginx created$ kubectl get po,svc,ingNAME READY STATUS RESTARTS AGEpod/nginx-966857787-78kth 1/1 Running 0 32sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 2dservice/nginx ClusterIP 10.104.180.119 &lt;none&gt; 80/TCP 32sNAME HOSTS ADDRESS PORTS AGEingress.extensions/nginx-ingress nginx.k8s.local 172.22.132.8 80 33s P.S. Ingress 規則也支援不同 Path 的服務轉發，可以參考上面提供的官方文件來設定。 完成後透過 cURL 工具來測試功能是否正常： $ curl 172.22.132.8 -H 'Host: nginx.k8s.local'&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...# 測試其他 domain name 是否會回傳 404$ curl 172.22.132.8 -H 'Host: nginx1.k8s.local'default backend - 404 雖然 Ingress 能夠讓我們透過域名方式存取 Kubernetes 內部服務，但是若域名於法被測試機器解析的話，將會顯示default backend - 404結果，而這經常發生在內部自建環境上，雖然可以透過修改主機/etc/hosts來描述，但並不彈性，因此下節將說明如何建立一個 External DNS 與 DNS 伺服器來提供自動解析 Ingress 域名。 External DNSExternal DNS 是 Kubernetes 社區的孵化專案，被用於定期同步 Kubernetes Service 與 Ingress 資源，並依據資源內容來自動設定公有雲 DNS 服務的資源紀錄(Record resources)。而由於部署不是公有雲環境，因此需要透過 CoreDNS 提供一個內部 DNS 伺服器，再由 ExternalDNS 與這個 CoreDNS 做串接。 首先在k8s-m1執行下述指令來建立 CoreDNS Server，並檢查是否部署正常： $ export DNS_VIP=172.22.132.8$ sed -i \"s/\\$&#123;DNS_VIP&#125;/$&#123;DNS_VIP&#125;/g\" addons/external-dns/coredns/coredns-svc-tcp.yml$ sed -i \"s/\\$&#123;DNS_VIP&#125;/$&#123;DNS_VIP&#125;/g\" addons/external-dns/coredns/coredns-svc-udp.yml$ kubectl create -f addons/external-dns/coredns/$ kubectl -n external-dns get po,svcNAME READY STATUS RESTARTS AGEpod/coredns-54bcfcbd5b-5grb5 1/1 Running 0 2mpod/coredns-etcd-6c9c68fd76-n8rhj 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/coredns-etcd ClusterIP 10.110.186.83 &lt;none&gt; 2379/TCP,2380/TCP 2mservice/coredns-tcp LoadBalancer 10.109.105.166 172.22.132.8 53:32169/TCP,9153:32150/TCP 2mservice/coredns-udp LoadBalancer 10.110.242.185 172.22.132.8 53:31210/UDP 這邊域名為k8s.local，可以修改檔案中的coredns-cm.yml來改變。 完成後，透過 dig 工具來檢查是否 DNS 是否正常： $ dig @172.22.132.8 SOA nginx.k8s.local +noall +answer +time=2 +tries=1...; (1 server found);; global options: +cmdk8s.local. 300 IN SOA ns.dns.k8s.local. hostmaster.k8s.local. 1531299150 7200 1800 86400 30 接著部署 ExternalDNS 來與 CoreDNS 同步資源紀錄： $ kubectl apply -f addons/external-dns/external-dns/$ kubectl -n external-dns get po -l k8s-app=external-dnsNAME READY STATUS RESTARTS AGEexternal-dns-86f67f6df8-ljnhj 1/1 Running 0 1m 完成後，透過 dig 與 nslookup 工具檢查上節測試 Ingress 的 NGINX 服務： $ dig @172.22.132.8 A nginx.k8s.local +noall +answer +time=2 +tries=1...; (1 server found);; global options: +cmdnginx.k8s.local. 300 IN A 172.22.132.8$ nslookup nginx.k8s.localServer: 172.22.132.8Address: 172.22.132.8#53** server can&apos;t find nginx.k8s.local: NXDOMAIN 這時會無法透過 nslookup 解析域名，這是因為測試機器並沒有使用這個 DNS 伺服器，可以透過修改/etc/resolv.conf來加入，或者類似下圖方式(不同 OS 有差異，不過都在網路設定中改)。 再次透過 nslookup 檢查，會發現可以解析了，這時也就能透過 cURL 來測試結果： $ nslookup nginx.k8s.localServer: 172.22.132.8Address: 172.22.132.8#53Name: nginx.k8s.localAddress: 172.22.132.8$ curl nginx.k8s.local&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;... DashboardDashboard 是 Kubernetes 官方開發的 Web-based 儀表板，目的是提升管理 Kubernetes 叢集資源便利性，並以資源視覺化方式，來讓人更直覺的看到整個叢集資源狀態， 在k8s-m1透過 kubeclt 執行下面指令來建立 Dashboard 至 Kubernetes，並檢查是否正確部署： $ cd ~/k8s-manual-files$ kubectl apply -f addons/dashboard/$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/kubernetes-dashboard-6948bdb78-w26qc 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes-dashboard ClusterIP 10.109.31.80 &lt;none&gt; 443/TCP 2m 在這邊會額外建立名稱為anonymous-dashboard-proxy的 Cluster Role(Binding) 來讓system:anonymous這個匿名使用者能夠透過 API Server 來 proxy 到 Kubernetes Dashboard，而這個 RBAC 規則僅能夠存取services/proxy資源，以及https:kubernetes-dashboard:資源名稱。 因此我們能夠在完成後，透過以下連結來進入 Kubernetes Dashboard： https://{YOUR_VIP}:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 由於 Kubernetes Dashboard v1.7 版本以後不再提供 Admin 權限，因此需要透過 kubeconfig 或者 Service Account 來進行登入才能取得資源來呈現，這邊建立一個 Service Account 來綁定cluster-admin 以測試功能： $ kubectl -n kube-system create sa dashboard$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk '/dashboard-token/ &#123;print $3&#125;')$ kubectl -n kube-system describe secrets $&#123;SECRET&#125; | awk '/token:/&#123;print $2&#125;'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw 複製token然後貼到 Kubernetes dashboard。注意這邊一般來說要針對不同 User 開啟特定存取權限。 Prometheus由於 Heapster 將要被移棄，因此這邊選用 Prometheus 作為第三方的叢集監控方案。而本次安裝採用 CoreOS 開發的 Prometheus Operator 用於管理在 Kubernetes 上的 Prometheus 叢集與資源，更多關於 Prometheus Operator 的資訊可以參考小弟的 Prometheus Operator 介紹與安裝 文章。 首先在k8s-m1執行下述指令來部署所有 Prometheus 需要的元件： $ kubectl apply -f addons/prometheus/$ kubectl apply -f addons/prometheus/operator/# 這邊要等 operator 起來並建立好 CRDs 才能進行$ kubectl apply -f addons/prometheus/alertmanater/$ kubectl apply -f addons/prometheus/node-exporter/$ kubectl apply -f addons/prometheus/kube-state-metrics/$ kubectl apply -f addons/prometheus/grafana/$ kubectl apply -f addons/prometheus/kube-service-discovery/$ kubectl apply -f addons/prometheus/prometheus/$ kubectl apply -f addons/prometheus/servicemonitor/ 完成後，透過 kubectl 檢查服務是否正常運行： $ kubectl -n monitoring get po,svc,ingNAME READY STATUS RESTARTS AGEpod/alertmanager-main-0 1/2 Running 0 1mpod/grafana-6d495c46d5-jpf6r 1/1 Running 0 43spod/kube-state-metrics-b84cfb86-4b8qg 4/4 Running 0 37spod/node-exporter-2f4lh 2/2 Running 0 59spod/node-exporter-7cz5s 2/2 Running 0 59spod/node-exporter-djdtk 2/2 Running 0 59spod/node-exporter-kfpzt 2/2 Running 0 59spod/node-exporter-qp2jf 2/2 Running 0 59spod/prometheus-k8s-0 3/3 Running 0 28spod/prometheus-k8s-1 3/3 Running 0 15spod/prometheus-operator-9ffd6bdd9-rvqsz 1/1 Running 0 1mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/alertmanager-main ClusterIP 10.110.188.2 &lt;none&gt; 9093/TCP 1mservice/alertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 1mservice/grafana ClusterIP 10.104.147.154 &lt;none&gt; 3000/TCP 43sservice/kube-state-metrics ClusterIP None &lt;none&gt; 8443/TCP,9443/TCP 51sservice/node-exporter ClusterIP None &lt;none&gt; 9100/TCP 1mservice/prometheus-k8s ClusterIP 10.96.78.58 &lt;none&gt; 9090/TCP 28sservice/prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 33sservice/prometheus-operator ClusterIP 10.99.251.16 &lt;none&gt; 8080/TCP 1mNAME HOSTS ADDRESS PORTS AGEingress.extensions/grafana-ing grafana.monitoring.k8s.local 172.22.132.8 80 45singress.extensions/prometheus-ing prometheus.monitoring.k8s.local 172.22.132.8 80 34s 確認沒問題後，透過瀏覽器查看 prometheus.monitoring.k8s.local 與 grafana.monitoring.k8s.local 是否正常，若沒問題就可以看到如下圖所示結果。 另外這邊也推薦用 Weave Scope 來監控容器的網路 Flow 拓樸圖。 Metrics ServerMetrics Server 是實現了 Metrics API 的元件，其目標是取代 Heapster 作為 Pod 與 Node 提供資源的 Usage metrics，該元件會從每個 Kubernetes 節點上的 Kubelet 所公開的 Summary API 中收集 Metrics。 首先在k8s-m1測試一下 kubectl top 指令： $ kubectl top nodeerror: metrics not available yet 發現 top 指令無法取得 Metrics，這表示 Kubernetes 叢集沒有安裝 Heapster 或是 Metrics Server 來提供 Metrics API 給 top 指令取得資源使用量。 由於上述問題，我們要在k8s-m1節點透過 kubectl 部署 Metrics Server 元件來解決： $ kubectl create -f addons/metric-server/$ kubectl -n kube-system get po -l k8s-app=metrics-serverNAME READY STATUS RESTARTS AGEpod/metrics-server-86bd9d7667-5hbn6 1/1 Running 0 1m 完成後，等待一點時間(約 30s - 1m)收集 Metrics，再次執行 kubectl top 指令查看： $ kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%k8s-g1 106m 2% 1037Mi 6%k8s-g2 212m 5% 1043Mi 8%k8s-m1 386m 9% 2125Mi 13%k8s-m2 320m 8% 1834Mi 11%k8s-m3 457m 11% 1818Mi 11% 而這時若有使用 HPA 的話，就能夠正確抓到 Pod 的 CPU 與 Memory 使用量了。 若想讓 HPA 使用 Prometheus 的 Metrics 的話，可以閱讀 Custom Metrics Server 來了解。 Helm Tiller ServerHelm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源。其中Tiller Server主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 Release)。 首先在k8s-m1安裝 Helm tool： $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz | tar -zx$ sudo mv linux-amd64/helm /usr/local/bin/ 另外在所有node節點安裝 socat： $ sudo apt-get install -y socat 接著初始化 Helm(這邊會安裝 Tiller Server)： $ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller...Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Happy Helming!$ kubectl -n kube-system get po -l app=helmNAME READY STATUS RESTARTS AGEtiller-deploy-759cb9df9-rfhqw 1/1 Running 0 19s$ helm versionClient: &amp;version.Version&#123;SemVer:\"v2.9.1\", GitCommit:\"20adb27c7c5868466912eebdf6664e7390ebe710\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.9.1\", GitCommit:\"20adb27c7c5868466912eebdf6664e7390ebe710\", GitTreeState:\"clean\"&#125; 測試 Helm 功能這邊部署簡單 Jenkins 來進行功能測試： $ helm install --name demo --set Persistence.Enabled=false stable/jenkins$ kubectl get po,svc -l app=demo-jenkinsNAME READY STATUS RESTARTS AGEdemo-jenkins-7bf4bfcff-q74nt 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemo-jenkins LoadBalancer 10.103.15.129 &lt;pending&gt; 8080:31161/TCP 2mdemo-jenkins-agent ClusterIP 10.103.160.126 &lt;none&gt; 50000/TCP 2m# 取得 admin 帳號的密碼$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=\"&#123;.data.jenkins-admin-password&#125;\" | base64 --decode);echor6y9FMuF2u 當服務都正常運作時，就可以透過瀏覽器查看 http://node_ip:31161 頁面。 測試完成後，就可以透過以下指令來刪除 Release： $ helm lsNAME REVISION UPDATED STATUS CHART NAMESPACEdemo 1 Tue Apr 10 07:29:51 2018 DEPLOYED jenkins-0.14.4 default$ helm delete demo --purgerelease \"demo\" deleted 想要了解更多 Helm Apps 的話，可以到 Kubeapps Hub 網站尋找。 測試叢集 HA 功能首先進入k8s-m1節點，然後關閉該節點： $ sudo poweroff 接著進入到k8s-m2節點，透過 kubectl 來檢查叢集是否能夠正常執行： # 先檢查 etcd 狀態，可以發現 etcd-0 因為關機而中斷$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-0 Unhealthy Get https://172.22.132.10:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)# 測試是否可以建立 Pod$ kubectl run nginx --image nginx --restart=Never --port 80$ kubectl get poNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 22s","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Calico","slug":"Calico","permalink":"https://k2r2bai.com/tags/Calico/"}]},{"title":"Prometheus 高可靠實現方式","slug":"devops/prometheus/prometheus-ha","date":"2018-07-01T04:23:01.000Z","updated":"2019-12-02T01:49:42.386Z","comments":true,"path":"2018/07/01/devops/prometheus/prometheus-ha/","link":"","permalink":"https://k2r2bai.com/2018/07/01/devops/prometheus/prometheus-ha/","excerpt":"前面幾篇提到了 Prometheus 儲存系統與 Federation 功能，其中在儲存系統可以得知 Local on-disk 方式雖然能夠帶來很好的效能，但是卻也存在著單點故障的問題，並且限制了 Prometehsu 的可擴展性，引發資料的持久等問題，也因此 Prometheus 提供了遠端儲存(Remote storage)的特性來解決擴展性問題。 而除了儲存問題外，另一方面就是要考量單一 Prometheus 在大規模環境下的採集樣本效能與乘載量(所能夠處理的時間序列資料)，因此這時候可以利用 Federation 來將不同監測任務劃分到不同實例當中，以解決單台 Prometheus 無法有效處理的狀況。","text":"前面幾篇提到了 Prometheus 儲存系統與 Federation 功能，其中在儲存系統可以得知 Local on-disk 方式雖然能夠帶來很好的效能，但是卻也存在著單點故障的問題，並且限制了 Prometehsu 的可擴展性，引發資料的持久等問題，也因此 Prometheus 提供了遠端儲存(Remote storage)的特性來解決擴展性問題。 而除了儲存問題外，另一方面就是要考量單一 Prometheus 在大規模環境下的採集樣本效能與乘載量(所能夠處理的時間序列資料)，因此這時候可以利用 Federation 來將不同監測任務劃分到不同實例當中，以解決單台 Prometheus 無法有效處理的狀況。 而本節主要探討各種 Prometheus 的高可靠(High Availability)架構。 這邊不探討 Alert Manager 如何實現高可靠性架構。 服務的高可靠性架構(最基本的 HA)從前面介紹可以得知 Promehteus 是以 Pull-based 進行設計，因此收集時間序列資料(Mtertics)都是透過 Prometheus 本身主動發起，而為了保證 Prometheus 服務能夠正常運作，這邊只需要建立多台 Prometheus 節點來收集同樣的 Metrics(同樣的 Exporter target)即可。 這種做法雖然能夠保證服務的高可靠，但是並無法解決不同 Prometheus Server 之間的資料一致性問題，也無法讓取得的資料進行長時間儲存，且當規模大到單一 Prometheus 無法負荷時，將延伸出效能瓶頸問題，因此這種架構只適合在小規模叢集進行監測，且 Prometheus Server 處於的環境比較不嚴苛，也不會頻繁發生遷移狀況與儲存長週期的資料(Long-term store)。 上述總結： Pros: 服務能夠提供可靠性 適合小規模監測、只需要短期資料儲存(5ms)、不用經常遷移節點 Cons: 無法動態擴展 資料會有不一致問題 資料無法長時間儲存 不適合在頻繁遷移的狀況 當乘載量過大時，單一 Prometheus Server 會無法負荷 服務高可靠性結合遠端儲存(基本 HA + Remote Storage)這種架構即在基本 HA 上加入遠端儲存功能，讓 Prometheus Server 的讀寫來至第三方儲存系統。 該架構解決了資料持久性儲存問題，且當 Prometheus Server 發生故障或者當機時，重新啟動能夠快速的恢復資料，同時 Prometheus Server 能夠更好睇進行遷移，但是這只適合在較小規模的監測使用。 上述總結： Pros: 服務能夠提供可靠性 適合小規模監測 資料能夠被持久性保存在第三方儲存系統 Prometheus Server 能夠遷移 能夠達到資料復原 Cons: 不適合大規模監測 當乘載量過大時，單一 Prometheus Server 會無法負荷 服務高可靠性結合遠端儲存與聯邦(基本 HA + Remote Storage + Federation)這種架構主要是解決單一 Promethes Server 無法處理大量資料收集任務問題，並且加強 Prometheus 的擴展性，透過將不同收集任務劃分到不同 Prometheus 實例上。 該架構通常有兩種使用場景： 單一資料中心，但是有大量的收集任務：這種場景下 Prometheus Server 可能會發生效能上瓶頸，主要是單一 Prometheus Server 要乘載大量的資料收集任務，這時候就能夠透過 Federation 來將不同類型的任務分到不同的子 Prometheus Server 上，再由最上層進行聚合資料。 多資料中心：在多資料中心下，這種架構也能夠適用，當不同資料中心的 Exporter 無法讓最上層的 Prometheus 去拉取資料時，就能透過 Federation 來進行分層處理，在每個資料中心建置一組收集該資料中心的子 Prometheus Server，再由最上層的 Prometheus 來進行抓取，並且也能夠依據每個收集任務的乘載量來部署與劃分層級，但是這需要確保上下層的 Prometheus Server 彼此能夠互相溝通。 上述總結： Pros: 服務能夠提供可靠性 資料能夠被持久性保存在第三方儲存系統 Prometheus Server 能夠遷移 能夠達到資料復原 能夠依據不同任務進行層級劃分 適合不同規模監測 能夠很好的擴展 Prometheus Server Cons: 部署架構複雜 維護困難性增加 在 Kubernetes 上部署不易 單一收集任務的實例(Scrape Target)過多問題這問題可能發生在單個 Job 設定太多 Target 數，這時候透過 Federation 來區分可能也無法解決問題，這種情況下只能透過在實例(Instance)級別進行功能劃分。這種做法是將不同實例的資料收集劃分到不同 Prometheus Server 實例，再透過 Relabel 設定來確保當前的 Prometheus Server 只收集當前收集任務的一部分實例監測資料。 一個簡單範例組態檔： global: external_labels: slave: 1 # This is the 2nd slave. This prevents clashes between slaves.scrape_configs: - job_name: some_job # Add usual service discovery here, such as static_configs relabel_configs: - source_labels: [__address__] modulus: 4 # 4 slaves target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: ^1$ # This is the 2nd slave action: keep References https://prometheus.io/docs/introduction/faq/#can-prometheus-be-made-highly-available https://github.com/coreos/prometheus-operator/blob/master/Documentation/high-availability.md https://github.com/coreos/prometheus-operator https://coreos.com/operators/prometheus/docs/latest/high-availability.html","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://k2r2bai.com/tags/Prometheus/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"}]},{"title":"了解 Prometheus Federation 功能","slug":"devops/prometheus/prometheus-federation","date":"2018-06-29T04:23:01.000Z","updated":"2019-12-02T01:49:42.386Z","comments":true,"path":"2018/06/29/devops/prometheus/prometheus-federation/","link":"","permalink":"https://k2r2bai.com/2018/06/29/devops/prometheus/prometheus-federation/","excerpt":"Prometheus 在效能上是能夠以單個 Server 支撐百萬個時間序列，當然根據不同規模的改變，Promethes 是能夠進行擴展的，這邊將介紹 Prometheus Federation 來達到此效果。 Prometheus Federation 允許一台 Prometheus Server 從另一台 Prometheus Server 刮取選定的時間序列資料。Federation 提供 Prometheus 擴展能力，這能夠讓 Prometheus 節點擴展至多個，並且能夠實現高可靠性(High Availability)與切片(Sharding)。對於 Prometheus 的 Federation 有不同的使用方式，一般分為Cross-service federation與Hierarchical federation。","text":"Prometheus 在效能上是能夠以單個 Server 支撐百萬個時間序列，當然根據不同規模的改變，Promethes 是能夠進行擴展的，這邊將介紹 Prometheus Federation 來達到此效果。 Prometheus Federation 允許一台 Prometheus Server 從另一台 Prometheus Server 刮取選定的時間序列資料。Federation 提供 Prometheus 擴展能力，這能夠讓 Prometheus 節點擴展至多個，並且能夠實現高可靠性(High Availability)與切片(Sharding)。對於 Prometheus 的 Federation 有不同的使用方式，一般分為Cross-service federation與Hierarchical federation。 Cross-service federation這種方式的 Federation 會將一個 Prometheus Server 設定成從另一個 Prometheus Server 中獲取選中的時間序列資料，使得這個 Prometheus 能夠對兩個資料來源進行查詢(Query)與警告(Alert)，比如說有一個 Prometheus A 收集了多個服務叢集排程器曝露的資訊使用資訊(CPU、Memory 等)，而另一個在叢集上的 Promethues B 則只收集應用程式指定的服務 Metrics，這時想讓 Prometheus B 收集 Prometheus A 的資源使用量的話，就可以利用 Federation 來取得。 又或者假設想要監控 mysqld 與 node 的資訊，但是這兩個在不同叢集中，這時可以採用一個 Master Prometheus + 兩個 Sharding Prometheus，其中 Sharding Prometheus 一個收集 node_exporter 的 Metrics，另一個則收集 mysql_exporter，最後 Master Prometheus 透過 Federation 來匯總兩個 Sharding 的時間序列資料。 Hierarchical federation這種方式能夠讓 Prometheus 擴展到多個資料中心，或者多個節點數量，當建立一個 Federation 叢集時，其拓樸結構會類似一個樹狀結構，並且每一層級會有所對應的級別，比如說較高層級的 Prometheus Server 會從大量低層級的 Prometheus Server 中檢索或聚合時間序列資料。 這種方式適合當單一的 Prometheus 收集 Metrics 的任務(Job)量過大而無法負荷時，可將任務的實例(Instance)進行水平擴展，讓任務的目標實例拆分到不同 Prometheus 中，再由當前資料中心的主 Prometheus 來收集聚合。 Federation 部署節點資訊測試環境將利用當一節點執行多個 Prometheus 來模擬，作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： Name Role Port Prometheus-global Master 9090 Prometheus-node Collector 9091 Prometheus-docker Collector 9092 事前準備開始安裝前需要確保以下條件已達成： 安裝與設定 Dockerd 提供 Metrics： $ curl -fsSL \"https://get.docker.com/\" | sh# 編輯 /etc/docker/daemon.json 加入下面內容$ sudo vim /etc/docker/daemon.json&#123; \"metrics-addr\" : \"127.0.0.1:9323\", \"experimental\" : true&#125;# 完成後重新啟動$ sudo systemctl restart docker$ curl 127.0.0.1:9323/metrics 透過 Docker 部署 Node Exporter： $ docker run -d \\ --net=\"host\" \\ --pid=\"host\" \\ --name node-exporter \\ quay.io/prometheus/node-exporter$ curl 127.0.0.1:9100/metrics 在模擬節點下載 Prometheus 伺服器執行檔： $ wget https://github.com/prometheus/prometheus/releases/download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz$ tar xvfz prometheus-*.tar.gz$ mv prometheus-2.3.0.linux-amd64 prometheus-2.3.0$ cd prometheus-2.3.0 部署 Prometheus Federation首先新增三個設定檔案，分別給 Global、Docker 與 Node 使用。 新增一個檔案prometheus-docker.yml，並加入以下內容: global: scrape_interval: 15s evaluation_interval: 15s external_labels: server: 'docker-monitor'scrape_configs: - job_name: 'docker' scrape_interval: 5s static_configs: - targets: ['localhost:9323'] 新增一個檔案prometheus-node.yml，並加入以下內容: global: scrape_interval: 15s evaluation_interval: 15s external_labels: server: 'node-monitor'scrape_configs: - job_name: 'node' scrape_interval: 5s static_configs: - targets: ['localhost:9100'] 新增一個檔案prometheus-global.yml，並加入以下內容: global: scrape_interval: 15s evaluation_interval: 15s external_labels: server: 'global-monitor'scrape_configs: - job_name: 'federate' scrape_interval: 15s honor_labels: true metrics_path: '/federate' params: 'match[]': - '&#123;job=~\"prometheus.*\"&#125;' - '&#123;job=\"docker\"&#125;' - '&#123;job=\"node\"&#125;' static_configs: - targets: - 'localhost:9091' - 'localhost:9092' 當設定 Federation 時，將透過 URL 中的 macth[] 參數指定需要獲取的時間序列資料，match[] 必須是一個向量選擇器資訊，如 up 或者 {job=&quot;api-server&quot;} 等。 設定honor_labels是避免資料衝突。 完成後，開啟三個 Terminal 來啟動 Prometheus Server： # 啟動收集 Docker metrics 的 Prometheus server$ ./prometheus --config.file=prometheus-docker.yml \\ --storage.tsdb.path=./data-docker \\ --web.listen-address=\"0.0.0.0:9092\"# 啟動收集 Node metrics 的 Prometheus server$ ./prometheus --config.file=prometheus-node.yml \\ --storage.tsdb.path=./data-node \\ --web.listen-address=\"0.0.0.0:9091\"# 啟動收集 Global 的 Prometheus server$ ./prometheus --config.file=prometheus-global.yml \\ --storage.tsdb.path=./data-global \\ --web.listen-address=\"0.0.0.0:9090\" 正常啟動後分別透過瀏覽器觀察:9090、:9091與:9092會發現 Master 會擁有 Node 與 Docker 的 Metrics，而其他兩者只會有自己所屬 Metrics。 注意，在 Alert 部分還是建議在各自 Sharding 的 Prometheus Server 處理，因為放到 Global 有可能會有接延遲。 部署 Grafana在測試節點透過 Docker 部署 Grafana 來提供資料視覺化用： $ docker run \\ -d \\ -p 3000:3000 \\ --name=grafana \\ -e \"GF_SECURITY_ADMIN_PASSWORD=secret\" \\ grafana/grafana 完成後透過瀏覽器查看:3000，並設定 Grafana 將 Prometheus Global 資料做呈現，請至Configuration的Data Sources進行設定。 接著分別下載以下 Dashbaord JSON 檔案： Node Exporter Server Metrics Docker Metrics 並在 Grafana 點選 Import 選擇上面兩個下載的 JSON 檔案。 Import 後選擇 Prometheus data source： 確認沒問題後點選Import，這時候就可以在 Dashboard 看到視覺化的 Metrics 了。 Docker Metrics 資訊： 更多的 Dashboard 可以至官方 Dashboards 尋找。 Prometheus Federation 不適用地方經上述兩者說明，可以知道 Prometheus Federation 大多被用來從另一個 Prometheus 拉取受限或聚合的時間序列資料集，但是不只上述功能，該 Prometheus 本身還是要肩負警報(Alert)與圖形(Graph)資料查詢工作。而什麼狀況是 Prometheus Federation 不適用的？那就是使用在從另一個 Prometheus 拉取大量時間序列(甚至所有時間序列資料)，並且只從該 Prometheus 做警報(Alert)與圖形(Graph)處理。 這邊列出三個原因： 效能(Performance)與縮放(Scaling)問題：Prometheus 的限制因素主要是一台機器所能處理的時間序列資料量，然而讓所有資料路由到一個 Global 的 Prometheus Server 將限制這台 Server 所能處理的監控。取而代之，若只拉取聚合的時間序列資料，只限於一個資料中心的 Prometheus 能夠處理，因此請允許新增資料中心來避免擴大 Global Prometheus。而 Federation 請求本身也能夠大量地服務於接收 Prometheus。 可靠性(Reliability)：如果需要進行警報(Alert)的資料從一個 Prometheus 移動到另一個時，那麼這樣就會多出一個額外的故障點。當牽扯到諸如互聯網之類的廣域網路連接時，是特別危險的。在可能的情況下，應該盡量將警報(Alert)推送到 Federation 層級較深的 Prometheus上。 正確性(Correctness)：由於工作原理關析，Federation 會在被刮取(scraped)後的某一段時間拉取資料，並且可能因 Race 問題而遺失一些資料。雖然這問題在 Global Promethesu 能夠被容忍，但是用於處理警報(Alert)與圖表查詢的資料中心 Prometheus 就可能造成問題。 References http://ylzheng.com/2018/03/08/prometheus-fedreation/ http://blog.51cto.com/lee90/2062252 https://banzaicloud.com/blog/prometheus-federation/ https://prometheus.io/docs/prometheus/latest/federation/ https://www.robustperception.io/federation-what-is-it-good-for/","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://k2r2bai.com/tags/Prometheus/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"}]},{"title":"Prometheus 儲存系統解析","slug":"devops/prometheus/prometheus-storage","date":"2018-06-27T04:23:01.000Z","updated":"2019-12-02T01:49:42.386Z","comments":true,"path":"2018/06/27/devops/prometheus/prometheus-storage/","link":"","permalink":"https://k2r2bai.com/2018/06/27/devops/prometheus/prometheus-storage/","excerpt":"Prometheus 系統包含了 Local on-disk storage 與 Remote storage，本節將說明兩者差異，以了解作為實現資料儲存可靠性的基礎。","text":"Prometheus 系統包含了 Local on-disk storage 與 Remote storage，本節將說明兩者差異，以了解作為實現資料儲存可靠性的基礎。 Time Series Database時間序列資料庫(Time Series Database, TSDB) 是經過優化後，專門用來儲存與管理時間序列資料(Time Series Data)的資料庫系統，目標是提供一套高效能讀寫的資料庫系統。時間序列資料庫一般在資料中以時間作為索引(index)。 TSDB 通常會具備以下特點： 系統主要以寫入為主。 寫入資料時是依序新增，並且大多數時間做資料排序。 不會頻繁更新資料，且寫入時間通常非常短，約幾秒內完成。 資料以區塊為單位進行刪除，很少單除某時間的資料。 讀取一般以升冪或降冪做循序讀取。 支援高並行與叢集式。 常見的 TSDB 可以查看 TSDB Projects。 Local StorageOn-disk LayoutPrometheus 從 2.0 版本開始引入自定義儲存格式，來將拉取的時間序列資料保存到 Local on-disk。其中存在 Local 的資料樣本會以兩小時(預設)為一個區塊被儲存(表示一個 Sliding Window)，而每個區塊會包含該 Sliding Window 的所有樣本資料(Chunks)、詮釋資料檔案(meta.json)與索引檔案(index)。 而 Prometheus 透過 write-ahead-log(WAL) 機制來防止當前區塊在收集樣本資料(剛抓取的資料會保存在記憶體中)時，發生伺服器錯誤或重啟等問題。一旦 Promethesu 重新啟動時，將依據 WAL 進行恢復資料，並以此重播(replayed)。然而當在這期間透過 API 刪除時間序列資料時，刪除紀錄將被儲存在單獨的邏輯檔案(tombstone)中(不是立即從區塊檔案中刪除資料)。 Prometheus 的資料目錄結構如下所示： # 透過以下指令觀察$ ./prometheus --storage.tsdb.min-block-duration=5m# 當經過 5m 後，用 tree 查看 data 結構$ tree datadata├── 01CGDH53DJXHE1M1971QM0NHMD│ ├── chunks│ │ └── 000001│ ├── index│ ├── meta.json│ └── tombstones├── lock└── wal ├── 000001 └── 000003 TSDB 的儲存格式可以參考 TSDB format。 可以看到每五分鐘會產生一個區塊的 Sliding Window，透過這種形式來保存所有樣本資料，可以有效提升 Prometheus 的查詢效率，比如說要查詢某個時間範圍內的資料，只需要查詢該時間範圍的區塊資料即可。 另外這種儲存方式也能夠簡化歷史資料的刪除，只要某一個區塊的時間範圍落在設置的保留範圍外，就會將該區塊丟棄。 而 Prometheus 的每個樣本大約為 1 - 2 位元組(byte)，因此規劃 Prometheus 伺服器容量可以使用以下公式來做簡單計算： needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample 因此在規劃時若想降低儲存容量需求的話，要依據上述規則來計算估計，如 retention_time_seconds 與 bytes_per_sample 不變下，就只能對 ingested_samples_per_second 做處理。 注意! Local on-disk 方式雖然能夠提供高效能的查詢與檢索，但也存在著侷限性問題，因為儲存資料並不會被叢集同步或進行複製等功能，這表示當磁碟出錯或節點故障時，將有可能遺失儲存的時間序列資料。然而若本身對於這問題不嚴謹的話，依然能夠使用 Local on-disk 方式來儲存樣本資料。 Memory UsagePrometheus 2.0 版本以前會在記憶體中儲存當前使用的所有 Chunks，並且會盡可能使用最近使用過的 Chunks。由於可能會使用到很多記憶體來快取資料，因此 Prometheus 可以透過設定 Heap memory size(bytes) 來確保不會發生 OOM(Out of memory)。而設定 Heap size 時要注意 Prometheus 使用的實體記憶體是 Go runtime 與作業系統的複雜溝通結果，因此很難精準預定大小，因此建議設定實體記憶體的 2/3大小(預設為 2G)。 假設設定 Heap size 為 2G，這表示 Prometheus 真正使用的實體記憶體約為 3G 左右。 Remote StoragePrometheus 的 Local storage 可擴展性與耐用性會受到單節點限制，然而 Prometheus 並沒有實作自身的叢集式儲存，而是以一套 gRPC 介面來實作遠程儲存系統(Remote Storage)的整合。 Prometheus 透過兩種方式來整合遠端儲存系統，分別為： Remote write: Prometheus 能夠以標準的格式寫入樣本到遠程的 URL。使用者能夠在 組態檔案設定指定 Remote write 的 URL，一但設定後 Prometheus 將樣本資料透過 HTTP 形式發送給 Adaptor。而使用者則可以在 Adaptor 中串接外部任意服務，其中外部服務可以是分散式儲存系統、公有雲儲存服務，或是是 Message Queue 等等。 Remote read: Prometheus 能夠以標準的格式從遠端 URL 讀取(返回)樣本資料。如 Remote write 類似，Remote read 也能夠透過 Adaptor 來實作與儲存服務的整合，在 Remote read 中，當使用者發送查詢請求後，Prometheus 將向 remote_read 中設定的 URL 發起查詢請求(mathchers, ranges)，而 Adaptor 將根據請求條件從第三方儲存服務中取得響應的資料，同時將資料轉換成 Prometheus 的原始樣本資料(Raw samples)傳給 Prometheus Server。 從 Prometheus 原始碼中，可以找到 Remote storage gRPC 的 ProtoBuff 的定義資訊，開發者能夠透過實作這些介面來串接儲存服務： syntax = \"proto3\";package prometheus;option go_package = \"prompb\";import \"types.proto\";message WriteRequest &#123; repeated prometheus.TimeSeries timeseries = 1;&#125;message ReadRequest &#123; repeated Query queries = 1;&#125;message ReadResponse &#123; // In same order as the request's queries. repeated QueryResult results = 1;&#125;message Query &#123; int64 start_timestamp_ms = 1; int64 end_timestamp_ms = 2; repeated prometheus.LabelMatcher matchers = 3; prometheus.ReadHints hints = 4;&#125;message QueryResult &#123; // Samples within a time series must be ordered by time. repeated prometheus.TimeSeries timeseries = 1;&#125; 由於 Prometheus 的 Remote storage 能夠自行實作資料處理邏輯，因此當接收到 remote_write 的 HTTP 服務時，能夠將內容轉換成 WriteRequests 再由開發者自行處理。舉例下列範例： package mainimport ( \"fmt\" \"io/ioutil\" \"net/http\" \"github.com/gogo/protobuf/proto\" \"github.com/golang/snappy\" \"github.com/prometheus/common/model\" \"github.com/prometheus/prometheus/prompb\")func main() &#123; http.HandleFunc(\"/receive\", func(w http.ResponseWriter, r *http.Request) &#123; compressed, err := ioutil.ReadAll(r.Body) if err != nil &#123; http.Error(w, err.Error(), http.StatusInternalServerError) return &#125; reqBuf, err := snappy.Decode(nil, compressed) if err != nil &#123; http.Error(w, err.Error(), http.StatusBadRequest) return &#125; var req prompb.WriteRequest if err := proto.Unmarshal(reqBuf, &amp;req); err != nil &#123; http.Error(w, err.Error(), http.StatusBadRequest) return &#125; for _, ts := range req.Timeseries &#123; m := make(model.Metric, len(ts.Labels)) for _, l := range ts.Labels &#123; m[model.LabelName(l.Name)] = model.LabelValue(l.Value) &#125; fmt.Println(m) for _, s := range ts.Samples &#123; fmt.Printf(\" %f %d\\n\", s.Value, s.Timestamp) &#125; &#125; &#125;) http.ListenAndServe(\":1234\", nil)&#125; 下面是一個 Remote storage 的設定檔案，可以在prometheus.yml 設定檔加入： remote_write: url: &lt;string&gt; [ remote_timeout: &lt;duration&gt; | default = 30s ] write_relabel_configs: [ - &lt;relabel_config&gt; ... ] basic_auth: [ username: &lt;string&gt; ] [ password: &lt;string&gt; ] [ bearer_token: &lt;string&gt; ] [ bearer_token_file: /path/to/bearer/token/file ] tls_config: [ &lt;tls_config&gt; ] [ proxy_url: &lt;string&gt; ]remote_read: url: &lt;string&gt; required_matchers: [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ] [ remote_timeout: &lt;duration&gt; | default = 30s ] [ read_recent: &lt;boolean&gt; | default = false ] basic_auth: [ username: &lt;string&gt; ] [ password: &lt;string&gt; ] [ bearer_token: &lt;string&gt; ] [ bearer_token_file: /path/to/bearer/token/file ] [ &lt;tls_config&gt; ] [ proxy_url: &lt;string&gt; ] 而 Prometheus 官方也列出了目前已整合的第三方儲存服務，可以到 Remote Endpoints and Storage 中查看。 需注意不同的儲存服務能提供的 Write 與 Read 實作都不同，有些可能只能進行寫入操作，而有些只能做讀取操作，因此要注意選擇時是否滿足需求。 Prometheus Storage FlagsPrometheus 支援針對儲存的 Flags： Flags 預設值 描述 –storage.tsdb.path data/ Metrics 儲存路徑 –storage.tsdb.retention 15d 儲存的資料樣本會保留多長的時間 –storage.tsdb.min-block-duration 2h 一個資料區塊的最小持續時間 –storage.tsdb.max-block-duration 36h 壓縮區塊的最大持續時間(預設為 retention period 的 10% 時間) –storage.tsdb.no-lockfile false 設定是否建立 lockfile 在資料目錄下 –storage.remote.flush-deadline 1m 在關機或者組態重新讀取時，清除樣本的等待時間 References http://liubin.org/blog/2016/02/18/tsdb-intro/ https://www.hi-linux.com/posts/25047.html https://prometheus.io/docs/prometheus/latest/storage/ https://caicloud.io/article_detail/5a5db4203255f5063f2bd462","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://k2r2bai.com/tags/Prometheus/"}]},{"title":"Kubernetes exec API 串接分析","slug":"kubernetes/coding/exec-api","date":"2018-06-25T09:08:54.000Z","updated":"2019-12-02T01:49:42.392Z","comments":true,"path":"2018/06/25/kubernetes/coding/exec-api/","link":"","permalink":"https://k2r2bai.com/2018/06/25/kubernetes/coding/exec-api/","excerpt":"本篇將說明 Kubernetes exec API 的運作方式，並以簡單範例進行開發在前後端上。雖然 Kubernetes 提供了不同資源的 RESTful API 來進行 CRUD 操作，但是部分 API 並非單純的回傳一個資料，有些是需要透過 SPDY 或 WebSocket 建立長連線串流，這種 API 以 exec、attach 為主，目標是對一個 Pod 執行指定指令，或者進入該 Pod 進行互動等等。","text":"本篇將說明 Kubernetes exec API 的運作方式，並以簡單範例進行開發在前後端上。雖然 Kubernetes 提供了不同資源的 RESTful API 來進行 CRUD 操作，但是部分 API 並非單純的回傳一個資料，有些是需要透過 SPDY 或 WebSocket 建立長連線串流，這種 API 以 exec、attach 為主，目標是對一個 Pod 執行指定指令，或者進入該 Pod 進行互動等等。 Exec API Endpoint首先了解一下 Kubernetes exec API endpoint，由於 Kubernetes 官方文件並未提供相關資訊，因此這邊透過 kubectl 指令來了解 API 的結構： $ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: ubuntuspec: containers: - name: ubuntu image: ubuntu:16.04 command: ['/bin/bash', '-c', 'while :; do echo Hello; sleep 1; done ']EOF$ kubectl -v=8 exec -ti ubuntu bash...I0625 10:39:33.716271 93099 round_trippers.go:383] POST https://xxx.xxx.xxx.xxx:8443/api/v1/namespaces/default/pods/ubuntu/exec?command=bash&amp;container=ubuntu&amp;container=ubuntu&amp;stdin=true&amp;stdout=true&amp;tty=true... 從上述得知 exec API 結構大致如下圖所示： 其中 API 中的 Queries 又可細分以下資訊： command：將被執行的指令。若指令為ping 8.8.8.8，則 API 為command=ping&amp;command=8.8.8.8。類型為string值。 container：哪個容器將被執行指令。若 Pod 只有一個容器，一般會用 API 找出名稱塞到該參數中，若多個則選擇讓人輸入名稱。類型為string值。 stdin：是否開啟標準輸入，通常由使用者決定是否開啟。類型為bool值。 stdout：是否開啟標準輸出，通常是預設開啟。類型為bool值。 stderr：是否開啟標準錯誤輸出，通常是預設開啟。類型為bool值。 tty：是否分配一個偽終端設備(Pseudo TTY, PTY)。ㄒ為bool值。 ProtocolExecute 是利用 SPDY 與 WebSocket 協定進行串流溝通的 API，其中 SPDY 在 Kubernetes 官方的 client-go 已經有實現(參考 Remote command)，而 kubectl 正是使用 SPDY，但是 SPDY 目前已經規劃在未來將被移棄，因此建議選擇使用 WebSocket 來作為串流溝通。但而無論是使用哪一個協定，都要注意請求的 Header 必須有Connection: Upgrade、Upgrade: xxx等，不然 API Server 會拒絕存取請求。 HTTP Headers除了 SPDY 與 WebSocket 所需要的 Headers(如 Upgrade 等)外，使用者與開發者還必須提供兩個 Headers 來確保能夠正確授權並溝通： Authorization：該 Header 是用來提供給 API Server 做認證請求的資訊，通常會是以Authorization: Bearer &lt;token&gt;的形式。 Accept：指定客戶端能夠接收的內容類型，一般為Accept: application/json，若輸入不支援的類型將會被 API 以406 Not Acceptable 拒絕請求。 溝通協定一旦符合上述所有資訊後，WebSocket(或 SPDY)就能夠建立連線，並且與 API Server 進行溝通。而當寫入 WebSocket 時，資料將被傳送到標準輸入(stdin)，而 WebSocket 的接收將會是標準輸出(stdout)與輸出錯誤(stderr)。Kubernetes API Server 簡單定義了一個協定來復用 stdout 與 stderr。因此可以理解當 WebSocket 建立連線後，傳送資料時需要再 Buffer 的第一個字元定義為 stdin(buf[0] = 0)，而接收資料時要判斷 stdout(buf[0] = 1) 與 stderr(buf[0] = 2)。其資訊如下： Code 標準串流 0 stdin 1 stdout 2 stderr 下面簡單以發送ls指令為例： # 傳送`ls`指令，必須 buf[0] 自行塞入 0 字元來表示 stdin。buf = [0 108 115 10]# Receivebuf = [1 108 115 13 10 27 91 48 109 27 91 ...] 最後需要注意 Timeout 問題，由於可能對 WebSocket 設定 TCP Timeout，因此建議每一段時間發送一個 stdin 空訊息來保持連線。 實作參考一些專案自行練習寫了 Go 語言版本 CLI。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"Prometheus Operator 介紹與安裝","slug":"devops/prometheus/prometheus-operator","date":"2018-06-23T04:23:01.000Z","updated":"2019-12-02T01:49:42.386Z","comments":true,"path":"2018/06/23/devops/prometheus/prometheus-operator/","link":"","permalink":"https://k2r2bai.com/2018/06/23/devops/prometheus/prometheus-operator/","excerpt":"Prometheus Operator 是 CoreOS 開源的一套用於管理在 Kubernetes 上的 Prometheus 控制器，目標當然就是簡化部署與維護 Prometheus 上的事情，其架構如下所示：","text":"Prometheus Operator 是 CoreOS 開源的一套用於管理在 Kubernetes 上的 Prometheus 控制器，目標當然就是簡化部署與維護 Prometheus 上的事情，其架構如下所示： 架構中的每一個部分都執行於 Kubernetes 的資源，這些資源分別負責不同作用與意義： Operator：Operator 是整個系統的主要控制器，會以 Deployment 方式執行於 Kubernetes 叢集上，並根據自定義的資源(Custom Resource Definition，CRDs)來負責管理與部署 Prometheus Server。而 Operator 會透過監聽這些自定義資源的事件變化來做對應處理。 Prometheus Server：由 Operator 依據一個自定義資源 Prometheus 類型中，所描述的內容而部署的 Prometheus Server 叢集，可以將這個自定義資源看作是一種特別用來管理 Prometheus Server 的 StatefulSets 資源。 apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata: name: k8s labels: prometheus: k8sspec: version: v2.3.0 replicas: 2 serviceMonitors: - selector: matchLabels: k8s-app: kubelet... ServiceMonitor：一個 Kubernetes 自定義資源，該資源描述了 Prometheus Server 的 Target 列表，Operator 會監聽這個資源的變化來動態的更新 Prometheus Server 的 Scrape targets。而該資源主要透過 Selector 來依據 Labels 選取對應的 Service Endpoint，並讓 Prometheus Server 透過 Service 進行拉取(Pull) Metrics 資料。 apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata: name: kubelet labels: k8s-app: kubeletspec: jobLabel: k8s-app endpoints: - port: cadvisor interval: 30s # scrape the endpoint every 10 seconds honorLabels: true selector: matchLabels: k8s-app: kubelet namespaceSelector: matchNames: - kube-system 這是一個抓取 Cadvisor metrics 的範例。 Service：Kubernetes 中的 Service 資源，這邊主要用來對應 Kubernetes 中 Metrics Server Pod，然後提供給 ServiceMonitor 選取讓 Prometheus Server 拉取資料。在 Prometheus 術語中，可以稱為 Target，即被 Prometheus 監測的對象，如一個部署在 Kubernetes 上的 Node Exporter Service。 Alertmanager：Prometheus Operator 不只提供 Prometheus Server 管理與部署，也包含了 AlertManager，並且一樣透過一個 Alertmanager 自定義資源來描述資訊，再由 Operator 依據描述內容部署 Alertmanager 叢集。 apiVersion: monitoring.coreos.com/v1kind: Alertmanagermetadata: name: main labels: alertmanager: mainspec: replicas: 3... 部署 Prometheus Operator本節將說明如何部署 Prometheus Operator 來管理 Kubernetes 上的 Prometheus 資源。 節點資訊測試環境將需要一套 Kubernetes 叢集，作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： IP Address Role vCPU RAM 172.22.132.10 k8s-m1 8 16G 172.22.132.11 k8s-n1 8 16G 172.22.132.12 k8s-n2 8 16G 這邊m 為 K8s master，n為 K8s node。 事前準備開始安裝前需要確保以下條件已達成： 所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 用 kubeadm 部署 Kubernetes 叢集。 在 Kubernetes 叢集部署 Helm 與 Tiller server。 $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx$ sudo mv linux-amd64/helm /usr/local/bin/$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller 在k8s-m1透過 kubectl 來建立 Ingress Controller 即可： $ kubectl create ns ingress-nginx$ wget https://kairen.github.io/files/manual-v1.10/addon/ingress-controller.yml.conf -O ingress-controller.yml$ sed -i ingress-controller.yml 's/192.16.35.10/172.22.132.10/g'$ kubectl apply -f ingress-controller.yml.conf 部署 Prometheus OperatorPrometheus Operator 提供了多種方式部署至 Kubernetes 上，一般會使用手動(or 腳本)與 Helm 來進行部署。 手動(腳本)部署透過 Git 取得最新版本腳本： $ git clone https://github.com/camilb/prometheus-kubernetes.git$ cd prometheus-kubernetes 接著執行deploy腳本來部署到 Kubernetes： $ ./deployCheck for uncommitted changesOK! No uncommitted changes detectedCreating 'monitoring' namespace.Error from server (AlreadyExists): namespaces \"monitoring\" already exists1) AWS2) GCP3) Azure4) CustomPlease select your cloud provider:4Deploying on custom providers without persistenceSetting components versionEnter Prometheus Operator version [v0.19.0]:Enter Prometheus version [v2.2.1]:Enter Prometheus storage retention period in hours [168h]:Enter Prometheus storage volume size [40Gi]:Enter Prometheus memory request in Gi or Mi [1Gi]:Enter Grafana version [5.1.1]:Enter Alert Manager version [v0.15.0-rc.1]:Enter Node Exporter version [v0.16.0-rc.3]:Enter Kube State Metrics version [v1.3.1]:Enter Prometheus external Url [http://127.0.0.1:9090]:Enter Alertmanager external Url [http://127.0.0.1:9093]:Do you want to use NodeSelector to assign monitoring components on dedicated nodes?Y/N [N]:Do you want to set up an SMTP relay?Y/N [N]:Do you want to set up slack alerts?Y/N [N]:# 這邊會跑一下部署階段，完成後要接著輸入一些資訊，如 Grafana username and passwdEnter Grafana administrator username [admin]:Enter Grafana administrator password: ******...Done 沒有輸入部分請直接按Enter。 當確認看到 Done 後就可以查看 monitoring namespace： $ kubectl -n monitoring get poNAME READY STATUS RESTARTS AGEalertmanager-main-0 2/2 Running 0 4malertmanager-main-1 2/2 Running 0 3malertmanager-main-2 2/2 Running 0 3mgrafana-568b569696-nltbh 2/2 Running 0 14skube-state-metrics-86467959c6-kxtl4 2/2 Running 0 3mnode-exporter-526nw 1/1 Running 0 4mnode-exporter-c828w 1/1 Running 0 4mnode-exporter-r2qq2 1/1 Running 0 4mnode-exporter-s25x6 1/1 Running 0 4mnode-exporter-xpgh7 1/1 Running 0 4mprometheus-k8s-0 1/2 Running 0 10sprometheus-k8s-1 2/2 Running 0 10sprometheus-operator-f596c68cf-wrpqc 1/1 Running 0 4m 查看 Kubernetes CRDs 與 SM： $ kubectl -n monitoring get crdNAME AGEalertmanagers.monitoring.coreos.com 4mprometheuses.monitoring.coreos.com 4mservicemonitors.monitoring.coreos.com 4m$ kubectl -n monitoring get servicemonitorsNAME AGEalertmanager 1mkube-apiserver 1mkube-controller-manager 1mkube-dns 1mkube-scheduler 1mkube-state-metrics 1mkubelet 1mnode-exporter 1mprometheus 1mprometheus-operator 1m 接著修改 Service 的 Grafana 的 Type： $ kubectl -n monitoring edit svc grafana# 修改成 NodePort 也可以建立 Ingress 來存取 Grafana。 apiVersion: extensions/v1beta1kind: Ingressmetadata: namespace: monitoring name: grfana-ingress annotations: ingress.kubernetes.io/rewrite-target: /spec: rules: - host: grafana.k8s-local.k2r2bai.com http: paths: - path: / backend: serviceName: grafana servicePort: 3000 這邊也可以建立 Prometheus Ingress 來使用 Web-based console。 apiVersion: extensions/v1beta1kind: Ingressmetadata: namespace: monitoring name: prometheus-ingress annotations: ingress.kubernetes.io/rewrite-target: /spec: rules: - host: prometheus.k8s-local.k2r2bai.com http: paths: - path: / backend: serviceName: prometheus-k8s servicePort: 9090 最後就可以存取 Grafana 來查看 Metric 視覺化資訊了。 Helm首先透過 Helm 加入 coreos 的 repo： $ helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/ 然後透過 kubectl 建立一個 Namespace 來管理 Prometheus，並用 Helm 部署 Prometheus Operator： $ kubectl create namespace monitoring$ helm install coreos/prometheus-operator \\ --name prometheus-operator \\ --set rbacEnable=true \\ --namespace=monitoring 接著部署 Prometheus、AlertManager 與 Grafana： # Prometheus$ helm install coreos/prometheus --name prometheus \\ --set serviceMonitorsSelector.app=prometheus \\ --set ruleSelector.app=prometheus \\ --namespace=monitoring# Alert Manager$ helm install coreos/alertmanager --name alertmanager --namespace=monitoring# Grafana$ helm install coreos/grafana --name grafana --namespace=monitoring 部署 kube-prometheus 來提供 Kubernetes 監測的 Exporter 與 ServiceMonitor： $ helm install coreos/kube-prometheus --name kube-prometheus --namespace=monitoring 完成後檢查安裝結果： $ kubectl -n monitoring get po,svcNAME READY STATUS RESTARTS AGEpod/alertmanager-alertmanager-0 2/2 Running 0 1mpod/alertmanager-kube-prometheus-0 2/2 Running 0 31spod/grafana-grafana-77cfcdff66-jwxfp 2/2 Running 0 1mpod/kube-prometheus-exporter-kube-state-56857b596f-knt8q 1/2 Running 0 21spod/kube-prometheus-exporter-kube-state-844bb6f589-n7xfg 1/2 Running 0 31spod/kube-prometheus-exporter-node-665kc 1/1 Running 0 31spod/kube-prometheus-exporter-node-bjvbx 1/1 Running 0 31spod/kube-prometheus-exporter-node-j8jf8 1/1 Running 0 31spod/kube-prometheus-exporter-node-pxn8p 1/1 Running 0 31spod/kube-prometheus-exporter-node-vft8b 1/1 Running 0 31spod/kube-prometheus-grafana-57d5b4d79f-lq5cr 1/2 Running 0 31spod/prometheus-kube-prometheus-0 3/3 Running 1 29spod/prometheus-operator-d75587d6-qhz4h 1/1 Running 0 2mpod/prometheus-prometheus-0 3/3 Running 1 1mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/alertmanager ClusterIP 10.99.170.79 &lt;none&gt; 9093/TCP 1mservice/alertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 1mservice/grafana-grafana ClusterIP 10.100.217.27 &lt;none&gt; 80/TCP 1mservice/kube-prometheus ClusterIP 10.102.165.173 &lt;none&gt; 9090/TCP 31sservice/kube-prometheus-alertmanager ClusterIP 10.99.221.122 &lt;none&gt; 9093/TCP 32sservice/kube-prometheus-exporter-kube-state ClusterIP 10.100.233.129 &lt;none&gt; 80/TCP 32sservice/kube-prometheus-exporter-node ClusterIP 10.97.183.222 &lt;none&gt; 9100/TCP 32sservice/kube-prometheus-grafana ClusterIP 10.110.134.52 &lt;none&gt; 80/TCP 32sservice/prometheus ClusterIP 10.105.229.141 &lt;none&gt; 9090/TCP 1mservice/prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 1m","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://k2r2bai.com/tags/Prometheus/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"}]},{"title":"Prometheus 介紹與基礎入門","slug":"cncf/prometheus","date":"2018-06-10T04:23:01.000Z","updated":"2019-12-02T01:49:42.380Z","comments":true,"path":"2018/06/10/cncf/prometheus/","link":"","permalink":"https://k2r2bai.com/2018/06/10/cncf/prometheus/","excerpt":"Prometheus 是一套開放式原始碼的系統監控警報框架與TSDB(Time Series Database)，該專案是由 SoundCloud 的工程師(前 Google 工程師)建立，Prometheus 啟發於 Google 的 Borgmon 監控系統。目前 Prometheus 已貢獻到 CNCF 成為孵化專案(2016-)，其受歡迎程度僅次於 Kubernetes。","text":"Prometheus 是一套開放式原始碼的系統監控警報框架與TSDB(Time Series Database)，該專案是由 SoundCloud 的工程師(前 Google 工程師)建立，Prometheus 啟發於 Google 的 Borgmon 監控系統。目前 Prometheus 已貢獻到 CNCF 成為孵化專案(2016-)，其受歡迎程度僅次於 Kubernetes。 Prometheus 具備了以下特性： 多維度資料模型 時間序列資料透過 Metric 名稱與鍵值(Key-value)來區分。 所有 Metrics 可以設定任意的多維標籤。 資料模型彈性度高，不需要刻意設定為以特定符號(ex: ,)分割。 可對資料模型進行聚合、切割與切片操作。 支援雙精度浮點數類型，標籤可以設定成 Unicode。 靈活的查詢語言(PromQL)，如可進行加減乘除等。 不依賴分散式儲存，因為 Prometheus Server 是一個二進制檔，可在單個服務節點自主運行。 基於 HTTP 的 Pull 方式收集時序資料。 可以透過 Push Gateway 進行資料推送。 支援多種視覺化儀表板呈現，如 Grafana。 能透過服務發現(Service discovery)或靜態組態去獲取監控的 Targets。 Prometheus 架構 Prometheus 生態圈中是由多個元件組成，其中有些是選擇性的元件： Prometheus Server：收集與儲存時間序列資料，並提供 PromQL 查詢語言支援。 Client Library：客戶端函式庫，提供語言開發來開發產生 Metrics 並曝露 Prometheus Server。當 Prometheus Server 來 Pull 時，直接返回即時狀態的 Metrics。 Pushgateway：主要用於臨時性 Job 推送。這類 Job 存在期間較短，有可能 Prometheus 來 Pull 時就消失，因此透過一個閘道來推送。適合用於服務層面的 Metrics。 Exporter：用來曝露已有第三方服務的 Metrics 給 Prometheus Server，即以 Client Library 開發的 HTTP server。 AlertManager：接收來至 Prometheus Server 的 Alert event，並依據定義的 Notification 組態發送警報，ex: E-mail、Pagerduty、OpenGenie 與 Webhook 等等。 Prometheus 運作流程 Prometheus Server 定期從組態好的 Jobs 或者 Exporters 中拉取 Metrics，或者接收來自 Pushgateway 發送的 Metrics，又或者從其他的 Prometheus Server 中拉取 Metrics。 Prometheus Server 在 Local 儲存收集到的 Metrics，並運行已定義好的 alert.rules，然後紀錄新時間序列或者像 AlertManager 發送警報。 AlertManager 根據組態檔案來對接受到的 Alert event 進行處理，然後發送警報。 在視覺化介面呈現採集資料。 Prometheus Server 拉取 Exporter 資料，然後透過 PromQL 語法進行查詢，再將資料給 Web UI or Dashboard。 Prometheus Server 觸發 Alert Definition 定義的事件，並發送給 AelertManager。 AlertManager 依據設定發送警報給 E-mail、Slack 等等。 Prometheus 資料模型與 Metric 類型本節將介紹 Prometheus 的資料模型與 Metrics 類型。 資料模型Prometheus 儲存的資料為時間序列，主要以 Metrics name 以及一系列的唯一標籤(key-value)組成，不同標籤表示不同時間序列。模型資訊如下： Metrics Name：該名稱通常用來表示 Metric 功能，例如 http_requests_total，即表示 HTTP 請求的總數。而 Metrics Name 是以 ASCII 字元、數字、英文、底線與冒號組成，並且要滿足[a-zA-Z_:][a-zA-Z0-9_:]* 正規表示法。 標籤：用來識別同一個時間序列不同維度。如 http_request_total{method=&quot;Get&quot;}表示所有 HTTP 的 Get Request 數量，因此當 method=&quot;Post&quot; 時又是另一個新的 Metric。標籤也需要滿足[a-zA-Z_:][a-zA-Z0-9_:]* 正規表示法。 樣本：實際的時間序列，每個序列包含一個 float64 值與一個毫秒的時間戳。 格式：一般為&lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;,...}，例如：http_requests_total{method=&quot;POST&quot;,endpoint=&quot;/api/tracks&quot;}。 Metrics 類型Prometheus Client 函式庫支援了四種主要 Metric 類型： Counter: 可被累加的 Metric，比如一個 HTTP Get 錯誤的出現次數。 Gauge: 屬於瞬時、與時間無關的任意更動 Metric，如記憶體使用率。 Histogram: 主要使用在表示一段時間範圍內的資料採樣。 Summary： 類似 Histogram，用來表示一端時間範圍內的資料採樣總結。 Job 與 InstancePrometheus 中會將任意獨立資料來源(Target)稱為 Instance。而包含多個相同 Instance 的集合稱為 Job。如以下範例： - job: api-server - instance 1: 1.2.3.4:5670 - instance 2: 1.2.3.4:5671 - instance 3: 5.6.7.8:5670 - instance 4: 5.6.7.8:5671 Instance: 被抓取目標 URL 的&lt;host&gt;:&lt;port&gt;部分。 Job: 一個同類型的 Instances 集合。(主要確保可靠性與擴展性) Prometheus 簡單部署與使用Prometheus 官方提供了已建構完成的二進制執行檔可以下載，只需要至 Download 頁面下載即可。首先下載符合作業系統的檔案，這邊以 Linux 為例： $ wget https://github.com/prometheus/prometheus/releases/download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz$ tar xvfz prometheus-*.tar.gz$ tree prometheus-2.3.0.linux-amd64├── console_libraries # Web console templates│ ├── menu.lib│ └── prom.lib├── consoles # Web console templates│ ├── index.html.example│ ├── node-cpu.html│ ├── node-disk.html│ ├── node.html│ ├── node-overview.html│ ├── prometheus.html│ └── prometheus-overview.html├── LICENSE├── NOTICE├── prometheus # Prometheus 執行檔├── prometheus.yml # Prometheus 設定檔└── promtool # 2.x+ 版本用來將一些 rules 格式轉成 YAML 用。 解壓縮完成後，編輯prometheus.yml檔案來調整設定： global: scrape_interval: 15s # 設定預設 scrape 的拉取間隔時間 external_labels: # 外通溝通時標示在 time series 或 Alert 的 Labels。 monitor: 'codelab-monitor'scrape_configs: # 設定 scrape jobs - job_name: 'prometheus' scrape_interval: 5s # 若設定間隔時間，將會覆蓋 global 的預設時間。 static_configs: - targets: ['localhost:9090'] 完成後，直接執行 prometheus 檔案來啟動伺服器： $ ./prometheus --config.file=prometheus.yml --storage.tsdb.path /tmp/data...level=info ts=2018-06-19T08:46:37.42756438Z caller=main.go:500 msg=\"Server is ready to receive web requests.\" --storage.tsdb.path 預設會直接存放在./data底下。 啟動後就可以瀏覽 :9090 來查看 Web-based console。 另外也可以進入 :9090/metrics 查看 Export metrics 資訊，並且可以在 console 來查詢指定 Metrics，並以圖表呈現。 Prometheus 提供了 Functional Expression Language 進行查詢與聚合時間序列資料，比如用sum(http_requests_total{method=&quot;GET&quot;} offset 5m)來查看指定時間的資訊總和。 Prometheus 提供拉取第三方或者自己開發的 Exporter metrics 作為監測資料，這邊可以透過簡單的 Go Client 範例來簡單部署 Exporter： $ git clone https://github.com/prometheus/client_golang.git$ cd client_golang/examples/random$ go get -d$ go build 完成後，開啟三個 Terminals 分別啟動以下 Exporter： # terminal 1$ ./random -listen-address=:8081# terminal 2$ ./random -listen-address=:8082# terminal 3$ ./random -listen-address=:8083 啟動後可以在:8081等 Ports 中查看 Metrics 資訊。 確定沒問題後，修改prometheus.yml來新增 target，並重新啟動 Prometheus Server： global: scrape_interval: 15s # 設定預設 scrape 的拉取間隔時間 external_labels: # 外通溝通時標示在 time series 或 Alert 的 Labels。 monitor: 'codelab-monitor'scrape_configs: # 設定 scrape jobs - job_name: 'prometheus' scrape_interval: 5s # 若設定間隔時間，將會覆蓋 global 的預設時間。 static_configs: - targets: ['localhost:9090'] - job_name: 'example-random' scrape_interval: 5s static_configs: - targets: ['localhost:8080', 'localhost:8081'] labels: group: 'production' - targets: ['localhost:8082'] labels: group: 'canary' 啟動完成後，就可以 Web-console 的 Execute 執行以下來查詢： avg(rate(rpc_durations_seconds_count[5m])) by (job, service) 另外 Prometheus 也提供自定義 Group rules 來將指定的 Expression query 當作一個 Metric，這邊建立一個檔案prometheus.rules.yml，並新增以下內容： groups:- name: example rules: - record: job_service:rpc_durations_seconds_count:avg_rate5m expr: avg(rate(rpc_durations_seconds_count[5m])) by (job, service) 接著修改prometheus.yml加入以下內容，並重新啟動 Prometheus Server： global: ...scrape_configs: ...rule_files: - 'prometheus.rules.yml' global 與 scrape_configs 不做任何修改，只需加入rule_files即可，另外注意檔案路徑位置。 正常啟動後，就可以看到新的 Metric 被加入。","categories":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/categories/CNCF/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://k2r2bai.com/tags/Prometheus/"}]},{"title":"以 Keystone 作為 Kubernetes 使用者認證","slug":"kubernetes/keystone-auth","date":"2018-05-30T09:08:54.000Z","updated":"2019-12-02T01:49:42.395Z","comments":true,"path":"2018/05/30/kubernetes/keystone-auth/","link":"","permalink":"https://k2r2bai.com/2018/05/30/kubernetes/keystone-auth/","excerpt":"本文章將說明如何整合 Keystone 來提供給 Kubernetes 進行使用者認證。但由於 Keystone 整合 Kubernetes 認證在 1.10.x 版本已從原生移除(--experimental-keystone-url, --experimental-keystone-ca-file)，並轉而使用 cloud-provider-openstack 中的 Webhook 來達成，而篇將說明如何建置與設定以整合該 Webhook。","text":"本文章將說明如何整合 Keystone 來提供給 Kubernetes 進行使用者認證。但由於 Keystone 整合 Kubernetes 認證在 1.10.x 版本已從原生移除(--experimental-keystone-url, --experimental-keystone-ca-file)，並轉而使用 cloud-provider-openstack 中的 Webhook 來達成，而篇將說明如何建置與設定以整合該 Webhook。 節點資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統以Ubuntu 16.x進行測試： IP Address Hostname CPU Memory 172.22.132.20 k8s 4 8G 172.22.132.21 keystone 4 8G k8s為 all-in-one Kubernetes 節點(就只是個執行 kubeadm init 的節點)。 keystone利用 DevStack 部署一台 all-in-one OpenStack。 事前準備開始安裝前需要確保以下條件已達成： k8s節點以 kubeadm 部署成 Kubernetes v1.9+ all-in-one 環境。請參考 用 kubeadm 部署 Kubernetes 叢集。 在k8s節點安裝 openstack-client： $ sudo apt-get update &amp;&amp; sudo apt-get install -y python-pip$ export LC_ALL=C; sudo pip install python-openstackclient keystone節點部署成 OpenStack all-in-one 環境。請參考 DevStack。 Kubernetes 與 Keystone 整合本節將逐節說明如何設定以整合 Keystone。 建立 Keystone User 與 Roles當keystone節點的 OpenStack 部署完成後，進入到節點建立測試用 User 與 Roles： $ sudo su - stack$ cd devstack$ source openrc admin admin# 建立 Roles$ for role in \"k8s-admin\" \"k8s-viewer\" \"k8s-editor\"; do openstack role create $role; done# 建立 User$ openstack user create demo_editor --project demo --password secret$ openstack user create demo_admin --project demo --password secret# 加入 User 至 Roles$ openstack role add --user demo --project demo k8s-viewer$ openstack role add --user demo_editor --project demo k8s-editor$ openstack role add --user demo_admin --project demo k8s-admin 在 Kubernetes 安裝 Keystone Webhook進入k8s節點，首先導入下載的檔案來源： $ export URL=\"https://kairen.github.io/files/openstack/keystone\" 新增一些腳本，來提供導入不同使用者環境變數給 OpenStack Client 使用： $ export KEYSTONE_HOST=\"172.22.132.21\"$ export USER_PASSWORD=\"secret\"$ for n in \"admin\" \"demo\" \"demoadmin\" \"demoeditor\" \"altdemo\"; do wget $&#123;URL&#125;/openrc-$&#123;n&#125; -O ~/openrc-$&#123;n&#125; sed -i \"s/KEYSTONE_HOST/$&#123;KEYSTONE_HOST&#125;/g\" ~/openrc-$&#123;n&#125; sed -i \"s/USER_PASSWORD/$&#123;USER_PASSWORD&#125;/g\" ~/openrc-$&#123;n&#125; done 下載 Keystone Webhook Policy 檔案，然後執行指令修改內容： $ sudo wget $&#123;URL&#125;/webhook-policy.json -O /etc/kubernetes/webhook-policy.json$ source ~/openrc-demo$ PROJECT_ID=$(openstack project list | awk '/demo/ &#123;print$2&#125;')$ sudo sed -i \"s/PROJECT_ID/$&#123;PROJECT_ID&#125;/g\" /etc/kubernetes/webhook-policy.json 然後下載與部署 Keystone Webhook YAML 檔： $ wget $&#123;URL&#125;/keystone-webhook-ds.conf -O keystone-webhook-ds.yml$ KEYSTONE_HOST=\"172.22.132.21\"$ sed -i \"s/KEYSTONE_HOST/$&#123;KEYSTONE_HOST&#125;/g\" keystone-webhook-ds.yml$ kubectl create -f keystone-webhook-ds.ymlconfigmap \"keystone-webhook-kubeconfig\" createddaemonset.apps \"keystone-auth-webhook\" created 透過 kubectl 確認 Keystone Webhook 是否部署成功： $ kubectl -n kube-system get po -l component=k8s-keystoneNAME READY STATUS RESTARTS AGEkeystone-auth-webhook-5qqwn 1/1 Running 0 1m 透過 cURL 確認是否能夠正確存取： $ source ~/openrc-demo$ TOKEN=$(openstack token issue -f yaml -c id | awk '&#123;print $2&#125;')$ cat &lt;&lt; EOF | curl -kvs -XPOST -d @- https://localhost:8443/webhook | python -mjson.tool&#123; \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"metadata\": &#123; \"creationTimestamp\": null &#125;, \"spec\": &#123; \"token\": \"$TOKEN\" &#125;&#125;EOF# output&#123; \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"metadata\": &#123; \"creationTimestamp\": null &#125;, \"spec\": &#123; \"token\": \"gAAAAABbFi1SacEPNstSuSuiBXiBG0Y_DikfbiR75j3P-CJ8CeaSKXa5kDQvun4LZUq8U6ehuW_RrQwi-N7j8t086uN6a4hLnPPGmvc6K_Iw0BZHZps7G1R5WniHZ8-WTUxtkMJROSz9eG7m33Bp18mvgx-P179QiwNYxLivf_rjnxePmvujNow\" &#125;, \"status\": &#123; \"authenticated\": true, \"user\": &#123; \"extra\": &#123; \"alpha.kubernetes.io/identity/project/id\": [ \"3ebcb1da142d427db04b8df43f6cb76a\" ], \"alpha.kubernetes.io/identity/project/name\": [ \"demo\" ], \"alpha.kubernetes.io/identity/roles\": [ \"k8s-viewer\", \"Member\", \"anotherrole\" ] &#125;, \"groups\": [ \"3ebcb1da142d427db04b8df43f6cb76a\" ], \"uid\": \"19748c0131504b87a4117e49c67383c6\", \"username\": \"demo\" &#125; &#125;&#125; 設定 kube-apiserver 使用 Webhook進入k8s節點，然後修改/etc/kubernetes/manifests/kube-apiserver.yaml檔案，加入以下內容： ...spec: containers: - command: ... # authorization-mode 加入 Webhook - --authorization-mode=Node,RBAC,Webhook - --runtime-config=authentication.k8s.io/v1beta1=true - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-auth - --authorization-webhook-config-file=/srv/kubernetes/webhook-auth - --authentication-token-webhook-cache-ttl=5m volumeMounts: ... - mountPath: /srv/kubernetes/webhook-auth name: webhook-auth-file readOnly: true volumes: ... - hostPath: path: /srv/kubernetes/webhook-auth type: File name: webhook-auth-file 完成後重新啟動 kubelet(或者等待 static pod 自己更新)： $ sudo systemctl restart kubelet 驗證部署結果進入k8s節點，然後設定 kubectl context 並使用 openstack provider： $ kubectl config set-credentials openstack --auth-provider=openstack$ kubectl config \\ set-context --cluster=kubernetes \\ --user=openstack \\ openstack@kubernetes \\ --namespace=default$ kubectl config use-context openstack@kubernetes 測試 demo 使用者的存取權限是否有被限制： $ source ~/openrc-demo$ kubectl get podsNo resources found.$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: nginx-podspec: restartPolicy: Never containers: - image: nginx name: nginx-appEOF# outputError from server (Forbidden): error when creating \"STDIN\": pods is forbidden: User \"demo\" cannot create pods in the namespace \"default\" 由於 demo 只擁有 k8s-viewer role，因此只能進行 get, list 與 watch API。 測試 demo_editor 使用者是否能夠建立 Pod： $ source ~/openrc-demoeditor$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: nginx-podspec: restartPolicy: Never containers: - image: nginx name: nginx-appEOF# outputpod \"nginx-pod\" created 這邊可以看到 demo_editor 因為擁有 k8s-editor role，因此能夠執行 create API。 測試 alt_demo 是否被禁止存取任何 API： $ source ~/openrc-altdemo$ kubectl get poError from server (Forbidden): pods is forbidden: User \"alt_demo\" cannot list pods in the namespace \"default\" 由於 alt_demo 不具備任何 roles，因此無法存取任何 API。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Keystone","slug":"Keystone","permalink":"https://k2r2bai.com/tags/Keystone/"}]},{"title":"在 AWS 上建立跨地區的 Kubernetes Federation 叢集","slug":"kubernetes/aws-federation-v1","date":"2018-04-21T09:08:54.000Z","updated":"2019-12-02T01:49:42.392Z","comments":true,"path":"2018/04/21/kubernetes/aws-federation-v1/","link":"","permalink":"https://k2r2bai.com/2018/04/21/kubernetes/aws-federation-v1/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本篇延續先前 On-premises Federation 與 Kops 經驗來嘗試在 AWS 上建立 Federaion 叢集，這邊架構如下圖所示：","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本篇延續先前 On-premises Federation 與 Kops 經驗來嘗試在 AWS 上建立 Federaion 叢集，這邊架構如下圖所示： 本次安裝的軟體版本： Kubernetes v1.9.3 kops v1.9.0 kubefed v1.10 節點資訊測試環境為 AWS EC2 虛擬機器，共有三組叢集： US West(Oregon) 叢集，也是 Federation 控制平面叢集： Host vCPU RAM us-west-m1 1 2G us-west-n1 1 2G us-west-n2 1 2G US East(Ohio) 叢集: Host vCPU RAM us-east-m1 1 2G us-east-n1 1 2G us-east-n2 1 2G Asia Pacific(Tokyo) 叢集: Host vCPU RAM ap-northeast-m1 1 2G ap-northeast-n1 1 2G ap-northeast-n2 1 2G 事前準備開始前，需要先安裝下列工具到操作機器上來提供使用： kubectl：用來操作部署完成的 Kubernetes 叢集。 kops：用來部署與管理公有雲上的 Kubernetes 叢集。 Mac OS X： $ brew update &amp;&amp; brew install kops Linux distro： $ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops kubefed：用來建立 Federation 控制平面與管理 Federation 叢集的工具。 Mac OS X： $ git clone https://github.com/kubernetes/federation.git $GOPATH/src/k8s.io/federation$ cd $GOPATH/src/k8s.io/federation$ make quick-release$ cp _output/dockerized/bin/linux/amd64/kubefed /usr/local/bin/kubefed Linux distro： $ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz$ tar xvf federation-client-linux-amd64.tar.gz$ cp federation/client/bin/kubefed /usr/local/bin/$ kubefed versionClient Version: version.Info&#123;Major:\"1\", Minor:\"9+\", GitVersion:\"v1.9.0-alpha.3\", GitCommit:\"85c06145286da663755b140efa2b65f793cce9ec\", GitTreeState:\"clean\", BuildDate:\"2018-02-14T12:54:40Z\", GoVersion:\"go1.9.1\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;Server Version: version.Info&#123;Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.6\", GitCommit:\"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1\", GitTreeState:\"clean\", BuildDate:\"2018-03-21T15:13:31Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125; AWS CLI：用來操作 AWS 服務的工具。 $ sudo pip install awscli$ aws --versionaws-cli/1.15.4 上述工具完成後，我們還要準備一下資訊： 申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。 一般來說只需開啟 S3、Route53、EC2、EBS、ELB 與 VPC 就好，但由於偷懶就全開。以下為各 AWS 服務在本次安裝的用意： IAM: 提供身份認證與存取管理。 EC2: Kubernetes 叢集部署的虛擬機環境。 ELB: Kubernetes 元件與 Service 負載平衡。 Route53: 提供 Public domain 存取 Kubernetes 環境。 S3: 儲存 Kops 狀態。 VPC: 提供 Kubernetes 與 EC2 的網路環境。 擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。 部署 Kubernetes Federation 叢集本節將說明如何利用自己撰寫好的腳本 aws-k8s-federation 來部署 Kubernetes 叢集與 Federation 叢集。首先在操作節點下載： $ git clone https://github.com/kairen/aws-k8s-federation$ cd aws-k8s-federation$ cp .env.sample .env 編輯.env檔案來提供後續腳本的環境變數： # 你的 Domain Name(這邊為 &lt;hoste_dzone_name&gt;.&lt;domain_name&gt;)export DOMAIN_NAME=\"k8s.example.com\"# Regions and zonesexport US_WEST_REGION=\"us-west-2\"export US_EAST_REGION=\"us-east-2\"export AP_NORTHEAST_REGION=\"ap-northeast-1\"export ZONE=\"a\"# Cluster contexts nameexport FED_CONTEXT=\"aws-fed\"export US_WEST_CONTEXT=\"us-west.$&#123;DOMAIN_NAME&#125;\"export US_EAST_CONTEXT=\"us-east.$&#123;DOMAIN_NAME&#125;\"export AP_NORTHEAST_CONTEXT=\"ap-northeast.$&#123;DOMAIN_NAME&#125;\"# S3 buckets nameexport US_WEST_BUCKET_NAME=\"us-west-k8s\"export US_EAST_BUCKET_NAME=\"us-east-k8s\"export AP_NORTHEAST_BUCKET_NAME=\"ap-northeast-k8s\"# Get domain name idexport HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \\ | jq -r '.HostedZones[] | select(.Name==\"'$&#123;DOMAIN_NAME&#125;'.\") | .Id' \\ | sed 's/\\/hostedzone\\///')# Kubernetes master and node size, and node count.export MASTER_SIZE=\"t2.micro\"export NODE_SIZE=\"t2.micro\"export NODE_COUNT=\"2\"# Federation simple apps deploy and service nameexport DNS_RECORD_PREFIX=\"nginx\"export SERVICE_NAME=\"nginx\" 建立 Route53 Hosted Zone首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey： $ aws configureAWS Access Key ID [****************QGEA]:AWS Secret Access Key [****************zJ+w]:Default region name [None]:Default output format [None]: 設定的 Keys 可以在~/.aws/credentials找到。 接著需要在 Route53 建立一個 Hosted Zone，並在 Domain Name 供應商上設定 NameServers： $ ./0-create-hosted-domain.sh# output...&#123; \"HostedZone\": &#123; \"ResourceRecordSetCount\": 2, \"CallerReference\": \"2018-04-25-16:16\", \"Config\": &#123; \"PrivateZone\": false &#125;, \"Id\": \"/hostedzone/Z2JR49ADZ0P3WC\", \"Name\": \"k8s.example.com.\" &#125;, \"DelegationSet\": &#123; \"NameServers\": [ \"ns-1547.awsdns-01.co.uk\", \"ns-1052.awsdns-03.org\", \"ns-886.awsdns-46.net\", \"ns-164.awsdns-20.com\" ] &#125;, \"Location\": \"https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC\", \"ChangeInfo\": &#123; \"Status\": \"PENDING\", \"SubmittedAt\": \"2018-04-25T08:16:57.462Z\", \"Id\": \"/change/C3802PE0C1JVW2\" &#125;&#125; 之後將上述NameServers新增至自己的 Domain name 的 record 中，如 Godaddy： 在每個 Region 建立 Kubernetes 叢集當 Hosted Zone 建立完成後，就可以接著建立每個 Region 的 Kubernetes 叢集，這邊腳本已包含建立叢集與 S3 Bucket 指令，因此只需要執行以下腳本即可： $ ./1-create-clusters.sh....Cluster is starting. It should be ready in a few minutes.... 這邊會需要等待一點時間進行初始化與部署，也可以到 AWS Console 查看狀態。 完成後，即可透過 kubectl 來操作叢集： $ ./us-east/kc get no+ kubectl --context=us-east.k8s.example.com get noNAME STATUS ROLES AGE VERSIONip-172-20-43-26.us-east-2.compute.internal Ready node 1m v1.9.3ip-172-20-56-167.us-east-2.compute.internal Ready master 3m v1.9.3ip-172-20-63-133.us-east-2.compute.internal Ready node 2m v1.9.3$ ./ap-northeast/kc get no+ kubectl --context=ap-northeast.k8s.example.com get noNAME STATUS ROLES AGE VERSIONip-172-20-42-184.ap-northeast-1.compute.internal Ready master 2m v1.9.3ip-172-20-52-176.ap-northeast-1.compute.internal Ready node 20s v1.9.3ip-172-20-56-88.ap-northeast-1.compute.internal Ready node 22s v1.9.3$ ./us-west/kc get no+ kubectl --context=us-west.k8s.example.com get noNAME STATUS ROLES AGE VERSIONip-172-20-33-22.us-west-2.compute.internal Ready node 1m v1.9.3ip-172-20-55-237.us-west-2.compute.internal Ready master 2m v1.9.3ip-172-20-63-77.us-west-2.compute.internal Ready node 35s v1.9.3 建立 Kubernetes Federation 叢集當三個地區的叢集建立完成後，接著要在 US West 的叢集上部署 Federation 控制平面元件： $ ./2-init-federation.sh...Federation API server is running at: abba6864f490111e8b4bd028106a7a79-793027324.us-west-2.elb.amazonaws.com$ ./us-west/kc -n federation-system get po+ kubectl --context=us-west.k8s.example.com -n federation-system get poNAME READY STATUS RESTARTS AGEapiserver-5d46898995-tmzvl 2/2 Running 0 1mcontroller-manager-6cc78c68d5-2pbg5 0/1 Error 3 1m 這邊會發現controller-manager會一直掛掉，這是因為它需要取得 AWS 相關權限，因此需要透過 Patch 方式來把 AccessKey 與 SecretKey 注入到 Deployment 中： $ ./3-path-federation.shSwitched to context \"us-west.k8s.example.com\".deployment \"controller-manager\" patched$ ./us-west/kc -n federation-system get po+ kubectl --context=us-west.k8s.example.com -n federation-system get poNAME READY STATUS RESTARTS AGEapiserver-5d46898995-tmzvl 2/2 Running 0 3mcontroller-manager-769bd95fbc-dkssr 1/1 Running 0 21s 確認上述沒問題後，透過 kubectl 確認 contexts： $ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE ap-northeast.k8s.example.com ap-northeast.k8s.example.com ap-northeast.k8s.example.com aws-fed aws-fed aws-fed us-east.k8s.example.com us-east.k8s.example.com us-east.k8s.example.com* us-west.k8s.example.com us-west.k8s.example.com us-west.k8s.example.com 接著透過以下腳本來加入us-west叢集至 aws-fed 的 Federation 中： $ ./4-join-us-west.sh+ kubectl config use-context aws-fedSwitched to context \"aws-fed\".+ kubefed join us-west --host-cluster-context=us-west.k8s.example.com --cluster-context=us-west.k8s.example.comcluster \"us-west\" created 加入ap-northeast叢集至 aws-fed 的 Federation 中： $ ./5-join-ap-northeast.sh+ kubectl config use-context aws-fedSwitched to context \"aws-fed\".+ kubefed join ap-northeast --host-cluster-context=us-west.k8s.example.com --cluster-context=ap-northeast.k8s.example.comcluster \"ap-northeast\" created 加入us-east叢集至 aws-fed 的 Federation 中： $ ./6-join-us-east.sh+ kubectl config use-context aws-fedSwitched to context \"aws-fed\".+ kubefed join us-east --host-cluster-context=us-west.k8s.example.com --cluster-context=us-east.k8s.example.comcluster \"us-east\" created 完成後，在 Federation 建立 Federated Namespace，並列出叢集： $ ./7-create-fed-ns.sh+ kubectl --context=aws-fed create namespace defaultnamespace \"default\" created+ kubectl --context=aws-fed get clustersNAME AGEap-northeast 2mus-east 1mus-west 2m 完成這些過程表示你已經建立了一套 Kubernetes Federation 叢集了，接下來就可以進行測試。 測試叢集首先建立一個簡單的 Nginx 來提供服務的測試，這邊可以透過以下腳本達成： $ ./8-deploy-fed-nginx.sh+ cat+ kubectl --context=aws-fed apply -f -deployment \"nginx\" created+ cat+ kubectl --context=aws-fed apply -f -service \"nginx\" created$ kubectl get deploy,svcNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeploy/nginx 3 3 3 3 3mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/nginx LoadBalancer &lt;none&gt; a4d86547a4903... 80/TCP 2m 這裡的 nginx deployment 有設定deployment-preferences，因此在 scale 時會依據下面資訊來分配： &#123; \"rebalance\": true, \"clusters\": &#123; \"us-west\": &#123; \"minReplicas\": 2, \"maxReplicas\": 10, \"weight\": 200 &#125;, \"us-east\": &#123; \"minReplicas\": 0, \"maxReplicas\": 2, \"weight\": 150 &#125;, \"ap-northeast\": &#123; \"minReplicas\": 1, \"maxReplicas\": 5, \"weight\": 150 &#125; &#125; &#125; 檢查每個叢集的 Pod： # us-west context(這邊策略為 2 - 10)$ ./us-west/kc get po+ kubectl --context=us-west.k8s.example.com get poNAME READY STATUS RESTARTS AGEnginx-679dc9c764-4x78c 1/1 Running 0 3mnginx-679dc9c764-fzv9z 1/1 Running 0 3m# us-east context(這邊策略為 0 - 2)$ ./us-east/kc get po+ kubectl --context=us-east.k8s.example.com get poNo resources found.# ap-northeast context(這邊策略為 1 - 5)$ ./ap-northeast/kc get po+ kubectl --context=ap-northeast.k8s.example.com get poNAME READY STATUS RESTARTS AGEnginx-679dc9c764-hmwzq 1/1 Running 0 4m 透過擴展副本數來查看分配狀況： $ ./9-scale-fed-nginx.sh+ kubectl --context=aws-fed scale deploy nginx --replicas=10deployment \"nginx\" scaled$ kubectl get deployNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx 10 10 10 10 8m 再次檢查每個叢集的 Pod： # us-west context(這邊策略為 2 - 10)$ ./us-west/kc get po+ kubectl --context=us-west.k8s.example.com get poNAME READY STATUS RESTARTS AGEnginx-679dc9c764-4x78c 1/1 Running 0 8mnginx-679dc9c764-7958k 1/1 Running 0 50snginx-679dc9c764-fzv9z 1/1 Running 0 8mnginx-679dc9c764-j6kc9 1/1 Running 0 50snginx-679dc9c764-t6rvj 1/1 Running 0 50s# us-east context(這邊策略為 0 - 2)$ ./us-east/kc get po+ kubectl --context=us-east.k8s.example.com get poNAME READY STATUS RESTARTS AGEnginx-679dc9c764-8t7qz 1/1 Running 0 1mnginx-679dc9c764-zvqmx 1/1 Running 0 1m# ap-northeast context(這邊策略為 1 - 5)$ ./ap-northeast/kc get po+ kubectl --context=ap-northeast.k8s.example.com get poNAME READY STATUS RESTARTS AGEnginx-679dc9c764-f79v7 1/1 Running 0 1mnginx-679dc9c764-hmwzq 1/1 Running 0 9mnginx-679dc9c764-vj7hb 1/1 Running 0 1m 可以看到結果符合我們預期範圍內。 最後因為服務是透過 ELB 來提供，為了統一透過 Domain name 存取相同服務，這邊更新 Hosted Zone Record 來轉發： $ ./10-update-fed-nginx-record.sh 完成後透過 cURL 工作來測試： $ curl nginx.k8s.example.com...&lt;title&gt;Welcome to nginx!&lt;/title&gt;... 最後透過該腳本來清楚叢集與 AWS 服務上建立的東西： $ ./99-purge.sh","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"AWS","slug":"AWS","permalink":"https://k2r2bai.com/tags/AWS/"},{"name":"Kops","slug":"Kops","permalink":"https://k2r2bai.com/tags/Kops/"},{"name":"Federation","slug":"Federation","permalink":"https://k2r2bai.com/tags/Federation/"}]},{"title":"使用 Kops 部署 Kubernetes 至公有雲(AWS)","slug":"kubernetes/deploy/kops-aws","date":"2018-04-18T09:08:54.000Z","updated":"2019-12-02T01:49:42.392Z","comments":true,"path":"2018/04/18/kubernetes/deploy/kops-aws/","link":"","permalink":"https://k2r2bai.com/2018/04/18/kubernetes/deploy/kops-aws/","excerpt":"Kops 是 Kubernetes 官方維護的專案，是一套 Production ready 的 Kubernetes 部署、升級與管理工具，早期用於 AWS 公有雲上建置 Kubernetes 叢集使用，但隨著社群的推進已支援 GCP、vSphere(Alpha)，未來也會有更多公有雲平台慢慢被支援(Maybe)。本篇簡單撰寫使用 Kops 部署一個叢集，過去自己因為公司都是屬於建置 On-premises 的 Kubernetes，因此很少使用 Kops，剛好最近社群分享又再一次接觸的關析，所以就來寫個文章。 本次安裝的軟體版本： Kubernetes v1.9.3 Kops v1.9.0","text":"Kops 是 Kubernetes 官方維護的專案，是一套 Production ready 的 Kubernetes 部署、升級與管理工具，早期用於 AWS 公有雲上建置 Kubernetes 叢集使用，但隨著社群的推進已支援 GCP、vSphere(Alpha)，未來也會有更多公有雲平台慢慢被支援(Maybe)。本篇簡單撰寫使用 Kops 部署一個叢集，過去自己因為公司都是屬於建置 On-premises 的 Kubernetes，因此很少使用 Kops，剛好最近社群分享又再一次接觸的關析，所以就來寫個文章。 本次安裝的軟體版本： Kubernetes v1.9.3 Kops v1.9.0 事前準備開始使用 Kops 前，需要先安裝下列工具到操作機器上來提供使用： kubectl：用來操作部署完成的 Kubernetes 叢集。 kops：本次使用工具，用來部署與管理公有雲上的 Kubernetes 叢集。 Mac OS X： $ brew update &amp;&amp; brew install kops Linux distro： $ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops AWS CLI：用來操作 AWS 服務的工具。 $ sudo pip install awscli$ aws --versionaws-cli/1.15.4 上述工具完成後，我們還要準備一下資訊： 申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。 一般來說只需開啟 S3、Route53、EC2、EBS 與 ELB 就好，但由於偷懶就全開。 擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。 建立 S3 Bucket 與 Route53 Hosted Zone首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey： $ aws configureAWS Access Key ID [****************QGEA]:AWS Secret Access Key [****************zJ+w]:Default region name [None]:Default output format [None]: 設定的 Keys 可以在~/.aws/credentials找到。 完成後建立一個 S3 bucket 用來儲存 Kops 狀態： $ aws s3 mb s3://kops-k8s-1 --region us-west-2make_bucket: kops-k8s-1 這邊 region 可自行選擇，這邊選用 Oregon。 接著建立一個 Route53 Hosted Zone： $ aws route53 create-hosted-zone \\ --name k8s.example.com \\ --caller-reference $(date '+%Y-%m-%d-%H:%M')# output&#123; \"HostedZone\": &#123; \"ResourceRecordSetCount\": 2, \"CallerReference\": \"2018-04-25-16:16\", \"Config\": &#123; \"PrivateZone\": false &#125;, \"Id\": \"/hostedzone/Z2JR49ADZ0P3WC\", \"Name\": \"k8s.example.com.\" &#125;, \"DelegationSet\": &#123; \"NameServers\": [ \"ns-1547.awsdns-01.co.uk\", \"ns-1052.awsdns-03.org\", \"ns-886.awsdns-46.net\", \"ns-164.awsdns-20.com\" ] &#125;, \"Location\": \"https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC\", \"ChangeInfo\": &#123; \"Status\": \"PENDING\", \"SubmittedAt\": \"2018-04-25T08:16:57.462Z\", \"Id\": \"/change/C3802PE0C1JVW2\" &#125;&#125; 請修改--name為自己所擁有的 domain name。 之後將上述NameServers新增至自己的 Domain name 的 record 中，如 Godaddy： 部署 Kubernetes 叢集當上述階段完成後，在自己機器建立 SSH key，就可以使用 Kops 來建立 Kubernetes 叢集： $ ssh-keygen -t rsa$ kops create cluster \\ --name=k8s.example.com \\ --state=s3://kops-k8s-1 \\ --zones=us-west-2a \\ --master-size=t2.micro \\ --node-size=t2.micro \\ --node-count=2 \\ --dns-zone=k8s.example.com# output...Finally configure your cluster with: kops update cluster k8s.example.com --yes 若過程沒有發生錯誤的話，最後會提示再執行 update 來正式進行部署： $ kops update cluster k8s.example.com --state=s3://kops-k8s-1 --yes# output...Cluster is starting. It should be ready in a few minutes. 當看到上述資訊時，表示叢集已建立，這時候等待環境初始化完成後就可以使用 kubectl 來操作： $ kubectl get nodeNAME STATUS ROLES AGE VERSIONip-172-20-32-194.us-west-2.compute.internal Ready master 1m v1.9.3ip-172-20-32-21.us-west-2.compute.internal Ready node 22s v1.9.3ip-172-20-54-100.us-west-2.compute.internal Ready node 28s v1.9.3 測試完成後就可以進行功能測試，這邊簡單建立 Nginx app： $ kubectl run nginx --image nginx --port 80$ kubectl expose deploy nginx --type=LoadBalancer --port 80$ kubectl get po,svcNAME READY STATUS RESTARTS AGEpo/nginx-7587c6fdb6-7qtlr 1/1 Running 0 50sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/kubernetes ClusterIP 100.64.0.1 &lt;none&gt; 443/TCP 8msvc/nginx LoadBalancer 100.68.96.3 ad99f206f486e... 80:30174/TCP 28s 這邊會看到EXTERNAL-IP會直接透過 AWS ELB 建立一個 Load Balancer，這時只要更新 Route53 的 record set 就可以存取到服務： $ export DOMAIN_NAME=k8s.example.com$ export NGINX_LB=$(kubectl get svc/nginx \\ --template=\"&#123;&#123;range .status.loadBalancer.ingress&#125;&#125; &#123;&#123;.hostname&#125;&#125; &#123;&#123;end&#125;&#125;\")$ cat &lt;&lt;EOF &gt; dns-record.json&#123; \"Comment\": \"Create/Update a latency-based CNAME record for a federated Deployment\", \"Changes\": [ &#123; \"Action\": \"UPSERT\", \"ResourceRecordSet\": &#123; \"Name\": \"nginx.$&#123;DOMAIN_NAME&#125;\", \"Type\": \"CNAME\", \"Region\": \"us-west-2\", \"TTL\": 300, \"SetIdentifier\": \"us-west-2\", \"ResourceRecords\": [ &#123; \"Value\": \"$&#123;NGINX_LB&#125;\" &#125; ] &#125; &#125; ]&#125;EOF$ export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \\ | jq -r '.HostedZones[] | select(.Name==\"'$&#123;DOMAIN_NAME&#125;'.\") | .Id' \\ | sed 's/\\/hostedzone\\///')$ aws route53 change-resource-record-sets \\ --hosted-zone-id $&#123;HOSTED_ZONE_ID&#125; \\ --change-batch file://dns-record.json# output&#123; \"ChangeInfo\": &#123; \"Status\": \"PENDING\", \"Comment\": \"Create/Update a latency-based CNAME record for a federated Deployment\", \"SubmittedAt\": \"2018-04-25T10:06:02.545Z\", \"Id\": \"/change/C79MFJRHCF05R\" &#125;&#125; 完成後透過 cURL 工作來測試： $ curl nginx.k8s.example.com...&lt;title&gt;Welcome to nginx!&lt;/title&gt;... 刪除節點當叢集測完後，可以利用以下指令來刪除： $ kops delete cluster \\ --name=k8s.example.com \\ --state=s3://kops-k8s-1 --yesDeleted cluster: \"k8s.k2r2bai.com\"$ aws s3 rb s3://kops-k8s-1 --forceremove_bucket: kops-k8s-1 接著清除 Route53 所有 record 並刪除 hosted zone： $ aws route53 list-resource-record-sets \\ --hosted-zone-id $&#123;HOSTED_ZONE_ID&#125; |jq -c '.ResourceRecordSets[]' |while read -r resourcerecordset; do read -r name type &lt;&lt;&lt;$(echo $(jq -r '.Name,.Type' &lt;&lt;&lt;\"$resourcerecordset\")) if [ $type != \"NS\" -a $type != \"SOA\" ]; then aws route53 change-resource-record-sets \\ --hosted-zone-id $&#123;HOSTED_ZONE_ID&#125; \\ --change-batch '&#123;\"Changes\":[&#123;\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"$resourcerecordset\"' &#125;]&#125;' \\ --output text --query 'ChangeInfo.Id' fidone$ aws route53 delete-hosted-zone --id $&#123;HOSTED_ZONE_ID&#125;","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"AWS","slug":"AWS","permalink":"https://k2r2bai.com/tags/AWS/"}]},{"title":"使用 kubefed 建立 Kubernetes Federation(On-premises)","slug":"kubernetes/federation-v1","date":"2018-03-21T09:08:54.000Z","updated":"2019-12-02T01:49:42.394Z","comments":true,"path":"2018/03/21/kubernetes/federation-v1/","link":"","permalink":"https://k2r2bai.com/2018/03/21/kubernetes/federation-v1/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) Kubernetes Federation(聯邦) 是實現跨地區與跨服務商多個 Kubernetes 叢集的管理機制。Kubernetes Federation 的架構非常類似純 Kubenretes 叢集，Federation 會擁有自己的 API Server 與 Controller Manager 來提供一個標準的 Kubernetes API，以及管理聯邦叢集，並利用 Etcd 來儲存所有狀態，不過差異在於 Kubenretes 只管理多個節點，而 Federation 是管理所有被註冊的 Kubernetes 叢集。","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) Kubernetes Federation(聯邦) 是實現跨地區與跨服務商多個 Kubernetes 叢集的管理機制。Kubernetes Federation 的架構非常類似純 Kubenretes 叢集，Federation 會擁有自己的 API Server 與 Controller Manager 來提供一個標準的 Kubernetes API，以及管理聯邦叢集，並利用 Etcd 來儲存所有狀態，不過差異在於 Kubenretes 只管理多個節點，而 Federation 是管理所有被註冊的 Kubernetes 叢集。 Federation 使管理多個叢集更為簡單，這主要是透過兩個模型來實現： 跨叢集的資源同步(Sync resources across clusters)：提供在多個叢集中保持資源同步的功能，如確保一個 Deployment 可以存在於多個叢集中。 跨叢集的服務發現(Cross cluster discovery:)：提供自動配置 DNS 服務以及在所有叢集後端上進行負載平衡功能，如提供全域 VIP 或 DNS record，並透過此存取多個叢集後端。 Federation 有以下幾個好處： 跨叢集的資源排程，能讓 Pod 分配至不同叢集的不同節點上執行，如果當前叢集超出負荷，能夠將額外附載分配到空閒叢集上。 叢集的高可靠，能夠做到 Pod 故障自動遷移。 可管理多個 Kubernetes 叢集。 跨叢集的服務發現。 雖然 Federation 能夠降低管理多叢集門檻，但是目前依據不建議放到生產環境。以下幾個原因： 成熟度問題，目前還處與 Alpha 階段，故很多功能都還處於實現性質，或者不太穩定。 提升網路頻寬與成本，由於 Federation 需要監控所有叢集以確保當前狀態符合預期，因是會增加額外效能開銷。 跨叢集隔離差，Federation 的子叢集有可能因為 Bug 的引發而影響其他叢集運行狀況。 個人用起來不是很穩定，例如建立的 Deployment 刪除很常會 Timeout。 支援的物件資源有限，如不支援 StatefulSets。可參考 API resources。 Federation 主要包含三個元件： federation-apiserver：主要提供跨叢集的 REST API 伺服器，類似 kube-apiserver。 federation-controller-manager：提供多個叢集之間的狀態同步，類似 kube-controller-manager。 kubefed：Federation CLI 工具，用來初始化 Federation 元件與加入子叢集。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器，共有三組叢集： Federation 控制平面叢集(簡稱 F): IP Address Host vCPU RAM 172.22.132.31 k8s-f-m1 4 16G 172.22.132.32 k8s-f-n1 4 16G 叢集 A: IP Address Host vCPU RAM 172.22.132.41 k8s-a-m1 8 16G 172.22.132.42 k8s-a-n1 8 16G 叢集 B: IP Address Host vCPU RAM 172.22.132.51 k8s-b-m1 8 16G 172.22.132.52 k8s-b-n1 8 16G 事前準備安裝與進行 Federation 之前，需要確保以下條件達成： 所有叢集的節點各自部署成一個 Kubernetes 叢集，請參考 用 kubeadm 部署 Kubernetes 叢集。 修改 F、A 與 B 叢集的 Kubernetes config，並將 A 與 B 複製到 F 節點，如修改成以下： ...... name: k8s-a-clustercontexts:- context: cluster: k8s-a-cluster user: a-cluster-admin name: a-cluster-contextcurrent-context: a-cluster-contextkind: Configpreferences: &#123;&#125;users:- name: a-cluster-admin user:... 這邊需要修改每個叢集 config。 接著在 F 叢集合併 F、A 與 B 三個 config，透過以下方式進行： $ lsa-cluster.conf b-cluster.conf f-cluster.conf$ KUBECONFIG=f-cluster.conf:a-cluster.conf:b-cluster.conf kubectl config view --flatten &gt; ~/.kube/config$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE a-cluster-context k8s-a-cluster a-cluster-admin b-cluster-context k8s-b-cluster b-cluster-admin* f-cluster-context k8s-f-cluster f-cluster-admin 在 F 叢集安裝 kubefed 工具： $ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz$ tar xvf federation-client-linux-amd64.tar.gz$ cp federation/client/bin/kubefed /usr/local/bin/$ kubefed versionClient Version: version.Info&#123;Major:\"1\", Minor:\"9+\", GitVersion:\"v1.9.0-alpha.3\", GitCommit:\"85c06145286da663755b140efa2b65f793cce9ec\", GitTreeState:\"clean\", BuildDate:\"2018-02-14T12:54:40Z\", GoVersion:\"go1.9.1\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;Server Version: version.Info&#123;Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.6\", GitCommit:\"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1\", GitTreeState:\"clean\", BuildDate:\"2018-03-21T15:13:31Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125; 在 F 叢集安裝 Helm 工具，並進行初始化： $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zxf$ sudo mv linux-amd64/helm /usr/local/bin/$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller# wait for a few minutes$ helm versionClient: &amp;version.Version&#123;SemVer:\"v2.8.1\", GitCommit:\"6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.8.1\", GitCommit:\"6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2\", GitTreeState:\"clean\"&#125; 部署 Kubernetes Federation由於本篇是使用實體機器部署 Kubernetes 叢集，因此無法像是 GCP 可以提供 DNS 服務來給 Federation 使用，故這邊要用 CoreDNS 建立自定義 DNS 服務。 CoreDNS 安裝首先透過 Helm 來安裝 CoreDNS 使用到的 Etcd： $ helm install --namespace federation --name etcd-operator stable/etcd-operator$ helm upgrade --namespace federation --set cluster.enabled=true etcd-operator stable/etcd-operator$ kubectl -n federation get poNAME READY STATUS RESTARTS AGEetcd-operator-etcd-operator-etcd-backup-operator-577d56449zqkj2 1/1 Running 0 1metcd-operator-etcd-operator-etcd-operator-56679fb56-fpgmm 1/1 Running 0 1metcd-operator-etcd-operator-etcd-restore-operator-65b6cbccl7kzr 1/1 Running 0 1m 完成後就可以安裝 CoreDNS 來提供自定義 DNS 服務了： $ cat &lt;&lt;EOF &gt; Values.yamlisClusterService: falseserviceType: NodePortmiddleware: kubernetes: enabled: false etcd: enabled: true zones: - \"kairen.com.\" endpoint: \"http://etcd-cluster.federation:2379\"EOF$ kubectl create clusterrolebinding federation-admin --clusterrole=cluster-admin --user=system:serviceaccount:federation:default$ helm install --namespace federation --name coredns -f Values.yaml stable/coredns# 測試 CoreDNS 可以查詢 Domain Name$ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstoolsdnstools# host kuberneteskubernetes.default.svc.cluster.local has address 10.96.0.1 安裝與初始化 Federation 控制平面元件完成 CoreDNS 後，接著透過 kubefed 安裝控制平面元件，由於使用到 CoreDNS，因此這邊要傳入相關 conf 檔，首先建立coredns-provider.conf檔案，加入以下內容： $ cat &lt;&lt;EOF &gt; coredns-provider.conf[Global]etcd-endpoints = http://etcd-cluster.federation:2379zones = kairen.com.EOF 請自行修改etcd-endpoints與zones。 檔案建立並確認沒問題後，透過 kubefed 工具來初始化主叢集： $ kubefed init federation \\ --host-cluster-context=f-cluster-context \\ --dns-provider=\"coredns\" \\ --dns-zone-name=\"kairen.com.\" \\ --apiserver-enable-basic-auth=true \\ --apiserver-enable-token-auth=true \\ --dns-provider-config=\"coredns-provider.conf\" \\ --apiserver-arg-overrides=\"--anonymous-auth=false,--v=4\" \\ --api-server-service-type=\"NodePort\" \\ --api-server-advertise-address=\"172.22.132.31\" \\ --etcd-persistent-storage=true$ kubectl -n federation-system get poNAME READY STATUS RESTARTS AGEapiserver-848d584b5d-cwxdh 2/2 Running 0 1mcontroller-manager-5846c555c6-mw2jz 1/1 Running 1 1m 這邊可以改變--etcd-persistent-storage來選擇使用或不使用 PV，若使用請先建立一個 PV 來提供給 Federation Pod 的 PVC 索取使用，可以參考 Persistent Volumes。 加入 Federation 的 Kubernetes 子叢集$ kubectl config use-context federation# 加入 k8s-a-cluster$ kubefed join f-a-cluster \\ --cluster-context=a-cluster-context \\ --host-cluster-context=f-cluster-context# 加入 k8s-b-cluster$ kubefed join f-b-cluster \\ --cluster-context=b-cluster-context \\ --host-cluster-context=f-cluster-context$ kubectl get clusterNAME AGEf-a-cluster 57sf-b-cluster 53s 測試 Federation 叢集這邊利用 Nginx Deployment 來進行測試，先簡單建立一個副本為 4 的 Nginx： $ kubectl config use-context federation$ kubectl create ns default$ kubectl run nginx --image nginx --port 80 --replicas=4 查看 Cluster A： $ kubectl --context=a-cluster-context get poNAME READY STATUS RESTARTS AGEnginx-7587c6fdb6-dpjv5 1/1 Running 0 25snginx-7587c6fdb6-sjv8v 1/1 Running 0 25s 查看 Cluster B： $ kubectl --context=b-cluster-context get poNAME READY STATUS RESTARTS AGEnginx-7587c6fdb6-dv45v 1/1 Running 0 1mnginx-7587c6fdb6-wxsmq 1/1 Running 0 1m 其他可測試功能： 設定 Replica set preferences，參考 Spreading Replicas in Underlying Clusters。 Federation 在 v1.7+ 加入了 ClusterSelector Annotation Scheduling Policy。 Refers Minikube Federation Global Kubernetes in 3 Steps","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Federation","slug":"Federation","permalink":"https://k2r2bai.com/tags/Federation/"}]},{"title":"[Kubeflow] 初探基本功能與概念","slug":"kubernetes/kubeflow/quick-start","date":"2018-03-15T09:08:54.000Z","updated":"2019-12-02T01:49:42.396Z","comments":true,"path":"2018/03/15/kubernetes/kubeflow/quick-start/","link":"","permalink":"https://k2r2bai.com/2018/03/15/kubernetes/kubeflow/quick-start/","excerpt":"Kubeflow 是 Google 開源的機器學習工具，目標是簡化在 Kubernetes 上運行機器學習的過程，使之更簡單、可攜帶與可擴展。Kubeflow 目標不是在於重建其他服務，而是提供一個最佳開發系統來部署到各種基礎設施架構中，另外由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠執行 Kubeflow。","text":"Kubeflow 是 Google 開源的機器學習工具，目標是簡化在 Kubernetes 上運行機器學習的過程，使之更簡單、可攜帶與可擴展。Kubeflow 目標不是在於重建其他服務，而是提供一個最佳開發系統來部署到各種基礎設施架構中，另外由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠執行 Kubeflow。 該工具能夠建立以下幾項功能： 用於建議與管理互動式 Jupyter notebook 的 JupyterHub。 可以設定使用 CPU 或 GPU，並透過單一設定調整單個叢集大小的 Tensorflow Training Controller。 用 TensorFlow Serving 容器來提供模型服務。 Kubeflow 目標是透過 Kubernetes 的特性使機器學習更加簡單與快速： 在不同基礎設施上實現簡單、可重複的攜帶性部署(Laptop &lt;-&gt; ML rig &lt;-&gt; Training cluster &lt;-&gt; Production cluster)。 部署與管理松耦合的微服務。 根據需求進行縮放。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： IP Address Role vCPU RAM Extra Device 172.22.132.51 gpu-node1 8 16G GTX 1060 3G 172.22.132.52 gpu-node2 8 16G GTX 1060 3G 172.22.132.53 master1 8 16G 無 事前準備使用 Kubeflow 之前，需要確保以下條件達成： 所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker，請參考 安裝 Nvidia Docker 2。 (option)所有 GPU 節點安裝 cuDNN v7.1.2 for CUDA 9.1，請至 NVIDIA cuDNN 下載。 $ tar xvf cudnn-9.1-linux-x64-v7.1.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ 所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集，請參考 用 kubeadm 部署 Kubernetes 叢集。 Kubernetes 叢集需要安裝 NVIDIA Device Plugins，請參考 安裝 Kubernetes NVIDIA Device Plugins。 建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用： # 在 master 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server$ sudo mkdir /nfs-data$ echo \"/nfs-data *(rw,sync,no_root_squash,no_subtree_check)\" | sudo tee -a /etc/exports$ sudo /etc/init.d/nfs-kernel-server restart# 在 node 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common 安裝ksonnet 0.9.2，請參考以下： $ wget https://github.com/ksonnet/ksonnet/releases/download/v0.9.2/ks_0.9.2_linux_amd64.tar.gz$ tar xvf ks_0.9.2_linux_amd64.tar.gz$ sudo cp ks_0.9.2_linux_amd64/ks /usr/local/bin/$ ks versionksonnet version: 0.9.2jsonnet version: v0.9.5client-go version: 1.8 部署 Kubeflow本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。首先在master節點初始化 ksonnet 應用程式目錄： $ ks init my-kubeflow 如果遇到以下問題的話，可以自己建立 GitHub Token 來存取 GitHub API，請參考 Github rate limiting errors。 ERROR GET https://api.github.com/repos/ksonnet/parts/commits/master: 403 API rate limit exceeded for 122.146.93.152. 接著安裝 Kubeflow 套件至應用程式目錄： $ cd my-kubeflow$ ks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow$ ks pkg install kubeflow/core$ ks pkg install kubeflow/tf-serving$ ks pkg install kubeflow/tf-job 然後建立 Kubeflow 核心元件，該元件包含 JupyterHub 與 TensorFlow job controller： $ kubectl create namespace kubeflow$ kubectl create clusterrolebinding tf-admin --clusterrole=cluster-admin --serviceaccount=default:tf-job-operator$ ks generate core kubeflow-core --name=kubeflow-core --namespace=kubeflow# 啟動收集匿名使用者使用量資訊，如果不想開啟則忽略$ ks param set kubeflow-core reportUsage true$ ks param set kubeflow-core usageId $(uuidgen)# 部署 Kubeflow$ ks param set kubeflow-core jupyterHubServiceType LoadBalancer$ ks apply default -c kubeflow-core 詳細使用量資訊請參考 Usage Reporting 。 完成後檢查 Kubeflow 元件部署結果： $ kubectl -n kubeflow get po -o wideNAME READY STATUS RESTARTS AGE IP NODEambassador-7956cf5c7f-6hngq 2/2 Running 0 34m 10.244.41.132 kube-gpu-node1ambassador-7956cf5c7f-jgxnd 2/2 Running 0 34m 10.244.152.134 kube-gpu-node2ambassador-7956cf5c7f-jww2d 2/2 Running 0 34m 10.244.41.133 kube-gpu-node1spartakus-volunteer-8c659d4f5-bg7kn 1/1 Running 0 34m 10.244.152.135 kube-gpu-node2tf-hub-0 1/1 Running 0 34m 10.244.152.133 kube-gpu-node2tf-job-operator-78757955b-2jbdh 1/1 Running 0 34m 10.244.41.131 kube-gpu-node1 這時候就可以登入 Jupyter Notebook，但這邊需要修改 Kubernetes Service，透過以下指令進行： $ kubectl -n kubeflow get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador ClusterIP 10.101.157.91 &lt;none&gt; 80/TCP 45m service=ambassadorambassador-admin ClusterIP 10.107.24.138 &lt;none&gt; 8877/TCP 45m service=ambassadork8s-dashboard ClusterIP 10.111.128.104 &lt;none&gt; 443/TCP 45m k8s-app=kubernetes-dashboardtf-hub-0 ClusterIP None &lt;none&gt; 8000/TCP 45m app=tf-hubtf-hub-lb ClusterIP 10.105.47.253 &lt;none&gt; 80/TCP 45m app=tf-hub# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。$ kubectl -n kubeflow edit svc tf-hub-lb...spec: type: LoadBalancer externalIPs: - 172.22.132.41... 測試 Kubeflow開始測試前先建立一個 NFS PV 來提供給 Kubeflow Jupyter 使用： $ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.41 path: /nfs-dataEOF 完成後連接 http://Master_IP，並輸入任意帳號密碼進行登入。 登入後點選Start My Server按鈕來建立 Server 的 Spawner options，預設會有多種映像檔可以使用： CPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-cpu。 GPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-gpu。 這邊也使用以下 GCP 建構的映像檔做測試使用(GPU 當前為 CUDA 8)： gcr.io/kubeflow/tensorflow-notebook-cpu:latest gcr.io/kubeflow/tensorflow-notebook-gpu:latest 若 CUDA 版本不同，請自行修改 GCP Tensorflow Notebook image 或是 Kubeflow Tensorflow Notebook image 重新建構。 如果使用 GPU 請執行以下指令確認是否可被分配資源： $ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"NAME GPUkube-gpu-master1 &lt;none&gt;kube-gpu-node1 1kube-gpu-node2 1 最後點選Spawn來完成建立 Server，如下圖所示： 這邊先用 CPU 進行測試，由於本篇是安裝 CUDA 9.1 + cuDNN 7，因此要自己建構映像檔。 接著等 Kubernetes 下載映像檔後，就會正常啟動，如下圖所示： 當正常啟動後，點選New &gt; Python 3建立一個 Notebook 並貼上以下範例程式： from __future__ import print_functionimport tensorflow as tfhello = tf.constant('Hello TensorFlow!')s = tf.Session()print(s.run(hello)) 正確執行會如以下圖所示： 若想關閉叢集的話，可以點選Control Plane。 另外由於 Kubeflow 會安裝 TF Operator 來管理 TFJob，這邊可以透過 Kubernetes 來手動建立 Job： $ kubectl create -f https://raw.githubusercontent.com/kubeflow/tf-operator/master/examples/tf_job.yaml$ kubectl get poNAME READY STATUS RESTARTS AGEexample-job-ps-qq6x-0-pdx7v 1/1 Running 0 5mexample-job-ps-qq6x-1-2mpfp 1/1 Running 0 5mexample-job-worker-qq6x-0-m5fm5 1/1 Running 0 5m 若想從 Kubernetes 叢集刪除 Kubeflow 相關元件的話，可執行下列指令達成： $ ks delete default -c kubeflow-core","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"GPU","slug":"GPU","permalink":"https://k2r2bai.com/tags/GPU/"},{"name":"ML/DL","slug":"ML-DL","permalink":"https://k2r2bai.com/tags/ML-DL/"},{"name":"Kubeflow","slug":"Kubeflow","permalink":"https://k2r2bai.com/tags/Kubeflow/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://k2r2bai.com/tags/TensorFlow/"}]},{"title":"安裝 NVIDIA Docker 2 來讓容器使用 GPU","slug":"container/nvidia-docker-install","date":"2018-02-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.382Z","comments":true,"path":"2018/02/17/container/nvidia-docker-install/","link":"","permalink":"https://k2r2bai.com/2018/02/17/container/nvidia-docker-install/","excerpt":"本篇主要介紹如何使用 NVIDIA Docker v2 來讓容器使用 GPU，過去 NVIDIA Docker v1 需要使用 nvidia-docker 來取代 Docker 執行 GPU image，或是透過手動掛載 NVIDIA driver 與 CUDA 來使 Docker 能夠編譯與執行 GPU 應用程式，而新版本的 Docker 則可以透過 –runtime 來選擇使用 NVIDIA Docker v2 的 Runtime 來執行 GPU 應用程式。","text":"本篇主要介紹如何使用 NVIDIA Docker v2 來讓容器使用 GPU，過去 NVIDIA Docker v1 需要使用 nvidia-docker 來取代 Docker 執行 GPU image，或是透過手動掛載 NVIDIA driver 與 CUDA 來使 Docker 能夠編譯與執行 GPU 應用程式，而新版本的 Docker 則可以透過 –runtime 來選擇使用 NVIDIA Docker v2 的 Runtime 來執行 GPU 應用程式。 安裝前需要確認滿足以下幾點： GNU/Linux x86_64 with kernel version &gt; 3.10 Docker CE or EE &gt;= v18.03.1 NVIDIA GPU with Architecture &gt; Fermi (2.1) NVIDIA drivers ~= 361.93 (untested on older versions) 首先透過 APT 安裝 Docker CE or EE 版本： $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ echo \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial edge\" | sudo tee /etc/apt/sources.list.d/docker.list$ sudo apt-get update &amp;&amp; sudo apt-get install -y docker-ce=5:18.09.4~3-0~ubuntu-xenial 接著透過 APT 安裝 NVIDIA Driver 與 CUDA： $ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update $ sudo apt-get install -y linux-headers-$(uname -r)$ sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install -y cuda-10-0 cuda-drivers 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成： $ cat /usr/local/cuda/version.txtCUDA Version 10.0.130$ sudo nvidia-smiThu Apr 11 03:30:48 2019+-----------------------------------------------------------------------------+| NVIDIA-SMI 418.40.04 Driver Version: 418.40.04 CUDA Version: 10.1 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A || 47% 34C P0 28W / 120W | 0MiB / 3017MiB | 0% Default |+-------------------------------+----------------------+----------------------+| 1 GeForce GTX 106... Off | 00000000:06:00.0 Off | N/A || 47% 33C P5 9W / 120W | 0MiB / 3019MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 確認上述無誤後，接著安裝 NVIDIA Docker v2，這邊透過 APT 來進行安裝： $ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$ curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2=2.0.3+docker18.09.4-1 nvidia-container-runtime=2.0.0+docker18.09.4-1$ sudo pkill -SIGHUP dockerd 測試 NVIDIA runtime，這邊下載 NVIDIA image 來進行測試： $ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi...+-----------------------------------------------------------------------------+| NVIDIA-SMI 418.40.04 Driver Version: 418.40.04 CUDA Version: 10.1 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A || 36% 34C P0 28W / 120W | 0MiB / 3017MiB | 0% Default |+-------------------------------+----------------------+----------------------+| 1 GeForce GTX 106... Off | 00000000:06:00.0 Off | N/A || 41% 32C P0 12W / 120W | 0MiB / 3019MiB | 2% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 最後可以透過 TensorFlow GPU 的 Image 來進行測試，執行後登入 IP:8888 執行簡單範例程式： $ docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter...2019-04-11 06:44:21.719705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845pciBusID: 0000:01:00.0totalMemory: 2.95GiB freeMemory: 2.88GiB2019-04-11 06:44:21.719728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 02019-04-11 06:44:21.919097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2598 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"GPU","slug":"GPU","permalink":"https://k2r2bai.com/tags/GPU/"}]},{"title":"Ceph Luminous CRUSH map 400000000000000 問題","slug":"ceph/luminous-crush-issue","date":"2018-02-11T09:08:54.000Z","updated":"2019-12-02T01:49:42.378Z","comments":true,"path":"2018/02/11/ceph/luminous-crush-issue/","link":"","permalink":"https://k2r2bai.com/2018/02/11/ceph/luminous-crush-issue/","excerpt":"在 Ceph Luminous(v12) 版本中，預設開啟了一些 Kernel 特性，其中首先遇到的一般是 400000000000000 問題，即CEPH_FEATURE_NEW_OSDOPREPLY_ENCODING特性(可以從對照表得知CEPH_FEATURE Table and Kernel Version)，剛問題需要在 Kernel 4.5+ 才能夠被支援，但如果不想升級可以依據本篇方式解決。","text":"在 Ceph Luminous(v12) 版本中，預設開啟了一些 Kernel 特性，其中首先遇到的一般是 400000000000000 問題，即CEPH_FEATURE_NEW_OSDOPREPLY_ENCODING特性(可以從對照表得知CEPH_FEATURE Table and Kernel Version)，剛問題需要在 Kernel 4.5+ 才能夠被支援，但如果不想升級可以依據本篇方式解決。 在 L 版本中，當建立 RBD 並且想要 Map 時，會發生 timeout 問題，這時候可以透過 journalctl 來查看問題，如以下： $ journalctl -xeFeb 12 08:36:57 kube-server2 kernel: libceph: mon0 172.22.132.51:6789 feature set mismatch, my 106b84a842a42 &lt; server's 40106b84a842a42, missing 400000000000000 查詢發現是 400000000000000 問題，這時可以選擇兩個解決方式： 將作業系統更新到 Linux kernel v4.5+ 的版本。 修改 CRUSH 中的 tunables 參數。 若想修改 CRUSH tunnables 參數，可以先到任一 Monitor 或者 Admin 節點中，執行以下指令： $ ceph osd crush tunables jewel$ ceph osd crush reweight-all 只要執行以上指令即可。","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"}]},{"title":"利用 RBAC + SA 進行 Kubectl 權限控管","slug":"kubernetes/rbac-sa-kubectl","date":"2018-01-08T09:08:54.000Z","updated":"2019-12-02T01:49:42.396Z","comments":true,"path":"2018/01/08/kubernetes/rbac-sa-kubectl/","link":"","permalink":"https://k2r2bai.com/2018/01/08/kubernetes/rbac-sa-kubectl/","excerpt":"這邊說明如何建立不同 Service account user，以及 RBAC 來定義存取規則，並綁定於指定 Service account ，以對指定 Namespace 中資源進行存取權限控制。","text":"這邊說明如何建立不同 Service account user，以及 RBAC 來定義存取規則，並綁定於指定 Service account ，以對指定 Namespace 中資源進行存取權限控制。 Service accountService account 一般使用情境方便是 Pod 中的行程呼叫 Kubernetes API 或者其他服務設計而成，這可能會跟 Kubernetes user account 有所混肴，但是由於 Service account 有別於 User account 是可以針對 Namespace 進行建立，因此這邊嘗試拿 Service account 來提供資訊給 kubectl 使用，並利用 RBAC 來設定存取規則，以限制該 Account 存取 API 的資源。 RBACRBAC(Role-Based Access Control)是從 Kubernetes 1.6 開始支援的存取控制機制，叢集管理者能夠對 User 或 Service account 的角色設定指定資源存取權限，在 RBAC 中，權限與角色相互關聯，其透過成為適當的角色成員，以獲取這些角色的存取權限，這比起過去 ABAC 來的方便使用、更簡化等好處。 簡單範例首先建立一個 Namespace 與 Service account： $ kubectl create ns dev$ kubectl -n dev create sa dev# 取得 secret 資訊$ SECRET=$(kubectl -n dev get sa dev -o go-template='&#123;&#123;range .secrets&#125;&#125;&#123;&#123;.name&#125;&#125;&#123;&#123;end&#125;&#125;') 建立一個 dev.conf 設定檔，添加以下內容： $ API_SERVER=\"https://172.22.132.51:6443\"$ CA_CERT=$(kubectl -n dev get secret $&#123;SECRET&#125; -o yaml | awk '/ca.crt:/&#123;print $2&#125;')$ cat &lt;&lt;EOF &gt; dev.confapiVersion: v1kind: Configclusters:- cluster: certificate-authority-data: $CA_CERT server: $API_SERVER name: clusterEOF$ TOKEN=$(kubectl -n dev get secret $&#123;SECRET&#125; -o go-template='&#123;&#123;.data.token&#125;&#125;')$ kubectl config set-credentials dev-user \\ --token=`echo $&#123;TOKEN&#125; | base64 -d` \\ --kubeconfig=dev.conf$ kubectl config set-context default \\ --cluster=cluster \\ --user=dev-user \\ --kubeconfig=dev.conf$ kubectl config use-context default \\ --kubeconfig=dev.conf 在不同作業系統中，base64 的 decode 指令不一樣，有些是 -D(OS X)。 新增 RBAC role 來限制 dev-user 存取權限: $ cat &lt;&lt;EOF &gt; dev-user-role.ymlkind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: dev name: dev-user-podrules:- apiGroups: [\"*\"] resources: [\"pods\", \"pods/log\"] verbs: [\"get\", \"watch\", \"list\", \"update\", \"create\", \"delete\"]EOF$ kubectl create rolebinding dev-view-pod \\ --role=dev-user-pod \\ --serviceaccount=dev:dev \\ --namespace=dev apiGroups 為不同 API 的群組，如 rbac.authorization.k8s.io，[“*”] 為允許存取全部。 resources 為 API 存取資源，如 pods、pods/log、pod/exec，[“*”] 為允許存取全部。 verbs 為 API 存取方法，如 get、list、watch、create、update、 delete、proxy，[“*”] 為允許存取全部。 透過 kubectl 確認權限設定沒問題： $ kubectl --kubeconfig=dev.conf get poError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:dev:dev\" cannot list pods in the namespace \"default\"$ kubectl -n dev --kubeconfig=dev.conf run nginx --image nginx --port 80 --restart=Never$ kubectl -n dev --kubeconfig=dev.conf get poNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 39s$ kubectl -n dev --kubeconfig=dev.conf logs -f nginx10.244.102.64 - - [04/Jan/2018:06:42:36 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"$ kubectl -n dev --kubeconfig=dev.conf exec -ti nginx shError from server (Forbidden): pods \"nginx\" is forbidden: User \"system:serviceaccount:dev:dev\" cannot create pods/exec in the namespace \"dev\" 也可以用export KUBECONFIG=dev.conf來設定使用的 config。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Kubernetes RBAC","slug":"Kubernetes-RBAC","permalink":"https://k2r2bai.com/tags/Kubernetes-RBAC/"}]},{"title":"多租戶 Kubernetes 部署方案 Stackube","slug":"openstack/stackube","date":"2017-12-20T08:23:01.000Z","updated":"2019-12-02T01:49:42.401Z","comments":true,"path":"2017/12/20/openstack/stackube/","link":"","permalink":"https://k2r2bai.com/2017/12/20/openstack/stackube/","excerpt":"Stackube是一個 Kubernetes-centric 的 OpenStack 發行版本(架構如下圖所示)，該專案結合 Kubernetes 與 OpenStack 的技術來達到真正的 Kubernetes 租戶隔離，如租戶實例採用 Frakti 來進行隔離、網路採用 Neutron OVS 達到每個 Namespace 擁有獨立的網路資源等。本篇會簡單介紹如何用 DevStack 建立測試用 Stackube。","text":"Stackube是一個 Kubernetes-centric 的 OpenStack 發行版本(架構如下圖所示)，該專案結合 Kubernetes 與 OpenStack 的技術來達到真正的 Kubernetes 租戶隔離，如租戶實例採用 Frakti 來進行隔離、網路採用 Neutron OVS 達到每個 Namespace 擁有獨立的網路資源等。本篇會簡單介紹如何用 DevStack 建立測試用 Stackube。 P.S. 目前 Stackube 已經不再維護，僅作為測試與研究程式碼使用。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： IP Address Host vCPU RAM 172.22.132.42 stackube1 8 32G 部署 Stackube首先新增 Devstack 使用的 User： $ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack$ sudo su - stack 透過 Git 取得 Ocata 版本的 Devstack： $ git clone https://git.openstack.org/openstack-dev/devstack -b stable/ocata$ cd devstack 取得單節範例設定檔： $ curl -sSL https://raw.githubusercontent.com/kairen/stackube/master/devstack/local.conf.sample -o local.conf 完成後即可進行安裝： $ ./stack.sh 測試基本功能完成後，就可以透過以下指令來引入 Kubernetes 與 OpenStack client 需要的環境變數： $ export KUBECONFIG=/opt/stack/admin.conf$ source /opt/stack/devstack/openrc admin admin Stackube 透過 CRD 新增了一個新抽象物件 Tenant，可以直接透過 Kubernetes API 來建立一個租戶，並將該租戶與 Kubernettes namespace 做綁定： $ cat &lt;&lt;EOF | kubectl create -f -apiVersion: \"stackube.kubernetes.io/v1\"kind: Tenantmetadata: name: testspec: username: \"test\" password: \"password\"EOF$ kubectl get namespace testNAME STATUS AGEtest Active 2h$ kubectl -n test get network test -o yamlapiVersion: stackube.kubernetes.io/v1kind: Networkmetadata: clusterName: \"\" creationTimestamp: 2017-12-20T06:03:33Z generation: 0 name: test namespace: test resourceVersion: \"4631\" selfLink: /apis/stackube.kubernetes.io/v1/namespaces/test/networks/test uid: e9aef6fa-3316-11e8-8b66-448a5bd481f0spec: cidr: 10.244.0.0/16 gateway: 10.244.0.1 networkID: \"\"status: state: Active 檢查 Neutron 網路狀況： $ neutron net-list+--------------------------------------+----------------------+----------------------------------+----------------------------------------------------------+| id | name | tenant_id | subnets |+--------------------------------------+----------------------+----------------------------------+----------------------------------------------------------+| 2a8e3b54-d76f-48a9-8380-7c2a5513b1fe | kube-test-test | f2f25d24fd9a4616bff41b018e8725d2 | 625909a9-6abf-4661-b259-ffc625bdf681 10.244.0.0/16 | P.S. 這邊個人只是研究 Stackube CNI，故不針對其於進行測試，可自行參考 Stackube。","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Openstack","slug":"Openstack","permalink":"https://k2r2bai.com/tags/Openstack/"}]},{"title":"Deploy OpenStack on Kubernetes using OpenStack-helm","slug":"openstack/openstack-helm","date":"2017-11-29T08:23:01.000Z","updated":"2019-12-02T01:49:42.401Z","comments":true,"path":"2017/11/29/openstack/openstack-helm/","link":"","permalink":"https://k2r2bai.com/2017/11/29/openstack/openstack-helm/","excerpt":"OpenStack Helm 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。","text":"OpenStack Helm 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。 而本篇文章將說明如何建置多節點的 OpenStack Helm 環境來進行功能驗證。 節點與安裝版本以下為各節點的硬體資訊。 IP Address Role CPU Memory 172.22.132.10 vip - - 172.22.132.101 master1 4 16G 172.22.132.22 node1 4 16G 172.22.132.24 node2 4 16G 172.22.132.28 node3 4 16G 使用 Kernel、作業系統與軟體版本： 資訊描述 作業系統版本 16.04.3 LTS (Xenial Xerus) Kernel 版本 4.4.0-101-generic Kubernetes v1.8.4 Docker Docker 17.09.0-ce Calico v2.6.2 Etcd v3.2.9 Ceph v10.2.10 Helm v2.7.0 Kubernetes 叢集本節說明如何建立 Kubernetes Cluster，這邊採用 kube-ansible 工具來建立。 初始化與設定基本需求安裝前需要確認以下幾個項目： 所有節點的網路之間可以互相溝通。 部署節點對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝Python。 所有節點需要設定/etc/host解析到所有主機。 部署節點需要安裝 Ansible &gt;= 2.4.0。 # Ubuntu install$ sudo apt-get install -y software-properties-common$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git make# CentOS install$ sudo yum install -y epel-release$ sudo yum -y install ansible cowsay 安裝與設定 Kube-ansible首先取得最新穩定版本的 Kubernetes Ansible: $ git clone https://github.com/kairen/kube-ansible.git$ cd kube-ansible 然後新增inventory檔案來描述要部屬的主機角色: [etcds]172.22.132.101 ansible_user=ubuntu[masters]172.22.132.101 ansible_user=ubuntu[nodes]172.22.132.22 ansible_user=ubuntu172.22.132.24 ansible_user=ubuntu172.22.132.28 ansible_user=ubuntu[kube-cluster:children]mastersnodes[kube-addon:children]masters 接著編輯group_vars/all.yml檔案來添加與修改以下內容： # Kubenrtes version, only support 1.8.0+.kube_version: 1.8.4# CNI plugin# Support: flannel, calico, canal, weave or router.network: calicopod_network_cidr: 10.244.0.0/16# CNI opts: flannel(--iface=enp0s8), calico(interface=enp0s8), canal(enp0s8).cni_iface: \"\"# Kubernetes cluster network.cluster_subnet: 10.96.0kubernetes_service_ip: \"&#123;&#123; cluster_subnet &#125;&#125;.1\"service_ip_range: \"&#123;&#123; cluster_subnet &#125;&#125;.0/12\"service_node_port_range: 30000-32767api_secure_port: 5443# Highly Available configuration.haproxy: truekeepalived: true # set `lb_vip_address` as keepalived vip, if this enable.keepalived_vip_interface: \"&#123;&#123; ansible_default_ipv4.interface &#125;&#125;\"lb_vip_address: 172.22.132.10lb_secure_port: 6443lb_api_url: \"https://&#123;&#123; lb_vip_address &#125;&#125;:&#123;&#123; lb_secure_port &#125;&#125;\"etcd_iface: \"\"insecure_registrys:- \"172.22.132.253:5000\" # 有需要的話ceph_cluster: true 這邊insecure_registrys為 deploy 節點的 Docker registry ip 與 port。 Extra addons 部分針對需求開啟，預設不會開啟。 若想把 Etcd, VIP 與 Network plugin 綁定在指定網路的話，請修改etcd_iface, keepalived_vip_interface 與 cni_iface。其中cni_iface需要針對不同 Plugin 來改變。 若想要修改部署版本的 Packages 的話，請編輯roles/commons/packages/defaults/main.yml來修改版本。 接著由於 OpenStack-helm 使用的 Kubernetes Controller Manager 不同，因此要修改roles/commons/container-images/defaults/main.yml的 Image 來源如下： ... manager: name: kube-controller-manager repos: kairen/ tag: \"v&#123;&#123; kube_version &#125;&#125;\"... 完後成修改 storage roles 設定版本並進行安裝。 首先編輯roles/storage/ceph/defaults/main.yml修改版本為以下： ceph_version: jewel 接著編輯roles/storage/ceph/tasks/main.yml修改成以下內容： ---- name: Install Ceph dependency packages include_tasks: install-ceph.yml# - name: Create and copy generator config file# include_tasks: gen-config.yml# delegate_to: \"&#123;&#123; groups['masters'][0] &#125;&#125;\"# run_once: true## - name: Deploy Ceph components on Kubernetes# include_tasks: ceph-on-k8s.yml# delegate_to: \"&#123;&#123; groups['masters'][0] &#125;&#125;\"# run_once: true# - name: Label all storage nodes# shell: \"kubectl label nodes node-type=storage\"# delegate_to: \"&#123;&#123; groups['masters'][0] &#125;&#125;\"# run_once: true# ignore_errors: true 部屬 Kubernetes 叢集確認group_vars/all.yml與其他設定都完成後，就透過 ansible ping 來檢查叢集狀態： $ ansible -i inventory all -m ping...172.22.132.101 | SUCCESS =&gt; &#123; \"changed\": false, \"failed\": false, \"ping\": \"pong\"&#125;... 接著就可以透過以下指令進行部署叢集： $ ansible-playbook cluster.yml...TASK [cni : Apply calico network daemonset] *********************************************************************************************************************************changed: [172.22.132.101 -&gt; 172.22.132.101]PLAY RECAP ******************************************************************************************************************************************************************172.22.132.101 : ok=155 changed=58 unreachable=0 failed=0172.22.132.22 : ok=117 changed=28 unreachable=0 failed=0172.22.132.24 : ok=50 changed=18 unreachable=0 failed=0172.22.132.28 : ok=51 changed=19 unreachable=0 failed=0 完成後，進入master節點執行以下指令確認叢集： $ kubectl get nodeNAME STATUS ROLES AGE VERSIONkube-master1 Ready master 1h v1.8.4kube-node1 Ready &lt;none&gt; 1h v1.8.4kube-node2 Ready &lt;none&gt; 1h v1.8.4kube-node3 Ready &lt;none&gt; 1h v1.8.4$ kubectl -n kube-system get poNAME READY STATUS RESTARTS AGEcalico-node-js6qp 2/2 Running 2 1hcalico-node-kx9xn 2/2 Running 2 1hcalico-node-lxrjl 2/2 Running 2 1hcalico-node-vwn5f 2/2 Running 2 1hcalico-policy-controller-d549764f6-9kn9l 1/1 Running 1 1hhaproxy-kube-master1 1/1 Running 1 1hkeepalived-kube-master1 1/1 Running 1 1hkube-apiserver-kube-master1 1/1 Running 1 1hkube-controller-manager-kube-master1 1/1 Running 1 1hkube-dns-7bd4879dc9-kxmx6 3/3 Running 3 1hkube-proxy-7tqkm 1/1 Running 1 1hkube-proxy-glzmm 1/1 Running 1 1hkube-proxy-krqxs 1/1 Running 1 1hkube-proxy-x9zdb 1/1 Running 1 1hkube-scheduler-kube-master1 1/1 Running 1 1h 檢查 kube-dns 是否連 host 都能夠解析: $ nslookup kubernetesServer: 10.96.0.10Address: 10.96.0.10#53Non-authoritative answer:Name: kubernetes.default.svc.cluster.localAddress: 10.96.0.1 接著安裝 Ceph 套件： $ ansible-playbook storage.yml OpenStack-helm 叢集本節說明如何建立 OpenStack on Kubernetes 使用 Helm，部署是使用 openstack-helm。過程將透過 OpenStack-helm 來在 Kubernetes 建置 OpenStack 叢集。以下所有操作都在kube-master1上進行。 Helm init在開始前需要先將 Helm 進行初始化，以提供後續使用，然而這邊由於使用到 RBAC 的關係，因此需建立一個 Service account 來提供給 Helm 使用： $ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller 由於 kube-ansible 本身包含 Helm 工具, 因此不需要自己安裝，只需要依據上面指令進行 init 即可。 新增一個檔案openrc來提供環境變數： export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk '/Endpoints/&#123;print $2&#125;')export OSD_CLUSTER_NETWORK=172.22.132.0/24export OSD_PUBLIC_NETWORK=172.22.132.0/24export WORK_DIR=localexport CEPH_RGW_KEYSTONE_ENABLED=true OSD_CLUSTER_NETWORK與OSD_PUBLIC_NETWORK都是使用實體機器網路，這邊 daemonset 會使用 hostNetwork。 CEPH_RGW_KEYSTONE_ENABLED 在 Kubernetes 版本有點不穩，可依需求關閉。 完成後，透過 source 指令引入: $ source openrc$ helm versionClient: &amp;version.Version&#123;SemVer:\"v2.7.0\", GitCommit:\"08c1144f5eb3e3b636d9775617287cc26e53dba4\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.7.0\", GitCommit:\"08c1144f5eb3e3b636d9775617287cc26e53dba4\", GitTreeState:\"clean\"&#125; 事前準備首先透過 Kubernetes label 來標示每個節點的角色： kubectl label nodes openstack-control-plane=enabled --allkubectl label nodes ceph-mon=enabled --allkubectl label nodes ceph-osd=enabled --allkubectl label nodes ceph-mds=enabled --allkubectl label nodes ceph-rgw=enabled --allkubectl label nodes ceph-mgr=enabled --allkubectl label nodes openvswitch=enabled --allkubectl label nodes openstack-compute-node=enabled --all 這邊為了避免過度的節點污染，因此不讓 masters 充當任何角色： kubectl label nodes kube-master1 openstack-control-plane-kubectl label nodes kube-master1 ceph-mon-kubectl label nodes kube-master1 ceph-osd-kubectl label nodes kube-master1 ceph-mds-kubectl label nodes kube-master1 ceph-rgw-kubectl label nodes kube-master1 ceph-mgr-kubectl label nodes kube-master1 openvswitch-kubectl label nodes kube-master1 openstack-compute-node- 由於使用 Kubernetes RBAC，而目前 openstack-helm 有 bug，不會正確建立 Service account 的 ClusterRoleBindings，因此要手動建立(這邊偷懶一下直接使用 Admin roles)： $ cat &lt;&lt;EOF | kubectl create -f -apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: ceph-sa-adminroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:serviceaccount:ceph:defaultEOF$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: openstack-sa-adminroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:serviceaccount:openstack:defaultEOF 若沒有建立的話，會有類似以下的錯誤資訊： Error from server (Forbidden): error when creating &quot;STDIN&quot;: secrets is forbidden: User &quot;system:serviceaccount:ceph:default&quot; cannot create secrets in the namespace &quot;ceph&quot; 下載最新版本 openstack-helm 專案： $ git clone https://github.com/openstack/openstack-helm.git$ cd openstack-helm 現在須建立 openstack-helm chart 來提供部署使用： $ helm serve &amp;$ helm repo add local http://localhost:8879/charts$ make# output...1 chart(s) linted, no failuresif [ -d congress ]; then helm package congress; fiSuccessfully packaged chart and saved it to: /root/openstack-helm/congress-0.1.0.tgzmake[1]: Leaving directory '/root/openstack-helm' Ceph Chart在部署 OpenStack 前，需要先部署 Ceph 叢集，這邊透過以下指令建置： $ helm install --namespace=ceph $&#123;WORK_DIR&#125;/ceph --name=ceph \\ --set endpoints.identity.namespace=openstack \\ --set endpoints.object_store.namespace=ceph \\ --set endpoints.ceph_mon.namespace=ceph \\ --set ceph.rgw_keystone_auth=$&#123;CEPH_RGW_KEYSTONE_ENABLED&#125; \\ --set network.public=$&#123;OSD_PUBLIC_NETWORK&#125; \\ --set network.cluster=$&#123;OSD_CLUSTER_NETWORK&#125; \\ --set deployment.storage_secrets=true \\ --set deployment.ceph=true \\ --set deployment.rbd_provisioner=true \\ --set deployment.client_secrets=false \\ --set deployment.rgw_keystone_user_and_endpoints=false \\ --set bootstrap.enabled=true CEPH_RGW_KEYSTONE_ENABLED是否啟動 Ceph RGW Keystone。 OSD_PUBLIC_NETWORK與OSD_PUBLIC_NETWORK為 Ceph 叢集網路。 成功安裝 Ceph chart 後，就可以透過 kubectl 來查看結果： $ kubectl -n ceph get poNAME READY STATUS RESTARTS AGEceph-mds-57798cc8f6-r898r 1/1 Running 2 10minceph-mon-96p9r 1/1 Running 0 10minceph-mon-check-bd8875f87-whvhd 1/1 Running 0 10minceph-mon-qkj95 1/1 Running 0 10minceph-mon-zx7tw 1/1 Running 0 10minceph-osd-5fvfl 1/1 Running 0 10minceph-osd-kvw9b 1/1 Running 0 10minceph-osd-wcf5j 1/1 Running 0 10minceph-rbd-provisioner-599ff9575-mdqnf 1/1 Running 0 10minceph-rbd-provisioner-599ff9575-vpcr6 1/1 Running 0 10minceph-rgw-7c8c5d4f6f-8fq9c 1/1 Running 3 10min 確認 Ceph 叢集建立正確： $ MON_POD=$(kubectl get pods \\ --namespace=ceph \\ --selector=\"application=ceph\" \\ --selector=\"component=mon\" \\ --no-headers | awk '&#123; print $1; exit &#125;')$ kubectl exec -n ceph $&#123;MON_POD&#125; -- ceph -s cluster 02ad8724-dee0-4f55-829f-3cc24e2c7571 health HEALTH_WARN too many PGs per OSD (856 &gt; max 300) monmap e2: 3 mons at &#123;kube-node1=172.22.132.22:6789/0,kube-node2=172.22.132.24:6789/0,kube-node3=172.22.132.28:6789/0&#125; election epoch 8, quorum 0,1,2 kube-node1,kube-node2,kube-node3 fsmap e5: 1/1/1 up &#123;0=mds-ceph-mds-57798cc8f6-r898r=up:active&#125; osdmap e21: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v6053: 856 pgs, 10 pools, 3656 bytes data, 191 objects 43091 MB used, 2133 GB / 2291 GB avail 856 active+clean Warn 這邊忽略，OSD 機器太少….。 接著為了讓 Ceph 可以在其他 Kubernetes namespace 中存取 PVC，這邊要產生 client secret key 於 openstack namespace 中來提供給 OpenStack 元件使用，這邊執行以下 Chart 來產生： $ helm install --namespace=openstack $&#123;WORK_DIR&#125;/ceph --name=ceph-openstack-config \\ --set endpoints.identity.namespace=openstack \\ --set endpoints.object_store.namespace=ceph \\ --set endpoints.ceph_mon.namespace=ceph \\ --set ceph.rgw_keystone_auth=$&#123;CEPH_RGW_KEYSTONE_ENABLED&#125; \\ --set network.public=$&#123;OSD_PUBLIC_NETWORK&#125; \\ --set network.cluster=$&#123;OSD_CLUSTER_NETWORK&#125; \\ --set deployment.storage_secrets=false \\ --set deployment.ceph=false \\ --set deployment.rbd_provisioner=false \\ --set deployment.client_secrets=true \\ --set deployment.rgw_keystone_user_and_endpoints=false 檢查 pod 與 secret 是否建立成功： $ kubectl -n openstack get secret,po -aNAME TYPE DATA AGEsecrets/default-token-q2r87 kubernetes.io/service-account-token 3 2msecrets/pvc-ceph-client-key kubernetes.io/rbd 1 2mNAME READY STATUS RESTARTS AGEpo/ceph-namespace-client-key-generator-w84n4 0/1 Completed 0 2m OpenStack Chart確認沒問題後，就可以開始部署 OpenStack chart 了。首先先安裝 Mariadb cluster: $ helm install --name=mariadb ./mariadb --namespace=openstack 這邊跑超久…34mins…，原因可能是 Storage 效能問題。 這邊正確執行後，會依序依據 StatefulSet 建立起 Pod 組成 Cluster： $ kubectl -n openstack get poNAME READY STATUS RESTARTS AGEmariadb-0 1/1 Running 0 37mmariadb-1 1/1 Running 0 4mmariadb-2 1/1 Running 0 2m 當 Mariadb cluster 完成後，就可以部署一些需要的服務，如 RabbitMQ, OVS 等： helm install --name=memcached ./memcached --namespace=openstackhelm install --name=etcd-rabbitmq ./etcd --namespace=openstackhelm install --name=rabbitmq ./rabbitmq --namespace=openstackhelm install --name=ingress ./ingress --namespace=openstackhelm install --name=libvirt ./libvirt --namespace=openstackhelm install --name=openvswitch ./openvswitch --namespace=openstack 上述指令若正確執行的話，會分別建立起以下服務： $ kubectl -n openstack get poNAME READY STATUS RESTARTS AGEetcd-5c9bc8c97f-jpm2k 1/1 Running 0 4mingress-api-jhjjv 1/1 Running 0 4mingress-api-nx5qm 1/1 Running 0 4mingress-api-vr8xf 1/1 Running 0 4mingress-error-pages-86b9db69cc-mmq4p 1/1 Running 0 4mlibvirt-94xq5 1/1 Running 0 4mlibvirt-lzfzs 1/1 Running 0 4mlibvirt-vswxb 1/1 Running 0 4mmariadb-0 1/1 Running 0 42mmariadb-1 1/1 Running 0 9mmariadb-2 1/1 Running 0 7mmemcached-746fcc894-cwhpr 1/1 Running 0 4mopenvswitch-db-7fjr2 1/1 Running 0 4mopenvswitch-db-gtmcr 1/1 Running 0 4mopenvswitch-db-hqmbt 1/1 Running 0 4mopenvswitch-vswitchd-gptp9 1/1 Running 0 4mopenvswitch-vswitchd-s4cwd 1/1 Running 0 4mopenvswitch-vswitchd-tvxlg 1/1 Running 0 4mrabbitmq-6fdb8879df-6vmz8 1/1 Running 0 4mrabbitmq-6fdb8879df-875zz 1/1 Running 0 4mrabbitmq-6fdb8879df-h5wj6 1/1 Running 0 4m 一旦所有基礎服務與元件都建立完成後，就可以開始部署 OpenStack 的專案 Chart，首先建立 Keystone 來提供身份認證服務： $ helm install --namespace=openstack --name=keystone ./keystone \\ --set pod.replicas.api=1$ kubectl -n openstack get po -l application=keystoneNAME READY STATUS RESTARTS AGEkeystone-api-74c774d448-dkqmj 0/1 Init:0/1 0 4mkeystone-bootstrap-xpdtl 0/1 Init:0/1 0 4mkeystone-db-sync-2bxtp 1/1 Running 0 4m 0 29s 這邊由於叢集規模問題，副本數都為一份。 這時候會先建立 Keystone database tables，完成後將啟動 API pod，如以下結果： $ kubectl -n openstack get po -l application=keystoneNAME READY STATUS RESTARTS AGEkeystone-api-74c774d448-dkqmj 1/1 Running 0 11m 如果安裝支援 RGW 的 Keystone endpoint 的話，可以使用以下方式建立： $ helm install --namespace=openstack $&#123;WORK_DIR&#125;/ceph --name=radosgw-openstack \\ --set endpoints.identity.namespace=openstack \\ --set endpoints.object_store.namespace=ceph \\ --set endpoints.ceph_mon.namespace=ceph \\ --set ceph.rgw_keystone_auth=$&#123;CEPH_RGW_KEYSTONE_ENABLED&#125; \\ --set network.public=$&#123;OSD_PUBLIC_NETWORK&#125; \\ --set network.cluster=$&#123;OSD_CLUSTER_NETWORK&#125; \\ --set deployment.storage_secrets=false \\ --set deployment.ceph=false \\ --set deployment.rbd_provisioner=false \\ --set deployment.client_secrets=false \\ --set deployment.rgw_keystone_user_and_endpoints=true$ kubectl -n openstack get po -a -l application=cephNAME READY STATUS RESTARTS AGEceph-ks-endpoints-vfg4l 0/3 Completed 0 1mceph-ks-service-tr9xt 0/1 Completed 0 1mceph-ks-user-z5tlt 0/1 Completed 0 1m 完成後，安裝 Horizon chart 來提供 OpenStack dashbaord： $ helm install --namespace=openstack --name=horizon ./horizon \\ --set network.enable_node_port=true \\ --set network.node_port=31000$ kubectl -n openstack get po -l application=horizonNAME READY STATUS RESTARTS AGEhorizon-7c54878549-45668 1/1 Running 0 3m 接著安裝 Glance chart 來提供 OpenStack image service。目前 Glance 支援幾個 backend storage: pvc: 一個簡單的 Kubernetes PVCs 檔案後端。 rbd: 使用 Ceph RBD 來儲存 images。 radosgw: 使用 Ceph RGW 來儲存 images。 swift: 另用 OpenStack switf 所提供的物件儲存服務來儲存 images. 這邊可以利用以下方式來部署不同的儲存後端： $ export GLANCE_BACKEND=radosgw$ helm install --namespace=openstack --name=glance ./glance \\ --set pod.replicas.api=1 \\ --set pod.replicas.registry=1 \\ --set storage=$&#123;GLANCE_BACKEND&#125;$ kubectl -n openstack get po -l application=glanceNAME READY STATUS RESTARTS AGEglance-api-6cd8b856d6-lhzfs 1/1 Running 0 14mglance-registry-599f8b857b-gt4c6 1/1 Running 0 14m 接著安裝 Neutron chart 來提供 OpenStack 虛擬化網路服務： $ helm install --namespace=openstack --name=neutron ./neutron \\ --set pod.replicas.server=1$ kubectl -n openstack get po -l application=neutronNAME READY STATUS RESTARTS AGEneutron-dhcp-agent-2z49d 1/1 Running 0 9hneutron-dhcp-agent-d2kn8 1/1 Running 0 9hneutron-dhcp-agent-mrstl 1/1 Running 0 9hneutron-l3-agent-9f9mw 1/1 Running 0 9hneutron-l3-agent-cshzw 1/1 Running 0 9hneutron-l3-agent-j5vb9 1/1 Running 0 9hneutron-metadata-agent-6bfb2 1/1 Running 0 9hneutron-metadata-agent-kxk9c 1/1 Running 0 9hneutron-metadata-agent-w8cnl 1/1 Running 0 9hneutron-ovs-agent-j2549 1/1 Running 0 9hneutron-ovs-agent-plj9t 1/1 Running 0 9hneutron-ovs-agent-xlx7z 1/1 Running 0 9hneutron-server-6f45d74b87-6wmck 1/1 Running 0 9h 接著安裝 Nova chart 來提供 OpenStack 虛擬機運算服務: $ helm install --namespace=openstack --name=nova ./nova \\ --set pod.replicas.api_metadata=1 \\ --set pod.replicas.osapi=1 \\ --set pod.replicas.conductor=1 \\ --set pod.replicas.consoleauth=1 \\ --set pod.replicas.scheduler=1 \\ --set pod.replicas.novncproxy=1$ kubectl -n openstack get po -l application=novaNAME READY STATUS RESTARTS AGEnova-api-metadata-84fdc84fd7-ldzrh 1/1 Running 1 9hnova-api-osapi-57f599c6d6-pqrjv 1/1 Running 0 9hnova-compute-8rvm9 2/2 Running 0 9hnova-compute-cbk7h 2/2 Running 0 9hnova-compute-tf2jb 2/2 Running 0 9hnova-conductor-7f5bc76d79-bxwnb 1/1 Running 0 9hnova-consoleauth-6946b5884f-nss6n 1/1 Running 0 9hnova-novncproxy-d789dccff-7ft9q 1/1 Running 0 9hnova-placement-api-f7c79578f-hj2g9 1/1 Running 0 9hnova-scheduler-778866f555-mmksg 1/1 Running 0 9h 接著安裝 Cinfer chart 來提供 OpenStack 區塊儲存服務: $ helm install --namespace=openstack --name=cinder ./cinder \\ --set pod.replicas.api=1$ kubectl -n openstack get po -l application=cinderNAME READY STATUS RESTARTS AGEcinder-api-5cc89f5467-ssm8k 1/1 Running 0 32mcinder-backup-67c4d8dfdb-zfsq4 1/1 Running 0 32mcinder-scheduler-65f9dd49bf-6htwg 1/1 Running 0 32mcinder-volume-69bfb67b4-bmst2 1/1 Running 0 32m (option)都完成後，將 Horizon 服務透過 NodePort 方式曝露出來(如果上面 Horizon chart 沒反應的話)，執行以下指令編輯： $ kubectl -n openstack edit svc horizon-int# 修改 type: type: NodePort 最後連接 Horizon Dashboard，預設使用者為admin/password。 其他 Chart 可以利用以下方式來安裝，如 Heat chart： $ helm install --namespace=openstack --name=heat ./heat$ kubectl -n openstack get po -l application=heatNAME READY STATUS RESTARTS AGEheat-api-5cf45d9d44-qrt69 1/1 Running 0 13mheat-cfn-79dbf55789-bq4wh 1/1 Running 0 13mheat-cloudwatch-bcc4647f4-4c4ln 1/1 Running 0 13mheat-engine-55cfcc86f8-cct4m 1/1 Running 0 13m 測試 OpenStack 功能在kube-master1安裝 openstack client: $ sudo pip install python-openstackclient 建立adminrc來提供 client 環境變數： export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=passwordexport OS_AUTH_URL=http://keystone.openstack.svc.cluster.local:80/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 引入環境變數，並透過 openstack client 測試： $ source adminrc$ openstack user list+----------------------------------+-----------+| ID | Name |+----------------------------------+-----------+| 42f0d2e7823e413cb469f9cce731398a | glance || 556a2744811f450098f64b37d34192d4 | nova || a97ec73724aa4445b2d575be54f23240 | cinder || b28a5dcfd18948419e14acba7ecf6f63 | swift || d1f312b6bb7c460eb7d8d78c8bf350fc | admin || dc326aace22c4314a0100865fe4f57c2 | neutron || ec5d6d3c529847b29a1c9187599c8a6b | placement |+----------------------------------+-----------+ 接著需要設定對外網路來提供給 VM 存取，在有neutron-l3-agent節點上，新增一個腳本setup-gateway.sh： #!/bin/bashset -x# Assign IP address to br-exOSH_BR_EX_ADDR=\"172.24.4.1/24\"OSH_EXT_SUBNET=\"172.24.4.0/24\"sudo ip addr add $&#123;OSH_BR_EX_ADDR&#125; dev br-exsudo ip link set br-ex up# Setup masquerading on default route dev to public subnetDEFAULT_ROUTE_DEV=\"enp3s0\"sudo iptables -t nat -A POSTROUTING -o $&#123;DEFAULT_ROUTE_DEV&#125; -s $&#123;OSH_EXT_SUBNET&#125; -j MASQUERADE 網卡記得修改DEFAULT_ROUTE_DEV。 這邊因為沒有額外提供其他張網卡，所以先用 bridge 處理。 然後透過執行該腳本建立一個 bridge 網路： $ chmod u+x setup-gateway.sh$ ./setup-gateway.sh 確認完成後，接著建立 Neutron ext net，透過以下指令進行建立： $ openstack network create \\ --share --external \\ --provider-physical-network external \\ --provider-network-type flat ext-net$ openstack subnet create --network ext-net \\ --allocation-pool start=172.24.4.10,end=172.24.4.100 \\ --dns-nameserver 8.8.8.8 --gateway 172.24.4.1 \\ --subnet-range 172.24.4.0/24 \\ --no-dhcp ext-subnet$ openstack router create router1$ neutron router-gateway-set router1 ext-net 直接進入 Dashboard 新增 Self-service Network: 加入到 router1: 完成後，就可以建立 instance，這邊都透過 Dashboard 來操作： 透過 SSH 進入 instance： Refers sydney-workshop Multi Node","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Openstack","slug":"Openstack","permalink":"https://k2r2bai.com/tags/Openstack/"},{"name":"Helm","slug":"Helm","permalink":"https://k2r2bai.com/tags/Helm/"}]},{"title":"利用 Kuryr 整合 OpenStack 與 Kubernetes 網路","slug":"openstack/kuryr-kubernetes","date":"2017-08-29T08:23:01.000Z","updated":"2019-12-02T01:49:42.400Z","comments":true,"path":"2017/08/29/openstack/kuryr-kubernetes/","link":"","permalink":"https://k2r2bai.com/2017/08/29/openstack/kuryr-kubernetes/","excerpt":"Kubernetes Kuryr 是 OpenStack Neutron 的子專案，其主要目標是透過該專案來整合 OpenStack 與 Kubernetes 的網路。該專案在 Kubernetes 中實作了原生 Neutron-based 的網路，因此使用 Kuryr-Kubernetes 可以讓你的 OpenStack VM 與 Kubernetes Pods 能夠選擇在同一個子網路上運作，並且能夠使用 Neutron 的 L3 與 Security Group 來對網路進行路由，以及阻擋特定來源 Port。","text":"Kubernetes Kuryr 是 OpenStack Neutron 的子專案，其主要目標是透過該專案來整合 OpenStack 與 Kubernetes 的網路。該專案在 Kubernetes 中實作了原生 Neutron-based 的網路，因此使用 Kuryr-Kubernetes 可以讓你的 OpenStack VM 與 Kubernetes Pods 能夠選擇在同一個子網路上運作，並且能夠使用 Neutron 的 L3 與 Security Group 來對網路進行路由，以及阻擋特定來源 Port。 Kuryr-Kubernetes 整合有兩個主要組成部分： Kuryr Controller: Controller 主要目的是監控 Kubernetes API 來獲取 Kubernetes 資源的變化，然後依據 Kubernetes 資源的需求來執行子資源的分配和資源管理。 Kuryr CNI：主要是依據 Kuryr Controller 分配的資源來綁定網路至 Pods 上。 本篇我們將說明如何利用DevStack與Kubespray建立一個簡單的測試環境。 環境資源與事前準備準備兩台實體機器，這邊測試的作業系統為CentOS 7.x，該環境將在扁平(flat)的網路下進行。 IP Address 1 Role 172.24.0.34 controller, k8s-master 172.24.0.80 compute, k8s-node1 更新每台節點的 CentOS 7.x packages: $ sudo yum --enablerepo=cr update -y 然後關閉 firewalld 以及 SELinux 來避免實現發生問題： $ sudo setenforce 0$ sudo systemctl disable firewalld &amp;&amp; sudo systemctl stop firewalld OpenStack Controller 安裝首先進入172.24.0.34（controller），並且執行以下指令。 然後執行以下指令來建立 DevStack 專用使用者： $ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack 選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。 接著切換至該使用者環境來建立 OpenStack： $ sudo su - stack 下載 DevStack 安裝套件： $ git clone https://git.openstack.org/openstack-dev/devstack$ cd devstack 新增local.conf檔案，來描述部署資訊： [[local|localrc]]HOST_IP=172.24.0.34GIT_BASE=https://github.comADMIN_PASSWORD=passwdDATABASE_PASSWORD=passwdRABBIT_PASSWORD=passwdSERVICE_PASSWORD=passwdSERVICE_TOKEN=passwdMULTI_HOST=1 [color=#fc9fca]Tips:修改 HOST_IP 為自己的 IP 位置。 完成後，執行以下指令開始部署： $ ./stack.sh Openstack Compute 安裝進入到172.24.0.80（compute），並且執行以下指令。 然後執行以下指令來建立 DevStack 專用使用者： $ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack 選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。 接著切換至該使用者環境來建立 OpenStack： $ sudo su - stack 下載 DevStack 安裝套件： $ git clone https://git.openstack.org/openstack-dev/devstack$ cd devstack 新增local.conf檔案，來描述部署資訊： [[local|localrc]]HOST_IP=172.24.0.80GIT_BASE=https://github.comMULTI_HOST=1LOGFILE=/opt/stack/logs/stack.sh.logADMIN_PASSWORD=passwdDATABASE_PASSWORD=passwdRABBIT_PASSWORD=passwdSERVICE_PASSWORD=passwdDATABASE_TYPE=mysqlSERVICE_HOST=172.24.0.34MYSQL_HOST=$SERVICE_HOSTRABBIT_HOST=$SERVICE_HOSTGLANCE_HOSTPORT=$SERVICE_HOST:9292ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-clientNOVA_VNC_ENABLED=TrueNOVNCPROXY_URL=&quot;http://$SERVICE_HOST:6080/vnc_auto.html&quot;VNCSERVER_LISTEN=$HOST_IPVNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN Tips:修改 HOST_IP 為自己的主機位置。修改 SERVICE_HOST 為 Master 的IP位置。 完成後，執行以下指令開始部署： $ ./stack.sh 建立 Kubernetes 叢集環境首先確認所有節點之間不需要 SSH 密碼即可登入，接著進入到172.24.0.34（k8s-master）並且執行以下指令。 接著安裝所需要的套件： $ sudo yum -y install software-properties-common ansible git gcc python-pip python-devel libffi-devel openssl-devel$ sudo pip install -U kubespray 完成後，新增 kubespray 設定檔： $ cat &lt;&lt;EOF &gt; ~/.kubespray.ymlkubespray_git_repo: \"https://github.com/kubernetes-incubator/kubespray.git\"# Logging optionsloglevel: \"info\"EOF 然後利用 kubespray-cli 快速產生環境的inventory檔，並修改部分內容： $ sudo -i$ kubespray prepare --masters master --etcds master --nodes node1 編輯/root/.kubespray/inventory/inventory.cfg，修改以下內容： [all]master ansible_host=172.24.0.34 ansible_user=root ip=172.24.0.34node1 ansible_host=172.24.0.80 ansible_user=root ip=172.24.0.80[kube-master]master[kube-node]masternode1[etcd]master[k8s-cluster:children]kube-node1kube-master 完成後，即可利用 kubespray-cli 指令來進行部署： $ kubespray deploy --verbose -u root -k .ssh/id_rsa -n calico 經過一段時間後就會部署完成，這時候檢查節點是否正常： $ kubectl get noNAME STATUS AGE VERSIONmaster Ready,master 2m v1.7.4node1 Ready 2m v1.7.4 接著為了方便讓 Kuryr Controller 簡單取得 K8s API Server，這邊修改/etc/kubernetes/manifests/kube-apiserver.yml檔案，加入以下內容： - &quot;--insecure-bind-address=0.0.0.0&quot;- &quot;--insecure-port=8080&quot; Tips:將 insecure 綁定到 0.0.0.0 之上，以及開啟 8080 Port。 安裝 Openstack Kuryr進入到172.24.0.34（controller），並且執行以下指令。 首先在節點安裝所需要的套件： $ sudo yum -y install gcc libffi-devel python-devel openssl-devel install python-pip 然後下載 kuryr-kubernetes 並進行安裝： $ git clone http://git.openstack.org/openstack/kuryr-kubernetes$ pip install -e kuryr-kubernetes 新增kuryr.conf至/etc/kuryr目錄： $ cd kuryr-kubernetes$ ./tools/generate_config_file_samples.sh$ sudo mkdir -p /etc/kuryr/$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf 接著使用 OpenStack Dashboard 建立相關專案，在瀏覽器輸入Dashboard，並執行以下步驟。 新增 K8s project。 修改 K8s project member 加入到 service project。 在該 Project 中新增 Security Groups，參考 kuryr-kubernetes manually。 在該 Project 中新增 pod_subnet 子網路。 在該 Project 中新增 service_subnet 子網路。 完成後，修改/etc/kuryr/kuryr.conf檔案，加入以下內容： [DEFAULT]use_stderr = truebindir = /usr/local/libexec/kuryr[kubernetes]api_root = http://172.24.0.34:8080[neutron]auth_url = http://172.24.0.34/identityusername = adminuser_domain_name = Defaultpassword = adminproject_name = serviceproject_domain_name = Defaultauth_type = password[neutron_defaults]ovs_bridge = br-intpod_security_groups = &#123;id_of_secuirity_group_for_pods&#125;pod_subnet = &#123;id_of_subnet_for_pods&#125;project = &#123;id_of_project&#125;service_subnet = &#123;id_of_subnet_for_k8s_services&#125; 完成後執行 kuryr-k8s-controller： $ kuryr-k8s-controller --config-file /etc/kuryr/kuryr.conf 安裝 Kuryr-CNI進入到172.24.0.80（node1）並且執行以下指令。 首先在節點安裝所需要的套件： $ sudo yum -y install gcc libffi-devel python-devel openssl-devel python-pip 然後安裝 Kuryr-CNI 來提供給 kubelet 使用： $ git clone http://git.openstack.org/openstack/kuryr-kubernetes$ sudo pip install -e kuryr-kubernetes 新增kuryr.conf至/etc/kuryr目錄： $ cd kuryr-kubernetes$ ./tools/generate_config_file_samples.sh$ sudo mkdir -p /etc/kuryr/$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf 修改/etc/kuryr/kuryr.conf檔案，加入以下內容： [DEFAULT]use_stderr = truebindir = /usr/local/libexec/kuryr[kubernetes]api_root = http://172.24.0.34:8080 建立 CNI bin 與 Conf 目錄： $ sudo mkdir -p /opt/cni/bin$ sudo ln -s $(which kuryr-cni) /opt/cni/bin/$ sudo mkdir -p /etc/cni/net.d/ 新增/etc/cni/net.d/10-kuryr.conf CNI 設定檔： &#123; &quot;cniVersion&quot;: &quot;0.3.0&quot;, &quot;name&quot;: &quot;kuryr&quot;, &quot;type&quot;: &quot;kuryr-cni&quot;, &quot;kuryr_conf&quot;: &quot;/etc/kuryr/kuryr.conf&quot;, &quot;debug&quot;: true&#125; 完成後，更新 oslo 與 vif python 函式庫： $ sudo pip install 'oslo.privsep&gt;=1.20.0' 'os-vif&gt;=1.5.0' 最後重新啟動相關服務： $ sudo systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service 測試結果我們這邊開一個 Pod 與 OpenStack VM 來進行溝通：","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"}]},{"title":"利用 OpenStack Ironic 提供裸機部署服務","slug":"openstack/ironic","date":"2017-08-16T08:23:01.000Z","updated":"2019-12-02T01:49:42.400Z","comments":true,"path":"2017/08/16/openstack/ironic/","link":"","permalink":"https://k2r2bai.com/2017/08/16/openstack/ironic/","excerpt":"Ironic 是 OpenStack 專案之一，主要目的是提供裸機機器部署服務(Bare-metal service)。它能夠單獨或整合 OpenStack 其他服務被使用，而可整合服務包含 Keystone、Nova、Neutron、Glance 與 Swift 等核心服務。當使用 Compute 與 Network 服務對 Bare-metal 進行適當的配置時，OpenStack 可以透過 Compute API 同時部署虛擬機(Virtual machines)與裸機(Bare machines)。 本篇為了精簡安裝過程，故這邊不採用手動安裝教學(會在 Gitbook 書上更新)，因此採用 DevStack 來部署服務，再手動設定一些步驟。 本環境安裝資訊： OpenStack Pike DevStack Pike Pike Pike Pike ….","text":"Ironic 是 OpenStack 專案之一，主要目的是提供裸機機器部署服務(Bare-metal service)。它能夠單獨或整合 OpenStack 其他服務被使用，而可整合服務包含 Keystone、Nova、Neutron、Glance 與 Swift 等核心服務。當使用 Compute 與 Network 服務對 Bare-metal 進行適當的配置時，OpenStack 可以透過 Compute API 同時部署虛擬機(Virtual machines)與裸機(Bare machines)。 本篇為了精簡安裝過程，故這邊不採用手動安裝教學(會在 Gitbook 書上更新)，因此採用 DevStack 來部署服務，再手動設定一些步驟。 本環境安裝資訊： OpenStack Pike DevStack Pike Pike Pike Pike …. P.S. 這邊因為我的 Manage net 已經有 MAAS 的服務，所以才用其他張網卡進行部署。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體主機： Role CPU Memory controller 4 16G bare-node1 4 16G 這邊 controller 為主要控制節點，將安裝大部分 OpenStack 服務。而 bare-node 為被用來做裸機部署的機器。 網卡若是實體主機，請設定為固定 IP，如以下： auto eth0iface eth0 inet static address 172.20.3.93/24 gateway 172.20.3.1 dns-nameservers 8.8.8.8 若想修改主機的網卡名稱，可以編輯/etc/udev/rules.d/70-persistent-net.rules。 其中controller的eth2需設定為以下： auto &lt;ethx&gt;iface &lt;ethx&gt; inet manual up ip link set dev $IFACE up down ip link set dev $IFACE down 事前準備安裝前需要確認叢集滿足以下幾點： 確認所有節點網路可以溝通。 Bare-node IPMI 設定完成。包含 Address、User 與 Password。 修改 Controller 的 /etc/apt/sources.list，使用tw.archive.ubuntu.com。 安裝 OpenStack 服務這邊採用 DevStack 來部署測試環境，首先透過以下指令取得 DevStack： $ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack$ sudo su - stack$ git clone https://git.openstack.org/openstack-dev/devstack$ cd devstack 接著撰寫 local.conf 來描述部署過程所需的服務： $ wget https://kairen.github.io/files/devstack/ironic-local.conf -O local.conf$ sed -i 's/HOST_IP=.*/HOST_IP=172.22.132.93/g' local.conf HOST_IP請更換為自己環境 IP。有其他 Driver 請記得加入。 完成後執行部署腳本進行建置： $ ./stack.sh 大約經過 15 min 就可以完成整個環境安裝。 測試 OpenStack 環境： $ source openrc admin$ openstack user list+----------------------------------+----------------+| ID | Name |+----------------------------------+----------------+| 3ba4e813270e4e98ad781f4103284e0d | demo || 40c6014bc18f407fbfbc22aadedb1ca0 | placement || 567156ad1c7b4ccdbcd4ea02e7c44ce3 | alt_demo || 7a22ce5036614993a707dd976c505ccd | swift || 8d392f051afe45008289abca4dadf3ca | swiftusertest1 || a6e616af3bf04611bc23625e71a22e64 | swiftusertest4 || a835f1674648427396a7c6ac7e5eef06 | neutron || b2bf73ef2eaa425c93e4f552e9266056 | swiftusertest2 || b7de1af8522b495c8a9fb743eb6e7f59 | nova || cada5913a03e4f2794066902144264d3 | admin || f03e39680b234474b139d00c3fbca989 | swiftusertest3 || f0a4033463f64c00858ff05525545b6d | glance-swift || f2a1b186e7e84b10ae7e8f810e5c2412 | glance || ff31787d136f4fba96c19af419b8559c | ironic |+----------------------------------+----------------+ 測試 ironic 是否正常運行： $ ironic driver-list+---------------------+----------------+| Supported driver(s) | Active host(s) |+---------------------+----------------+| agent_ipmitool | ironic-dev || fake | ironic-dev || ipmi | ironic-dev || pxe_ipmitool | ironic-dev |+---------------------+----------------+ 建立 Bare metal 網路首先我們需要設定一個網路來提供 DHCP, PXE 與其他需求使用，這部分會說明如何建立一個 Flat network 來提供裸機配置用。詳細可參考 Configure the Networking service for bare metal provisioning。 首先編輯/etc/neutron/plugins/ml2/ml2_conf.ini修改以下內容： [ml2_type_flat]flat_networks = public, physnet1[ovs]datapath_type = systembridge_mappings = public:br-ex, physnet1:br-eth2tunnel_bridge = br-tunlocal_ip = 172.22.132.93 接著建立 bridge 來處理實體網路與 OpenStack 之間的溝通： $ sudo ovs-vsctl add-br br-eth2$ sudo ovs-vsctl add-port br-eth2 eth2 完成後重新啟動 Neutron server 與 agent： $ sudo systemctl restart devstack@q-svc.service$ sudo systemctl restart devstack@q-agt.service 建立完成後，OVS bridges 會類似如下： $ sudo ovs-vsctl show Bridge br-int fail_mode: secure Port \"int-br-eth2\" Interface \"int-br-eth2\" type: patch options: &#123;peer=\"phy-br-eth2\"&#125; Port br-int Interface br-int type: internal Bridge \"br-eth2\" Port \"phy-br-eth2\" Interface \"phy-br-eth2\" type: patch options: &#123;peer=\"int-br-eth2\"&#125; Port \"eth2\" Interface \"eth2\" Port \"br-eth2\" Interface \"br-eth2\" type: internal 接著建立 Neutron flat 網路來提供使用： $ neutron net-create sharednet1 \\ --shared \\ --provider:network_type flat \\ --provider:physical_network physnet1$ neutron subnet-create sharednet1 172.22.132.0/24 \\ --name sharedsubnet1 \\ --ip-version=4 --gateway=172.22.132.254 \\ --allocation-pool start=172.22.132.180,end=172.22.132.200 \\ --enable-dhcp P.S. neutron-client 在未來會被移除，故請轉用 Provider network。 設定 Ironic cleaning network當使用到 Node cleaning 時，我們必須設定cleaning_network選項來提供使用。首先取得 Network 資訊，透過以下指令： $ openstack network list+--------------------------------------+------------+----------------------------------------------------------------------------+| ID | Name | Subnets |+--------------------------------------+------------+----------------------------------------------------------------------------+| 03de10a0-d4d2-43ce-83db-806a5277dd29 | private | 2a651bfb-776d-47f4-a958-f8a418f7fcd5, 99bdbd78-7a20-41b7-afa3-7cf7bf25b95b || 349a6a5b-1e26-4e36-8444-f6a6bbbdd227 | public | 032a516e-3d55-4623-995d-06ee033eaee4, daf733a9-492e-4ea6-8a45-6364b88a8f6f || ade096bd-6a86-4d90-9cf4-bce9921f7257 | sharednet1 | 3f9f2a47-fdd9-472b-a6a2-ce6570e490ff |+--------------------------------------+------------+----------------------------------------------------------------------------+ 編輯/etc/ironic/ironic.conf修改一下內容： [neutron]cleaning_network = sharednet1 完成後，重新啟動 Ironic 服務： $ sudo systemctl restart devstack@ir-api.service$ sudo systemctl restart devstack@ir-cond.service 建立 Deploy 與 User 映像檔裸機服務在配置時需要兩組映像檔，分別為 Deploy 與 User 映像檔，其功能如下： Deploy images: 用來準備裸機服務機器以進行實際的作業系統部署，在 Cleaning 等階段會使用到。 User images:最後安裝至裸機服務提供給使用者使用的作業系統映像檔。 由於 DevStack 預設會建立一組 Deploy 映像檔，這邊只針對 User 映像檔做手動建構說明，若要建構 Deploy 映像檔可以參考 Building or downloading a deploy ramdisk image。 首先我們必須先安裝disk-image-builder工具來提供建構映像檔： $ virtualenv dib$ source dib/bin/activate(dib) $ pip install diskimage-builder 接著執行以下指令來進行建構映像檔： $ cat &lt;&lt;EOF &gt; k8s.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF$ DIB_YUM_REPO_CONF=k8s.repo \\ DIB_DEV_USER_USERNAME=kyle \\ DIB_DEV_USER_PWDLESS_SUDO=yes \\ DIB_DEV_USER_PASSWORD=r00tme \\ disk-image-create \\ centos7 \\ dhcp-all-interfaces \\ devuser \\ yum \\ epel \\ baremetal \\ -o k8s.qcow2 \\ -p vim,docker,kubelet,kubeadm,kubectl,kubernetes-cni...Converting image using qemu-img convertImage file k8s.qcow2 created... 完成後會看到以下檔案： $ lsdib k8s.d k8s.initrd k8s.qcow2 k8s.repo k8s.vmlinuz 上傳至 Glance 以提供使用： # 上傳 Kernel$ openstack image create k8s.kernel \\ --public \\ --disk-format aki \\ --container-format aki &lt; k8s.vmlinuz# 上傳 Initrd$ openstack image create k8s.initrd \\ --public \\ --disk-format ari \\ --container-format ari &lt; k8s.initrd# 上傳 Qcow2$ export MY_VMLINUZ_UUID=$(openstack image list | awk '/k8s.kernel/ &#123; print $2 &#125;')$ export MY_INITRD_UUID=$(openstack image list | awk '/k8s.initrd/ &#123; print $2 &#125;')$ openstack image create k8s \\ --public \\ --disk-format qcow2 \\ --container-format bare \\ --property kernel_id=$MY_VMLINUZ_UUID \\ --property ramdisk_id=$MY_INITRD_UUID &lt; k8s.qcow2 建立 Ironic 節點在所有服務配置都完成後，這時候要註冊實體機器資訊，來提供給 Compute 服務部署時使用。首先確認 Ironic 的 Driver 是否有資源機器的 Power driver： $ ironic driver-list+---------------------+----------------+| Supported driver(s) | Active host(s) |+---------------------+----------------+| agent_ipmitool | ironic-dev || fake | ironic-dev || ipmi | ironic-dev || pxe_ipmitool | ironic-dev |+---------------------+----------------+ 若有缺少的話，請參考 Set up the drivers for the Bare Metal service。 確認有支援後，透過以下指令來建立 Node，並進行註冊： $ export DEPLOY_VMLINUZ_UUID=$(openstack image list | awk '/ipmitool.kernel/ &#123; print $2 &#125;')$ export DEPLOY_INITRD_UUID=$(openstack image list | awk '/ipmitool.initramfs/ &#123; print $2 &#125;')$ ironic node-create -d agent_ipmitool \\ -n bare-node-1 \\ -i ipmi_address=172.20.3.194 \\ -i ipmi_username=maas \\ -i ipmi_password=passwd \\ -i ipmi_port=623 \\ -i deploy_kernel=$DEPLOY_VMLINUZ_UUID \\ -i deploy_ramdisk=$DEPLOY_INITRD_UUID 若使用 Console 的話，要加入-i ipmi_terminal_port=9000，可參考 Configuring Web or Serial Console。 接著更新機器資訊，這邊透過手動方式來更新資訊： $ export NODE_UUID=$(ironic node-list | awk '/bare-node-1/ &#123; print $2 &#125;')$ ironic node-update $NODE_UUID add \\ properties/cpus=4 \\ properties/memory_mb=8192 \\ properties/local_gb=100 \\ properties/root_gb=100 \\ properties/cpu_arch=x86_64 (option)也可以使用 inspector 來識別裸機機器的硬體資訊，但需要修改/etc/ironic-inspector/dnsmasq.conf修改一下： no-daemonport=0interface=eth1bind-interfacesdhcp-range=172.22.132.200,172.22.132.210dhcp-match=ipxe,175dhcp-boot=tag:!ipxe,undionly.kpxedhcp-boot=tag:ipxe,http://172.22.132.93:3928/ironic-inspector.ipxedhcp-sequential-ip 完成後，透過 systemctl 重新啟動背景服務devstack@ironic-inspector-dhcp.service與devstack@ironic-inspector.service。 透過 port create 來把 Node 的所有網路資訊進行註冊： $ ironic port-create -n $NODE_UUID -a NODE_MAC_ADDRESS 這邊NODE_MAC_ADDRESS是指bare-node-1節點的 PXE(eth1)網卡 Mac Address，如 54:a0:50:85:d5:fa。 完成後透過 validate 指令來檢查： $ ironic node-validate $NODE_UUID+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Interface | Result | Reason |+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| boot | False | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: ['ramdisk', 'kernel', 'image_source'] || console | False | Missing 'ipmi_terminal_port' parameter in node\\'s driver_info. || deploy | False | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: ['ramdisk', 'kernel', 'image_source'] || inspect | True | || management | True | || network | True | || power | True | || raid | True | || storage | True | |+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ P.S. 這邊boot與deploy的錯誤若是如上所示的話，可以直接忽略，這是因為使用 Nova 來管理 baremetal 會出現的問題。 最後利用 provision 指令來測試節點是否能夠提供服務： $ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID manage$ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID provide$ ironic node-list+--------------------------------------+--------+---------------+-------------+--------------------+-------------+| UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |+--------------------------------------+--------+---------------+-------------+--------------------+-------------+| 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | None | power off | cleaning | False |+--------------------------------------+--------+---------------+-------------+--------------------+-------------+ 這時候機器會進行 clean 過程，經過一點時間就會完成，若順利完成則該節點就可以進行部署了。若要了解細節狀態，可以參考 Ironic’s State Machine。 透過 Nova 部署 baremetal 機器最後我們要透過 Nova API 來部署裸機，在開始前要建立一個 flavor 跟上傳 keypair 來提供使用： $ ssh-keygen -t rsa$ openstack keypair create --public-key ~/.ssh/id_rsa.pub default$ openstack flavor create --vcpus 4 --ram 8192 --disk 100 baremetal.large 完成後，即可透過以下指令進行部署： $ NET_ID=$(openstack network list | awk '/sharednet1/ &#123; print $2 &#125;')$ openstack server create --flavor baremetal.large \\ --nic net-id=$NET_ID \\ --image k8s \\ --key-name default k8s-01 經過一段時間後，就會看到部署完成，這時候可以透過以下指令來確認部署結果： $ openstack server list+--------------------------------------+--------+--------+---------------------------+-------+-----------------+| ID | Name | Status | Networks | Image | Flavor |+--------------------------------------+--------+--------+---------------------------+-------+-----------------+| a40e5cb1-dfc6-44d5-b638-648e8c0975fb | k8s-01 | ACTIVE | sharednet1=172.22.132.187 | k8s | baremetal.large |+--------------------------------------+--------+--------+---------------------------+-------+-----------------+$ openstack baremetal list+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+| UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+| 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | a40e5cb1-dfc6-44d5-b638-648e8c0975fb | power on | active | False |+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+ 最後透過 ssh 來進入部署機器來建立應用： $ ssh kyle@172.22.132.187[kyle@host-172-22-132-187 ~]$ sudo systemctl start kubelet.service[kyle@host-172-22-132-187 ~]$ sudo systemctl start docker.service[kyle@host-172-22-132-187 ~]$ sudo kubeadm init --service-cidr 10.96.0.0/12 \\ --kubernetes-version v1.7.4 \\ --pod-network-cidr 10.244.0.0/16 \\ --apiserver-advertise-address 172.22.132.187 \\ --token b0f7b8.8d1767876297d85c 整合Magnum有空再寫，先簡單玩玩吧。 若是懶人可以用 Dashboard 來部署，另外本教學的 DevStack 有使用 Ironic UI，因此可以在以下頁面看到 node 資訊。","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"},{"name":"DevStack","slug":"DevStack","permalink":"https://k2r2bai.com/tags/DevStack/"},{"name":"Bare-metal","slug":"Bare-metal","permalink":"https://k2r2bai.com/tags/Bare-metal/"}]},{"title":"利用 LinuxKit 建立 Kubernetes 叢集","slug":"kubernetes/deploy/linuxkit","date":"2017-07-21T16:00:00.000Z","updated":"2019-12-02T01:49:42.393Z","comments":true,"path":"2017/07/22/kubernetes/deploy/linuxkit/","link":"","permalink":"https://k2r2bai.com/2017/07/22/kubernetes/deploy/linuxkit/","excerpt":"LinuxKit 是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，先前有簡單介紹與操作過，可以參考LinuxKit。本篇則將利用 LinuxKit 來建立 Kubernetes 的映像檔，並部署簡單的 Kubernetes 叢集。","text":"LinuxKit 是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，先前有簡單介紹與操作過，可以參考LinuxKit。本篇則將利用 LinuxKit 來建立 Kubernetes 的映像檔，並部署簡單的 Kubernetes 叢集。 本次教學會在Mac OS X作業系統上進行，而部署的軟體資訊如下： Kubernetes v1.7.2(2017-08-07, Update) Etcd v3 Weave Docker v17.06.0-ce 預先準備資訊 主機已安裝與啟動Docker工具。 主機已安裝Git工具。 主機以下載 LinuxKit 專案，並建構了 Moby 與 LinuxKit 工具。 建構 Moby 與 LinuxKit 方法如以下操作： $ git clone https://github.com/linuxkit/linuxkit.git$ cd linuxkit$ make$ ./bin/moby versionmoby version 0.0commit: c2b081ed8a9f690820cc0c0568238e641848f58f$ ./bin/linuxkit versionlinuxkit version 0.0commit: 0e3ca695d07d1c9870eca71fb7dd9ede31a38380 建構 Kubernetes 系統映像檔首先我們要建立一個包好 Kubernetes 的 Linux 映像檔，而官方已經有做好範例，只要利用以下方式即可建構： $ cd linuxkit/projects/kubernetes/$ make build-vm-images...Create outputs: kube-node-kernel kube-node-initrd.img kube-node-cmdline 建置 Kubernetes cluster完成建構映像檔後，就可以透過以下指令來啟動 Master OS 映像檔，然後獲取節點 IP： $ ./boot.sh(ns: getty) linuxkit-025000000002:~\\# ip addr show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 02:50:00:00:00:02 brd ff:ff:ff:ff:ff:ff inet 192.168.65.3/24 brd 192.168.65.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::abf0:9fa4:d0f4:8da2/64 scope link valid_lft forever preferred_lft forever 啟動後，開啟新的 Console 來 SSH 進入 Master，來利用 kubeadm 初始化 Master： $ cd linuxkit/projects/kubernetes/$ ./ssh_into_kubelet.sh 192.168.65.3linuxkit-025000000002:/\\# kubeadm-init.sh...kubeadm join --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 一旦 kubeadm 完成後，就會看到 Token，這時請記住 Token 資訊。接著開啟新 Console，然後執行以下指令來啟動 Node： console1&gt;$ ./boot.sh 1 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 P.S. 開啟節點格式為./boot.sh &lt;n&gt; [&lt;join_args&gt; ...]。 接著分別在開兩個 Console 來加入叢集： console2&gt; $ ./boot.sh 2 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443console3&gt; $ ./boot.sh 3 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 完成後回到 Master 節點上，執行以下指令來查看節點狀況： $ kubectl get noNAME STATUS AGE VERSIONlinuxkit-025000000002 Ready 16m v1.7.2linuxkit-025000000003 Ready 6m v1.7.2linuxkit-025000000004 Ready 1m v1.7.2linuxkit-025000000005 Ready 1m v1.7.2 簡單部署 Nginx 服務Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務： $ kubectl run nginx --image=nginx --replicas=1 --port=80$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-1423793266-v0hpb 1/1 Running 0 38s 10.42.0.1 linuxkit-025000000004 完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立： $ kubectl expose deploy nginx --port=80 --type=NodePort$ kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.96.0.1 &lt;none&gt; 443/TCP 19mnginx 10.108.41.230 &lt;nodes&gt; 80:31773/TCP 5s 由於這邊不是使用實體機器部署，因此網路使用 Docker namespace 網路，故這邊透過ubuntu-desktop-lxde-vnc來瀏覽 Nginx 應用： $ docker run -it --rm -p 6080:80 dorowu/ubuntu-desktop-lxde-vnc 完成後透過瀏覽器連接 HTLM VNC 最後關閉節點只需要執行以下即可： $ halt[ 1503.034689] reboot: Power down","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"LinuxKit","slug":"LinuxKit","permalink":"https://k2r2bai.com/tags/LinuxKit/"}]},{"title":"智能合約(Smart contracts)","slug":"blockchain/ethereum/smart-contracts","date":"2017-05-28T09:08:54.000Z","updated":"2019-12-02T01:49:42.377Z","comments":true,"path":"2017/05/28/blockchain/ethereum/smart-contracts/","link":"","permalink":"https://k2r2bai.com/2017/05/28/blockchain/ethereum/smart-contracts/","excerpt":"智能合約(Smart Contracts) 是在 Ethereum 區塊鏈中所屬的物件。它們包含程式碼函式以及能夠與其他合約進行互動、做出決策、儲存資料與傳送乙太幣給其他人。合約是由創建者所定義，但是它們的執行與他們所提供的服務，都是由 Ethereum 網路本身提供。它們將存在且可被執行，只要整個網路存在，並且只會因程式中有撰寫自我銷毀的功能才會消失。 我可以用合約做什麼呢？只要想像力夠豐富，要做什麼幾乎都沒問題，但以下指南只會是入門，讓我們去實現一些簡單的事情。","text":"智能合約(Smart Contracts) 是在 Ethereum 區塊鏈中所屬的物件。它們包含程式碼函式以及能夠與其他合約進行互動、做出決策、儲存資料與傳送乙太幣給其他人。合約是由創建者所定義，但是它們的執行與他們所提供的服務，都是由 Ethereum 網路本身提供。它們將存在且可被執行，只要整個網路存在，並且只會因程式中有撰寫自我銷毀的功能才會消失。 我可以用合約做什麼呢？只要想像力夠豐富，要做什麼幾乎都沒問題，但以下指南只會是入門，讓我們去實現一些簡單的事情。 Smart Sponsor本節將說明一智能合約範例，透過建構一個合約來允許以下賬戶持有人進行互動。 一個慈善機構舉行籌款活動，我們稱之為 thebenefactor。 一個受贊助的 runner 想為慈善機構募款，我們稱之為 therunner。 其他的人想要贊助 runner，我們稱之為 thesponsor。 一個 Ethereum 節點，用來開採區塊鏈以驗證交易，我們稱之為 theminer。 我們的合約(smartSponsor)： 是由一位 runner 透過贊助的執行來為慈善機構募款。 當建立合約時，runner 會任命為募集錢的捐助者。 runner 則邀情其他人去進行贊助。用戶透過呼叫一個在智能合約上的函式，將乙太幣從 贊助商的帳戶 轉移到 合約，並保持乙太幣於合約，直到有進一步的通知。 在合約的時限期間的所有人都能看到誰是 捐助者，有多少的乙太幣被從誰捐(雖然贊助者可以匿名，當然:p)。 那麼有兩件事情可能發生： 執行都按計劃進行，以及 runner 指示合約轉移到所有資金的捐助者。 執行由於謀些原因無法承擔，而 runner 指示合約將退還贊助商的抵押。 Ethereum 允許智能合約由撰寫 Solidity 語言來定義。Solidity 的合約是類似於 Java 的類別定義。成員變數的儲存採用區塊鍊交易與合約的方法，來詢問合約或改變的其狀態。作為區塊鏈的副本會分散到網路中的所有節點，任何人都可以詢問合約，以從中搜尋公開的訊息。 合約有以下幾種方法： smartSponsor：合約的建構子。它初始化合約的狀態。合約的建立者傳入賬戶的位址，有利於合約的 drawdown。 pledge：任何人都可以呼叫捐贈乙太幣贊助基金。贊助商提供支援的選擇性訊息 getPot：回傳當前儲存在合約的總乙太幣。 refund：把贊助商的錢退回給贊助商。只有合約的擁有者才能呼叫這個函式。 drawdown：傳送合約的總價值給捐助者賬戶。只有合約的擁有者才能呼叫這個函式。 這個想法是使一個合約擁有約束力。他們不能拿回任何資金，除非整個合約被退還。在這種情況下，所有資料都是被公開存取的，這意味著任何人都有存取 Ethereum 區塊鏈，來查看誰建立了合約，誰是捐助者，以及誰透過存取合約程式碼本身保證了每一筆資金。 要注意很重要的一點，任何改變合約的狀態(建立、承若、退還或 drawing down)都需要在區塊鏈上建立交易，這意味著這些交易不會被儲存，要直到這些交易的區塊被開採。操作只能讀取到一個現有合約狀態(getPot 或讀取公有成員變數)都不需要進行挖礦。這是一個很重要且微妙的點：寫入操作是很慢的(因為我們要等到採礦完成)。由於這情況合約可能永遠不會被建立到區塊鍊中，因此呼叫方需要提供一些獎勵，來促進礦工去工作。這是被稱為 gas 的 Ethereum 術語，所有的寫入操作都是需要 gas 的支出來改變區塊鍊的狀態。 幸運的是我們不需要購買真正的乙太幣，以及參與 Ethereum 網路。我們可以使用相同的軟體，但要配置它運行一個本地測試區塊鏈，以及產生自己的假乙太幣。 以下為一個 Solidity 語言的智能合約範例： contract smartSponsor &#123; address public owner; address public benefactor; bool public refunded; bool public complete; uint public numPledges; struct Pledge &#123; uint amount; address eth_address; bytes32 message; &#125; mapping(uint =&gt; Pledge) public pledges; // constructor function smartSponsor(address _benefactor) &#123; owner = msg.sender; numPledges = 0; refunded = false; complete = false; benefactor = _benefactor; &#125; // add a new pledge function pledge(bytes32 _message) &#123; if (msg.value == 0 || complete || refunded) throw; pledges[numPledges] = Pledge(msg.value, msg.sender, _message); numPledges++; &#125; function getPot() constant returns (uint) &#123; return this.balance; &#125; // refund the backers function refund() &#123; if (msg.sender != owner || complete || refunded) throw; for (uint i = 0; i &lt; numPledges; ++i) &#123; pledges[i].eth_address.send(pledges[i].amount); &#125; refunded = true; complete = true; &#125; // send funds to the contract benefactor function drawdown() &#123; if (msg.sender != owner || complete || refunded) throw; benefactor.send(this.balance); complete = true; &#125;&#125; 一個Pledge結構模型的捐贈，儲存著贊助商的帳戶 ID、承若押金，以及一些訊息字串。 這個pledges陣列儲存了一個承若方的列表。 合約中的所有成員變數都是公開的，所以getters將自動被建立。 throw被稱為某些函式(functions)，用以防止資料被寫入錯誤的資料到該區塊鏈中。 參考連結 Our thoughts on Ethereum Building a smart contract using the command line Block chain technology, smart contracts and Ethereum","categories":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/categories/Blockchain/"}],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/tags/Blockchain/"},{"name":"Ethereum","slug":"Ethereum","permalink":"https://k2r2bai.com/tags/Ethereum/"},{"name":"Solidity","slug":"Solidity","permalink":"https://k2r2bai.com/tags/Solidity/"},{"name":"Smart Contract","slug":"Smart-Contract","permalink":"https://k2r2bai.com/tags/Smart-Contract/"}]},{"title":"利用 Browser Solidity 部署智能合約","slug":"blockchain/ethereum/browser-solidity","date":"2017-05-27T09:08:54.000Z","updated":"2019-12-02T01:49:42.376Z","comments":true,"path":"2017/05/27/blockchain/ethereum/browser-solidity/","link":"","permalink":"https://k2r2bai.com/2017/05/27/blockchain/ethereum/browser-solidity/","excerpt":"Browser Solidity 是一個 Web-based 的 Solidity 編譯器與 IDE。本節將說明如何安裝於 Linux 與 Docker 中。 這邊可以連結官方的 https://ethereum.github.io/browser-solidity 來使用; 該網站會是該專案的最新版本預覽。","text":"Browser Solidity 是一個 Web-based 的 Solidity 編譯器與 IDE。本節將說明如何安裝於 Linux 與 Docker 中。 這邊可以連結官方的 https://ethereum.github.io/browser-solidity 來使用; 該網站會是該專案的最新版本預覽。 Ubuntu Server 手動安裝首先安裝 Browser Solidity 要使用到的相關套件： $ sudo apt-get install -y apache2 make g++ git 接著安裝 node.js 平台，來建置 App： $ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -$ sudo apt-get install nodejs 然後透過 git 將專案抓到 local 端，並進入目錄： $ git clone https://github.com/ethereum/browser-solidity.git$ cd browser-solidity 安裝相依套件與建置應用程式： $ sudo npm install$ sudo npm run build 完成後，將所以有目錄的資料夾與檔案搬移到 Apache HTTP Server 的網頁根目錄： $ sudo cp ./* /var/www/html/ 完成後就可以開啟網頁了。 Docker 快速安裝目前 Browser Solidity 有提供 Docker Image 下載。這邊只需要透過以下指令就能夠建立 Browser Solidity Dashboard 環境： $ docker run -d \\ -p 80:80 \\ --name solidity \\ kairen/solidity","categories":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/categories/Blockchain/"}],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/tags/Blockchain/"},{"name":"Ethereum","slug":"Ethereum","permalink":"https://k2r2bai.com/tags/Ethereum/"},{"name":"Solidity","slug":"Solidity","permalink":"https://k2r2bai.com/tags/Solidity/"},{"name":"Smart Contract","slug":"Smart-Contract","permalink":"https://k2r2bai.com/tags/Smart-Contract/"}]},{"title":"監控 Go Ethereum 的區塊鏈狀況","slug":"blockchain/ethereum/geth-monitoring","date":"2017-05-26T09:08:54.000Z","updated":"2019-12-02T01:49:42.376Z","comments":true,"path":"2017/05/26/blockchain/ethereum/geth-monitoring/","link":"","permalink":"https://k2r2bai.com/2017/05/26/blockchain/ethereum/geth-monitoring/","excerpt":"Ethereum 提供了一個 Web-based 的監控儀表板，可以部署該儀表板，並透過 Clinet 端傳送 Ethereum 節點的資訊，來查看整個區塊鏈狀態。本節將說明如何安裝監控儀表板於 Linux 與 Docker 容器中。 這邊可以連結官方的 https://ethstats.net/ 來查看主節點網路的狀態。","text":"Ethereum 提供了一個 Web-based 的監控儀表板，可以部署該儀表板，並透過 Clinet 端傳送 Ethereum 節點的資訊，來查看整個區塊鏈狀態。本節將說明如何安裝監控儀表板於 Linux 與 Docker 容器中。 這邊可以連結官方的 https://ethstats.net/ 來查看主節點網路的狀態。 Ubuntu Server 手動安裝本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分： Monitoring site Client side Monitoring site首先安裝 Browser Solidity 要使用到的相關套件： $ sudo apt-get install -y make g++ git 接著安裝 node.js 平台，來建置 App： $ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -$ sudo apt-get install nodejs 然後透過 git 將專案抓到 local 端，並進入目錄： $ git clone https://github.com/cubedro/eth-netstats$ cd eth-netstats 安裝相依套件與建置應用程式，並啟動服務： $ sudo npm install$ sudo npm install -g grunt-cli$ grunt$ PORT=\"3000\" WS_SECRET=\"admin\" npm start 接著就可以開啟 eth-netstats。 在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。 撰寫一個腳本eth-netstats.sh放置到背景服務執行： #!/bin/bash# History:# 2016/05/22 Kyle Bai Release#export PORT=\"3000\"export WS_SECRET=\"admin\"echo \"Starting private eth-netstats ...\"screen -dmS netstats /usr/bin/npm start 透過以下方式執行： $ chmod u+x eth-netstats.sh$ ./eth-netstats.shStarting private eth-netstats ... 透過screen -x netstats取得當前畫面。 Client side首先安裝 Browser Solidity 要使用到的相關套件： $ sudo apt-get install -y make g++ git 接著安裝 node.js 平台，來建置 App： $ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -$ sudo apt-get install nodejs 然後透過 git 將專案抓到 local 端，並進入目錄： $ git clone https://github.com/cubedro/eth-net-intelligence-api$ cd eth-net-intelligence-api 安裝相依套件與建置應用程式： $ sudo npm install &amp;&amp; sudo npm install -g pm2 編輯app.json設定檔，並修改以下內容： [ &#123; \"name\" : \"mynode\", \"cwd\" : \".\", \"script\" : \"app.js\", \"log_date_format\" : \"YYYY-MM-DD HH:mm Z\", \"merge_logs\" : false, \"watch\" : false, \"exec_interpreter\" : \"node\", \"exec_mode\" : \"fork_mode\", \"env\": &#123; \"NODE_ENV\" : \"production\", \"RPC_HOST\" : \"localhost\", \"RPC_PORT\" : \"8545\", \"INSTANCE_NAME\" : \"mynode-1\", \"WS_SERVER\" : \"http://localhost:3000\", \"WS_SECRET\" : \"admin\", &#125; &#125;,] RPC_HOST為 ethereum 的 rpc ip address。 RPC_PORT為 ethereum 的 rpc port。 INSTANCE_NAME為 ethereum 的監控實例名稱。 WS_SERVER為 eth-netstats 的 URL。 WS_SECRET為 eth-netstats 的 secret。 確認完成後，即可啟動服務： $ pm2 start app.json$ sudo tail -f $HOME/.pm2/logs/mynode-out-0.log Docker 快速安裝本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分： Docker Monitoring site Docker Client side Docker Monitoring site自動建置的映像檔現在可以在 DockerHub 找到，建議直接執行以下指令來啟動 eth-netstats 容器： $ docker run -d \\ -p 3000:3000 \\ -e WS_SECRET=\"admin\" \\ --name ethstats \\ kairen/ethstats 接著就可以開啟 eth-netstats。 在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。 Docker Client side自動建置的映像檔現在可以在 DockerHub 找到，也推薦透過執行以下指令來啟動 eth-netintel 容器： $ docker run -d \\ -p 30303:30303 \\ -p 30303:30303/udp \\ -e NAME_PREFIX=\"geth-1\" \\ -e WS_SERVER=\"http://172.17.1.200:3000\" \\ -e WS_SECRET=\"admin\" \\ -e RPC_HOST=\"172.17.1.199\" \\ -e RPC_PORT=\"8545\" \\ --name geth-1 \\ kairen/ethnetintel Monitor 與 Client 需要統一WS_SECRET。","categories":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/categories/Blockchain/"}],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/tags/Blockchain/"},{"name":"Ethereum","slug":"Ethereum","permalink":"https://k2r2bai.com/tags/Ethereum/"}]},{"title":"建立 Go Ethereum 私有網路鏈","slug":"blockchain/ethereum/multi-node-geth","date":"2017-05-25T09:08:54.000Z","updated":"2019-12-02T01:49:42.376Z","comments":true,"path":"2017/05/25/blockchain/ethereum/multi-node-geth/","link":"","permalink":"https://k2r2bai.com/2017/05/25/blockchain/ethereum/multi-node-geth/","excerpt":"Ethereum 專案是以區塊鏈原理，並進一步增加容納值、儲存資料，並且能封裝程式碼來建立智能合約(Smart Contracts)，形成區塊鏈應用程式，來執行運算任務。類似於比特幣(Bitcoin)，Ethereum 也具有一種貨幣，它叫做乙太幣(Ether)。乙太幣是開採於儲存在共享一致性的區塊鏈前驗證交易節點。乙太幣可以在賬戶(公有金鑰, Pubilc keys)與智能合約(Smart Contracts)之間進行轉移。 本節將說明如何透過 Ubuntu 部署 Go Ethereum。並利用簡單的指令來進行 Demo。","text":"Ethereum 專案是以區塊鏈原理，並進一步增加容納值、儲存資料，並且能封裝程式碼來建立智能合約(Smart Contracts)，形成區塊鏈應用程式，來執行運算任務。類似於比特幣(Bitcoin)，Ethereum 也具有一種貨幣，它叫做乙太幣(Ether)。乙太幣是開採於儲存在共享一致性的區塊鏈前驗證交易節點。乙太幣可以在賬戶(公有金鑰, Pubilc keys)與智能合約(Smart Contracts)之間進行轉移。 本節將說明如何透過 Ubuntu 部署 Go Ethereum。並利用簡單的指令來進行 Demo。 事前準備本次會使用到兩個節點來建立 Geth Instances，其規格如下： Role CPUs RAM Disk geth-1 2vCPU 4 GB 40 GB geth-2 2vCPU 4 GB 40 GB 首先在每個節點安裝 Ethereum 最新版本，可以依照官方透過以下方式快速安裝： $ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:ethereum/ethereum$ sudo apt-get update &amp;&amp; sudo apt-get install ethereum 在每個節點建立一個private.json檔案來定義起源區塊(Genesis Block)，內容如下： &#123; \"coinbase\" : \"0x0000000000000000000000000000000000000000\", \"difficulty\" : \"0x40000\", \"extraData\" : \"Custem Ethereum Genesis Block\", \"gasLimit\" : \"0xffffffff\", \"nonce\" : \"0x0000000000000042\", \"mixhash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\", \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\", \"timestamp\" : \"0x00\", \"config\": &#123; \"chainId\": 123, \"homesteadBlock\": 0, \"eip155Block\": 0, \"eip158Block\": 0 &#125;, \"alloc\": &#123; &#125;&#125; 初始化創世區塊： $ geth init --datadir=data/ private.json 在每個節點新增一名稱為geth-private.sh的腳本程式，將用於啟動 geth，並放置背景： #!/bin/bash# Program:# This program is a private geth runner.# History:# 2016/05/22 Kyle Bai Release#echo \"Starting private geth\"screen -dmS geth /usr/bin/geth \\ --datadir data/ \\ --networkid 123 \\ --nodiscover \\ --maxpeers 5 \\ --port 30301 \\ --rpc \\ --rpcaddr \"0.0.0.0\" \\ --rpcport \"8545\" \\ --rpcapi \"admin,db,eth,debug,miner,net,shh,txpool,personal,web3\" \\ --rpccorsdomain \"*\" \\ -verbosity 6 更多的參數，請參考 Command-Line-Options。 建立完成後，修改執行權限： $ chmod u+x geth-private.sh 建立 Ethereum 環境首先進入到geth-1節點透過以下方式來啟動： $ ./geth-private.shStarting private geth 這時候會透過 screen 執行於背景，我們可以透過screen -x geth來進行前景。若要回到背景則透過[Ctrl-A] + [Ctrl-D]來 detached。要關閉 screen 則透過 [Ctrl-C]。 接著為了確認是否正確啟動，我們可以透過 geth 的 attach 指令來連接 console： $ geth attach ipc:data/geth.ipc 也可以透過 HTTP 方式 attach，geth attach http://localhost:8545。 若一開始建立沒有 RPC，但想要加入 RPC 可以 attach 後，輸入以下 function： &gt; admin.startRPC(\"0.0.0.0\", 8545, \"*\", \"web3,db,net,eth\") 進入後透過 admin API 來取得節點的資訊： &gt; admin.nodeInfo.enode\"enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@[::]:30301?discport=0\" 這邊要取代[::]為主機 IP，如以下： &quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot; 上面沒問題後，接著進入到geth-2節點，然後透過以下指令開啟 console： $ geth init --datadir=data/ private.json$ geth --datadir data/ \\ --networkid 123 \\ --nodiscover \\ --maxpeers 5 \\ --port 30301 \\ --rpc \\ --rpcaddr \"0.0.0.0\" \\ --rpcport \"8545\" \\ --rpcapi \"admin,db,eth,debug,miner,net,shh,txpool,personal,web3\" \\ --rpccorsdomain \"*\" \\ -verbosity 6 \\ console 也可以透過上一個節點的方式將服務放到背景，在 attach。 完成上面指令會直接進入 console，接著透過以下方式來連接```sh&gt; admin.addPeer(&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot;)trueI0525 12:56:40.623642 eth/downloader/downloader.go:239] Registering peer e3dd0392a2971c4bI0525 12:57:10.622920 p2p/server.go:467] &lt;-taskdone: wait for dial hist expire (29.99999387s) 接著透過 net API 進行查看連接狀態： &gt; net.peerCount1&gt; admin.peers[&#123; caps: [\"eth/61\", \"eth/62\", \"eth/63\"], id: \"e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257\", name: \"Geth/v1.4.5-stable/linux/go1.5.1\", network: &#123; localAddress: \"172.16.1.100:51038\", remoteAddress: \"172.16.1.99:30301\" &#125;, protocols: &#123; eth: &#123; difficulty: 131072, head: \"882048e0d045ea48903eddb4c50825a4e3c6c1a055df6a32244e9a9239f8c5e8\", version: 63 &#125; &#125;&#125;] 驗證服務這部分將透過幾個指令與流程來驗證服務，首先在geth-1透過 attach 進入，並建立一個賬戶與查看乙太幣： $ geth attach http://localhost:8545&gt; kairen = personal.newAccount();Passphrase:Repeat passphrase:\"0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b\"&gt; personal.listAccounts[\"0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b\"]&gt; web3.fromWei(eth.getBalance(kairen), \"ether\");0 P.S. 若要移除帳號，可以刪除data/keystore底下的檔案。 接著在geth-2透過以下指令建立一個賬戶與查看乙太幣： &gt; pingyu = personal.newAccount();Passphrase:Repeat passphrase:\"0xf8c70df559cb9225f6e426d0f139fd6e8752c644\"&gt; personal.listAccounts[\"0xf8c70df559cb9225f6e426d0f139fd6e8752c644\"]&gt; web3.fromWei(eth.getBalance(pingyu), \"ether\");0 接著回到geth-1來賺取一些要交易的乙太幣： &gt; miner.setEtherbase(kairen)true 當賬戶設定完成後，就可以執行以下指令進行採礦： &gt; miner.start(1)true 這邊需要一點時間產生 DAG，可以開一個新的命令列透過screen -x geth查看。經過一段時間後，當 DAG 完成並開始採擴時就可以miner.stop()。 接著在geth-1查看賬戶的乙太幣： &gt; web3.fromWei(eth.getBalance(kairen), \"ether\");40.78125 當成開採區塊後，就可以查看geth-1共採集的 ether balance 的數值： &gt; eth.getBalance(eth.coinbase).toNumber()40781250000000000000 即為40.78125乙太幣。 接著我們要在將geth-1的賬戶乙太幣轉移到geth-2上，首先在geth-1上建立一個變數來存geth-2的賬戶位址： &gt; consumer = \"0xf8c70df559cb9225f6e426d0f139fd6e8752c644\"\"0xf8c70df559cb9225f6e426d0f139fd6e8752c644\" 完成上述後，首先要將賬戶解鎖： &gt; personal.unlockAccount(kairen)true 輸入當初建立賬戶的密碼。 並透過 eth API 的交易函式還將 ether balance 數值轉移： $ eth.sendTransaction(&#123;from: kairen, to: consumer, value: web3.toWei(10, \"ether\")&#125;)\"0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36\" 若有在每一台 geth 節點上進入 debug 模式的話，會發現該交易資訊被存到一個區塊，這邊也可以透過 txpool 與 eth API 來查看： &gt; txpool.status&#123; pending: 1, queued: 0&#125;&gt; eth.getBlock(\"pending\", true).transactions[&#123; blockHash: \"0x0b58d0b17e02f56746b0b5b22f195b6ae71d47343bf778763c4c476386ad7db7\", blockNumber: 112, from: \"0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b\", gas: 90000, gasPrice: 20000000000, hash: \"0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36\", input: \"0x\", nonce: 0, to: \"0xf8c70df559cb9225f6e426d0f139fd6e8752c644\", transactionIndex: 0, value: 10000000000000000000&#125;] 這邊的pending表示目前還沒有被驗證，因此我們需要一些節點來進行採礦驗證。這邊也可以發現該交易資訊被存在區塊編號112，可以提供往後查詢之用。 接著回到geth-2節點，查看目前的數值變化： &gt; web3.fromWei(eth.getBalance(pingyu), \"ether\");0 這邊會發現沒有任何錢進來，Why? so sad。其實是因為該區塊還沒有被採集與認證，因此該交易不會被執行。 因此我們需要在任一節點提供運算，這邊在geth-1執行以下指令來進行採礦，這樣就可以看到該交易被驗證與接受： &gt; miner.start(1)trueTX(1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36)Contract: falseFrom: cb41ad8ba28c4b8b52eee159ef3bb6da197ff60bTo: f8c70df559cb9225f6e426d0f139fd6e8752c644Nonce: 0GasPrice: 20000000000GasLimit 90000Value: 10000000000000000000Data: 0xV: 0x1cR: 0x9de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43S: 0x287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fedHex: f86d808504a817c80083015f9094f8c70df559cb9225f6e426d0f139fd6e8752c644888ac7230489e80000801ca009de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43a0287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fed 當該區塊的交易確認沒問題被執行後，就可以透過miner.stop()停止採礦。 這時再回到geth-2節點，查看目前的數值變化，會發現增加了 10 枚乙太幣： &gt; web3.fromWei(eth.getBalance(pingyu), \"ether\");10 之後可以在任一節點透過 eth web3 的 API 來查找指定區塊的交易資訊： &gt; eth.getTransactionFromBlock(40)&#123; blockHash: \"0xe839c1392657731417fc04b9aecf7a181dd339086d5f7cdea0bccc2b1483b885\", blockNumber: 112, from: \"0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b\", gas: 90000, gasPrice: 20000000000, hash: \"0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36\", input: \"0x\", nonce: 0, to: \"0xf8c70df559cb9225f6e426d0f139fd6e8752c644\", transactionIndex: 0, value: 10000000000000000000&#125; 簡單的 Contract這邊將說明如何建立一個簡單的合約(Contract)來部署於區塊鏈上，首先複製以下內容： contract SimpleStorage &#123; uint storedData; function set(uint x) &#123; storedData = x; &#125; function get() constant returns (uint retVal) &#123; return storedData; &#125;&#125; 接著將內容貼到 browser-solidity 進行編譯成 JavaScript。如快照畫面所示。 透過這個 IDE 可以將 Solidity 語言轉換成 web3 code(JavaScript)，複製 web3 code 的內容，並儲存成SimpleStorage.js檔案放置到geth-1上。接著 attach 進入 geth 執行以下指令： &gt; loadScript('SimpleStorage.js'); 若有自行安裝browser-solidity的話，則可以使用如下圖一樣的方式連接。","categories":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/categories/Blockchain/"}],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/tags/Blockchain/"},{"name":"Ethereum","slug":"Ethereum","permalink":"https://k2r2bai.com/tags/Ethereum/"}]},{"title":"透過 Docker 體驗 Hyperledger","slug":"blockchain/hyperledger/quick-start","date":"2017-05-16T09:08:54.000Z","updated":"2019-12-02T01:49:42.377Z","comments":true,"path":"2017/05/16/blockchain/hyperledger/quick-start/","link":"","permalink":"https://k2r2bai.com/2017/05/16/blockchain/hyperledger/quick-start/","excerpt":"Hyperledger 專案是 IBM 與 Linux 基金會於 2015 年底共同推動的區塊鏈基礎專案，該專案吸引來自多行業巨頭的參與，並且該專案也很有可能成為未來企業級的區塊鏈技術底層架構基礎，因此該專案被受到高度關注。 以 Hyperledger 作為企業對企業(B2B)、企業對消費者(B2C)的一種交易協定，既符合規章制度，又能夠支援各種類型需求的實現。其設計的核心元素是智能合約(Smart Contracts)、數位資產、記錄儲存庫、一致性對等網路、安全的加密與分散式總賬本等。此外，區塊鍊主要還涉及行業效能要求、身份認證與私下機密交易等。Hyperledger 基於以上建立一個對等網路的分散式總帳技術交易應用程式，透過建立信任、問責制與透明度，並同時簡化業務流程，把它看成是一個作業系統來互動，因此大大的化簡了成本與複雜性。","text":"Hyperledger 專案是 IBM 與 Linux 基金會於 2015 年底共同推動的區塊鏈基礎專案，該專案吸引來自多行業巨頭的參與，並且該專案也很有可能成為未來企業級的區塊鏈技術底層架構基礎，因此該專案被受到高度關注。 以 Hyperledger 作為企業對企業(B2B)、企業對消費者(B2C)的一種交易協定，既符合規章制度，又能夠支援各種類型需求的實現。其設計的核心元素是智能合約(Smart Contracts)、數位資產、記錄儲存庫、一致性對等網路、安全的加密與分散式總賬本等。此外，區塊鍊主要還涉及行業效能要求、身份認證與私下機密交易等。Hyperledger 基於以上建立一個對等網路的分散式總帳技術交易應用程式，透過建立信任、問責制與透明度，並同時簡化業務流程，把它看成是一個作業系統來互動，因此大大的化簡了成本與複雜性。 透過 Docker 執行 Hyperledger本節將說明如何透過 Docker 來快速體驗 Hyperledger。 事前準備準備一台 Ubuntu Server 16.04 LTS 主機規格如下： Role RAM Disk CPUs IP Address hyperledger 4 GB 記憶體 40 GB 儲存空間 兩核處理器 172.16.1.78 然後安裝 Docker Engine 與相關套件，透過以下指令： $ sudo apt-get install -y python-pip git$ curl -fsSL \"https://get.docker.com/\" | sh 接著安裝 Docker-compose，透過以下指令安裝： $ sudo pip install --upgrade pip$ sudo pip install docker-compose 安裝 Hyperledger安裝前首先下載 Hyperledger compose 專案，透過以下指令： $ git clone https://github.com/yeasy/docker-compose-files.git$ cd docker-compose-files/hyperledger 執行以下步驟來下載與取代映像檔： $ docker pull openblockchain/baseimage:0.0.9$ docker pull yeasy/hyperledger:latest$ docker tag yeasy/hyperledger:latest hyperledger/fabric-baseimage:latest$ docker pull yeasy/hyperledger-peer:noops$ docker pull yeasy/hyperledger-peer:pbft$ docker pull yeasy/hyperledger-membersrvc:latest 接著執行 docker compose 來部署四個節點的 Hyperledger： $ docker-compose upAttaching to hyperledger_vp0_1, hyperledger_vp3_1, hyperledger_vp1_1, hyperledger_vp2_1vp0_1 | 07:42:07.870 [crypto] main -&gt; INFO 001 Log level recognized 'info', set to INFO... 驗證 Hyperledger peer進入到第一個 hyperledger 容器裡面，透過以下指令： $ docker exec -ti hyperledger_vp0_1 bash 查看目前所有建立的節點： $ peer node status08:09:14.715 [crypto] main -&gt; INFO 001 Log level recognized 'info', set to INFO08:09:14.715 [logging] LoggingInit -&gt; DEBU 002 Setting default logging level to DEBUG for command 'node'08:09:14.715 [peer] func1 -&gt; INFO 003 Auto detected peer address: 172.17.0.2:3030308:09:14.716 [peer] func1 -&gt; INFO 004 Auto detected peer address: 172.17.0.2:3030308:09:14.716 [peer] func1 -&gt; INFO 005 Auto detected peer address: 172.17.0.2:30303status:STARTED 進入到容器後，首先部署一個 chaincode： $ peer chaincode deploy -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02 \\-c '&#123;\"Function\":\"init\", \"Args\": [\"kairen\",\"100\", \"pingyu\", \"200\"]&#125;'...81a73fa1fabe6e385f3c609cef8915a732ee74179abde55f4ac7addf4e7c35ac4a669a7d9a17b2c9a6b3c28b45565b97dc69f4c8f53381ba13251adf5ac6d23d 上面會取得一組 Key。 首先查詢 kairen 的金錢有多少： $ my_key=\"81a73fa1fabe6e385f3c609cef8915a732ee74179abde55f4ac7addf4e7c35ac4a669a7d9a17b2c9a6b3c28b45565b97dc69f4c8f53381ba13251adf5ac6d23d\"$ peer chaincode query -n $&#123;my_key&#125; \\-c '&#123;\"Function\": \"query\", \"Args\": [\"kairen\"]&#125;'...100 接著執行一個交易，我們讓 kairen 付保護費給 pingyu： $ peer chaincode invoke -n $&#123;my_key&#125; \\-c '&#123;\"Function\": \"invoke\", \"Args\": [\"kairen\", \"pingyu\", \"10\"]&#125;' 確認完成交易後，可以查看 pingyu： $ peer chaincode query -n $&#123;my_key&#125; \\-c '&#123;\"Function\": \"query\", \"Args\": [\"pingyu\"]&#125;' References hyperledger-py hyperledger chaincode_example02 8btc Technical Introduction to Hyperledger Fabric","categories":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/categories/Blockchain/"}],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"https://k2r2bai.com/tags/Blockchain/"},{"name":"Hyperledger","slug":"Hyperledger","permalink":"https://k2r2bai.com/tags/Hyperledger/"}]},{"title":"Harbor: 開源 Container Registry","slug":"cncf/harbor","date":"2017-05-10T09:08:54.000Z","updated":"2019-12-02T01:49:42.379Z","comments":true,"path":"2017/05/10/cncf/harbor/","link":"","permalink":"https://k2r2bai.com/2017/05/10/cncf/harbor/","excerpt":"Harbor 是基於 Docker Distribution 進行擴展的 Container Registry 專案，其針對一些企業常用的功能做了許多整合，例如: LDAP、容器映像檔安全掃描的 Clair、容器映像檔簽署的 Notary、儲存後端(S3、Cloud Storage 等)、映像檔副本機制。除此之外，Harbor 亦提供了進階的安全性功能，如使用者管理(User managment)，存取控制(Access control)和活動稽核(Activity auditing)。 Harbor 早期是 VMware 開源作為企業級私有 Container Registry 的專案，但在 2018 年時貢獻給 CNCF 作為 Sandbox 專案進行維護。","text":"Harbor 是基於 Docker Distribution 進行擴展的 Container Registry 專案，其針對一些企業常用的功能做了許多整合，例如: LDAP、容器映像檔安全掃描的 Clair、容器映像檔簽署的 Notary、儲存後端(S3、Cloud Storage 等)、映像檔副本機制。除此之外，Harbor 亦提供了進階的安全性功能，如使用者管理(User managment)，存取控制(Access control)和活動稽核(Activity auditing)。 Harbor 早期是 VMware 開源作為企業級私有 Container Registry 的專案，但在 2018 年時貢獻給 CNCF 作為 Sandbox 專案進行維護。 功能特色 支援 RBAC(Role Based Access Control) 機制: 管理員可以定義 User、Project 與 Image 的權限。 P.S. 這邊的 Project 是 Image 隔離域。Image 可以被存放到不同 Project，然後讓不同 User 存取。 支援 LDAP/AD：可以整合 LDAP/AD 來管理使用者的認證與授權。 Web-based UI：使用者可以透過登入 Web-based UI 來管理 Image。 稽核管理：所有操作都被記錄。 RESTful API：提供 RESTful APIs 來讓其他系統進行整合。 漏洞掃描：整合 Clair 專案來進行 Image 的漏洞掃描。 映像檔簽署：整合 Notary 專案來確保 Image 是可信任的。 可以查看 Harbor Features 來了解更多功能特色。 Harbor 簡單部署與使用Harbor 提供兩種方法進行安裝： Online installer 這種安裝方式會從 Docker hub 下載 Harbor 所需的映像檔，因此 installer 檔案較輕量。 Offline installer 當無任何網際網路連接的情況下使用此種安裝方式，預先將所需的映像檔打包，因此 installer 檔案較大。 事前準備Harbor 會部署數個 Docker container，所以部署的主機需要能支援 Docker 的 Linux distribution。而部署主機需要安裝以下套件： Python 版本2.7+。 Docker Engine 版本 1.10+。Docker 安裝方式，請參考：Install Docker Docker Compose 版本 1.6.0+。Docker Compose 安裝方式，請參考：Install Docker Compose 官方安裝指南說明是 Linux 且要支援 Docker，但 Windows 支援 Docker 部署 Harbor 還需要驗證是否可行。 安裝步驟大致可分為以下階段： 下載 installer 設定 Harbor 執行安裝腳本 下載 installerinstaller 的二進制檔案可以從 release page 下載，選擇您需要 Online installer 或者 Offline installer，下載完成後，使用tar將 package 解壓縮： Online installer： $ tar xvf harbor-online-installer-&lt;version&gt;.tgz Offline installer： $ tar xvf harbor-offline-installer-&lt;version&gt;.tgz 設定 HarborHarbor 的設定與參數都在harbor.cfg中。 harbor.cfg中的參數分為required parameters與optional parameters required parameters 這類的參數是必須設定的，且會影響使用者更新harbor.cfg後，重新執行安裝腳本來重新安裝 Harbor。 optional parameters 這類的參數為使用者自行決定是否設定，且只會在第一次安裝時，這些參數的配置才會生效。而 Harbor 啟動後，可以透過 Web UI 進行修改。 Configuring storage backend (optional)預設的情況下，Harbor 會將 Docker image 儲存在本機的檔案系統上，在生產環境中，您可以考慮使用其他 storage backend 而不是本機的檔案系統，像是 S3, OpenStack Swift, Ceph 等。而僅需更改 common/templates/registry/config.yml。以下為一個接 OpenStack Swift 的範例： storage: swift: username: admin password: ADMIN_PASS authurl: http://keystone_addr:35357/v3/auth tenant: admin domain: default region: regionOne container: docker_images 更多 storage backend 的資訊，請參考：Registry Configuration Reference。 另外官方提供的是改 common/templates/registry/config.yml，感覺寫錯，需再測試其正確性。 執行安裝腳本一旦harbor.cfg與 storage backend (optional) 設定完成後，可以透過install.sh腳本開始安裝 Harbor。從 Harbor 1.1.0 版本之後，已經整合Notary，但是預設的情況下安裝是不包含Notary支援： $ sudo ./install.sh Online installer 會從 Docker hub 下載 Harbor 所需的映像檔，因此會花較久的時間。 如果安裝過程正常，您可以打開瀏覽器並輸入在harbor.cfg中設定的hostname，來存取 Harbor 的 Web UI。 預設的管理者帳號密碼為 admin/Harbor12345。 開始使用 Harbor登入成功後，可以創建一個新的 Project，並使用 Docker command 進行登入，但在登入之前，需要對 Docker daemon 新增--insecure-registry參數。新增--insecure-registry參數至/etc/default/docker中： DOCKER_OPTS=\"--insecure-registry &lt;your harbor.cfg hostname&gt;\" 其他細節，請參考：Test an insecure registry。 若在Ubuntu 16.04的作業系統版本，需要修改/lib/systemd/system/docker.service檔案，並加入一下內容。另外在 CentOS 7.x 版本則不需要加入-H fd://資訊： EnvironmentFile=/etc/default/dockerExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS 修改完成後，重新啟動服務： $ sudo systemctl daemon-reload 服務重啟成功後，透過 Docker command 進行 login： $ docker login &lt;your harbor.cfg hostname&gt; 將映像檔上 tag 之後，上傳至 Harbor： $ docker tag ubuntu:&lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubuntu:16.04$ docker push &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 從 Harbor 抓取上傳的映像檔： $ docker pull &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 更多使用者操作，請參考：Harbor User Guide。 References","categories":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/categories/CNCF/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Container Registry","slug":"Container-Registry","permalink":"https://k2r2bai.com/tags/Container-Registry/"}]},{"title":"品嚐 Moby LinuxKit 的 Linux 作業系統","slug":"container/moby-linuxkit","date":"2017-04-23T09:08:54.000Z","updated":"2019-12-02T01:49:42.382Z","comments":true,"path":"2017/04/23/container/moby-linuxkit/","link":"","permalink":"https://k2r2bai.com/2017/04/23/container/moby-linuxkit/","excerpt":"LinuxKit 是 DockerCon 2017 中推出的工具之一，其主要是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，Docker 公司一直透過 LinuxKit 來建立相關產品，如 Docker for Mac 等。由於要最快的了解功能，因此這邊透過建立簡單的映像檔來學習。","text":"LinuxKit 是 DockerCon 2017 中推出的工具之一，其主要是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，Docker 公司一直透過 LinuxKit 來建立相關產品，如 Docker for Mac 等。由於要最快的了解功能，因此這邊透過建立簡單的映像檔來學習。 在開始前需要準備完成一些事情： 安裝 Git client。 安裝 Docker engine，這邊建立使用 Docker-ce 17.04.0。 安裝 GUN make 工具。 安裝 GUN tar 工具。 建構 Moby 工具首先我們要建構名為 Moby 的工具，這個工具主要提供指定的 YAML 檔來執行描述的建構流程與功能，並利用 Docker 來建構出 Linux 作業系統。在本教學中，最後我們會利用 xhyve 這個 OS X 的虛擬化來提供執行系統實例，當然也可以透過官方的 HyperKit 來進行。 首先透過 Git 來抓取 LinuxKit repos，並進入建構 Moby： $ git clone https://github.com/linuxkit/linuxkit.git$ cd linuxkit$ make &amp;&amp; sudo make install$ moby versionmoby version 0.0commit: 34d508562d7821cb812dd7b9caf4d9fbcdbc9fef 建立 Linux 映像檔當完成建構 Moby 工具後，就可以透過撰寫 YAML 檔來描述 Linux 的建構功能與流程了，這邊建立一個 Docker + SSH 的 Linux 映像檔。首先建立檔名為docker-sshd.yml的檔案，然後加入以下內容： kernel: image: \"linuxkit/kernel:4.9.x\" cmdline: \"console=ttyS0 console=tty0 page_poison=1\"init: - linuxkit/init:63eed9ca7a09d2ce4c0c5e7238ac005fa44f564b - linuxkit/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9 - linuxkit/containerd:18eaf72f3f4f9a9f29ca1951f66df701f873060b - linuxkit/ca-certificates:e091a05fbf7c5e16f18b23602febd45dd690ba2fonboot: - name: sysctl image: \"linuxkit/sysctl:1f5ec5d5e6f7a7a1b3d2ff9dd9e36fd6fb14756a\" net: host pid: host ipc: host capabilities: - CAP_SYS_ADMIN readonly: true - name: sysfs image: linuxkit/sysfs:6c1d06f28ddd9681799d3950cddf044b930b221c - name: binfmt image: \"linuxkit/binfmt:c7e69ebd918a237dd086a5c58dd888df772746bd\" binds: - /proc/sys/fs/binfmt_misc:/binfmt_misc readonly: true - name: format image: \"linuxkit/format:53748000acf515549d398e6ae68545c26c0f3a2e\" binds: - /dev:/dev capabilities: - CAP_SYS_ADMIN - CAP_MKNOD - name: mount image: \"linuxkit/mount:d2669e7c8ddda99fa0618a414d44261eba6e299a\" binds: - /dev:/dev - /var:/var:rshared,rbind capabilities: - CAP_SYS_ADMIN rootfsPropagation: shared command: [\"/mount.sh\", \"/var/lib/docker\"]services: - name: rngd image: \"linuxkit/rngd:c42fd499690b2cb6e4e6cb99e41dfafca1cf5b14\" capabilities: - CAP_SYS_ADMIN oomScoreAdj: -800 readonly: true - name: dhcpcd image: \"linuxkit/dhcpcd:57a8ef29d3a910645b2b24c124f9ce9ef53ce703\" binds: - /var:/var - /tmp/etc:/etc capabilities: - CAP_NET_ADMIN - CAP_NET_BIND_SERVICE - CAP_NET_RAW net: host oomScoreAdj: -800 - name: ntpd image: \"linuxkit/openntpd:a570316d7fc49ca1daa29bd945499f4963d227af\" capabilities: - CAP_SYS_TIME - CAP_SYS_NICE - CAP_SYS_CHROOT - CAP_SETUID - CAP_SETGID net: host - name: docker image: \"linuxkit/docker-ce:741bf21513328f674e0cdcaa55492b0b75974e08\" capabilities: - all net: host mounts: - type: cgroup options: [\"rw\",\"nosuid\",\"noexec\",\"nodev\",\"relatime\"] binds: - /var/lib/docker:/var/lib/docker - /lib/modules:/lib/modules - name: sshd image: \"linuxkit/sshd:e108d208adf692c8a0954f602743e0eec445364e\" capabilities: - all net: host pid: host binds: - /root/.ssh:/root/.ssh - /etc/resolv.conf:/etc/resolv.conf - name: test-docker-bench image: \"linuxkit/test-docker-bench:2f941429d874c5dcf05e38005affb4f10192e1a8\" ipc: host pid: host net: host binds: - /run:/var/run capabilities: - allfiles: - path: etc/docker/daemon.json contents: '&#123;\"debug\": true&#125;' - path: root/.ssh/authorized_keys contents: 'SSH_KEY'trust: image: - linuxkit/kernel - linuxkit/binfmt - linuxkit/rngdoutputs: - format: kernel+initrd - format: iso-bios P.S.請修改SSH_KEY內容為你的系統 ssh public key。 這邊說明幾個 YAML 格式意義： kernel: 指定 Docker 映像檔的核心版本，會包含一個 Linux 核心與檔案系統的 tar 檔，會將核心建構在/kernel目錄中。 init: 是一個 Docker Container 的 init 行程基礎，裡面包含init、containerd、runC與其他等工具。 onboot: 指定要建構的系統層級工具，會依據定義順序來執行，該類型如: dhcpd 與 ntpd 等。 services: 指定要建構服務，通常會是系統開啟後執行，如 ngnix、apache2。 files:要複製到該 Linux 系統映像檔中的檔案。 outputs:輸出的映像檔格式。 更多 YAML 格式說明可以參考官方 LinuxKit YAML。目前 LinuxKit 的映像檔來源可以參考 Docker Hub 撰寫完後，就可以透過 Moby 工具進行建構 Linux 映像檔了： $ moby build docker-sshd.ymlExtract kernel image: linuxkit/kernel:4.9.xPull image: linuxkit/kernel:4.9.x...Create outputs: docker-sshd-kernel docker-sshd-initrd.img docker-sshd-cmdline docker-sshd.iso 完成後會看到以下幾個檔案： docker-sshd-kernel: 為 RAW Kernel 映像檔. docker-sshd-initrd.img: 為初始化 RAW Disk 檔案. docker-sshd-cmdline: Command line options 檔案. docker-sshd.iso: Docker SSHD ISO 格式映像檔. 測試映像檔當完成建構映像檔後，就可以透過一些工具來進行測試，這邊採用 xhyve 來執行實例，首先透過 Git 取得 xhyve repos，並建構與安裝： $ git clone https://github.com/mist64/xhyve$ cd xhyve$ make &amp;&amp; cp build/xhyve /usr/local/bin/$ xhyveUsage: xhyve [-behuwxMACHPWY] [-c vcpus] [-g &lt;gdb port&gt;] [-l &lt;lpc&gt;] [-m mem] [-p vcpu:hostcpu] [-s &lt;pci&gt;] [-U uuid] -f &lt;fw&gt; xhyve 是 FreeBSD 虛擬化技術 bhyve 的 OS X 版本，是以 Hypervisor.framework 為基底的上層工具，這是除了 VirtualBox 與 VMwar 的另外選擇，並且該工具非常的輕巧，只有幾 KB 的容量。 接著撰寫 xhyve 腳本來啟動映像檔： #!/bin/shKERNEL=\"docker-sshd-kernel\"INITRD=\"docker-sshd-initrd.img\"CMDLINE=\"console=ttyS0 console=tty0 page_poison=1\"MEM=\"-m 1G\"PCI_DEV=\"-s 0:0,hostbridge -s 31,lpc\"LPC_DEV=\"-l com1,stdio\"ACPI=\"-A\"#SMP=\"-c 2\"# sudo if you want networking enabledNET=\"-s 2:0,virtio-net\"xhyve $ACPI $MEM $SMP $PCI_DEV $LPC_DEV $NET -f kexec,$KERNEL,$INITRD,\"$CMDLINE\" 修改KERNEL與INITRD為 docker-sshd 的映像檔。 完成後就可以進行啟動測試： $ chmod u+x run.sh$ sudo ./run.shWelcome to LinuxKit ## . ## ## ## == ## ## ## ## ## === /&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\\___/ === ~~~ &#123;~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/.../ # lsbin etc lib root srv usrcontainers home media run sys vardev init proc sbin tmp/ # ip...4: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 inet 192.168.64.4/24 brd 192.168.64.255 scope global eth0 valid_lft forever preferred_lft forever14: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 scope global docker0 valid_lft forever preferred_lft forever 驗證映像檔服務當看到上述結果後，表示作業系統開啟無誤，這時候我們要測試系統服務是否正常，首先透過 SSH 來進行測試，在剛剛新增的 ssh public key 主機上執行以下： $ ssh root@192.168.64.4moby-aa16c789d03b:~# uname -r4.9.25-linuxkitmoby-aa16c789d03b:~# exit 查看 Docker 是否啟動： moby-aa16c789d03b:~# netstat -xpActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node PID/Program name Pathunix 2 [ ] DGRAM 33822 606/dhcpcdunix 3 [ ] STREAM CONNECTED 33965 748/ntpd: dns enginunix 3 [ ] STREAM CONNECTED 33960 747/ntpd: ntp enginunix 3 [ ] STREAM CONNECTED 33964 747/ntpd: ntp enginunix 3 [ ] STREAM CONNECTED 33959 642/ntpdunix 3 [ ] STREAM CONNECTED 34141 739/dockerdunix 3 [ ] STREAM CONNECTED 34142 751/docker-containe /var/run/docker/libcontainerd/docker-containerd.sock 最後關閉虛擬機可以透過以下指令完成： moby-aa16c789d03b:~# haltTerminated","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"Moby","slug":"Moby","permalink":"https://k2r2bai.com/tags/Moby/"},{"name":"Microkernel","slug":"Microkernel","permalink":"https://k2r2bai.com/tags/Microkernel/"}]},{"title":"TensorFlow 基本使用與分散式概念","slug":"tensorflow/introduction","date":"2017-04-10T08:23:01.000Z","updated":"2019-12-02T01:49:42.401Z","comments":true,"path":"2017/04/10/tensorflow/introduction/","link":"","permalink":"https://k2r2bai.com/2017/04/10/tensorflow/introduction/","excerpt":"TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。","text":"TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。 TensorFlow 其實在意思上是要用兩個部分來解釋，Tensor 與 Flow： Tensor：是中文翻譯是張量，其實就是一個n維度的陣列或列表。如一維 Tensor 就是向量，二維 Tensor 就是矩陣等等. Flow：是指 Graph 運算過程中的資料流. Data Flow Graphs資料流圖(Data Flow Graphs)是一種有向圖的節點(Node)與邊(Edge)來描述計算過程。圖中的節點表示數學操作，亦表示資料 I/O 端點; 而邊則表示節點之間的關析，用來傳遞操作之間互相使用的多維陣列(Tensors)，而 Tensor 是在圖中流動的資料表示。一旦節點相連的邊傳來資料流，這時節點就會被分配到運算裝置上異步(節點之間)或同步(節點之內)的執行。 TensorFlow 基本使用在開始進行 TensorFlow 之前，需要了解幾個觀念： 使用 tf.Graph 來表示計算任務. 採用tensorflow::Session的上下文(Context)來執行圖. 以 Tensor 來表示所有資料，可看成擁有靜態資料類型，但有動態大小的多維陣列與列表，如 Boolean 或 String 轉成數值類型. 透過tf.Variable來維護狀態. 透過 feed 與 fetch 來任意操作(Arbitrary operation)給予值或從中取得資料. TensorFlow 的圖中的節點被稱為 op(operation)。一個op會有 0 至多個 Tensor，而每個 Tensor 是一種類別化的多維陣列，例如把一個圖集合表示成四維浮點陣列，分別為[batch, height, width, channels]。 利用三種不同稱呼來描述 Tensor 的維度，Shape、Rank 與 Dimension。可參考 Rank, Shape, 和 Type。 一般只有 shape 能夠直接被 print，而 Tensor 則需要 Session 來提供，一般需要三個操作步驟： 建立 Tensor. 新增 op. 建立 Session(包含一個 Graph)來執行運算. 以下是一個簡單範例，說明如何建立運算： # coding=utf-8import tensorflow as tfa = tf.constant(1)b = tf.constant(2)c = tf.constant(3)d = tf.constant(4)add1 = tf.add(a, b)mul1 = tf.multiply(b, c)add2 = tf.add(c, d)output = tf.add(add1, mul1)with tf.Session() as sess: print sess.run(output) 執行流程如下圖： 以下是一個簡單範例，說明如何建立多個 Graph： # coding=utf-8import tensorflow as tflogs_path = './basic_tmp'# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點g1 = tf.Graph()with g1.as_default(): a = tf.constant([1.5, 6.0]) b = tf.constant([1.5, 3.2]) c = a * bwith tf.Graph().as_default() as g2: # 建立一個 1x2 矩陣與 2x1 矩陣 op m1 = tf.constant([[1., 0., 2.], [-1., 3., 1.]]) m2 = tf.constant([[3., 1.], [2., 1.], [1., 0.]]) m3 = tf.matmul(m1, m2) # 矩陣相乘# 在 session 執行 graph，並進行資料數據操作 `c`。# 然後指派給 cpu 做運算with tf.Session(graph=g1) as sess_cpu: with tf.device(\"/cpu:0\"): writer = tf.summary.FileWriter(logs_path, graph=g1) print(sess_cpu.run(c))with tf.Session(graph=g2) as sess_gpu: with tf.device(\"/gpu:0\"): result = sess_gpu.run(m3) print(result)# 使用 tf.InteractiveSession 方式來印出內容(不會實際執行)it_sess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# 使用初始器 initializer op 的 run() 方法初始化 'x'x.initializer.run()sub = tf.subtract(x, a)print sub.eval()it_sess.close() 範例來至 Basic Usage。 指定 Device 可以看這邊 Using GPU. 上面範例可以看到建立了一個 Graph 的計算過程c，而當直接執行到c時，並不會真的執行運算，而是在sess會話建立後，並透過sess執行分配給 CPU 或 GPU 之類設備進行運算後，才會回傳一個節點的 Tensor，在 Python 中 Tensor 是一個 Numpy 的 ndarry 物件。 TensorFlow 也可以透過變數來維護 Graph 的執行過程狀態，這邊提供一個簡單的累加器： # coding=utf-8import tensorflow as tf# 建立一個變數 counter，並初始化為 0state = tf.Variable(0, name=\"counter\")# 建立一個常數 op 為 1，並用來累加 stateone = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# 啟動 Graph 前，變數必須先被初始化(init) opinit_op = tf.global_variables_initializer()# 啟動 Graph 來執行 opwith tf.Session() as sess: sess.run(init_op) print sess.run(state) # 執行 op 並更新 state for _ in range(3): sess.run(update) print sess.run(state) 更多細節可以查看 Variables。 另外可以利用 Fetch 方式來一次取得多個節點的 Tensor，範例如下： # coding=utf-8import tensorflow as tfinput1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.multiply(input1, intermed)with tf.Session() as sess: # 一次取得多個 Tensor result = sess.run([mul, intermed]) print result 而當我們想要在執行 Session 時，臨時替換 Tensor 內容的話，就可以利用 TensorFlow 內建的 Feed 方法來解決： # coding=utf-8import tensorflow as tfinput1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)with tf.Session() as sess: # 透過 feed 來更改 op 內容，這只會在執行時有效 print sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;) print sess.run([output]) TensorFlow 分散式運算本節將以 TensorFlow 分散式深度學習為例。 gRPCgRPC(google Remote Procedure Call) 是 Google 開發的基於 HTTP/2 和 Protocol Buffer 3 的 RPC 框架，該框架有各種常見語言的實作，如 C、Java 與 Go 等語言，提供輕鬆跨語言的呼叫。 概念說明客戶端(Client)、叢集(Cluster)、工作(Job)、任務(Task)、TensorFlow 伺服器、Master 與 Worker 服務。 如圖所示，幾個流程說明如下： 整個系统映射到 TensorFlow 叢集. 參數伺服器映射到一個 Job. 每個模型(Model)副本映射到一個 Job. 每台實體運算節點映射到其 Job 中的 Task. 每個 Task 都有一個 TF Server，並利用 Master 服務來進行溝通與協調工作，而 Worker 服務則透過本地裝置(CPU 或 GPU)進行 TF graph 運算. TensorFlow 叢集裡包含了一個或多個工作(Job)，每個工作又可以拆分成一個或多個任務(Task)，簡單說 Cluster 是 Job 的集合，而 Job 是 Task 的集合。叢集概念主要用在一個特定層次對象，如訓練神經網路、平行操作多台機器等，一個叢集物件可以透過tf.train.ClusterSpec來定義。 如上所述，TensorFlow 的叢集就是一組工作任務，每個任務是一個服務，而服務又分成Master與Worker這兩種，並提供給Client進行操作。 Client：是用於建立 TensorFlow 計算 Graph，並建立與叢集進行互動的tensorflow::Session行程，一般由 Python 或 C++ 實作，單一客戶端可以同時連接多個 TF 伺服器連接，同時也能被多個 TF 伺服器連接. Master Service：是一個 RPC 服務行程，用來遠端連線一系列分散式裝置，主要提供tensorflow::Session介面，並負責透過 Worker Service 與工作的任務進行溝通. Worker Service：是一個可以使用本地裝置(CPU 或 GPU)對部分 Graph 進行運算的 RPC 邏輯，透過worker_service.proto介面來實作，所有 TensorFlow 伺服器均包含了 Worker Service 邏輯. TensorFlow 伺服器是運行tf.train.Server實例的行程，其為叢集一員，並有 Master 與 Worker 之分。 而 TensorFlow 的工作(Job)可拆成多個相同功能的任務(Task)，這些工作又分成Parameter server與Worker，兩者功能說明如下： Parameter server(ps):是分散式系統縮放至工業大小機器學習的問題，它提供工作節點與伺服器節點之間的非同步與零拷貝 key-value 的溝通，並支援資料的一致性模型的分散式儲存。在 TensorFlow 中主要根據梯度更新變數，並儲存於tf.Variable，可理解成只儲存 TF Model 的變數，並存放 Variable 副本. Worker:通常稱為計算節點，一般管理無狀態(Stateless)，且執行密集型的 Graph 運算資源，並根據變數運算梯度。存放 Graph 副本. Parameter Server 詳解 一般對於小型規模訓練，這種資料與參數量不多時，可以用一個 CPU 來同時執行兩種任務。而中型規模訓練，資料量較大，但參數量不多時，計算梯度的工作負載較高，而參數更新負載較低，所以計算梯度交給若干個 CPU 或 GPU 去執行，而更新參數則交給一個 CPU 即可。對於大型規模訓練，資料與參數量多時，不僅計算梯度需要部署多個 CPU 或 GPU，連更新參數也要不說到多個 CPU 中。 然而單一節點能夠裝載的 CPU 與 GPU 是有限的，所以在大量訓練時就需要多台機器來提供運算能力的擴展。 分散式變數伺服器(Parameter Server)當在較大規模的訓練時，隨著模型的變數越來越多，很可能造成單一節點因為效能問題，而無法負荷模型變數儲存與更新時，這時候就需要將變數分開到不同機器來做儲存與更新。而 TensorFlow 提供了變數伺服器的邏輯實現，並可以用多台機器來組成叢集，類似分散式儲存結構，主要用來解決變數的儲存與更新效能問題。 撰寫分散式程式注意概念當我們在寫分散式程式時，需要知道使用的副本與訓練模式。 In-graph 與 Between-graph 副本模式下圖顯示兩者差異，而這邊也在進行描述。 In-graph：只有一個 Clinet(主要呼叫tf::Session行程)，並將裡面變數與 op 指定給對應的 Job 完成，因此資料分發只由一個 Client 完成。這種方式設定簡單，其他節點只需要 join 操作，並提供一個 gRPC 位址來等待任務。但是訓練資料只在單一節點，因此要把資料分發到不同機器時，會影響平行訓練效能。可理解成所有 op 都在同一個 Graph 中，伺服器只需要做join()功能. Between-graph：多個獨立 Client 建立相同 Graph(包含變數)，並透過tf.train.replica_device_setter將這些參數映射到 ps 上，即訓練的變數儲存在 Parameter Server，而資料不用分發，資料分片(Shards)會存在個計算節點，因此個節點自己算自己的，算完後，把要更新變數告知 Parameter Server 進行更新。適合在 TB 級別的資料量使用，節省大量資料傳輸時間，也是深度學習推薦模式。 同步(Synchronous)訓練與非同步(Asynchronous)訓鍊TensorFlow 的副本擁有 in-graph 和 between-graph 模式，這兩者都支援了同步與非同步更新。本節將說明同步與非同步兩者的差異為何。 Synchronous：每個 Graph 的副本讀取相同 Parameter 的值，然後平行計算梯度(gradients)，將所有計算完的梯度放在一起處理，當每次更新梯度時，需要等所以分發的資料計算完成，並回傳結果來把梯度累加計算平均，在進行更新變數。好處在於使用 loss 的下降時比較穩定，壞處就是要等最慢的分片計算時間。 可以利用tf.train.SyncReplicasOptimizer來解決這個問題(在 Between-graph 情況下)，而在 In-graph 則將所有梯度平均即可。 Asynchronous：自己計算完梯度後，就去更新 paramenter，不同副本之前不會進行協調進度，因此計算資源被充分的利用。缺點是 loss 的下降不穩定。 一般在資料量小，且各節點計算能力平均下，適合使用同步模式; 反之在資料量大與各節點效能差異不同時，適合用非同步。 簡單分散式訓練程式TensorFlow 提供建立 Server 函式來進行測試使用，以下是建立一個分散式訓練 Server 程式server.py： # coding=utf-8import tensorflow as tf# 定義 Clustercluster = tf.train.ClusterSpec(&#123;\"worker\": [\"localhost:2222\"]&#125;)# 建立 Worker serverserver = tf.train.Server(cluster,job_name=\"worker\",task_index=0)server.join() 也可以透過tf.train.Server.create_local_server() 來建立 Local Server 當確認程式沒有任何問題後，就可以透過以下方式啟動： $ python server.py2017-04-10 18:19:41.953448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0)2017-04-10 18:19:41.983913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -&gt; &#123;0 -&gt; localhost:2222&#125;2017-04-10 18:19:41.984946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2222 接著我們要撰寫 Client 端來進行定義 Graph 運算的程式client.py： # coding=utf-8import tensorflow as tf# 執行目標 Sessionserver_target = \"grpc://localhost:2222\"logs_path = './basic_tmp'# 指定 worker task 0 使用 CPU 運算with tf.device(\"/job:worker/task:0\"): with tf.device(\"/cpu:0\"): a = tf.constant([1.5, 6.0], name='a') b = tf.Variable([1.5, 3.2], name='b') c = (a * b) + (a / b) d = c * a y = tf.assign(b, d)# 啟動 Sessionwith tf.Session(server_target) as sess: sess.run(tf.global_variables_initializer()) writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph()) print(sess.run(y)) 完成後即可透過以下指令測試： $ python client.py[ 4.875 126.45000458] 線性迴歸訓練程式上面範例提供了很簡單的 Client 與 Server 運算操作。而這邊建立一個 Between-graph 執行程式bg_dist.py： # coding=utf-8import tensorflow as tfimport numpy as npparameter_servers = [\"localhost:2222\"]workers = [\"localhost:2223\", \"localhost:2224\"]tf.app.flags.DEFINE_string(\"job_name\", \"\", \"輸入 'ps' 或是 'worker'\")tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Job 的任務 index\")FLAGS = tf.app.flags.FLAGSdef main(_): cluster = tf.train.ClusterSpec(&#123;\"ps\": parameter_servers, \"worker\": workers&#125;) server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index) if FLAGS.job_name == \"ps\": server.join() elif FLAGS.job_name == \"worker\": train_X = np.linspace(-1.0, 1.0, 100) train_Y = 2.0 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10.0 X = tf.placeholder(\"float\") Y = tf.placeholder(\"float\") # Assigns ops to the local worker by default. with tf.device(tf.train.replica_device_setter( worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster)): w = tf.Variable(0.0, name=\"weight\") b = tf.Variable(0.0, name=\"bias\") # 損失函式，用於描述模型預測值與真實值的差距大小，常見為`均方差(Mean Squared Error)` loss = tf.square(Y - tf.multiply(X, w) - b) global_step = tf.Variable(0) train_op = tf.train.AdagradOptimizer(0.01).minimize( loss, global_step=global_step) saver = tf.train.Saver() summary_op = tf.summary.merge_all() init_op = tf.global_variables_initializer() # 建立 \"Supervisor\" 來負責監督訓練過程 sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=\"/tmp/train_logs\", init_op=init_op, summary_op=summary_op, saver=saver, global_step=global_step, save_model_secs=600) with sv.managed_session(server.target) as sess: loss_value = 100 while not sv.should_stop() and loss_value &gt; 70.0: # 執行一個非同步 training 步驟. # 若要執行同步可利用`tf.train.SyncReplicasOptimizer` 來進行 for (x, y) in zip(train_X, train_Y): _, step = sess.run([train_op, global_step], feed_dict=&#123;X: x, Y: y&#125;) loss_value = sess.run(loss, feed_dict=&#123;X: x, Y: y&#125;) print(\"步驟: &#123;&#125;, loss: &#123;&#125;\".format(step, loss_value)) sv.stop()if __name__ == \"__main__\": tf.app.run() 若想指定 Device 可以用以下方式： tf.train.replica_device_setter(ps_tasks=0, ps_device='/job:ps', worker_device='/job:worker', merge_devices=True, cluster=None, ps_ops=None) 撰寫完成後，透過以下指令來進行測試： $ python liner_dist.py --job_name=ps --task_index=0$ python liner_dist.py --job_name=worker --task_index=0$ python liner_dist.py --job_name=worker --task_index=1 Tensorboard 視覺化工具Tensorboard 是 TensorFlow 內建的視覺化工具，我們可以透過讀取事件紀錄結構化的資料，來顯示以下幾個項目來提供視覺化： Event：訓練過程中統計資料(平均值等)變化狀態. Image：訓練過程中紀錄的 Graph. Audio：訓練過程中紀錄的 Audio. Histogram：順練過程中紀錄的資料分散圖 一個範例程式如下所示： # coding=utf-8import tensorflow as tflogs_path = './tmp/1'# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點g1 = tf.Graph()with g1.as_default(): a = tf.constant([1.5, 6.0], name='a') b = tf.Variable([1.5, 3.2], name='b') c = (a * b) + (a / b) d = c * a y = tf.assign(b, d)# 在 session 執行 graph，並進行資料數據操作 `c`。# 然後指派給 cpu 做運算with tf.Session(graph=g1) as sess_cpu: with tf.device(\"/cpu:0\"): sess_cpu.run(tf.global_variables_initializer()) writer = tf.summary.FileWriter(logs_path, graph=g1) print(sess_cpu.run(y)) 執行後會看到當前目錄產生tmp_mnist logs 檔案，這時候就可以透過 thensorboard 來視覺化訓練結果： $ tensorboard --logdir=run1:./tmp/1 --port=6006 run1 是當有多次 log 被載入時做為區別用。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://k2r2bai.com/categories/TensorFlow/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://k2r2bai.com/tags/Python/"},{"name":"ML/DL","slug":"ML-DL","permalink":"https://k2r2bai.com/tags/ML-DL/"}]},{"title":"[Helm] 基礎介紹與使用","slug":"kubernetes/helm/quick-start","date":"2017-03-25T09:08:54.000Z","updated":"2019-12-02T01:49:42.395Z","comments":true,"path":"2017/03/25/kubernetes/helm/quick-start/","link":"","permalink":"https://k2r2bai.com/2017/03/25/kubernetes/helm/quick-start/","excerpt":"Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。使用 Helm 有以下幾個好處： 查詢與使用熱門的 Kubernetes Chart 軟體套件。 以 Kuberntes Chart 來分享自己的應用程式。 可利用 Chart 來重複建立應用程式。 智能地管理 Kubernetes manifest 檔案。 管理釋出的 Helm 版本。","text":"Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。使用 Helm 有以下幾個好處： 查詢與使用熱門的 Kubernetes Chart 軟體套件。 以 Kuberntes Chart 來分享自己的應用程式。 可利用 Chart 來重複建立應用程式。 智能地管理 Kubernetes manifest 檔案。 管理釋出的 Helm 版本。 概念Helm 有三個觀念需要我們去了解，分別為 Chart、Release 與 Repository，其細節如下： Chart：主要定義要被執行的應用程式中，所需要的工具、資源、服務等資訊，有點類似 Homebrew 的 Formula 或是 APT 的 dpkg 檔案。 Release：一個被執行於 Kubernetes 的 Chart 實例。Chart 能夠在一個叢集中擁有多個 Release，例如 MySQL Chart，可以在叢集建立基於該 Chart 的兩個資料庫實例，其中每個 Release 都會有獨立的名稱。 Repository：主要用來存放 Chart 的倉庫，如 KubeApps。 可以理解 Helm 主要目標就是從 Chart Repository 中，查找部署者需要的應用程式 Chart，然後以 Release 形式來部署到 Kubernetes 中進行管理。 Helm 系統元件Helm 主要分為兩種元件，Helm Client 與 Tiller Server，兩者功能如下： Helm Client：一個安裝 Helm CLI 的機器，該機器透過 gRPC 連接 Tiller Server 來對 Repository、Chart 與 Release 等進行管理與操作，如建立、刪除與升級等操作，細節可以查看 Helm Documentation。 Tiller Server：主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 Release)。 兩者溝通架構圖如下所示： 事前準備安裝前需要確認環境滿足以下幾膽： 已部署 Kubernetes 叢集。 操作端安裝 kubectl 工具。 操作端可以透過 kubectl 工具管理到 Kubernetes（可用的 kubectl config）。 安裝 HelmHelm 有許多種安裝方式，這邊個人比較喜歡用 binary 檔案來進行安裝： $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx$ sudo mv linux-amd64/helm /usr/local/bin/$ helm version OS X 為下載 helm-v2.4.1-darwin-amd64.tar.gz。 初始化 Helm在開始使用 Helm 之前，我們需要建置 Tiller Server 來對 Kubernetes 的管理，而 Helm CLI 內建也提供了快速初始化指令，如下： $ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller$HELM_HOME has been configured at /root/.helm.Tiller (the helm server side component) has been installed into your Kubernetes Cluster.Happy Helming! 若之前只用舊版想要更新可以透過以下指令helm init --upgrade來達到效果。 完成後，就可以透過 kubectl 來查看 Tiller Server 是否被建立： $ kubectl get po,svc -n kube-system -l app=helmNAME READY STATUS RESTARTS AGEpo/tiller-deploy-1651596238-5lsdw 1/1 Running 0 3mNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/tiller-deploy 192.162.204.144 &lt;none&gt; 44134/TCP 3m 接著透過 helm ctl 來查看資訊： $ export KUBECONFIG=/etc/kubernetes/admin.conf$ export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk '/Endpoints/&#123;print $2&#125;')# wait for a few minutes$ helm versionClient: &amp;version.Version&#123;SemVer:\"v2.8.1\", GitCommit:\"6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.8.1\", GitCommit:\"6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2\", GitTreeState:\"clean\"&#125; 部署 Chart Release 實例當完成初始化後，就可以透過 helm ctl 來管理與部署 Chart Release，我們可以到 KubeApps 查找想要部署的 Chart，如以下快速部屬 Jenkins 範例，首先先透過搜尋來查看目前應用程式版本： $ helm search jenkinsNAME VERSION DESCRIPTIONstable/jenkins 0.6.3 Open source continuous integration server. It s... 接著透過inspect指令查看該 Chart 的參數資訊： $ helm inspect stable/jenkins...Persistence: Enabled: true 從中我們會發現需要建立一個 PVC 來提供持久性儲存。 因此需要建立一個 PVC 提供給 Jenkins Chart 來儲存使用，這邊我們自己手動建立jenkins-pv-pvc.yml檔案： apiVersion: v1kind: PersistentVolumemetadata: name: jenkins-pv labels: app: jenkinsspec: capacity: storage: 10Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /var/nfs/jenkins server: 172.20.3.91---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: jenkins-pvc labels: app: jenkinsspec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 接著透過 kubectl 來建立： $ kubectl create -f jenkins-pv-pvc.ymlpersistentvolumeclaim \"jenkins-pvc\" createdpersistentvolume \"jenkins-pv\" created$ kubectl get pv,pvcNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGEpv/jenkins-pv 10Gi RWO Recycle Bound default/jenkins-pvc 20sNAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGEpvc/jenkins-pvc Bound jenkins-pv 10Gi RWO 20s 當 PVC 建立完成後，就可以開始透過 Helm 來建立 Jenkins Release： $ export PVC_NAME=$(kubectl get pvc -l app=jenkins --output=template --template=\"&#123;&#123;with index .items 0&#125;&#125;&#123;&#123;.metadata.name&#125;&#125;&#123;&#123;end&#125;&#125;\")$ helm install --name demo --set Persistence.ExistingClaim=$&#123;PVC_NAME&#125; stable/jenkinsNAME: demoLAST DEPLOYED: Thu May 25 17:53:50 2017NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1beta1/DeploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdemo-jenkins 1 1 1 0 1s==&gt; v1/SecretNAME TYPE DATA AGEdemo-jenkins Opaque 2 1s==&gt; v1/ConfigMapNAME DATA AGEdemo-jenkins-tests 1 1sdemo-jenkins 3 1s==&gt; v1/ServiceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemo-jenkins 192.169.143.140 &lt;pending&gt; 8080:30152/TCP,50000:31806/TCP 1s... P.S. install 指令可以安裝來至Chart repository、壓縮檔 Chart、一個 Chart 目錄與Chart URL。 這邊 install 可以額外透過以下兩種方式來覆寫參數，在這之前可以先透過helm inspect values &lt;chart&gt;來取得使用的變數。 –values：指定一個 YAML 檔案來覆寫設定。$ echo -e 'Master:\\n AdminPassword: r00tme' &gt; config.yaml$ helm install -f config.yaml stable/jenkins –sets：指定一對 Key/value 指令來覆寫。$ helm install --set Master.AdminPassword=r00tme stable/jenkins 完成後就可以透過 helm 與 kubectl 來查看建立狀態： $ helm lsNAME REVISION UPDATED STATUS CHART NAMESPACEdemo 1 Thu May 25 17:53:50 2017 DEPLOYED jenkins-0.6.3 default$ kubectl get po,svcNAME READY STATUS RESTARTS AGEpo/demo-jenkins-3139496662-c0lzk 1/1 Running 0 1mNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/demo-jenkins 192.169.143.140 &lt;pending&gt; 8080:30152/TCP,50000:31806/TCP 1m 由於預設只使用 LoadBalancerSourceRanges 來定義存取策略，但沒有指定任何外部 IP，因此要手動加入以下內容： $ kubectl edit svc demo-jenkinsspec: externalIPs: - 172.20.3.90 完成後再次查看 Service 資訊： $ kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemo-jenkins 192.169.143.140 ,172.20.3.90 8080:30152/TCP,50000:31806/TCP 10m 這時候就可以透過 http://172.20.3.90:8080 連進去 Jenkins 了，其預設帳號為 admin。 透過以下指令來取得 Jenkins admin 密碼： $ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=\"&#123;.data.jenkins-admin-password&#125;\" | base64 --decode);echobuQ1ik2Q7x 該 Chart 會產生亂數密碼存放到 secret 中。 最後我們也可以透過upgrade指令來更新已經 Release 的 Chart： $ helm upgrade --set Master.AdminPassword=r00tme --set Persistence.ExistingClaim=jenkins-pvc demo stable/jenkinsRelease \"demo\" has been upgraded. Happy Helming!$ helm get values demoMaster: AdminPassword: r00tmePersistence: ExistingClaim: jenkins-pvc$ helm lsNAME REVISION UPDATED STATUS CHART NAMESPACEdemo 2 Tue May 30 21:18:43 2017 DEPLOYED jenkins-0.6.3 default 這邊會看到REVISION會 +1，這可以用來做 rollback 的版本號使用。 刪除 ReleaseHelm 除了基本的建立功能外，其還包含了整個 Release 的生命週期管理功能，如我們不需要該 Release 時，就可以透過以下方式刪除： $ helm del demo$ helm status demo | grep STATUSSTATUS: DELETED 當刪除後，該 Release 並沒有真的被刪除，我們可以透過 helm ls 來查看被刪除的 Release： $ helm ls --allNAME REVISION UPDATED STATUS CHART NAMESPACEdemo 2 Tue May 30 21:18:43 2017 DELETED jenkins-0.6.3 default 當執行 helm ls 指令為加入 --all 時，表示只列出DEPLOYED狀態的 Release。 而當 Release 處於 DELETED 狀態時，我們可以進行一些操作，如 Roll back 或完全刪除 Release： $ helm rollback demo 1Rollback was a success! Happy Helming!$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=\"&#123;.data.jenkins-admin-password&#125;\" | base64 --decode);echoBIsLlQTN9l$ helm del demo --purgerelease \"demo\" deleted# 這時執行以下指令就不會再看到已刪除的 Release.$ helm ls --all 建立簡單 Chart 結構Helm 提供了 create 指令來建立一個 Chart 基本結構： $ helm create example$ tree example/example/├── charts├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt│ └── service.yaml└── values.yaml 當我們設定完 Chart 後，就可以透過 helm 指令來打包： $ helm package example/example-0.1.0.tgz 最後可以用 helm 來安裝： $ helm install ./example-0.1.0.tgz 自己建立 RepositoryHelm 指令除了可以建立 Chart 基本結構外，很幸運的也提供了建立 Helm Repository 的功能，建立方式如下： $ helm serve --repo-path example-0.1.0.tgz$ helm repo add example http://repo-url 另外 helm repo 也可以加入來至於 Github 與 HTTP 伺服器的網址來提供服務。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Helm","slug":"Helm","permalink":"https://k2r2bai.com/tags/Helm/"}]},{"title":"Kubespray 部署實體機 Kubernetes v1.6 叢集","slug":"kubernetes/deploy/kubespray","date":"2017-03-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.393Z","comments":true,"path":"2017/03/17/kubernetes/deploy/kubespray/","link":"","permalink":"https://k2r2bai.com/2017/03/17/kubernetes/deploy/kubespray/","excerpt":"Kubespray 是 Kubernetes incubator 中的專案，目標是提供 Production Ready Kubernetes 部署方案，該專案基礎是透過 Ansible Playbook 來定義系統與 Kubernetes 叢集部署的任務，目前 Kubespray 有以下幾個特點： 可以部署在 AWS, GCE, Azure, OpenStack 或者 Baremetal. 部署 High Available Kubernetes 叢集. 可組合性(Composable)，可自行選擇 Network Plugin (flannel, calico, canal, weave) 來部署. 支援多種 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7). 本篇將說明如何透過 Kubespray 部署 Kubernetes 至實體機器節點，安裝版本如下所示： Kubernetes v1.6.4 Etcd v3.1.6 Flannel v0.7.1 Docker v17.04.0-ce","text":"Kubespray 是 Kubernetes incubator 中的專案，目標是提供 Production Ready Kubernetes 部署方案，該專案基礎是透過 Ansible Playbook 來定義系統與 Kubernetes 叢集部署的任務，目前 Kubespray 有以下幾個特點： 可以部署在 AWS, GCE, Azure, OpenStack 或者 Baremetal. 部署 High Available Kubernetes 叢集. 可組合性(Composable)，可自行選擇 Network Plugin (flannel, calico, canal, weave) 來部署. 支援多種 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7). 本篇將說明如何透過 Kubespray 部署 Kubernetes 至實體機器節點，安裝版本如下所示： Kubernetes v1.6.4 Etcd v3.1.6 Flannel v0.7.1 Docker v17.04.0-ce 節點資訊本次安裝測試環境的作業系統採用Ubuntu 16.04 Server，其他細節內容如下： IP Address Role CPU Memory 192.168.121.179 master1 + deploy 2 4G 192.168.121.106 node1 2 4G 192.168.121.197 node2 2 4G 192.168.121.123 node3 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 預先準備資訊 所有節點的網路之間可以互相溝通。 部署節點(這邊為 master1)對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝 Python。 所有節點需要設定/etc/host解析到所有主機。 修改所有節點/etc/resolv.conf： $ echo \"nameserver 8.8.8.8\" | sudo tee /etc/resolv.conf 部署節點(這邊為 master1)需要安裝 Ansible &gt; 2.3.0。 Ubuntu 16.04 安裝最新版 Ansible: $ sudo sed -i 's/us.archive.ubuntu.com/tw.archive.ubuntu.com/g' /etc/apt/sources.list$ sudo apt-get install -y software-properties-common$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git cowsay python-pip python-netaddr libssl-dev 安裝 Kubespray 與準備部署資訊首先透過 pypi 安裝 kubespray-cli，雖然官方說已經改成 Go 語言版本的工具，但是根本沒在更新，所以目前暫時用 pypi 版本： $ sudo pip install -U kubespray 安裝完成後，新增設定檔~/.kubespray.yml，並加入以下內容： $ mkdir /etc/kubespray$ cat &lt;&lt;EOF &gt; ~/.kubespray.ymlkubespray_git_repo: \"https://github.com/kubernetes-incubator/kubespray.git\"# Logging optionsloglevel: \"info\"EOF 接著用 kubespray cli 來產生 inventory 檔案： $ kubespray prepare --masters master1 --etcds master1 --nodes node1 node2 node3$ cat ~/.kubespray/inventory/inventory.cfg 也可以自己建立inventory來描述部署節點。 完成後就可以透過以下指令進行部署 Kubernetes 叢集： $ time kubespray deploy --verbose -u root -k .ssh/id_rsa -n flannelRun kubernetes cluster deployment with the above command ? [Y/n]y...master1 : ok=368 changed=89 unreachable=0 failed=0node1 : ok=305 changed=73 unreachable=0 failed=0node2 : ok=276 changed=62 unreachable=0 failed=0node3 : ok=276 changed=62 unreachable=0 failed=0Kubernetes deployed successfuly 其中-n為部署的網路插件類型，目前支援 calico、flannel、weave 與 canal。 驗證叢集當 Ansible 執行完成後，若沒發生錯誤就可以開始進行操作 Kubernetes，如取得版本資訊： $ kubectl versionClient Version: version.Info&#123;Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4+coreos.0\", GitCommit:\"9212f77ed8c169a0afa02e58dce87913c6387b3e\", GitTreeState:\"clean\", BuildDate:\"2017-04-04T00:32:53Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125;Server Version: version.Info&#123;Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4+coreos.0\", GitCommit:\"9212f77ed8c169a0afa02e58dce87913c6387b3e\", GitTreeState:\"clean\", BuildDate:\"2017-04-04T00:32:53Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125; 取得目前節點狀態： $ kubectl get nodeNAME STATUS AGE VERSIONmaster1 Ready,SchedulingDisabled 11m v1.6.4+coreos.0node1 Ready 11m v1.6.4+coreos.0node2 Ready 11m v1.6.4+coreos.0node3 Ready 11m v1.6.4+coreos.0 查看目前系統 Pod 狀態： $ kubectl get po -n kube-systemNAME READY STATUS RESTARTS AGEdnsmasq-975202658-6jj3n 1/1 Running 0 14mdnsmasq-975202658-h4rn9 1/1 Running 0 14mdnsmasq-autoscaler-2349860636-kfpx0 1/1 Running 0 14mflannel-master1 1/1 Running 1 14mflannel-node1 1/1 Running 1 14mflannel-node2 1/1 Running 1 14mflannel-node3 1/1 Running 1 14mkube-apiserver-master1 1/1 Running 0 15mkube-controller-manager-master1 1/1 Running 0 15mkube-proxy-master1 1/1 Running 1 14mkube-proxy-node1 1/1 Running 1 14mkube-proxy-node2 1/1 Running 1 14mkube-proxy-node3 1/1 Running 1 14mkube-scheduler-master1 1/1 Running 0 15mkubedns-1519522227-thmrh 3/3 Running 0 14mkubedns-autoscaler-2999057513-tx14j 1/1 Running 0 14mnginx-proxy-node1 1/1 Running 1 14mnginx-proxy-node2 1/1 Running 1 14mnginx-proxy-node3 1/1 Running 1 14m","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"}]},{"title":"Ubuntu 16.04 安裝 TensorFlow GPU GTX 1060","slug":"tensorflow/install-from-source","date":"2017-03-12T08:23:01.000Z","updated":"2019-12-02T01:49:42.401Z","comments":true,"path":"2017/03/12/tensorflow/install-from-source/","link":"","permalink":"https://k2r2bai.com/2017/03/12/tensorflow/install-from-source/","excerpt":"本篇主要因為自己買了一片NVIDIA GTX 1060 6G顯卡，但是購買至今只用來玩過一個遊戲，因此才拿來試跑 TensorFlow。","text":"本篇主要因為自己買了一片NVIDIA GTX 1060 6G顯卡，但是購買至今只用來玩過一個遊戲，因此才拿來試跑 TensorFlow。 本次安裝硬體與規格如下： 作業系統: Ubuntu 16.04 Desktop GPU: GeForce® GTX 1060 6G NVIDIA Driver: nvidia-367 Python: 2.7+ TensorFlow: r1.0.1 CUDA: v8.0 cuDNN: v5.1 環境部署如果要安裝 TensorFlow with GPU support 的話，需要滿足以下幾點： NVIDIA Driver. 已安裝 CUDA® Toolkit 8.0. 已安裝 cuDNN v5.1. GPU card with CUDA Compute Capability 6.1(GTX 10-series). libcupti-dev 函式庫. NVIDIA Driver 安裝由於預設 Ubuntu 的 NVIDIA 版本比較舊，或者並沒有安裝相關驅動，因此這邊需要安裝顯卡對應的版本才能夠正常使用，可以透過以下方式進行： $ sudo add-apt-repository -y ppa:graphics-drivers/ppa$ sudo apt-get update$ sudo apt-get install -y nvidia-367 完成後，需重新啟動機器。 CUDA Toolkit 8.0 安裝由於 TensorFlow 支援 GPU 運算時，會需要使用到 CUDA Toolkit 相關功能，可以到 CUDA Toolkit 頁面下載，這邊會下載 Ubuntu Run file 檔案，來進行安裝： $ wget \"https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run\"$ sudo chmod u+x cuda_8.0.61_375.26_linux-run$ ./cuda_8.0.61_375.26_linux-runDo you accept the previously read EULA?accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.77?(y)es/(n)o/(q)uit: nInstall the CUDA 8.0 Toolkit?(y)es/(n)o/(q)uit: yEnter Toolkit Location[ default is /usr/local/cuda-8.0]: enterDo you want to install a symbolic link at /usr/local/cuda?(y)es/(n)o/(q)uit:yInstall the CUDA 8.0 Samples?(y)es/(n)o/(q)uit:yEnter CUDA Samples Location[ defualt is /home/kylebai ]: enter 這邊enter為鍵盤直接按壓，而不是輸入 enter。 安裝完成後，編輯 Home 目錄底下的.bashrc檔案加入以下內容： export PATH=$&#123;PATH&#125;:/usr/local/cuda-8.0/binexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64 最後 Source Bash 檔案與測試 CUDA Toolkit： $ source .bashrc$ sudo nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.39 Driver Version: 375.39 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 0000:01:00.0 On | N/A || 28% 29C P8 6W / 120W | 130MiB / 6069MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1165 G /usr/lib/xorg/Xorg 98MiB || 0 1764 G compiz 29MiB |+-----------------------------------------------------------------------------+ cuDNN 5.1 安裝NVIDIA cuDNN 是一個深度神經網路運算的 GPU 加速原函式庫，這邊需要點選前面的連結，下載cuDNN v5.1 Library for Linux檔案： $ tar xvf cudnn-8.0-linux-x64-v5.1.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ TensorFlow GPU 套件建構本次教學將透過 Source code 建構安裝檔，再進行安裝 TensorFlow，首先安裝相依套件： $ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ sudo apt-get install -y libcupti-dev python-numpy python-dev python-setuptools python-pip python-wheel git oracle-java8-installer$ echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list$ curl -s \"https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg\" | sudo apt-key add -$ sudo apt-get update &amp;&amp; sudo apt-get -y install bazel$ sudo apt-get upgrade -y bazel 接著取得 TensorFlow 專案原始碼，然後進入到 TensorFlow 專案目錄進行 bazel 設定： $ git clone \"https://github.com/tensorflow/tensorflow\"$ cd tensorflow$ ./configure...Do you wish to build TensorFlow with CUDA support? [y/N] yPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5Please note that each additional compute capability significantly increases your build time and binary size.[Default is: \"3.5,5.2\"]: 6.1...Configuration finished 6.1為 GTX 10-series 系列顯卡，其他可以查看 CUDA GPUS。這邊除了上述特定要輸入外，其餘都是直接鍵盤enter。 當完成組態後，即可透過 bazel 進行建構 pip 套件腳本： $ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 當腳本建構完成後，即可透過以下指令來建構 .whl 檔案： $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tf_pkg 完成後，可以在/tmp/tf_pkg目錄底下找到安裝檔tensorflow-1.0.1-py2-none-any.whl，最後就可以透過 pip 來進行安裝了： $ sudo pip install /tmp/tf_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl 測試安裝結果最後透過簡單程式來驗證安裝是否成功： $ cat &lt;&lt;EOF &gt; simple.pyimport tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello))EOF$ python simple.py...roperties:name: GeForce GTX 1060 6GBmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845pciBusID 0000:01:00.0Total memory: 5.93GiBFree memory: 5.74GiB2017-03-12 21:43:56.477084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 02017-03-12 21:43:56.477092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0: Y2017-03-12 21:43:56.503464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)Hello, TensorFlow! ...部分會顯示一些 GPU 使用狀態。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://k2r2bai.com/categories/TensorFlow/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://k2r2bai.com/tags/Ubuntu/"},{"name":"ML/DL","slug":"ML-DL","permalink":"https://k2r2bai.com/tags/ML-DL/"}]},{"title":"TensorFlow on Docker","slug":"tensorflow/run-on-docker","date":"2017-03-10T08:23:01.000Z","updated":"2019-12-02T01:49:42.401Z","comments":true,"path":"2017/03/10/tensorflow/run-on-docker/","link":"","permalink":"https://k2r2bai.com/2017/03/10/tensorflow/run-on-docker/","excerpt":"本篇主要整理使用 Docker 來執行 TensorFlow 的一些問題，這邊 Google 官方已經提供了相關的映像檔提供使用，因此會簡單說明安裝過程與需求。","text":"本篇主要整理使用 Docker 來執行 TensorFlow 的一些問題，這邊 Google 官方已經提供了相關的映像檔提供使用，因此會簡單說明安裝過程與需求。 環境準備環境採用 Ubuntu 16.04 Desktop 作業系統，然後顯卡是撿朋友不要的來使用，環境硬體資源如下： 名稱 描述 CPU i7-4790 CPU @ 3.60GHz Memory 32GB GPU GeForce GTX 650 事前準備開始進行 TensorFlow on Docker 之前，需要確認環境已經安裝以下驅動與軟體等。 系統安裝了 Docker。 安裝最新版本 NVIDIA Driver。 安裝 NVIDIA Docker。 上面可以參考 安裝 Nvidia Docker 2 來進行安裝。 利用 Docker 執行 TensorFlowTensorFlow on Docker 官方已經提供了相關映像檔，這邊透過單一指令就可以取得該映像檔，並啟動提供使用，以下為只有 CPU 的版本： $ docker run -d -p 8888:8888 --name tf-cpu tensorflow/tensorflow$ docker logs tf-cpu...to login with a token: http://localhost:8888/?token=7ddd6ef31fed5f22696c1003a905782b9219a6ec9a19b97c 這時候就可以登入 Jupyter notebook，這邊登入需要token後面的值。 若要支援 GPU(CUDA) 的容器的話，可以透過以下指令來提供： $ docker run --runtime=nvidia -d -p 8888:8888 --name tf-gpu tensorflow/tensorflow:latest-gpu$ docker logs tf-cpu 其他版本可以參考 tags。 利用 Docker 提供 ServingTensorFlow Serving 是靈活、高效能的機器學習模型服務系統，是專門為生產環境而設計的，它可以很簡單部署新的演算法與實驗來提供同樣的架構與 API 進行服務。 首先我們下載官方寫好的 Dockerfile 來進行建置： $ mkdir serving &amp;&amp; cd serving$ wget \"https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/tools/docker/Dockerfile.devel\"$ sed -i 's/BAZEL_VERSION.*0.4.2/BAZEL_VERSION 0.4.5/g' Dockerfile.devel$ docker build --pull -t kyle/serving:0.1.0 -f Dockerfile.devel . 建置完成映像檔後，透過以下指令執行，並在容器內建置 Serving： $ docker run -itd --name=tf-serving kyle/serving:0.1.0$ docker exec -ti tf-serving bashroot@459a89a3cf5a$ git clone --recurse-submodules \"https://github.com/tensorflow/serving\"root@459a89a3cf5a$ cd serving/tensorflowroot@459a89a3cf5a$ ./configureroot@459a89a3cf5a$ cd .. &amp;&amp; bazel build -c opt tensorflow_serving/... 當建置完 Serving 後，就可以透過以下指令來確認是否正確： root@459a89a3cf5a$ bazel-bin/tensorflow_serving/model_servers/tensorflow_model_serverusage: bazel-bin/tensorflow_serving/model_servers/tensorflow_model_serverFlags: --port=8500 int32 port to listen on --enable_batching=false... 接著使用 Inception v3 模型來提供服務，透過以下步驟來完成： root@459a89a3cf5a$ curl -O \"http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz\"root@459a89a3cf5a$ tar xzf inception-v3-2016-03-01.tar.gzroot@459a89a3cf5a$ ls inception-v3README.txt checkpoint model.ckpt-157585root@459a89a3cf5a$ bazel-bin/tensorflow_serving/example/inception_saved_model --checkpoint_dir=inception-v3 --output_dir=inception-exportSuccessfully exported model to inception-exportroot@459a89a3cf5a$ ls inception-export1 當完成匯入後離開容器，並 commit 成新版本映像檔： $ docker commit tf-serving kyle/serving-inception:0.1.0$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEkyle/serving-inception 0.1.0 1d866ff60d38 3 minutes ago 5.55 GB 接著執行剛 commit 的映像檔，並啟動 Serving 服務： $ docker run -it kyle/serving-inception:0.1.0root@5b9a89eeef5a$ cd servingroot@5b9a89eeef5a$ bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception --model_base_path=inception-export &amp;&gt; inception_log &amp;[1] 15 最後透過 inception_client.py 來測試功能： root@5b9a89eeef5a$ curl \"https://s-media-cache-ak0.pinimg.com/736x/32/00/3b/32003bd128bebe99cb8c655a9c0f00f5.jpg\" --output rabbit.jpgroot@5b9a89eeef5a$ bazel-bin/tensorflow_serving/example/inception_client --server=localhost:9000 --image=rabbit.jpgoutputs &#123; key: \"classes\" value &#123; dtype: DT_STRING tensor_shape &#123; dim &#123; size: 1 &#125; dim &#123; size: 5 &#125; &#125; string_val: \"hare\" string_val: \"wood rabbit, cottontail, cottontail rabbit\" string_val: \"Angora, Angora rabbit\" string_val: \"mouse, computer mouse\" string_val: \"gazelle\" &#125;&#125;outputs &#123; key: \"scores\" value &#123; dtype: DT_FLOAT tensor_shape &#123; dim &#123; size: 1 &#125; dim &#123; size: 5 &#125; &#125; float_val: 10.3059120178 float_val: 8.19226741791 float_val: 4.00839996338 float_val: 2.34308481216 float_val: 2.00992465019 &#125;&#125;","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://k2r2bai.com/categories/TensorFlow/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"ML/DL","slug":"ML-DL","permalink":"https://k2r2bai.com/tags/ML-DL/"}]},{"title":"Ceph 使用 SPDK 加速 NVMe SSD","slug":"ceph/ceph-spdk","date":"2016-12-03T09:08:54.000Z","updated":"2019-12-02T01:49:42.377Z","comments":true,"path":"2016/12/03/ceph/ceph-spdk/","link":"","permalink":"https://k2r2bai.com/2016/12/03/ceph/ceph-spdk/","excerpt":"SPDK(Storage Performance Development Kit) 是 Intel 釋出的儲存效能開發工具，主要提供一套撰寫高效能、可擴展與 User-mode 的儲存應用程式工具與函式庫，而中國公司 XSKY 藉由該開發套件來加速 Ceph 在 NVMe SSD 的效能。","text":"SPDK(Storage Performance Development Kit) 是 Intel 釋出的儲存效能開發工具，主要提供一套撰寫高效能、可擴展與 User-mode 的儲存應用程式工具與函式庫，而中國公司 XSKY 藉由該開發套件來加速 Ceph 在 NVMe SSD 的效能。 首先進入 root，並 clone 專案到 local： $ sudo su -$ git clone http://github.com/ceph/ceph$ cd ceph 編輯CMakeLists.txt檔案，修改以下內容： option(WITH_SPDK \"Enable SPDK\" ON) 接著安裝一些相依套件與函式庫： $ ./install-deps.sh$ sudo apt-get install -y libpciaccess-dev 接著需要在環境安裝 DPDK 開發套件，首先進入 src 底下的 dpdk 目錄，編輯config/common_linuxapp檔案修改以下內容： CONFIG_RTE_BUILD_SHARED_LIB= 完成後建置與安裝 DPDK： $ make config T=x86_64-native-linuxapp-gcc$ make &amp;&amp; make install 接著回到 ceph root 目錄進行建構 Ceph 準備，透過以下指令進行： $ ./do_cmake.sh....-- Configuring done-- Generating done-- Build files have been written to: /root/ceph/build+ cat+ echo 40000+ echo done.done. 確認上面無誤後就可以進行 compile 包含 SPDK 的 Ceph： $ cd build$ make -j2 完成後就可以執行 test cluster，首先建構 vstart 程式： $ make vstart$ ../src/vstart.sh -d -n -x -l$ ./bin/ceph -s 若要關閉則使用以下方式： &gt; $ ../src/stop.sh&gt;","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Distribution System","slug":"Distribution-System","permalink":"https://k2r2bai.com/tags/Distribution-System/"},{"name":"SPDK","slug":"SPDK","permalink":"https://k2r2bai.com/tags/SPDK/"}]},{"title":"Using bluestore in Kraken","slug":"ceph/deploy/ceph-deploy-bluestore","date":"2016-11-28T09:08:54.000Z","updated":"2019-12-02T01:49:42.377Z","comments":true,"path":"2016/11/28/ceph/deploy/ceph-deploy-bluestore/","link":"","permalink":"https://k2r2bai.com/2016/11/28/ceph/deploy/ceph-deploy-bluestore/","excerpt":"本篇說明如何安裝 Kraken 版本的 Ceph，並將 objectstore backend 修改成 Bluestore，過程包含建立 RBD 等操作。","text":"本篇說明如何安裝 Kraken 版本的 Ceph，並將 objectstore backend 修改成 Bluestore，過程包含建立 RBD 等操作。 硬體規格說明本安裝由於實體機器數量受到限制，故只進行一台 MON 與兩台 OSD，而 OSD 數量則總共兩顆，硬體規格如下所示： Role RAM CPUs Disk IP Address mon1(deploy) 4 GB 4 core 500 GB 172.16.1.200 osd1 16 GB 8 core 2 TB 172.16.1.201 osd2 16 GB 8 core 2 TB 172.16.1.202 osd3 16 GB 8 core 2 TB 172.16.1.203 作業系統採用Ubuntu 16.04 LTS Server，Kernel 版本為Linux 4.4.0-31-generic。 事前準備在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有四台節點。 首先在每一台節點新增以下內容到/etc/hosts： 127.0.0.1 localhost172.16.1.200 mon1172.16.1.201 osd1172.16.1.202 osd2172.16.1.203 osd3 然後設定各節點 sudo 指令的權限，使之不用輸入密碼(若使用 root 則忽略)： $ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | \\sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 接著在設定deploy節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行： $ ssh-keygen -t rsa$ ssh-copy-id mon1$ ssh-copy-id osd1... 若不同節點之間使用不同 User 進行 SSH 部署的話，可以設定 ~/.ssh/config 之後在deploy節點安裝部署工具，首先使用 apt-get 來進行安裝基本相依套件，再透過 pypi 進行安裝 ceph-deploy 工具： $ sudo apt-get install -y python-pip$ sudo pip install -U ceph-deploy 節點部署首先建立一個名稱為 local 的目錄，並進到目錄底下： $ sudo mkdir local &amp;&amp; cd local 接著透過 ceph-deploy 在各節點安裝 ceph： $ ceph-deploy install --release kraken mon1 osd1 osd2 osd3 完成後建立 Monitor 節點資訊到 ceph.conf 中： $ ceph-deploy new mon1 &lt;other_mons&gt; 接著編輯目錄底下的 ceph.conf，並加入以下內容： [global]...rbd_default_features = 3osd pool default size = 3osd pool default min size = 1public network = 172.16.1.0/24cluster network = 172.16.1.0/24filestore_xattr_use_omap = trueenable experimental unrecoverable data corrupting features = bluestore rocksdbbluestore fsck on mount = truebluestore block db size = 134217728bluestore block wal size = 268435456bluestore block size = 322122547200osd objectstore = bluestore[osd]bluestore = true 若確認沒問題，即可透過以下指令初始化 mon： $ ceph-deploy mon create-initial 上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行： $ ceph-deploy osd prepare --bluestore osd1:&lt;device&gt; 系統驗證叢集檢查首先要驗證環境是否有部署成功，可以透過 ceph 提供的基本指令做檢查： $ ceph -vceph version v11.0.2 (697fe64f9f106252c49a2c4fe4d79aea29363be7)$ ceph -s cluster 6da24ae5-755f-4077-bfa0-78681dfc6bde health HEALTH_OK monmap e1: 1 mons at &#123;r-mon00=172.16.1.200:6789/0&#125; election epoch 7, quorum 0 mon1 mgr no daemons active osdmap e256: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v920162: 128 pgs, 1 pools, 6091 MB data, 1580 objects 12194 MB used, 588 GB / 600 GB avail 128 active+clean 另外也可以用 osd 指令來查看部屬的 osd 資訊： $ ceph osd treeID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY-1 0.58618 root default-2 0.29309 host osd1 0 0.29309 osd.0 up 1.00000 1.00000-3 0.29309 host osd2 1 0.29309 osd.1 up 1.00000 1.00000-4 0.29309 host osd3 1 0.29309 osd.2 up 1.00000 1.00000 RBD 建立本節說明在 Kraken 版本建立 RBD 來進行使用，在預設部署起來的叢集下會存在一個儲存池 rbd，因此可以省略建立新的儲存池。 首先透過以下指令建立一個區塊裝置映像檔： $ rbd create rbd/bd -s 50G 接著透過 info 指令查看區塊裝置映像檔資訊： $ rbd info rbd/bdrbd image 'bd': size 51200 MB in 12800 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.102d474b0dc51 format: 2 features: layering, striping flags: stripe unit: 4096 kB stripe count: 1 以下為目前支援的特性： 屬性名稱 說明 Bit Code layering 支援分層 1 striping 支援串連(v2) 2 exclusive-lock 支援互斥鎖定 4 object-map 支援物件映射(相依於 exclusive-lock ) 8 fast-diff 支援快速計算差異(相依於 object-map ) 16 deep-flatten 支援快照扁平化操作 32 journaling 支援紀錄 I/O 操作(相依於 exclusive-lock ) 64 P.S. 這邊由於 Kernel 版本問題有些特性無法支援，因此在 conf 檔只設定使用 layering, striping。P.S. 若預設未修改 feature 設定的話，可以透過以下指令修改: $ rbd feature disable rbd/bd &lt;feature_name&gt; 接著就可以透過 Linux mkfs 指令來格式化 rbd： $ sudo mkfs.ext4 /dev/rbd0$ sudo mount /dev/rbd0 /mnt 最後透過 dd 指令測試 rbd 寫入效能： $ dd if=/dev/zero of=/mnt/test bs=4096 count=40000004000000+0 records in4000000+0 records out16384000000 bytes (16 GB) copied, 119.947 s, 137 MB/s 另外有些需求為了測試 feature，卻又礙於 Kernel 不支援等問題，而造成無法 Map 時，可以透過 rbd-nbd 來進行 Map，安裝跟使用方式如下： $ sudo apt-get install -y rbd-nbd$ sudo rbd-nbd map rbd/bd/dev/nbd0 P.S. 在新版的 ceph 已經有內建 rbd nbd，參考 rbd - manage command。 最後透過 dd 指令測試 nbd 寫入效能： $ dd if=/dev/zero of=./mnt-nbd/test bs=4096 count=40000004000000+0 records in4000000+0 records out16384000000 bytes (16 GB) copied, 168.201 s, 97.4 MB/s","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"BlueStore","slug":"BlueStore","permalink":"https://k2r2bai.com/tags/BlueStore/"}]},{"title":"簡單部署 Docker Swarm 測試叢集","slug":"container/docker-swarm","date":"2016-11-16T09:08:54.000Z","updated":"2019-12-02T01:49:42.381Z","comments":true,"path":"2016/11/16/container/docker-swarm/","link":"","permalink":"https://k2r2bai.com/2016/11/16/container/docker-swarm/","excerpt":"Docker Swarm 是 Docker 公司的 Docker 編配引擎，最早是在 2014 年 12 月發佈。Docker Swarm 目的即管理多台節點的 Docker 上應用程式與節點資源的排程等，並提供標準的 Docker API 介面當作前端存取入口，因此可以跟現有 Docker 工具與函式庫進行整合，本篇將介紹簡單的建立 Swarm cluster。 Docker Swarm 具備了以下幾個特性： Docker engine 原生支援。(Docker 1.12+)。 去中心化設計。 宣告式服務模型(Declarative Service Model)。 服務可擴展與容錯。 可協調預期狀態與實際狀態的一致性。 多種網路支援。 提供服務發現、負載平衡與安全策略。 支援滾動升級(Rolling Update)。","text":"Docker Swarm 是 Docker 公司的 Docker 編配引擎，最早是在 2014 年 12 月發佈。Docker Swarm 目的即管理多台節點的 Docker 上應用程式與節點資源的排程等，並提供標準的 Docker API 介面當作前端存取入口，因此可以跟現有 Docker 工具與函式庫進行整合，本篇將介紹簡單的建立 Swarm cluster。 Docker Swarm 具備了以下幾個特性： Docker engine 原生支援。(Docker 1.12+)。 去中心化設計。 宣告式服務模型(Declarative Service Model)。 服務可擴展與容錯。 可協調預期狀態與實際狀態的一致性。 多種網路支援。 提供服務發現、負載平衡與安全策略。 支援滾動升級(Rolling Update)。 基本架構Docker Swarm 具備基本叢集功能，能讓多個 Docker 組合成一個群組，來提供容器服務。Docker 採用標準 Docker API 來管理容器的生命週期，而 Swarm 最主要核心是處理容器如何選擇一台主機來啟動容器這件事。以下為 Docker Swarm 架構： Docker Swarm 一般分為兩個角色Manager與Worker，兩者主要工作如下： Manager: 主要負責排程 Task，Task 可以表示為 Swarm 節點中的 Node 上啟動的容器。同時還負責編配容器與叢集管理功能，簡單說就是 Manager 具備管理 Node 的工作，除了以上外，Manager 還會維護叢集狀態。另外 Manager 也具備 Worker 的功能，當然也可以設定只做管理 Node 的職務。 Worker: Worker 主要接收來自 Manager 的 Task 指派，並依據指派內容啟動 Docker 容器服務，並在完成後向 Manager 匯報 Task 執行狀態。 預先準備資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Role CPU Memory 172.16.35.12 manager 1 2G 172.16.35.10 node1 1 2G 172.16.35.11 node2 1 2G 這邊 Manager 為主要控制節點，node 為應用程式工作節點。 首先安裝前要確認以下幾項都已將準備完成： 所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。 所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0 所有節點需要設定/etc/host解析到所有主機。 所有節點需要安裝Docker引擎，安裝方式如下： $ curl -fsSL \"https://get.docker.com/\" | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後，需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker Manager 節點建置當我們完成安裝 Docker Engine 後，就可以透過 Docker 指令來初始化 Manager 節點： $ docker swarm init --advertise-addr 172.16.35.12Swarm initialized: current node (olluuvvz340ze64zhjpw03uke) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-0q0ohnexs40lb9z4kmvqb6zcrmp22hul9tmh6zpfztxzv5cv61-73yubitun1ufm0yhwx7h38p85 172.16.35.12:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 當看到上述內容，表示 Manager 初始化完成，這時候可以透過以下指令檢查： $ docker info$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUSolluuvvz340ze64zhjpw03uke * manager Ready Active Leader 接著建立 Docker swarm network 來提供容器跨節點的溝通： # Deploy network$ docker network create --driver=overlay --attachable cnblogs# Docker flow proxy network$ docker network create --driver overlay proxy 檢查 Docker 網路狀態： $ docker network ls | grep swarmNETWORK ID NAME DRIVER SCOPE57nq0rux7akh cnblogs overlay swarmihyg6uixeiov ingress overlay swarmb8vqturisod8 proxy overlay swarm Worker 節點建置完成 Manager 初始化後，就可以透過以下指令來將節點加入叢集： $ docker swarm join --token SWMTKN-1-0q0ohnexs40lb9z4kmvqb6zcrmp22hul9tmh6zpfztxzv5cv61-73yubitun1ufm0yhwx7h38p85 172.16.35.12:2377This node joined a swarm as a worker. P.S. 其他節點一樣請用上述指令加入。 在Manager節點，查看節點狀態： $ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUScwkta4o37daxed3otrqab9zdq node2 Ready Activeolluuvvz340ze64zhjpw03uke * manager Ready Active Leadersfs49249kv8mad2qzr4ev4fy0 node1 Ready Active (option)將節點改為 Manager： $ docker node promote &lt;HOSTNAME&gt; 另外降級為docker node demote &lt;HOSTNAME&gt;。 透過指令建立簡單服務要建立 Docker 服務，可以使用docker service指令來達成，如下指令： $ docker service create --replicas 1 --name ping alpine ping 8.8.8.8$ docker service logs pingping.1.auqefe3iq9yk@node2 | PING 8.8.8.8 (8.8.8.8): 56 data bytesping.1.auqefe3iq9yk@node2 | 64 bytes from 8.8.8.8: seq=0 ttl=61 time=7.042 msping.1.auqefe3iq9yk@node2 | 64 bytes from 8.8.8.8: seq=1 ttl=61 time=7.029 msping.1.auqefe3iq9yk@node2 | 64 bytes from 8.8.8.8: seq=2 ttl=61 time=7.668 m... 建立兩份副本數的應用，如以下指令： $ docker service create --replicas 2 --name redis redis$ docker service ps redisID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSngtegx9vk4gu redis.1 redis:latest node1 Running Running 43 seconds agon95vu3dzewu7 redis.2 redis:latest manager Running Running 44 seconds ago 完成後，想要刪除可以使用以下指令： $ docker service rm ping$ docker service rm redis 部署簡單的 Stack這邊利用簡單範例來部署應用程式於 Swarm 叢集中，首先新增stack.yml檔案，並加入以下內容： version: '3.2'services: api: image: open-api:latest deploy: replicas: 2 update_config: delay: 5s labels: - com.df.notify=true - com.df.distribute=true - com.df.serviceDomain=api.cnblogs.com - com.df.port=80 networks: - cnblogs - proxynetworks: cnblogs: external: true proxy: external: true 完成後，透過以下指令來進行部署： $ docker stack deploy -c stack.yml openapi","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Container Orchestration","slug":"Container-Orchestration","permalink":"https://k2r2bai.com/tags/Container-Orchestration/"}]},{"title":"Minikube 部署 Local 測試環境","slug":"kubernetes/deploy/minikube","date":"2016-10-23T09:08:54.000Z","updated":"2019-12-02T01:49:42.394Z","comments":true,"path":"2016/10/23/kubernetes/deploy/minikube/","link":"","permalink":"https://k2r2bai.com/2016/10/23/kubernetes/deploy/minikube/","excerpt":"Minikube 是提供簡單與快速部署本地 Kubernetes 環境的工具，透過執行虛擬機來執行單節點 Kubernetes 叢集，以便開發者使用 Kubernetes 與開發用。 本環境安裝資訊： Minikube v0.22.3 Kubernetes v1.7.5","text":"Minikube 是提供簡單與快速部署本地 Kubernetes 環境的工具，透過執行虛擬機來執行單節點 Kubernetes 叢集，以便開發者使用 Kubernetes 與開發用。 本環境安裝資訊： Minikube v0.22.3 Kubernetes v1.7.5 事前準備安裝前需要確認叢集滿足以下幾點： 安裝 xhyve driver, VirtualBox 或 VMware Fusion。 安裝 kubectl 工具。 以下為Mac OS X下載方式： $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/darwin/amd64/kubectl$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 如果是Linux則使用以下方式： $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/linux/amd64/kubectl$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 快速開始Minikube 支援了許多作業系統，若是 OS X 的開發者，可以透過該指令安裝： $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.22.3/minikube-darwin-amd64$ chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 若是 Linux 開發者則利用以下指令安裝： $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.22.3/minikube-linux-amd64$ chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 下載完成後，就可以透過以下指令建立環境： $ minikube get-k8s-versionsThe following Kubernetes versions are available: - v1.8.0 - v1.7.5...$ minikube startStarting local Kubernetes v1.7.5 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster. 看到上述資訊表示已完成啟動 Kubernetes 虛擬機，這時候可以透過 kubectl 來查看資訊： $ kubectl get nodeNAME STATUS AGE VERSIONminikube Ready 2m v1.7.5$ kubectl get po,svc -n kube-systemNAME READY STATUS RESTARTS AGEpo/default-http-backend-2jk83 1/1 Running 0 2mpo/kube-addon-manager-minikube 1/1 Running 0 2mpo/kube-dns-1326421443-8br9x 3/3 Running 0 2mpo/kubernetes-dashboard-mdc9f 1/1 Running 0 2mpo/nginx-ingress-controller-dspc0 1/1 Running 0 2mNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/default-http-backend 10.0.0.237 &lt;nodes&gt; 80:30001/TCP 2msvc/kube-dns 10.0.0.10 &lt;none&gt; 53/UDP,53/TCP 2msvc/kubernetes-dashboard 10.0.0.222 &lt;nodes&gt; 80:30000/TCP 2m 新版本 Minikube 預設會自動啟動上述 Addons。這時可以透過瀏覽器進入 Dashboard。 想啟動 Extra addons 的話，可以透過以下指令來達成： $ minikube addons list$ minikube addons enable heapster 若要移除則使用minikube addons disable heapster指令。 取得虛擬機裡面的 Docker env： $ eval $(minikube docker-env)$ docker versionClient: Version: 17.10.0-ce API version: 1.23 Go version: go1.8.3 Git commit: f4ffd25 Built: Tue Oct 17 19:00:43 2017 OS/Arch: darwin/amd64Server: Version: 1.12.6 API version: 1.24 (minimum version ) Go version: go1.6.4 Git commit: 78d1802 Built: Wed Jan 11 00:23:16 2017 OS/Arch: linux/amd64 Experimental: false (option)若想要移除與刪除虛擬機的話，可以透過以下指令進行： $ minikube stop$ minikube delete 執行簡單測試應用程式這邊利用 echoserver 來測試 Minikube 功能，首先透過以下指令啟動一個 Deployment： $ kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080$ kubectl get deploy,poNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeploy/hello-minikube 1 1 1 1 11mNAME READY STATUS RESTARTS AGEpo/hello-minikube-938614450-31rtv 1/1 Running 0 11m 接著 expose 服務來進行存取： $ kubectl expose deployment hello-minikube --type=NodePort$ kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEhello-minikube 10.0.0.164 &lt;nodes&gt; 8080:30371/TCP 4skubernetes 10.0.0.1 &lt;none&gt; 443/TCP 29m 最後透過 cURL 來存取服務： $ curl $(minikube service hello-minikube --url)CLIENT VALUES:client_address=172.17.0.1command=GETreal path=/...","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"只要用 kubeadm 小朋友都能部署 Kubernetes","slug":"kubernetes/deploy/kubeadm","date":"2016-09-29T09:08:54.000Z","updated":"2019-12-02T01:49:42.393Z","comments":true,"path":"2016/09/29/kubernetes/deploy/kubeadm/","link":"","permalink":"https://k2r2bai.com/2016/09/29/kubernetes/deploy/kubeadm/","excerpt":"kubeadm是 Kubernetes 官方推出的部署工具，該工具實作類似 Docker swarm 一樣的部署方式，透過初始化 Master 節點來提供給 Node 快速加入，kubeadm 目前屬於測試環境用階段，但隨著時間推移會越來越多功能被支援，這邊可以看 kubeadm Roadmap 來更進一步知道功能發展狀態。 若想利用 Ansible 安裝的話，可以參考這邊 kubeadm-ansible。","text":"kubeadm是 Kubernetes 官方推出的部署工具，該工具實作類似 Docker swarm 一樣的部署方式，透過初始化 Master 節點來提供給 Node 快速加入，kubeadm 目前屬於測試環境用階段，但隨著時間推移會越來越多功能被支援，這邊可以看 kubeadm Roadmap 來更進一步知道功能發展狀態。 若想利用 Ansible 安裝的話，可以參考這邊 kubeadm-ansible。 本環境安裝資訊： Kubernetes v1.9.6 Etcd v3 Flannel v0.9.1 Docker v18.02.0-ce 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為 Vagrant with Libvirt： IP Address Role CPU Memory 172.16.35.12 master1 1 2G 172.16.35.10 node1 1 2G 172.16.35.11 node2 1 2G 目前 kubeadm 只支援在Ubuntu 16.04+、CentOS 7與HypriotOS v1.0.1+作業系統上使用。 事前準備安裝前需要確認叢集滿足以下幾點： 所有節點網路可以溝通。 所有節點需要設定 APT Docker Repository： $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" CentOS 7 EPEL 有支援 Docker Package: $ sudo yum install -y epel-release 所有節點需要設定 APT 與 YUM Kubernetes Repository： $ curl -s \"https://packages.cloud.google.com/apt/doc/apt-key.gpg\" | sudo apt-key add -$ echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list 若是 CentOS 7 則執行以下方式： $ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF CentOS 7 要額外確認 SELinux 或 Firewall 關閉。 Kubernetes v1.8+ 要求關閉系統 Swap，若不關閉則需要修改 kubelet 設定參數，這邊可以利用以下指令關閉： $ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0# 不同機器有差異$ sed '/swap.img/d' -i /etc/fstab 記得/etc/fstab也要註解掉SWAP掛載。 Kubernetes Master 建立首先更新 APT 來源，並且安裝 Kubernetes 元件與工具： $ export KUBE_VERSION=\"1.9.6\"$ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=$&#123;KUBE_VERSION&#125;-00 kubeadm=$&#123;KUBE_VERSION&#125;-00 kubectl=$&#123;KUBE_VERSION&#125;-00 docker-ce 進行初始化 Master，這邊需要進入root使用者執行以下指令： $ sudo su -$ kubeadm token generateb0f7b8.8d1767876297d85c$ kubeadm init --service-cidr 10.96.0.0/12 \\ --kubernetes-version v$&#123;KUBE_VERSION&#125; \\ --pod-network-cidr 10.244.0.0/16 \\ --token b0f7b8.8d1767876297d85c \\ --apiserver-advertise-address 172.16.35.12# output ...kubeadm join --token b0f7b8.8d1767876297d85c 172.16.35.12:6443 --discovery-token-ca-cert-hash sha256:739d936954a752d44d2f2282dd645083259826f2c24a651608a6ac2081106cd7 當出現如上面資訊後，表示 Master 初始化成功，不過這邊還是一樣透過 kubectl 測試一下： $ mkdir ~/.kube &amp;&amp; cp /etc/kubernetes/admin.conf ~/.kube/config$ kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster1 NotReady master 4m v1.9.6 當叢集正確部署後，要接著部署 Pod 網路，但要注意一個叢集只能用一種 Pod 網路(除非用 multus-cni)，這邊採用 Flannel： $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.ymlclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds created 若參數 --pod-network-cidr=10.244.0.0/16 改變時，在kube-flannel.yml檔案也需修改net-conf.json欄位的 CIDR。 若使用 Virtualbox 的話，請修改kube-flannel.yml中的 command 綁定 iface，如command: [ &quot;/opt/bin/flanneld&quot;, &quot;--ip-masq&quot;, &quot;--kube-subnet-mgr&quot;, &quot;--iface=eth1&quot; ]。 其他 Pod Network 可以參考 Installing a pod network。 接著透過 kubectl 查看 Flannel 是否正確在每個 Node 部署：確認 Flannel 部署正確： $ kubectl -n kube-system get po -l app=flannel -o wideNAME READY STATUS RESTARTS AGE IP NODEkube-flannel-ds-2ssnj 1/1 Running 0 58s 172.22.132.10 k8s-m1kube-flannel-ds-pgfpd 1/1 Running 0 58s 172.22.132.11 k8s-m2kube-flannel-ds-vmt2h 1/1 Running 0 58s 172.22.132.12 k8s-m3$ ip -4 a show flannel.15: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default inet 10.244.0.0/32 scope global flannel.1 valid_lft forever preferred_lft forever$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.244.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni010.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.110.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 Kubernetes Node 建立首先更新 APT 來源，並且安裝 Kubernetes 元件與工具： $ export KUBE_VERSION=\"1.9.6\"$ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=$&#123;KUBE_VERSION&#125;-00 kubeadm=$&#123;KUBE_VERSION&#125;-00 docker-ce 完成後就可以開始加入 Node，這邊需要進入root使用者執行以下指令： $ kubeadm join --token b0f7b8.8d1767876297d85c 172.16.35.12:6443 --discovery-token-ca-cert-hash sha256:739d936954a752d44d2f2282dd645083259826f2c24a651608a6ac2081106cd7# output...Run 'kubectl get nodes' on the master to see this machine join. 回到master1查看節點狀態： $ kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster1 Ready master 10m v1.9.6node1 Ready &lt;none&gt; 9m v1.9.6node2 Ready &lt;none&gt; 9m v1.9.6 為了多加利用資源這邊透過 taint 來讓 masters 也會被排程執行容器： $ kubectl taint nodes --all node-role.kubernetes.io/master- Add-ons 建立當完成後就可以建立一些 Addons，如 Dashboard。這邊執行以下指令進行建立： $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml Dashboard 1.7.x 版本有做一些改變，會用到 SSL Cert，可參考這邊 Installation。 確認沒問題後，透過 kubectl 查看： kubectl get svc --namespace=kube-systemNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 36mkubernetes-dashboard 10.111.162.184 &lt;nodes&gt; 80:32546/TCP 33s 最後就可以存取 Kube Dashboard。 在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role： $ kubectl -n kube-system create sa dashboard$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard$ kubectl -n kube-system get sa dashboard -o yamlapiVersion: v1kind: ServiceAccountmetadata: creationTimestamp: 2017-11-27T17:06:41Z name: dashboard namespace: kube-system resourceVersion: \"69076\" selfLink: /api/v1/namespaces/kube-system/serviceaccounts/dashboard uid: 56b880bf-d395-11e7-9528-448a5ba4bd34secrets:- name: dashboard-token-vg52j$ kubectl -n kube-system describe secrets dashboard-token-vg52j...token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdmc1MmoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNTZiODgwYmYtZDM5NS0xMWU3LTk1MjgtNDQ4YTViYTRiZDM0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.bVRECfNS4NDmWAFWxGbAi1n9SfQ-TMNafPtF70pbp9Kun9RbC3BNR5NjTEuKjwt8nqZ6k3r09UKJ4dpo2lHtr2RTNAfEsoEGtoMlW8X9lg70ccPB0M1KJiz3c7-gpDUaQRIMNwz42db7Q1dN7HLieD6I4lFsHgk9NPUIVKqJ0p6PNTp99pBwvpvnKX72NIiIvgRwC2cnFr3R6WdUEsuVfuWGdF-jXyc6lS7_kOiXp2yh6Ym_YYIr3SsjYK7XUIPHrBqWjF-KXO_AL3J8J_UebtWSGomYvuXXbbAUefbOK4qopqQ6FzRXQs00KrKa8sfqrKMm_x71Kyqq6RbFECsHPA 複製token，然後貼到 Kubernetes dashboard。 簡單部署一個服務這邊利用 Weave 公司提供的服務來驗證系統，透過以下方式建立： $ kubectl create namespace sock-shop$ kubectl apply -n sock-shop -f \"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true\" 接著透過 kubectl 查看資訊： $ kubectl describe svc front-end -n sock-shop$ kubectl get pods -n sock-shop 最後存取 http://172.16.35.12:30001 即可看到服務的 Frontend。 移除節點最後，若要將現有節點移除的話，kubeadm 已經有內建的指令來完成這件事，只要執行以下即可： $ kubeadm reset","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://k2r2bai.com/tags/Ubuntu/"},{"name":"CentOS","slug":"CentOS","permalink":"https://k2r2bai.com/tags/CentOS/"}]},{"title":"Go 語言環境安裝","slug":"golang/installation","date":"2016-08-19T09:08:54.000Z","updated":"2019-12-02T01:49:42.386Z","comments":true,"path":"2016/08/19/golang/installation/","link":"","permalink":"https://k2r2bai.com/2016/08/19/golang/installation/","excerpt":"Go 語言是 Google 開發的該世代 C 語言，延續 C 語言的一些優點，是一種靜態強刑別、編譯型，且具有並行機制與垃圾回收功能的語言。由於其並行機制讓 Go 在撰寫多執行緒與網路程式都非常容易。值得一提的是 Go 語言的設計者也包含過去設計 C 語言的 Ken Thompson。目前 Go 語言基於 1.x 每半年發布一個版本。","text":"Go 語言是 Google 開發的該世代 C 語言，延續 C 語言的一些優點，是一種靜態強刑別、編譯型，且具有並行機制與垃圾回收功能的語言。由於其並行機制讓 Go 在撰寫多執行緒與網路程式都非常容易。值得一提的是 Go 語言的設計者也包含過去設計 C 語言的 Ken Thompson。目前 Go 語言基於 1.x 每半年發布一個版本。 Go 語言安裝Go 語言安裝非常容易，目前已支援多個平台的作業系統，以下針對幾個常見的作業系統進行教學。 P.S. 以下教學皆使用 64 bit 進行安裝。 Linux首先透過網路下載 Go 語言的壓縮檔： $ wget https://storage.googleapis.com/golang/go1.8.linux-amd64.tar.gz 然後將壓縮檔內的資料全部解壓縮到/usr/local底下： $ sudo tar -C /usr/local -xzf go1.8.linux-amd64.tar.gz 之後編輯.bashrc檔案，在最下面加入以下內容： export GOROOT=/usr/local/goexport GOPATH=$&#123;HOME&#125;/goexport PATH=$PATH:$GOPATH/bin:$GOROOT/bin Mac OS XMac OS X 安裝可以透過官方封裝好的檔案來進行安裝，下載 go1.8.darwin-amd64.pkg，然後雙擊進行安裝。 完成後，編輯.bashrc檔案來加入套件的安裝路徑： export PATH=$PATH:/usr/local/go/binexport GOPATH=~/Goexport PATH=$PATH:$GOPATH/bin 簡單入門建立目錄hello-go，並新增檔案hello.go： $ mkdir hello-go$ cd hello-go &amp;&amp; touch hello.go 接著編輯hello.go加入以下內容： package mainimport \"fmt\"func main() &#123; fmt.Println(\"Hello world, GO !\")&#125; 完成後執行以下指令： $ go build$ ./helloHello world, GO ! 其他 Framework 與網站以下整理相關 Go 語言的套件與不錯網站。 種類 名稱 Web 框架 Beego、Martini、Gorilla、GoCraft、Net/HTTP、Revel、girl、XWeb、go-start、goku、web.go 系統處理框架 apifs、goIRC 影音處理 Gopher-Talkie、Videq Social 框架 ChannelMail.io 參考資訊 Gopher Gala 2015 Finalists Web application frameworks","categories":[{"name":"Golang","slug":"Golang","permalink":"https://k2r2bai.com/categories/Golang/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"Golang","slug":"Golang","permalink":"https://k2r2bai.com/tags/Golang/"},{"name":"OS X","slug":"OS-X","permalink":"https://k2r2bai.com/tags/OS-X/"}]},{"title":"簡單部署 DC/OS 於 CentOS 上","slug":"data-engineer/dcos-install","date":"2016-08-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.383Z","comments":true,"path":"2016/08/17/data-engineer/dcos-install/","link":"","permalink":"https://k2r2bai.com/2016/08/17/data-engineer/dcos-install/","excerpt":"DC/OS(Data Center Operating System，資料中心作業系統)是 Mesosphere 公司開源的系統，該平台提供了諸多巨量資料處理框架與系統的建置，並以分散式系統 Mesos 作為核心，提供系統資源的隔離與調度，使用者可以根據需求與策略來應用系統資源。 而本篇將說明如何透過 UI 與 CLI 進行安裝 DC/OS。","text":"DC/OS(Data Center Operating System，資料中心作業系統)是 Mesosphere 公司開源的系統，該平台提供了諸多巨量資料處理框架與系統的建置，並以分散式系統 Mesos 作為核心，提供系統資源的隔離與調度，使用者可以根據需求與策略來應用系統資源。 而本篇將說明如何透過 UI 與 CLI 進行安裝 DC/OS。 節點配置DC/OS 最低需求要四台主機，以下為本次安裝的硬體設備： Role RAM Disk CPUs IP Address bootstrap 16 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.101 master 8 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.102 agent-1 8 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.103 agent-2 8 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.104 請以上節點都分別安裝 RHEL 或者 CentOS 作業系統。並且設定 IP 為靜態固定，編輯/etc/sysconfig/network-scripts/ifcfg-&lt;name&gt;檔案，加入以下內容： ONBOOT=\"yes\"IPADDR=\"10.0.0.104\"PREFIX=\"24\"GATEWAY=\"10.0.0.1\"DNS1=\"8.8.8.8\"DNS2=\"8.8.8.4\" 安裝前準備在開始安裝以前，首先需要在每一台節點將基本環境的軟體更新： $ sudo yum upgrade -y 完成後檢查是否是最新版本，可以透過以下方式查看 Kernel： $ uname -r3.10.0-327.13.1.el7.x86_64 如果不是以上版本，請執行以下指令： $ sudo yum upgrade --assumeyes --tolerant$ sudo yum update --assumeyes 由於在 CentOS 與 RHEL 預設會開啟防火牆，故要關閉防火牆與開機時自動啟動： $ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld 接著安裝一些基本工具軟體： $ sudo yum install -y tar xz unzip curl ipset vim 設定啟動 OverlayFS : $ sudo tee /etc/modules-load.d/overlay.conf &lt;&lt;-'EOF'overlayEOF 設定關閉 SELinux 與設定一些資訊，並重新啟動： $ sudo sed -i s/SELINUX=enforcing/SELINUX=permissive/g /etc/selinux/config &amp;&amp;sudo groupadd nogroup &amp;&amp;sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 &amp;&amp;sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 &amp;&amp;sudo reboot 完成重新啟動後，在每一台節點安裝 Docker，首先要取得 Repos，設定以下來讓 yum 可以抓取： $ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF 設定 systemd 執行 Docker daemon 於 OverlayFS： $ sudo mkdir -p /etc/systemd/system/docker.service.d &amp;&amp; sudo tee /etc/systemd/system/docker.service.d/override.conf &lt;&lt;- EOF[Service]ExecStart=ExecStart=/usr/bin/docker daemon --storage-driver=overlay -H fd://EOF 安裝 Docker engine，並啟動 docker 與設定開機啟動： $ sudo yum install --assumeyes --tolerant docker-engine$ sudo systemctl start docker$ sudo systemctl enable docker 這邊可以設定使用者加入 docker 群組： $ sudo gpasswd -a $(whoami) docker 安裝 Bootstrap NodeBootstrap 節點主要提供佈署的功能，可以採用 UI 或 CLI 來進行部署，以下將說明如何透過建置 Bootstrap 來完成 DC/OS 佈署。 GUI 安裝這邊採用 GUI 方式來佈署 DC/OS，首先下載 DC/OS installer： $ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh 接著執行啟動 DC/OS UI： $ sudo bash dcos_generate_config.sh --web -v 完成後，即可開啟瀏覽器輸入 bootstrap web 這邊需要輸入 Bootstrap 節點的 SSH 私有金鑰，透過以下方式產生與印出： $ ssh-keygen -t rsa$ cat .ssh/id_rsa 並建立一個腳本檔案ip-detect，並加入以下內容： #!/bin/bashset -o nounset -o errexitIP=&lt;MASTER_IP&gt;echo $(ip route get $&#123;IP&#125; | awk '&#123;print $NF; exit&#125;') 上傳以上資訊，並填入 Master 與 Agent 的 IP Address。 CLI 安裝DC/OS 除了可以透過 GUI 進行安裝，也可以透過 CLI 的方式來佈署，首先建立目錄genconf： $ mkdir -p dcos_cluster$ cd dcos_cluster$ mkdir -p genconf 然後下載 DC/OS installer 來進行安裝，並且查看所有指令： $ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh$ sudo bash dcos_generate_config.sh --help 建立一個腳本檔案genconf/ip-detect，並加入以下內容： #!/bin/bashset -o nounset -o errexitIP=&lt;MASTER_IP&gt;echo $(ip route get $&#123;IP&#125; | awk '&#123;print $NF; exit&#125;') MASTER_IP為修改成主節點 IP。 接著建立設定檔genconf/config.yaml，並加入以下內容： ---agent_list:- &lt;agent-private-ip-1&gt;- &lt;agent-private-ip-2&gt;bootstrap_url: file:///opt/dcos_install_tmpcluster_name: \"DC/OS Cluster\"master_discovery: staticmaster_list:- &lt;master-private-ip-1&gt;resolvers:- 8.8.4.4- 8.8.8.8ssh_port: 22ssh_user: &lt;username&gt; &lt;agent-private-ip-1&gt; 修改成 10.0.0.103。 &lt;agent-private-ip-2&gt; 修改成 10.0.0.104。 &lt;master-private-ip-1&gt; 修改成 10.0.0.102。 &lt;username&gt; 修改成 cloud-user。P.S 這邊是用 cloud image。 完成後複製 Bootstrap 節點的 SSH 私有金鑰到目錄底下： $ cp &lt;path-to-key&gt; genconf/ssh_key &amp;&amp; chmod 0600 genconf/ssh_key 透過 DC/OS installer 產生設定資訊： $ sudo bash dcos_generate_config.sh --genconfExtracting image from this script and loading into docker daemon, this step can take a few minutesdcos-genconf.e060aa49ac4ab62d5e-1e14856f55e5d5d07b.tarRunning mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf====&gt; EXECUTING CONFIGURATION GENERATION... 成功的話目錄結構會類似以下： ├── dcos-genconf.14509fe1e7899f4395-3a2b7e03c45cd615da.tar├── dcos_generate_config.sh└── genconf ├── cluster_packages.json ├── config.yaml ├── ip-detect ├── serve │ ├── bootstrap │ │ ├── 3a2b7e03c45cd615da8dfb1b103943894652cd71.active.json │ │ └── 3a2b7e03c45cd615da8dfb1b103943894652cd71.bootstrap.tar.xz │ ├── bootstrap.latest │ ├── cluster-package-info.json │ ├── dcos_install.sh │ ├── fetch_packages.sh │ └── packages │ ├── dcos-config │ │ └── dcos-config--setup_6dac2d99011d6219b32d9f66cafa9845b7cf6d74.tar.xz │ └── dcos-metadata │ └── dcos-metadata--setup_6dac2d99011d6219b32d9f66cafa9845b7cf6d74.tar.xz ├── ssh_key └── state 若沒問題就可以 prerequisites 佈署階段： $ sudo bash dcos_generate_config.sh --install-prereqsRunning mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING INSTALL PREREQUISITES====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START install_prereqs 若沒問題就可以執行 preflight 來驗證叢集是否可以安裝： $ sudo bash dcos_generate_config.sh --preflightRunning mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING PREFLIGHT====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START run_preflight====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; STAGE preflight 一樣若沒問題就可以執行 deploy 來安裝 DC/OS： $ sudo bash dcos_generate_config.sh --deployRunning mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING DC/OS INSTALLATION====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START deploy_master 最後沒問題就可以執行 postflight 來確認服務是否有啟動： $ sudo bash dcos_generate_config.sh --postflightRunning mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING POSTFLIGHT====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START run_postflight 都完成後就可以查看 Zookpeer 與 DC/OS。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"DC/OS","slug":"DC-OS","permalink":"https://k2r2bai.com/tags/DC-OS/"},{"name":"Mesos","slug":"Mesos","permalink":"https://k2r2bai.com/tags/Mesos/"}]},{"title":"OpenStack Kolla 初體驗","slug":"openstack/kolla-ansible","date":"2016-07-21T08:23:01.000Z","updated":"2019-12-02T01:49:42.400Z","comments":true,"path":"2016/07/21/openstack/kolla-ansible/","link":"","permalink":"https://k2r2bai.com/2016/07/21/openstack/kolla-ansible/","excerpt":"Kolla 提供 OpenStack 生產環境就緒的容器與部署工具，其具備快速、擴展、可靠等特性，並提供社群版本的最佳本版升級實現。","text":"Kolla 提供 OpenStack 生產環境就緒的容器與部署工具，其具備快速、擴展、可靠等特性，並提供社群版本的最佳本版升級實現。 節點配置本安裝將使用三台實體主機與一台虛擬機器來進行簡單叢集部署，主機規格如以下所示： Role RAM CPUs Disk IP Address controller1 8 GB 4vCPU 250 GB 10.0.0.11 network1 2 GB 1vCPU 250 GB 10.0.0.21 compute1 8 GB 8vCPU 500 GB 10.0.0.31 docker-registry 2 GB 1vCPU 50 GB 10.0.0.90 作業系統皆為Ubuntu Server 14.04。 另外每台實體主機的網卡網路分別為以下： Role Management Tunnel Public controller1 eth0 eth1 eth2 network1 eth0 eth1 eth2 compute1 eth0 eth1 eth2 網卡若是實體主機，請設定為固定 IP，如以下： auto eth0iface eth0 inet static address 10.0.0.11 netmask 255.255.255.0 gateway 10.0.0.1 dns-nameservers 8.8.8.8 若想修改主機的網卡名稱，可以編輯/etc/udev/rules.d/70-persistent-net.rules。 其中network節點的 Public 需設定網卡為以下： auto &lt;ethx&gt;iface &lt;ethx&gt; inet manual up ip link set dev $IFACE up down ip link set dev $IFACE down 事前準備在開始部署 OpenStack 之前，我們需要先將一些相依軟體與函式庫安裝完成，並且需要部署一用於存取映像檔的倉庫(Docker registry)。 部署 Docker Registry首先進入到 docker-registry 節點，本教學採用一台虛擬機作為部署使用，並安裝 Docker engine： $ curl -fsSL \"https://get.docker.com/\" | sh 安裝完成 Docker engine 後，透過以下指令建置 Docker registry： $ docker run -d -p 5000:5000 --restart=always --name registry \\-v $(pwd)/data:/var/lib/registry \\registry:2 接著為了方便檢視 Docker image，這邊另外部署 Docker registry UI： $ docker run -d -p 8080:80 \\-e ENV_DOCKER_REGISTRY_HOST=10.26.1.49 \\-e ENV_DOCKER_REGISTRY_PORT=5000 \\konradkleine/docker-registry-frontend:v2 完成以上即可透過瀏覽器查看該主機 8080 Port。也可以透過以下指令檢查是否部署成功： $ docker pull ubuntu:14.04$ docker tag ubuntu:14.04 localhost:5000/ubuntu:14.04$ docker push localhost:5000/ubuntu:14.04The push refers to a repository [localhost:5000/ubuntu]447f88c8358f: Pusheddf9a135a6949: Pushed... 其他 Docker registry 列表： Portus Atomic Registry Private Registries in RancherOS 部署節點準備當完成上述後，即可進行部署節點的相依軟體安裝，首先在每台節點透過以下指令更新與安裝一些套件： $ sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y$ sudo apt-get install -y python-pip python-dev$ curl -fsSL \"https://get.docker.com/\" | sh$ sudo timedatectl set-timezone Asia/Taipei 由於使用Ubuntu server 14.04，故需要更新 Kernel: $ sudo apt-get install -y linux-image-generic-lts-wily 接著更新 pip，並安裝 docker-python 函式庫： $ sudo pip install -U pip docker-py 編輯每台節點的/etc/default/docker檔案，加入以下內容： DOCKER_OPTS=&quot;--insecure-registry &lt;registry-ip&gt;:5000&quot; 這邊{registry-ip}為 Docker registry 的 IP 位址。若使用 Ubuntu Server 16.04 版本的話，需要編輯/lib/systemd/system/docker.service檔案，修改以下： EnvironmentFile=-/etc/default/%pExecStart=/usr/bin/dockerd -H fd:// \\ $DOCKER_OPTS 完成上述後，需要 reload service： $ sudo systemctl daemon-reload 接著編輯每台節點的/etc/rc.local檔案，加入以下內容： mount --make-shared /run# 在 compute 節點額外加入mount --make-shared /var/lib/nova/mnt 接著在compute1節點執行以下指令： sudo mkdir -p /var/lib/nova/mnt /var/lib/nova/mnt1sudo mount --bind /var/lib/nova/mnt1 /var/lib/nova/mntsudo mount --make-shared /var/lib/nova/mnt 完成後重新啟動每台節點。 當上述步驟都完成後，進入到controller1的root使用者執行以下指令安裝與設定額外套件： $ sudo apt-get install -y software-properties-common$ sudo apt-add-repository -y ppa:ansible/ansible &amp;&amp; sudo apt-get update$ sudo apt-get install -y ansible libffi-dev libssl-dev gcc git NTP 部分可自行設定，這邊不再說明。 接著繼續在controller1節點的root使用者執行以下指令： $ ssh-keygen -t rsa$ cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys 複製controller1節點的root底下的.ssh/id_rsa.pub公有金鑰到其他節點的root使用者底下的.ssh/authorized_keys。 建立 OpenStack Kolla Docker 映像檔在使用 Kolla 部署 OpenStack 叢集之前，我們需要預先建立用於部署的映像檔，才能進行節點的部署。首先進入到controller1節點，並下載 Kolla 專案： $ git clone \"https://github.com/openstack/kolla.git\" -b stable/newton$ cd kolla &amp;&amp; pip install .$ pip install tox python-openstackclient &amp;&amp; tox -e genconfig$ cp -r etc/kolla /etc/ 接著執行指令進行建立 Docker 映像檔，若不指定名稱預設下將建立全部映像檔，如以下： $ kolla-build --base ubuntu --type source --registry &#123;registry-ip&#125;:5000 --push 這邊{registry-ip}為 Docker registry 的 IP 位址。 若要改變 OS 與版本，可以編輯/etc/kolla/kolla-build.conf檔案修改以下內容： base = centosbase_tag = 3.0.3push = trueinstall_type = rdoregistry = &#123;registry-ip&#125;:5000 可參考官方文件 Building Container Images。 當映像檔建置完成後，即可查看docker-regisrtry節點的 UI 介面，來確認是否上傳成功。 部署 OpenStack 節點當完成映像檔建立後，即可開始進行部署 OpenStack 節點。 首先到controller1進入剛下載的kolla-ansible專案目錄，並編輯ansible/inventory/multinode檔案，修改以下內容： [control]controller1[network]network1[compute]compute1[storage]controller1 這邊由於機器不夠，所以將 storage 也放到 controller。 接著編輯/etc/kolla/globals.yml檔案，修改以下內容： config_strategy: \"COPY_ALWAYS\"kolla_base_distro: \"ubuntu\"kolla_install_type: \"source\"openstack_release: \"3.0.3\"kolla_internal_vip_address: \"10.0.0.10\"docker_registry: \"10.0.0.90:5000\"network_interface: \"eth0\"tunnel_interface: \"eth1\"neutron_external_interface: \"eth2\"neutron_plugin_agent: \"openvswitch\"# OpenStack serviceenable_cinder: \"yes\"enable_neutron_lbaas: \"yes\"cinder_volume_group: \"cinder-volumes\" 若要將 API 開放給 Public network 存取的話，需要設定以下內容： kolla_external_vip_address: &quot;10.26.1.251&quot;kolla_external_vip_interface: &quot;eth3&quot; (Option)建立 Cinder LVM volume group: $ sudo apt-get install lvm2 -y$ sudo pvcreate /dev/sdb$ sudo vgcreate cinder-volumes /dev/sdb$ sudo pvdisplay--- Physical volume ---PV Name /dev/sdbVG Name cinder-volumesPV Size 465.76 GiB / not usable 4.02 MiBAllocatable yesPE Size 4.00 MiBTotal PE 119234Free PE 119234Allocated PE 0PV UUID 5g01hz-Ebds-EVSF-yGm4-evNm-5Kfp-2h8kUR 然後透過以下指令產生亂數密碼： $ kolla-genpwd 接著要執行預先檢查確認節點是否可以進行部署，透過以下指令： $ kolla-ansible prechecks -i ansible/inventory/multinodePLAY RECAP *********************************************************************compute1 : ok=8 changed=0 unreachable=0 failed=0controller1 : ok=35 changed=0 unreachable=0 failed=0network1 : ok=29 changed=0 unreachable=0 failed=0 若中途發生錯誤，請檢查錯誤訊息。 確認沒問題後即可部署 OpenStack，透過以下指令進行： $ kolla-ansible deploy -i ansible/inventory/multinodePLAY RECAP *********************************************************************compute1 : ok=44 changed=31 unreachable=0 failed=0controller1 : ok=154 changed=96 unreachable=0 failed=0network1 : ok=37 changed=28 unreachable=0 failed=0 P.S. 若發生映像檔無法下載，請自行透過以下指令傳到該節點： $ docker save &#123;image&#125; &gt; image.tar$ scp image.tar network1:~/$ ssh network1 \"docker load &lt; image.tar\" 驗證服務完成後，就可以產生 Credential 檔案來進行系統驗證： $ kolla-ansible post-deploy$ source /etc/kolla/admin-openrc.sh 透過 OpenStack Client 來檢查 Keystone，並查看所有使用者： $ openstack user list+----------------------------------+-------------------+| ID | Name |+----------------------------------+-------------------+| 19af62000d334c4d9de3aa29b9fd06df | heat_domain_admin || 83fa8ba1deff46e58e716470dbdaeb03 | heat || 9661e2a4b1f648a6a3ed5c52f22c52bb | nova || 9fa2a71716d046a893dcbaef3d7375ce | admin || acdfdc4fae2248b0af4c8788a9858cf3 | glance || b683b3c611dd4bdba2c25321a0f03cf0 | cinder || dc57ebf4e6c548e990767f00ffda19d8 | neutron |+----------------------------------+-------------------+ 透過 OpenStack Client 檢查 Nova 的服務列表： $ openstack compute service list+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+| Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason |+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+| 3 | nova-consoleauth | controller1 | internal | enabled | up | 2016-08-16T05:40:16.000000 | - || 5 | nova-scheduler | controller1 | internal | enabled | up | 2016-08-16T05:40:20.000000 | - || 6 | nova-conductor | controller1 | internal | enabled | up | 2016-08-16T05:40:12.000000 | - || 10 | nova-compute | compute1 | nova | enabled | up | 2016-08-16T05:40:16.000000 | - |+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+ 透過 OpenStack Client 檢查 Neutron 的服務列表： $ openstack network agent list+------------------+------------------+----------+-------------------+-------+----------------+------------------+| id | agent_type | host | availability_zone | alive | admin_state_up | binary |+------------------+------------------+----------+-------------------+-------+----------------+------------------+| 2254dcb0-9d5b- | Open vSwitch | network1 | | :-) | True | neutron- || 46da-b1f2-b03719 | agent | | | | | openvswitch- || 5913b1 | | | | | | agent || acb81021-2efd-4b | DHCP agent | network1 | nova | :-) | True | neutron-dhcp- || b6-9af7-a3df95ce | | | | | | agent || 479b | | | | | | || b564c005-779d- | Metadata agent | network1 | | :-) | True | neutron- || 45f4-b10f- | | | | | | metadata-agent || 788bb788491b | | | | | | || d95d75e5-6429-44 | Open vSwitch | compute1 | | :-) | True | neutron- || 65-aef1-a6a9f4e7 | agent | | | | | openvswitch- || 995f | | | | | | agent || e0dbbff4-d7a7-4f | L3 agent | network1 | nova | :-) | True | neutron-l3-agent || a2-8fa2-b3d3de42 | | | | | | || d899 | | | | | | |+------------------+------------------+----------+-------------------+-------+----------------+------------------+ 從網路上下載一個測試用映像檔cirros-0.3.4-x86_64，並上傳至 Glance 服務： $ wget \"http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img\"$ openstack image create \"cirros-0.3.4-x86_64\" \\--file cirros-0.3.4-x86_64-disk.img \\--disk-format qcow2 --container-format bare \\--public 這邊可以透過 Neutron client 來查看建立外部網路： $ openstack network create --external \\--provider-physical-network physnet1 \\--provider-network-type flat public1$ openstack subnet create --no-dhcp --network public1 \\--allocation-pool start=10.26.1.230,end=10.26.1.240 \\--subnet-range 10.26.1.0/24 --gateway 10.26.1.254 public1-subnet$ openstack network create --provider-network-type vxlan admin-net$ openstack subnet create --subnet-range 192.168.10.0/24 \\--network admin-net --gateway 192.168.10.1 \\--dns-nameserver 8.8.8.8 admin-subnet$ openstack router create admin-router$ openstack router add subnet admin-router admin-subnet$ openstack router set --external-gateway public1 admin-router 建立 flavor 來提供虛擬機樣板： openstack flavor create --id 1 --ram 512 --disk 1 --vcpus 1 m1.tinyopenstack flavor create --id 2 --ram 2048 --disk 20 --vcpus 1 m1.smallopenstack flavor create --id 3 --ram 4096 --disk 40 --vcpus 2 m1.mediumopenstack flavor create --id 4 --ram 8192 --disk 80 --vcpus 4 m1.largeopenstack flavor create --id 5 --ram 16384 --disk 160 --vcpus 8 m1.xlarge 上傳 Keypair 來提供虛擬機 SSH 登入認證用： $ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey 開啟一台虛擬機進行測試： $ openstack server create --image cirros-0.3.4-x86_64 \\--flavor m1.tiny \\--nic net-id=admin-net \\--key-name mykey \\admin-instance 查看 instance 列表： $ openstack server list+--------------------------------------+----------------+--------+------------------------+---------------------+| ID | Name | Status | Networks | Image Name |+--------------------------------------+----------------+--------+------------------------+---------------------+| bb3b31b6-56a4-4c68-bd53-b7efb451f0fc | admin-instance | ACTIVE | admin-net=192.168.10.6 | cirros-0.3.4-x86_64 |+--------------------------------------+----------------+--------+------------------------+---------------------+","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"},{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"}]},{"title":"Databricks spark-perf Benchmark","slug":"data-engineer/spark-perf","date":"2016-07-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.383Z","comments":true,"path":"2016/07/17/data-engineer/spark-perf/","link":"","permalink":"https://k2r2bai.com/2016/07/17/data-engineer/spark-perf/","excerpt":"本範例為利用 spark-perf 針對SparkStreaming進行效能測試 系統需求: Ubuntu 14.04 Hadoop2.6.0 Spark1.5.2 + Mesos 安裝","text":"本範例為利用 spark-perf 針對SparkStreaming進行效能測試 系統需求: Ubuntu 14.04 Hadoop2.6.0 Spark1.5.2 + Mesos 安裝 設定 Spark-perf首先透過 Git 下載 Sparo-perf 專案： $ git clone https://github.com/databricks/spark-perf 完成後，進入spark-perf/config目錄，並透過以下指令修改組態檔： $ cd spark-perf/config$ cp config.py.template config.py 這邊修改config.py檔案以下內容： 設定 Spark 根目錄SPARK_HOME_DIR = /opt/spark。 Mesos 模式：SPARK_CLUSTER_URL = mesos://10.26.1.101:5050。 YARN 模式：SPARK_CLUSTER_URL = yarn-cluster。 設定 Java memory：JavaOptionSet(&quot;spark.executor.memory&quot;, [&quot;2g&quot;])。 設定 Spark Driver memory：SPARK_DRIVER_MEMORY = &quot;512m&quot;。 設定 SparkStreaming 中的 Spark executor memory：JavaOptionSet(&quot;spark.executor.memory&quot;, [&quot;512m&quot;])。 設定 Spark 測試項目，因本範例為 Streaming 效能評估，故將STREAMING_TESTS設定True，其餘設定False spark-perf 能針對 SparkSQL、SparkMlib、SparkCoreRDD 以及 SparkStreaming 等項目做效能評估。 RUN_SPARK_TESTS = FalseRUN_PYSPARK_TESTS = FalseRUN_STREAMING_TESTS = TrueRUN_MLLIB_TESTS = FalseRUN_PYTHON_MLLIB_TESTS = FalsePREP_SPARK_TESTS = FalsePREP_PYSPARK_TESTS = FalsePREP_STREAMING_TESTS = TruePREP_MLLIB_TESTS = False 設定batch-duration批次時間設定： 項目state-by-key、group-by-key-and-window、reduce-by-key-and-window 的批次時間設定。 STREAMING_KEY_VAL_TEST_OPTS中的streaming_batch_duration_opts(1000)。 hdfs-recovery 的批次時間設定。 STREAMING_HDFS_RECOVERY_TEST_OPTS中的streaming_batch_duration_opts(5000)。 設定total-duration效能測試時間設定： STREAMING_COMMON_OPTS = [ OptionSet(\"total-duration\", [300]), OptionSet(\"hdfs-url\", [HDFS_URL]),] 設定 SparkStreaming 測試項目，於config.py檔案中的搜尋STREAMING_TESTS來查看。測試項目可分為 5 個項目，如下： basic，測試 setup 是否正確。 state-by-key，狀態紀錄的計數評估。 group-by-key-and-window，groupByKey 的評估。 reduce-by-key-and-window，reduceByKey 的評估。 hdfs-recovery，checkpoint 備份恢復效能評估。 開始測試首先執行測試指令： $ cd spark-perf/bin$ ./run --config ../config/config.py 評估state-by-key結果如下： Result: count: 30, avg: 0.846 s, stdev: 0.244 s, min: 0.630 s,25%: 0.707 s, 50%: 0.738 s, 75%: 0.824 s, max: 1.394 s 評估結果也可於bin/results中查看： $ ubuntu@spark-master:/opt/spark-perf/bin/results$ lsstreaming_perf_output__2016-03-12_07-19-19streaming_perf_output__2016-03-12_07-19-19_logs","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Mesos","slug":"Mesos","permalink":"https://k2r2bai.com/tags/Mesos/"},{"name":"Benchmark","slug":"Benchmark","permalink":"https://k2r2bai.com/tags/Benchmark/"}]},{"title":"用 Bcache 來加速硬碟效能","slug":"linux/ubuntu/bcache","date":"2016-07-05T08:23:01.000Z","updated":"2019-12-02T01:49:42.397Z","comments":true,"path":"2016/07/05/linux/ubuntu/bcache/","link":"","permalink":"https://k2r2bai.com/2016/07/05/linux/ubuntu/bcache/","excerpt":"Bcache 是按照固態硬碟特性來設計的技術，只按擦除 Bucket 的大小進行分配，並使用 btree 和 journal 混合方法來追蹤快取資料，快取資料可以是 Bucket 上的任意一個 Sector。Bcache 最大程度上減少了隨機寫入的代價，它按循序的方式填充一個 Bucket，重新使用時只需將 Bucket 設置為無效即可。Bcache 也支援了類似 Flashcache 的快取策略，如write-back、write-through 與 write-around。","text":"Bcache 是按照固態硬碟特性來設計的技術，只按擦除 Bucket 的大小進行分配，並使用 btree 和 journal 混合方法來追蹤快取資料，快取資料可以是 Bucket 上的任意一個 Sector。Bcache 最大程度上減少了隨機寫入的代價，它按循序的方式填充一個 Bucket，重新使用時只需將 Bucket 設置為無效即可。Bcache 也支援了類似 Flashcache 的快取策略，如write-back、write-through 與 write-around。 安裝與設定首先要先安裝 bcache-tools，這邊採用 ubuntu 的apt-get來進行安裝： $ sudo add-apt-repository ppa:g2p/storage$ sudo apt-get update$ sudo apt-get install -y bcache-tools 完成安裝後，要準備一顆 SSD 與 HDD，並安裝於同一台主機上，如以下硬碟結構： +-------+----------+ +--------+---------+ | [ 固態硬碟(SSD)] | | [ 傳統硬碟(HDD)] | | System disk +-------+ System disk +| (/dev/sdb) | | (/dev/sdc) |+------------------+ +------------------+ 當確認以上都沒問題後，即可用 bcache 指令來建立快取，首先建立後端儲存裝置： $ sudo make-bcache -B /dev/sdcUUID: 3b62c662-c739-4621-aca3-80efbf5e1da2Set UUID: 67828232-2427-46d3-a473-e92e1f213f87version: 1block_size: 1data_offset: 16 -C為快取層。 -B為 bcache 後端儲存層。 --block 為 Block Size，預設為 1k。 --discard為 SSD 上使用 TRIM。 --writeback為使用 writeback 模式，預設為 writethrough。 P.S 如果有任何錯誤，請使用以下指令： &gt; $ sudo wipefs -a /dev/sdb&gt; 之後在透過指令建立快取儲存裝置，如以下： $ sudo make-bcache --block 4k --bucket 2M -C /dev/sdb -B /dev/sdc --wipe-bcacheUUID: 192dfaf6-fd2a-4246-b4be-f159c3346850Set UUID: ed865522-96a7-43e5-8dab-e8c024fe85dbversion: 0nbuckets: 228946block_size: 1bucket_size: 1024nr_in_set: 1nr_this_dev: 0first_bucket: 1 完成後，可以用bcache-super-show指令確認是否有建立，並取得 UUID： $ sudo bcache-super-show /dev/sdb | grep cset.uuidcset.uuid b6295aac-34c3-4630-8872-9aa18618daea 也可以用其他指令查看儲存建立狀況： $ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 232.9G 0 disk└─sda1 8:1 0 232.9G 0 part /sdb 8:16 0 111.8G 0 disk└─bcache0 251:0 0 465.8G 0 disksdc 8:32 0 465.8G 0 disk└─bcache0 251:0 0 465.8G 0 disk 接著將快取儲存裝置附加到後端儲存裝置： $ echo \"&lt;cset.uuid&gt;\" &gt; /sys/block/bcache0/bcache/attach bcache0會隨建立的不同而改變。 之後可以依需求設定 cache mode，透過以下方式： $ echo writeback &gt; /sys/block/bcache0/bcache/cache_mode 一切完成後，可以透過以下方式來檢查 Cache 狀態： $ cat /sys/block/bcache0/bcache/stateclean no cache：表示沒有任何快取裝置連接到後台儲存裝置。 clean：表示快取已連接，且快取是乾淨的。 dirty：表示一切設定完成，但必須啟用 writeback，且快取不是乾淨的。 inconsistent：這表示後端是不被同步的高速快取儲存裝置。記得換爛一點。 測試寫入速度這邊採用 Linux 的 dd 工具來看寫入速度： $ dd if=/dev/zero of=/dev/bcache0 bs=1G count=1 oflag=direct","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"SSD","slug":"SSD","permalink":"https://k2r2bai.com/tags/SSD/"}]},{"title":"Building Spark Source Code","slug":"data-engineer/build-spark","date":"2016-06-24T09:08:54.000Z","updated":"2019-12-02T01:49:42.383Z","comments":true,"path":"2016/06/24/data-engineer/build-spark/","link":"","permalink":"https://k2r2bai.com/2016/06/24/data-engineer/build-spark/","excerpt":"本節將說明如何透過 mvn 與 sbt 來建置 Spark 最新版的相關檔案，透過提供最新版本來觀看 API 的變動。","text":"本節將說明如何透過 mvn 與 sbt 來建置 Spark 最新版的相關檔案，透過提供最新版本來觀看 API 的變動。 事前準備首先準備一台裝有 Ubuntu 14.04 LTS Server 的主機或 Docker 容器，然後在裡面安裝相依套件： sudo apt-get purge openjdk*sudo apt-get -y autoremovesudo apt-get install -y software-properties-commonsudo add-apt-repository -y ppa:webupd8team/javasudo apt-get updateecho debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selectionsecho debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selectionssudo apt-get -y install oracle-java8-installer git 接著安裝 maven 3.3.1 + 工具： wget http://ftp.tc.edu.tw/pub/Apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gztar -zxf apache-maven-3.3.9-bin.tar.gzsudo cp -R apache-maven-3.3.9 /usr/local/sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvnmvn --version 安裝 Scala 語言： wget www.scala-lang.org/files/archive/scala-2.11.7.debsudo dpkg -i scala-2.11.7.deb 安裝 sbt 工具： echo \"deb http://dl.bintray.com/sbt/debian /\" | sudo tee /etc/apt/sources.list.d/sbt.listsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823sudo apt-get updatesudo apt-get install sbt 安裝 Python 2.7 語言： $ sudo apt-get install -y python 透過 Git 指令取得 Spark 最新原始碼： $ git clone https://github.com/apache/spark.git 使用 sbt 來建置 sparksbt 的 spark 建置指令如下所示，若使用 sbt 需要大約 10 分鐘時間：： $ ./build/sbt -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean assembly 當建置完成後，可以透過 spark-shell 查看版本： $ ./bin/spark-shell --version 使用 Apache Maven 來建置 sparkApache Maven 的 spark 建置指令如下所示: $ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean install 當建置完成後，可以透過 spark-shell 查看版本： $ ./bin/spark-shell --version Making Distributionmake-distribution.sh 是一個 shell 腳本用於建立分散式應用。它使用跟 sbt 與 mvn 一樣的配置檔案。首先新增 Java 環境參數： $ export JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\" 使用--tgz選項建立一個 tar gz 的 Spark 分散檔案： $ ./dev/make-distribution.sh --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests 一旦完成後，你會在當前目錄看到檔案，名稱會是spark-2.0.0-SNAPSHOT-bin-2.6.0.tgz。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Maven","slug":"Maven","permalink":"https://k2r2bai.com/tags/Maven/"}]},{"title":"Docker 串接 OpenStack Neutron Kuryr 網路","slug":"openstack/kuryr-install","date":"2016-06-02T08:23:01.000Z","updated":"2019-12-02T01:49:42.400Z","comments":true,"path":"2016/06/02/openstack/kuryr-install/","link":"","permalink":"https://k2r2bai.com/2016/06/02/openstack/kuryr-install/","excerpt":"Kuryr 是 Docker network plugin 之一，主要是使用 Neutron 來提供網路服務給不同主機的 Docker 容器使用，目前也提供了容器化的 Neutron plugin 容器映像檔。 本篇說明如何透過 CentOS 來部署簡單的 Kuryr 與 Docker 串接。","text":"Kuryr 是 Docker network plugin 之一，主要是使用 Neutron 來提供網路服務給不同主機的 Docker 容器使用，目前也提供了容器化的 Neutron plugin 容器映像檔。 本篇說明如何透過 CentOS 來部署簡單的 Kuryr 與 Docker 串接。 節點配置OpenStack Kuryr 我們使用到三台節點，以下為本次安裝的規格： Role RAM Disk CPUs IP Address controller 4 GB 50GB 2vCPU 172.16.1.115 network-1 4 GB 50GB 2vCPU 172.16.1.118 network-2 4 GB 50 GB 2vCPU 172.16.1.119 請以上節點都分別安裝 RHEL 或者 CentOS 作業系統。並且設定 IP 為靜態固定，編輯/etc/sysconfig/network-scripts/ifcfg-&lt;name&gt;檔案，加入以下內容： ONBOOT=\"yes\"IPADDR=\"10.0.0.104\"PREFIX=\"24\"GATEWAY=\"10.0.0.1\"DNS1=\"8.8.8.8\"DNS2=\"8.8.8.4\" P.S. 若是虛擬機則不需要設定。 安裝前準備在開始安裝以前，首先需要在每一台節點將基本環境的軟體更新： $ sudo yum update -y 完成後檢查是否是最新版本，可以透過以下方式查看 Kernel： $ uname -r3.10.0-327.13.1.el7.x86_64 如果不是以上版本，請執行以下指令： $ sudo yum upgrade --assumeyes --tolerant$ sudo yum update --assumeyes 由於在 CentOS 與 RHEL 預設會開啟防火牆，故要關閉防火牆與開機時自動啟動： $ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld 設定關閉 SELinux 與設定一些資訊，並重新啟動： $ sudo sed -i s/SELINUX=enforcing/SELINUX=permissive/g /etc/selinux/config &amp;&amp;sudo groupadd nogroup &amp;&amp;sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 &amp;&amp;sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 &amp;&amp;sudo reboot 完成重新啟動後，在所有Network節點安裝 Docker，首先要取得 repos，設定以下來讓 yum 可以抓取： $ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF 在Network節點安裝 Docker engine，並啟動 docker 與設定開機啟動： $ sudo yum install --assumeyes --tolerant docker-engine$ sudo systemctl start docker$ sudo systemctl enable docker 在所有節點安裝一些基本工具軟體： $ sudo yum install -y tar xz unzip curl ipset vim wget git python-pip$ sudo pip install --upgrade pip 在Controller節點，進入root使用者，並建置 ssh keys： $ ssh-keygen -t rsa$ cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys 複製Controller節點的.ssh/id_rsa.pub內容，並貼到Network節點的root使用者的.ssh/authorized_keys。並驗證 ssh 是否可以無密碼登入： $ ssh 172.16.1.118 安裝 OpenStack這邊使用 RDO 進行安裝。由於只需要 Neutron 與 Keystone 服務，所以可以修改部署的answer-file設定檔。由於這邊使用的是虛擬機，因此 Neutron 網路採用 VXLAN 方式進行安裝。進入到Controller節點，並且進入到root使用者安裝 PackStack： $ yum install -y centos-release-openstack-mitaka.noarch$ yum install -y openstack-packstack$ wget https://gist.githubusercontent.com/kairen/637c707b960e188d32aba9044e652c0b/raw/6724a5177ca82ced555554635c6b0893e8c398ab/answer-file 這邊可以更改安裝版本，如更改成 Liberty 的穩定版centos-release-openstack-liberty.noarch 編輯answer-file設定檔，修改以下內容： CONFIG_CONTROLLER_HOST=172.16.1.115CONFIG_COMPUTE_HOSTS=172.16.1.118,172.16.1.119CONFIG_NETWORK_HOSTS=172.16.1.115,172.16.1.118,172.16.1.119CONFIG_STORAGE_HOST=172.16.1.115CONFIG_AMQP_HOST=172.16.1.115CONFIG_MARIADB_HOST=172.16.1.115CONFIG_KEYSTONE_LDAP_URL=ldap://172.16.1.115 設定檔都確認無誤後，透過以下指令進行安裝： $ packstack --answer-file=answer-file...**** Installation completed successfully ******* To access the OpenStack Dashboard browse to http://172.16.1.115/dashboard . 中途若發生安裝套件失敗問題，請直接重新執行一次。 當成功安裝完成後，透過簡單的 OpenStack 指令來確認： $ . keystonerc_admin$ openstack user list+----------------------------------+---------+| ID | Name |+----------------------------------+---------+| 70b80593320543bbb32e15d7f06036f0 | admin || 9600aaa2447940e789e548b2f5515690 | neutron |+----------------------------------+---------+ 安裝 Kuryr進入Network節點，並且進入到root使用者，下載 Kuryr 最新的專案： $ git clone https://github.com/openstack/kuryr.git$ pip install -r requirements.txt$ python setup.py install 建立 Kuryr 設定檔與 Log 目錄： $ mkdir -p /var/log/kuryr /etc/kuryr$ wget https://gist.githubusercontent.com/kairen/637c707b960e188d32aba9044e652c0b/raw/6724a5177ca82ced555554635c6b0893e8c398ab/kuryr.conf -O /etc/kuryr/kuryr.conf 編輯/etc/kuryr/kuryr.conf設定檔，修改一下內容： [keystone_client]auth_uri = http://172.16.1.115:35357/v2.0[neutron_client]neutron_uri = http://172.16.1.115:9696 接著啟動 Kuryr 服務： $ ./scripts/run_kuryr.sh &amp;2016-06-02 03:51:33.578 4758 INFO werkzeug [-] * Running on http://0.0.0.0:2377/ (Press CTRL+C to quit) 服務驗證當所有Network節點都完成 Kuryr 安裝後，就可以透過以下方式來驗證，首先在network-1建立網路： $ docker network create --driver=kuryr \\--ipam-driver=kuryr \\--subnet 10.0.0.0/16 \\--gateway 10.0.0.1 \\--ip-range 10.0.0.0/24 kuryr...821d6cd53af6c656969c1c96063a60c695a0313c9a119e44d4325ce2a9f2f935 透過 docker 指令來查看網路： $ docker network inspect kuryr[ &#123; \"Name\": \"kuryr\", \"Id\": \"821d6cd53af6c656969c1c96063a60c695a0313c9a119e44d4325ce2a9f2f935\", \"Scope\": \"local\", \"Driver\": \"kuryr\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"kuryr\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"10.0.0.0/16\", \"IPRange\": \"10.0.0.0/24\", \"Gateway\": \"10.0.0.1\" &#125; ] &#125;, \"Internal\": false, \"Containers\": &#123; \"367763a677ad180c57818631ee3e9151683d0218a15bb002d0995ab5c6e30446\": &#123; \"Name\": \"awesome_dijkstra\", \"EndpointID\": \"326aba6247266cfd0ca3771b0283e2142a3e934bb994d1b77fc93bb467c0df48\", \"MacAddress\": \"fa:16:3e:7c:d8:b6\", \"IPv4Address\": \"10.0.0.5/24\", \"IPv6Address\": \"\" &#125;, \"b740abd7976d274de233df0bad052c418516ba036132b92ad022a4b52a2d7d25\": &#123; \"Name\": \"awesome_engelbart\", \"EndpointID\": \"fcc642dae6323ce7d86ddda61c9583a19801eae70d8126d35b4b5846cd8598a4\", \"MacAddress\": \"fa:16:3e:5b:6e:7f\", \"IPv4Address\": \"10.0.0.3/24\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;] 接著建立一個 container 來取得 IP： $ docker run -it -d --net kuryr --privileged=true ubuntu:14.04$ docker exec -ti $(docker ps -lq) bashroot@367763a677ad$ ip -4 a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever14: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 10.0.0.3/24 scope global eth0 valid_lft forever preferred_lft forever 接著進入到docker 指令建立網路，這邊採用跟上一個同樣的網路：```sh$ docker network create --driver=kuryr \\--ipam-driver=kuryr \\--subnet 10.0.0.0/16 \\--gateway 10.0.0.1 \\--ip-range 10.0.0.0/24 \\-o neutron.net.uuid=8c069d2c-772c-47ae-90bb-d22148f37dc8 kuryr 這邊neutron.net.uuid可以透過以下在Controller方式取得： $ neutron net-list | awk '/kuryr/ &#123;print $2&#125;'8c069d2c-772c-47ae-90bb-d22148f37dc8 接著一樣建立一個 container 來取得 IP： $ docker run -it -d --net kuryr --privileged=true ubuntu:14.04$ docker exec -ti $(docker ps -lq) bashroot@f2b48a802e92$ ip -4 a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever15: eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 10.0.0.4/24 scope global eth0 valid_lft forever preferred_lft forever 透過 ping 來驗證網路是否有連接： root@f2b48a802e92$ ping -c 3 10.0.0.3PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.71 ms64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.37 ms64 bytes from 10.0.0.3: icmp_seq=3 ttl=64 time=1.23 ms","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"}]},{"title":"用 Flashcache 建立高容量與高效能儲存","slug":"linux/ubuntu/flashcache","date":"2016-05-27T08:23:01.000Z","updated":"2019-12-02T01:49:42.398Z","comments":true,"path":"2016/05/27/linux/ubuntu/flashcache/","link":"","permalink":"https://k2r2bai.com/2016/05/27/linux/ubuntu/flashcache/","excerpt":"Flashcache 是 Facebook 的一個開源專案，主要被用於資料庫加速。基本結構為在硬碟（HDD）前面加了一層快取，即採用固態硬碟（SSD）裝置，把熱資料保存於快取中，寫入的過程也是先寫到 SSD，然後由 SSD 同步到傳統硬碟，最後的資料將保存於硬碟中，這樣可以不用擔心 SSD 損壞造成資料遺失問題，同時又可以有大容量、高效能的儲存。","text":"Flashcache 是 Facebook 的一個開源專案，主要被用於資料庫加速。基本結構為在硬碟（HDD）前面加了一層快取，即採用固態硬碟（SSD）裝置，把熱資料保存於快取中，寫入的過程也是先寫到 SSD，然後由 SSD 同步到傳統硬碟，最後的資料將保存於硬碟中，這樣可以不用擔心 SSD 損壞造成資料遺失問題，同時又可以有大容量、高效能的儲存。 安裝本教學採用 Ubuntu 14.04 LTS 進行安裝，並建立快取。首先安裝相依套件： $ sudo apt-get install -y git build-essential dkms linux-headers-`uname -r` 完成後，透過 git 指令將專案下載至主機上： $ git clone https://github.com/facebook/flashcache.git$ cd flashcache 進入目錄編譯 flashcache 套件，並透過 make 進行安裝套件： $ make$ sudo make install 安裝完成後，就可以載入 flashcache 模組，透過以下指令： $ sudo modprobe flashcache 若要檢查是否載入成功的話，可以使用以下指令： $ dmesg | tail[24181.921706] flashcache: module verification failed: signature and/or required key missing - tainting kernel[24181.922785] flashcache: flashcache-3.1.1 initialized 設定開機時自動載入模組： $ echo \"flashcache\" | sudo tee -a /etc/modules 設定快取首先準備一顆 SSD 與 HDD，並安裝於同一台主機上，如以下硬碟結構： +-------+----------+ +--------+---------+ | [ 固態硬碟(SSD)] | | [ 傳統硬碟(HDD)] | | System disk +-------+ System disk +| (/dev/sdb) | | (/dev/sdc) |+------------------+ +------------------+ 在開始前，必須先將傳統硬碟進行格式化： $ sudo mkfs.ext4 /dev/sdc 接著要初始化 Flashcache，然後透過 Flashcache 指令來設定快取： $ sudo flashcache_create -p back -b 4k cachedev /dev/sdb /dev/sdccachedev cachedev, ssd_devname /dev/sdb, disk_devname /dev/sdc cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0Flashcache metadata will use 614MB of your 7950MB main memory 完成後，就可以透過 mount 來使用快取： $ sudo mount /dev/mapper/cachedev /mnt 若要在開機時自動 mount 為 Flashcache 的快取固態硬碟，可以在rc.local加入以下內容： flashcache_load /dev/sdbmount /dev/mapper/cachedev /mnt 若想監控 Flashcache 資訊的話，可以使用以下工具： $ flashstat 最後，若想要刪除 Flashcache 的話，可以使用以下指令： $ sudo umount /mnt$ sudo flashcache_destroy /dev/sdb$ sudo dmsetup remove cachedev fio 測試這邊採用 fio 來進行測試，首先透過apt-get安裝套件： $ sudo apt-get install fio 完成後，即可透過 fio 指令進行效能測試： $ fio --filename=/dev/sdb --direct=1 \\--rw=randrw --ioengine=libaio --bs=4k \\--rwmixread=100 --iodepth=16 \\--numjobs=16 --runtime=60 \\--group_reporting --name=4ktest fio 測試工具 options 參數： --filename=/dev/sdb：指定要測試的磁碟。 --direct=1：預設值為 0 ,必須設定為 1 才會測試到真實的 non-buffered I/O。 --rw=randrw：可以設定的參數如下 randrw 代表 random(隨機) 的 read(讀) write(寫),其他的請參考下面說明。 read : Sequential reads. (循序讀) write : Sequential writes. (循序寫) randread : Random reads. (隨機讀) randwrite : Random writes. (隨機寫) rw : Mixed sequential reads and writes. (循序讀寫) randrw : Mixed random reads and writes. (隨機讀寫) --ioengine=libaio：定義如何跑 I/O 的方式, libaio 是 Linux 本身非同步(asynchronous) I/O 的方式. 其他還有 sync , psync , vsync , posixaio , mmap , splice , syslet-rw , sg , null , net , netsplice , cpuio , guasi , external。 --bs=4k：bs 或是 blocksize ,也就是檔案寫入大小,預設值為 4K。 --rwmixread=100： 當設定為 Mixed ,同一時間 read 的比例為多少,預設為 50%。 --refill_buffers：refill_buffers 為預設值,應該是跟 I/O Buffer 有關 (refill the IO buffers on every submit),把 Buffer 填滿就不會跑到 Buffer 的值。 --iodepth=16：同一時間有多少 I/O 在做存取,越多不代表存儲裝置表現會更好,通常是 RAID 時須要設大一點。 --numjobs=16：跟前面的 iodepth 類似,但不一樣,在 Linux 下每一個 job 可以生出不同的 processes/threads ,numjobs 就是在同一個 workload 同時提出多個 I/O 請求,通常負載這個會比較大.預設值為 1。 --runtime=60：這一測試所需的時間,單位為 秒。 --group_reporting：如果 numjobs 有指定,設定 group_reporting 報告會以 per-group 的顯示方式。 --name=4ktest：代表這是一個新的測試 Job。 參考資料 Fio – Flexible I/O Tester Flashcache Wiki Flashcache初次体验 Ubuntu bonnie++硬碟測速 (Linux 適用)","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"SSD","slug":"SSD","permalink":"https://k2r2bai.com/tags/SSD/"}]},{"title":"Pacemaker + Corosync 做服務 HA","slug":"linux/ubuntu/corosync-pacemaker","date":"2016-05-26T08:23:01.000Z","updated":"2019-12-02T01:49:42.397Z","comments":true,"path":"2016/05/26/linux/ubuntu/corosync-pacemaker/","link":"","permalink":"https://k2r2bai.com/2016/05/26/linux/ubuntu/corosync-pacemaker/","excerpt":"Pacemaker 與 Corosync 是 Linux 中現今較常用的高可靠性叢集系統組合。Pacemaker 自身提供了很多常用的應用管理功能，不過若要使用 Pacemaker 來管理自己實作的服務，或是一些特別的東西時，就必須要自己實作管理資源。","text":"Pacemaker 與 Corosync 是 Linux 中現今較常用的高可靠性叢集系統組合。Pacemaker 自身提供了很多常用的應用管理功能，不過若要使用 Pacemaker 來管理自己實作的服務，或是一些特別的東西時，就必須要自己實作管理資源。 節點配置本安裝將使用三台實體主機與一台虛擬機器，主機規格如以下所示： Role IP Address pacemaker1 172.16.35.10 pacemaker2 172.16.35.11 作業系統皆為 Ubuntu 14.04 Server。 進行安裝與設定首先要在所有節點之間設定無密碼 ssh 登入，透過以下方式： $ ssh-keygen -t rsa$ ssh-copy-id pacemaker1 安裝相關套件軟體： $ sudo apt-get install -y corosync pacemaker heartbeat resource-agents fence-agents apache2 完成後，在pacemaker1進行以下步驟，首先編輯/etc/corosync/corosync.conf設定檔，修改一下內容： # Please read the openais.conf.5 manual pagetotem &#123; version: 2 # How long before declaring a token lost (ms) token: 3000 # How many token retransmits before forming a new configuration token_retransmits_before_loss_const: 10 # How long to wait for join messages in the membership protocol (ms) join: 60 # How long to wait for consensus to be achieved before starting a new round of membership configuration (ms) consensus: 3600 # Turn off the virtual synchrony filter vsftype: none # Number of messages that may be sent by one processor on receipt of the token max_messages: 20 # Limit generated nodeids to 31-bits (positive signed integers) clear_node_high_bit: yes # Disable encryption secauth: off #啟動認證功能 # How many threads to use for encryption/decryption threads: 0 # Optionally assign a fixed node id (integer) # nodeid: 1234 # This specifies the mode of redundant ring, which may be none, active, or passive. rrp_mode: none interface &#123; # The following values need to be set based on your environment ringnumber: 0 bindnetaddr: 10.11.8.0 # 主機所在網路位址 mcastaddr: 226.93.2.1 # 廣播地址，不要被佔用即可 P.S. 範圍:224.0.2.0～238.255.255.255 mcastport: 5405 # 廣播埠口 &#125;&#125;amf &#123; mode: disabled&#125;quorum &#123; # Quorum for the Pacemaker Cluster Resource Manager provider: corosync_votequorum expected_votes: 1&#125;aisexec &#123; user: root group: root&#125;logging &#123; fileline: off to_stderr: yes # 輸出到標準输出 to_logfile: yes # 輸出到日誌檔案 logfile: /var/log/corosync.log # 日誌檔案位置 to_syslog: no # 輸出到系统日誌 syslog_facility: daemon debug: off timestamp: on logger_subsys &#123; subsys: AMF debug: off tags: enter|leave|trace1|trace2|trace3|trace4|trace6 &#125;&#125;# 新增 pacemaker 服務配置service &#123; ver: 1 name: pacemaker&#125; 接著產生節點之間的溝通時的認證金鑰文件： $ corosync-keygen -l 然後將設定檔與金鑰複製到pacemaker2上： $ cd /etc/corosync/$ scp -p corosync.conf authkey pacemaker2:/etc/corosync/ 接著分別在兩個節點上編輯/etc/default/corosync檔案，修改以下： # start corosync at boot [yes|no]START=yes 接著將 Corosync 與 Pacemaker 服務啟動： $ sudo service corosync start$ sudo service pacemaker start 完成後透過 crm 指令來查看狀態： $ crm statusLast updated: Tue Dec 27 03:12:07 2016Last change: Tue Dec 27 02:35:18 2016 via cibadmin on pacemaker1Stack: corosyncCurrent DC: pacemaker1 (739255050) - partition with quorumVersion: 1.1.10-42f20632 Nodes configured0 Resources configuredOnline: [ pacemaker1 pacemaker2 ] 關閉 corosync 預設啟動的 stonith 與 quorum 在兩台節點之問題： $ crm configure property stonith-enabled=false$ crm configure property no-quorum-policy=ignore 完成後，透過指令檢查： $ crm configure shownode $id=\"739255050\" pacemaker1node $id=\"739255051\" pacemaker2property $id=\"cib-bootstrap-options\" \\ dc-version=\"1.1.10-42f2063\" \\ cluster-infrastructure=\"corosync\" \\ stonith-enabled=\"false\" \\ no-quorum-policy=\"ignore\" 設定資源Corosync 支援了多種資源代理，如 heartbeat、LSB(Linux Standard Base)與 OCF(Open Cluster Framework) 等。而 Corosync 也可以透過指令來查詢： $ crm ra classeslsbocf / heartbeat pacemaker redhatservicestonithupstart 而更細部的資訊可以透過以下查詢： $ crm ra list lsb$ crm ra list ocf heartbeat$ crm ra info ocf:heartbeat:IPaddr 首先新增一個 heartbeat 資源： $ crm configure# 設定 VIPcrm(live)configure# primitive vip ocf:heartbeat:IPaddr params ip=172.16.35.20 nic=eth2 cidr_netmask=24 op monitor interval=10s timeout=20s on-fail=restart# 設定 httpdcrm(live)configure# primitive httpd lsb:apache2crm(live)configure# exitThere are changes pending. Do you want to commit them? yes 設定 Group 來將 httpd 與 vip 資源放一起： crm(live)configure# group webservice vip httpd 完成後，透過 crm 指令查詢狀態： $ crm statusLast updated: Tue Dec 27 03:52:21 2016Last change: Tue Dec 27 03:52:20 2016 via cibadmin on pacemaker1Stack: corosyncCurrent DC: pacemaker1 (739255050) - partition with quorumVersion: 1.1.10-42f20632 Nodes configured2 Resources configuredOnline: [ pacemaker1 pacemaker2 ] Resource Group: webservice vip (ocf::heartbeat:IPaddr): Started pacemaker1 httpd (lsb:apache2): Started pacemaker2 最後就可以在pacemaker1或pacemaker2關閉服務來確認是否正常執行。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"Load Balancer","slug":"Load-Balancer","permalink":"https://k2r2bai.com/tags/Load-Balancer/"},{"name":"High Availability","slug":"High-Availability","permalink":"https://k2r2bai.com/tags/High-Availability/"}]},{"title":"利用 rados-java 存取 Ceph","slug":"ceph/rados-java","date":"2016-05-15T09:08:54.000Z","updated":"2019-12-02T01:49:42.379Z","comments":true,"path":"2016/05/15/ceph/rados-java/","link":"","permalink":"https://k2r2bai.com/2016/05/15/ceph/rados-java/","excerpt":"rados-java 透過 JNA 來綁定 librados (C) 的 API 來提供給 Java 使用，並且實作了 RADOS 與 RBD 的 API，由於透過 JNA 的關析，故不用建構任何的 Header 檔案(.h)。因此我們可以在擁有 JNA 與 librados 的系統上使用本函式庫。","text":"rados-java 透過 JNA 來綁定 librados (C) 的 API 來提供給 Java 使用，並且實作了 RADOS 與 RBD 的 API，由於透過 JNA 的關析，故不用建構任何的 Header 檔案(.h)。因此我們可以在擁有 JNA 與 librados 的系統上使用本函式庫。 環境準備在開始進行之前，需要滿足以下幾項要求： 需要部署一個 Ceph 叢集，可以參考 Ceph Docker 部署。 執行 rados-java 程式的環境，要能夠與 Ceph 叢集溝通(ceph.conf、admin key)。 需要安裝 Ceph 相關 library。可以透過以下方式安裝： $ wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -$ echo \"deb https://download.ceph.com/debian-kraken/ $(lsb_release -sc) main\" | sudo tee /etc/apt/sources.list.d/ceph.list$ sudo apt-get update &amp;&amp; sudo apt-get install -y ceph 建構 rados-java jar 檔首先需要安裝一些相關軟體來提供建構 rados-java 使用： $ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ sudo apt-get -y install oracle-java8-installer git libjna-java$ sudo ln -s /usr/share/java/jna-4.2.2.jar /usr/lib/jvm/java-8-oracle/jre/lib/ext/ 接著安裝 maven 3.3.1 + 工具： wget \"http://ftp.tc.edu.tw/pub/Apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz\"tar -zxf apache-maven-3.3.9-bin.tar.gzsudo cp -R apache-maven-3.3.9 /usr/local/sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvnmvn --version 然後透過 Git 取得 rados-java 原始碼： $ git clone \"https://github.com/ceph/rados-java.git\"$ cd rados-java &amp;&amp; git checkout v0.3.0$ mvn clean install -Dmaven.test.skip=true 完成後將 rados-java Jar 檔複製到/usr/share/java/底下，並設定 JAR 連結 JVM Class path： $ sudo cp target/rados-0.3.0.jar /usr/share/java$ sudo ln -s /usr/share/java/rados-0.3.0.jar /usr/lib/jvm/java-8-oracle/jre/lib/ext/ 這邊也可以直接透過下載 Jar 檔來完成： $ wget \"http://central.maven.org/maven2/com/ceph/rados/0.3.0/rados-0.3.0.jar\"$ sudo cp rados-0.3.0.jar /usr/share/java/ 最後就可以透過簡單範例程式存取 Ceph 了。 簡單測試程式這邊透過 Java 程式連結到 Ceph 叢集，並且存取data儲存池來寫入物件，建立與編輯Example.java檔，加入以下程式內容： import com.ceph.rados.Rados;import com.ceph.rados.exceptions.RadosException;import java.io.File;import com.ceph.rados.IoCTX;public class Example &#123; public static void main (String args[])&#123; try &#123; Rados cluster = new Rados(\"admin\"); File f = new File(\"/etc/ceph/ceph.conf\"); cluster.confReadFile(f); cluster.connect(); System.out.println(\"Connected to the cluster.\"); IoCTX io = cluster.ioCtxCreate(\"data\"); /* Pool Name */ String oidone = \"kyle-say\"; String contentone = \"Hello World!\"; io.write(oidone, contentone); String oidtwo = \"my-object\"; String contenttwo = \"This is my object.\"; io.write(oidtwo, contenttwo); String[] objects = io.listObjects(); for (String object: objects) System.out.println(\"Put \" + object); /* io.remove(oidone); io.remove(oidtwo); */ cluster.ioCtxDestroy(io); &#125; catch (RadosException e) &#123; System.out.println(e.getMessage() + \": \" + e.getReturnValue()); &#125; &#125;&#125; 撰寫完程式後，執行以下指令來看結果： $ javac Example.java$ sudo java ExampleConnected to the cluster.Put kyle-sayPut my-object 透過 rados 指令檢查當程式正確執行後，就可以透過 rados 指令來確認物件是否正確被寫入： $ sudo rados -p data lskyle-saymy-object 透過 Get 指令來取得物件的內容： $ sudo rados -p data get kyle-say -Hello World!$ sudo rados -p data get my-object -This is my object.","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Java","slug":"Java","permalink":"https://k2r2bai.com/tags/Java/"}]},{"title":"Alluxio 分散式虛擬儲存系統","slug":"data-engineer/alluxio","date":"2016-05-06T09:08:54.000Z","updated":"2019-12-02T01:49:42.382Z","comments":true,"path":"2016/05/06/data-engineer/alluxio/","link":"","permalink":"https://k2r2bai.com/2016/05/06/data-engineer/alluxio/","excerpt":"Alluxio 是分散式虛擬儲存系統，早期名稱為 Tachyon ，而現在已正式改名 Alluxio，並發佈 1.0 版本 Aluxion 是一個記憶體虛擬分散式儲存系統，具有高效能、高容錯以及高可靠性的特色，它能夠統一資料的存取去串接機算框架和儲存系統的橋梁，像是同時可相容於 Hadoop MapReduce 和 Apache Spark 以及 Apache Flink 的計算框架和 Alibaba OSS、Amazon S3、OpenStack Swift,、GlusterFS 及 Ceph 的儲存系統","text":"Alluxio 是分散式虛擬儲存系統，早期名稱為 Tachyon ，而現在已正式改名 Alluxio，並發佈 1.0 版本 Aluxion 是一個記憶體虛擬分散式儲存系統，具有高效能、高容錯以及高可靠性的特色，它能夠統一資料的存取去串接機算框架和儲存系統的橋梁，像是同時可相容於 Hadoop MapReduce 和 Apache Spark 以及 Apache Flink 的計算框架和 Alibaba OSS、Amazon S3、OpenStack Swift,、GlusterFS 及 Ceph 的儲存系統 Install Java7首先安裝相關套件： $ sudo apt-get purge openjdk*$ sudo apt-get -y autoremove$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections$ sudo apt-get -y install oracle-java7-installer Download Alluxio 1.0.0接著下載 Alluxio： $ wget http://alluxio.org/downloads/files/1.0.0/alluxio-1.0.0-bin.tar.gz$ tar xvfz alluxio-1.0.0-bin.tar.gz$ cd alluxio-1.0.0 複製一個conf/alluxio-env.sh檔案 $ cp conf/alluxio-env.sh.template conf/alluxio-env.sh conf/alluxio-env.sh中加入ALLUXIO_UNDERFS_ADDRESS參數 export ALLUXIO_UNDERFS_ADDRESS=/tmp 確認 ssh localhost 可成功 $ssh-copy-id localhsot 格式化 Alluxio FileSystem 並開啟它 $ ./bin/alluxio format$ ./bin/alluxio-start.sh local 驗證 Alluxio 可於瀏覽器輸入http://localhost:19999，也可執行簡單的程式，如下: $ ./bin/alluxio runTest Basic CACHE THROUGH 執行後，如下: 2015-11-20 08:32:22,271 INFO (ClientBase.java:connect) - Alluxio client (version 1.0.0) is trying to connect with FileSystemMaster master @ localhost/127.0.0.1:199982015-11-20 08:32:22,294 INFO (ClientBase.java:connect) - Client registered with FileSystemMaster master @ localhost/127.0.0.1:199982015-11-20 08:32:22,387 INFO (BasicOperations.java:createFile) - createFile with fileId 33554431 took 127 ms.2015-11-20 08:32:22,552 INFO (ClientBase.java:connect) - Alluxio client (version 1.0.0) is trying to connect with BlockMaster master @ localhost/127.0.0.1:199982015-11-20 08:32:22,553 INFO (ClientBase.java:connect) - Client registered with BlockMaster master @ localhost/127.0.0.1:199982015-11-20 08:32:22,604 INFO (WorkerClient.java:connect) - Connecting local worker @ /192.168.2.15:299982015-11-20 08:32:22,698 INFO (BasicOperations.java:writeFile) - writeFile to file /default_tests_files/BasicFile_CACHE_THROUGH took 311 ms.2015-11-20 08:32:22,759 INFO (FileUtils.java:createStorageDirPath) - Folder /Volumes/ramdisk/alluxioworker/7226211928567857329 was created!2015-11-20 08:32:22,809 INFO (LocalBlockOutStream.java:&lt;init&gt;) - LocalBlockOutStream created new file block, block path: /Volumes/ramdisk/alluxioworker/7226211928567857329/167772162015-11-20 08:32:22,886 INFO (BasicOperations.java:readFile) - readFile file /default_tests_files/BasicFile_CACHE_THROUGH took 187 ms.Passed the test! 執行更複雜的測試: $ ./bin/alluxio runTests 停止服務： $ ./bin/alluxio-stop.sh all","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Java","slug":"Java","permalink":"https://k2r2bai.com/tags/Java/"},{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"}]},{"title":"DM-cache 建立混和區塊裝置","slug":"linux/ubuntu/dm-cache","date":"2016-04-21T08:23:01.000Z","updated":"2019-12-02T01:49:42.398Z","comments":true,"path":"2016/04/21/linux/ubuntu/dm-cache/","link":"","permalink":"https://k2r2bai.com/2016/04/21/linux/ubuntu/dm-cache/","excerpt":"DM-cache 是一種利用高速的儲存裝置給低速儲存裝置當作快取的技術，透過此一技術使儲存系統兼容容量與效能之間的平衡。DM-cache 目前是 Linunx 核心的一部份，透過裝置映射(Device Mapper)機制允許管理者建立混合的磁區(Volume)。","text":"DM-cache 是一種利用高速的儲存裝置給低速儲存裝置當作快取的技術，透過此一技術使儲存系統兼容容量與效能之間的平衡。DM-cache 目前是 Linunx 核心的一部份，透過裝置映射(Device Mapper)機制允許管理者建立混合的磁區(Volume)。 快取建立流程DM-cache 在比較新版本的 Linux Kernel 已經整合，以下為建置流程： $ sudo blockdev --getsize64 /dev/sdb250059350016# ssd-metadata : 4194304 + (250059350016 * 16 / 262144) / 512 = 38001# ssd-blocks : 250059350016 / 512 - 38001 = 488359166$ sudo dmsetup create ssd-metadata --table '0 38001 linear /dev/sdb 0'$ sudo dd if=/dev/zero of=/dev/mapper/ssd-metadata$ sudo dmsetup create ssd-blocks --table '0 189008622 linear /dev/sdb 38001'$ sudo blockdev --getsz /dev/sdc1953525168$ sudo dmsetup create home-cached --table '0 1953525168 cache /dev/mapper/ssd-metadata /dev/mapper/ssd-blocks /dev/sdc 512 1 writeback default 0'$ ls -l /dev/mapper/home-cached$ sudo mkdir /mnt/cache$ sudo mount /dev/mapper/home-cached /mnt/cache","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"SSD","slug":"SSD","permalink":"https://k2r2bai.com/tags/SSD/"}]},{"title":"Akka 基本介紹","slug":"data-engineer/akka-intro","date":"2016-04-06T09:08:54.000Z","updated":"2019-12-02T01:49:42.382Z","comments":true,"path":"2016/04/06/data-engineer/akka-intro/","link":"","permalink":"https://k2r2bai.com/2016/04/06/data-engineer/akka-intro/","excerpt":"Akka 是基於 Actor 模型以 Scala 程式語言開發而成的開源工具，被用在建置可擴展、彈性、高度並行、分散式與快速響應的 JVM 應用程式平台。現在許多高度並行（Concurrent） JVM 應用程式被廣泛應用，尤其以巨量資料處理框架為甚，諸如：Spark、Storm等，甚至可以基於 Akka 建置高平行的 Web 框架，更能建置分散式系統。Akka 最主要的目是要解決同步造成的效能問題，以及可能發生的死鎖問題。 Akka 目前擁有以下幾個特點： 高度並行與分散式 可擴展 擁有容錯機制 去中心化，且彈性 基於 Actors 模型 事務性 Actors 支援 JAVA 與 Scala API。 支援叢集","text":"Akka 是基於 Actor 模型以 Scala 程式語言開發而成的開源工具，被用在建置可擴展、彈性、高度並行、分散式與快速響應的 JVM 應用程式平台。現在許多高度並行（Concurrent） JVM 應用程式被廣泛應用，尤其以巨量資料處理框架為甚，諸如：Spark、Storm等，甚至可以基於 Akka 建置高平行的 Web 框架，更能建置分散式系統。Akka 最主要的目是要解決同步造成的效能問題，以及可能發生的死鎖問題。 Akka 目前擁有以下幾個特點： 高度並行與分散式 可擴展 擁有容錯機制 去中心化，且彈性 基於 Actors 模型 事務性 Actors 支援 JAVA 與 Scala API。 支援叢集 Akka 是基於 Actor 模型來開發，透過 Actor 能夠簡化死鎖與執行緒管理，可以非常容易開發正確並行化行程與系統，在 Akka 中 Actor 是最基本、最重要的元素，被用來完成工作。Actor 具有以下特性： 提供高級別的抽象，能簡化在並行（Concurrency）/平行（Parallelism）應用下的程式開發。 提供異步（Async）非阻塞、高效能的事件驅動程式模型。 非常輕量的事件處理（每 GB Heap 記憶體有幾百萬的 Actor） Actor 是一個運算實體，在開發程式中就是對實體之間所回應接受到的訊息做互動，同時並行的傳送有限數量的訊息給其他 Actor、建立有限數量的 Actor 以及設計指定接收到下一個訊息時的行為。 Actor 之間是獨立的，多個 Actor 進行互動只能透過自定的訊息（Message）來完成發送與接收處理。如果一個 Actor 在某一個時刻收到多個 Actor 發送的訊息，就會發生並行問題，這時就需要一個訊息佇列來進行訊息的儲存與分散。可參考 Akka 為範例，介紹 Actor 模型。 Akka 應用場景有以下幾個項目，當然這不是全部： 交易處理（Transaction Processing） 後端服務（Backend Service） 平行運算（Concurrency/Parallelism） 通訊 Hub（Communications Hub） 複雜事件串流處理（Complex Event Stream Processing） 安裝Akka 一般在 Java 有兩種安裝方式，如以下： 當作 Library 使用，就是直接 Import JAR 使用。可參考 Java Documentation。 將應用放到獨立的微核心（Microkernel）裡使用。可參考 Microkernel。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://k2r2bai.com/tags/Java/"},{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Concurrent","slug":"Concurrent","permalink":"https://k2r2bai.com/tags/Concurrent/"}]},{"title":"Apache Flume 快速上手","slug":"data-engineer/apache-flume","date":"2016-04-04T09:08:54.000Z","updated":"2019-12-02T01:49:42.383Z","comments":true,"path":"2016/04/04/data-engineer/apache-flume/","link":"","permalink":"https://k2r2bai.com/2016/04/04/data-engineer/apache-flume/","excerpt":"Apache Flume 是一個分散式日誌收集系統，是由 Cloudera 公司開發的一款高效能、高可靠性和高恢復性的系統。它能從不同來源的大量日誌資料進行高效收集、聚合、移動，最後儲存到一個資料中心儲存系統當中。架構經過重構後，從原來的 Flume OG 到現在的 Flume NG。Flume NG 更像一個輕量化的小套件，簡單使用且容易適應不同方式收集日誌，且支援 Failover 和 Load Balancing","text":"Apache Flume 是一個分散式日誌收集系統，是由 Cloudera 公司開發的一款高效能、高可靠性和高恢復性的系統。它能從不同來源的大量日誌資料進行高效收集、聚合、移動，最後儲存到一個資料中心儲存系統當中。架構經過重構後，從原來的 Flume OG 到現在的 Flume NG。Flume NG 更像一個輕量化的小套件，簡單使用且容易適應不同方式收集日誌，且支援 Failover 和 Load Balancing 架構角色說明Flume 架構中主要有以下幾個核心: Event：一個資料單元，會附帶一個可選的訊息來源。ex:日誌紀錄、avro。 Client：操作位在原點的 Event 且將它傳送到 Flume Agent，主要是產生資料，運行在一個獨立程式。 Agent：一個獨立的 Flume 程式，包含 Source、Channel、Sink。 Source：用來消費從 Client 端收集資料到此的 Event，然後傳送到 Channel。 Channel：轉換 Event 的一個臨時儲存空間，保有從 Source 傳送過來的 Event。 Sink:從 Channel 中讀取並且移除 Event，將 Event 傳遞到 Flow Pipeline 的下一個 Agent（如果存在的話）。 安裝 Apache Flume本節將說明如何部署 Apache Flume，其中包含單機與多機部署方式。 單機首先節點需先安裝 Java，這邊採用 Oracle 的 Java8 來進行安裝： $ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ sudo apt-get install -y oracle-java8-installer 完成後，在主機上安裝下載 Flume 套件，使用wget下載： $ wget \"ftp://ftp.twaren.net/Unix/Web/apache/flume/1.6.0/apache-flume-1.6.0-src.tar.gz\"$ wget \"ftp://ftp.twaren.net/Unix/Web/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz\"$ tar zxvf apache-flume-1.6.0-src.tar.gz$ tar zxvf apache-flume-1.6.0-bin.tar.gz 下載完後，將 src 覆蓋到 bin 底下，並解壓縮到/opt底下: $ sudo cp -ri apache-flume-1.6.0-src/* apache-flume-1.6.0-bin$ sudo mv /opt/apache-flume-1.5.0-bin /opt/flume 之後到/opt/flume/conf底下建立 example 配置檔: $ sudo vim example.conf 設定以下內容: # example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 之後啟動 Flume: $ bin/flume-ng agent -c conf -f example.conf -n a1 -Dflume.root.logger=INFO,console -c/--conf設定檔目錄，-f/--conf-file設定檔案路徑，-n/--name指定 agent 的名稱 驗證 Flume 開啟是否已開啟 $ jps6760 Jps6623 Application 最後開 shell 終端窗口，telnet 到配置監聽 port: $ telnet localhost 44444# 輸入HI!OK# 輸出2016-02-24 11:40:30,389 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D HI!. &#125; 多節點部署本節說明多機部署方式，流程為 Agent1 和 Agent2 主要是兩個來源蒐集端，本身會監聽且接收 Flume 本地端的訊息，然後將資料整合到 Collector 做資料日誌整理 部署節點角色規則如下: IP Address Role 192.168.100.94 Agent1 192.168.100.96 Agent2 192.168.100.97 Collector 一開始安裝配置與單機相同，從第一步驟到下載完後，將 src 覆蓋到 bin 底下，並解壓縮到/opt底下 然後到各自的/opt/flume/conf底下建立配置檔 Agent1和Agent2配置內容如下: # flume-client.properties: Agent1 Flume configuration#agent1 nameagent1.sources = r1agent1.sinks = k1agent1.channels = c1#set gruopagent1.sinkgroups = g1#set channelagent1.channels.c1.type = memoryagent1.channels.c1.capacity = 1000agent1.channels.c1.transactionCapacity = 100#set sourceagent1.sources.r1.channels = c1agent1.sources.r1.type = netcatagent1.sources.r1.bind = localhostagent1.sources.r1.port = 52020agent1.sources.r1.interceptors = i1agent1.sources.r1.interceptors.i1.type = staticagent1.sources.r1.interceptors.i1.key = Typeagent1.sources.r1.interceptors.i1.value = LOGIN# set sinkagent1.sinks.k1.channel = c1agent1.sinks.k1.type = avroagent1.sinks.k1.hostname = 192.168.100.97agent1.sinks.k1.port = 44444#set sink groupagent1.sinkgroups.g1.sinks = k1#set failoveragent1.sinkgroups.g1.processor.type = failoveragent1.sinkgroups.g1.processor.priority.k1 = 10agent1.sinkgroups.g1.processor.maxpenalty = 10000 Collector 配置內容如下: # flume-server.properties: Agent1 Flume configuration#set Agent namea1.sources = r1a1.sinks = k1a1.channels = c1#set channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100#set sourcea1.sources.r1.type = avroa1.sources.r1.bind = 192.168.100.97a1.sources.r1.port = 44444a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = Collectora1.sources.r1.interceptors.i1.value = NNAa1.sources.r1.channels = c1# set sinka1.sinks.k1.type=loggera1.sinks.k1.channel=c1 最後分別啓動Agent和Collector的 Flume： Agent: $ bin/flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console Collector: $ bin/flume-ng agent -n a1 -c conf -f flume-server.properties -Dflume.root.logger=DEBUG,console","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Logging","slug":"Logging","permalink":"https://k2r2bai.com/tags/Logging/"}]},{"title":"DRBD 進行跨節點的區塊儲存備份","slug":"linux/ubuntu/drbd","date":"2016-04-01T08:23:01.000Z","updated":"2019-12-02T01:49:42.398Z","comments":true,"path":"2016/04/01/linux/ubuntu/drbd/","link":"","permalink":"https://k2r2bai.com/2016/04/01/linux/ubuntu/drbd/","excerpt":"DRBD（Distributed Replicated BlockDevice）是一個分散式區塊裝置備份系統，DRBD 是由 Kernel 模組與相關腳本組成，被用來建置高可靠的叢集服務。實現方式是透過網路來 mirror 整個區塊裝置，一般可作為是網路 RAID 的一類。DRBD 允許使用者在遠端機器上建立一個 Local 區塊裝置的即時 mirror。","text":"DRBD（Distributed Replicated BlockDevice）是一個分散式區塊裝置備份系統，DRBD 是由 Kernel 模組與相關腳本組成，被用來建置高可靠的叢集服務。實現方式是透過網路來 mirror 整個區塊裝置，一般可作為是網路 RAID 的一類。DRBD 允許使用者在遠端機器上建立一個 Local 區塊裝置的即時 mirror。 安裝 DRBD本教學將使用以下主機數量與角色： IP Address Role Disk 172.16.1.184 master /dev/vdb 172.16.1.182 backup /dev/vdb 在 Ubuntu 14.04 LTS Server 可以直接透過apt-get來安裝 DRBD，指令如下： $ sudo apt-get install linux-image-extra-virtual$ sudo apt-get install -y drbd8-utils 完成後可以透過 lsmod 檢查： &gt; $ lsmod | grep drbd&gt;&gt; # 若沒有則使用以下指令&gt; $ sudo modprobe drbd&gt; P.S 若出現錯誤請重新啟動主機。 DRBD 設定首先在各兩個節點透過fdisk來建立分區： $ fdisk /dev/vdbCommand (m for help): nPartition type: p primary (0 primary, 0 extended, 4 free) e extendedSelect (default p): pPartition number (1-4, default 1): 1First sector (2048-20971519, default 2048): 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-20971519, default 20971519):Using default value 20971519Command (m for help): w 之後建立/etc/drbd.d/ha.res設定檔，並加入以下內容： resource ha &#123; on drbd-master &#123; device /dev/drbd0; disk /dev/vdb1; address 172.16.1.184:1166; meta-disk internal; &#125; on drbd-backup &#123; device /dev/drbd0; disk /dev/vdb1; address 172.16.1.182:1166; meta-disk internal; &#125;&#125; 上面都設定完成後，到master接著透過drbdadm指令建立： $ drbdadm create-md haWriting meta data...md_offset 10736365568al_offset 10736332800bm_offset 10736005120Found some data ==&gt; This might destroy existing data! &lt;==Do you want to proceed?[need to type 'yes' to confirm] yesinitializing activity logNOT initializing bitmapNew drbd meta data block successfully created. 透過指令啟用： $ drbdadm up ha$ drbd-overview0:ha/0 WFConnection Secondary/Unknown Inconsistent/DUnknown C r----s 設定某一節點為主節點： $ drbdadm -- --force primary ha$ drbd-overview0:ha/0 WFConnection Primary/Unknown UpToDate/DUnknown C r----s 檢查是否有正確啟動： $ cd /dev/drbd$ lsby-disk by-res$ ls -al by-disk/total 0drwxr-xr-x 2 root root 60 Mar 24 16:46 .drwxr-xr-x 4 root root 80 Mar 24 16:46 ..lrwxrwxrwx 1 root root 11 Mar 24 16:49 vdb1 -&gt; ../../drbd0$ ls -al by-res/ha/lrwxrwxrwx 1 root root 11 Mar 24 16:49 by-res/ha -&gt; ../../drbd0 若沒問題後，即可 mount 使用： $ mount /dev/drbd0 /mnt/ 若出現mount: you must specify the filesystem type的話，記得格式化： &gt; $ mkfs.ext4 /dev/drbd0&gt; 這時候再透過指令查詢，可以看到已成功同步： $ drbd-overview0:ha/0 WFConnection Primary/Unknown UpToDate/DUnknown C r----s /mnt ext4 9.8G 23M 9.2G 1% 接著到backup節點，執行類似上面做法： $ drbdadm create-md ha$ drbdadm up ha$ drbd-overview 0:ha/0 SyncTarget Secondary/Primary Inconsistent/UpToDate C r----- [========&gt;...........] sync'ed: 47.1% (5420/10236)Mfinish: 0:02:10 speed: 42,600 (45,252) want: 0 K/se","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"}]},{"title":"OpenStack 貢獻基本流程","slug":"openstack/how-to-contribute","date":"2016-04-01T08:23:01.000Z","updated":"2019-12-02T01:49:42.399Z","comments":true,"path":"2016/04/01/openstack/how-to-contribute/","link":"","permalink":"https://k2r2bai.com/2016/04/01/openstack/how-to-contribute/","excerpt":"本文章主要記錄自己在貢獻 OpenStack 時，所需要了解的流程。一但完成這些過程後，就可以在 OpenStack 社群上尋找自己喜歡的專案進行貢獻。","text":"本文章主要記錄自己在貢獻 OpenStack 時，所需要了解的流程。一但完成這些過程後，就可以在 OpenStack 社群上尋找自己喜歡的專案進行貢獻。 申請 OpenStack 帳號流程首先註冊 Launchpad.net 帳號： 到 Launchpad 註冊一個帳號，使用 inwinStack 信箱 完成申請後，進入 inwinSTACK Org 點選加入請求 確認自己出現在 inwinSTACK Menber List 之後註冊 OpenStack Foundation 的 Foundation Member，到 Register 申請 Foundation Member。並完成以下步驟： 填寫使用者資訊。 在 Affiliations 部分，點選 Add New Affiliations的組織輸入inwinSTACK，選擇開始時間，勾選 Is Current? 最後填寫住址與密碼資訊，住址翻譯網站 : http://goo.gl/qez9tt 然後使用 Launchpad.net 登入 OpenStack的 Gerrit 平台，到 Gerrit 點選 sign in，登入 Launchpad 帳號。當第一次登入成功後，會需要你設定唯一的 username（注意設定後就不能更改）。並完成以下步驟： 簽署 ICLA， 到 New Agreement 選擇ICLA（OpenStack Individual Contributor License Agreement） 上傳 SSH 公有金鑰， 到 Git Review SSH Keys 上傳 Key。 用ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; 指令產生金鑰，複製 ~/.ssh/id_rsa.pub到 review.openstack.org 上。產生 key 參考 generating-ssh-keys 完成後，設定 Git 資訊： git config --global user.name \"Firstname Lastname\"git config --global user.email \"your_email@youremail.com\"git config --global gitreview.username \"yourgerritusername\" 之後安裝 git-review，參考 Git Review install 進行安裝。 貢獻程式碼（已 openstack-manuals 為例）一個基本的貢獻流程如下圖所示： 首先透過 git clone 來下載程式專案，並設定 review： git clone https://github.com/openstack/openstack-manualscd openstack-manualsgit review -s 成功的話，會在目錄底下產生檔案.gitreview。若 auth 有問題請檢查 ssh key 是否正確。 並透過 git 來切換到最新版本： git checkout mastergit pull 新建一個 branch，在單獨的一行中撰寫 summary（小於50個字），然後第二段進行詳細的描述。如果是實現 bp 或修改 bug，需要註明： blueprint BP-NAME bug BUG-NUMBER 一個簡單範例： Adds some summary less than 50 characters ...Long multiline description of the change... Implements: blueprint authentication Fixes: bug # 123456 詳細的程式碼提交資訊，參考 GitCommitMessages。 修改完程式碼後，記得跑過UT的測試。然後提交程式碼，並申請 review： git commit -agit review 提交 review 之後，會出現在 Git review，可以查看狀態和資訊，並自動執行 CI，然後程式碼會由 review 人員進行程式碼的 review。 如果 jenkins 回報了 failure，可以查看 Logs 除錯。如果確認不是自己的 patch 導致，可以在 comment 上留言 recheck no bug，重新再跑 Test。 如果 review 過程中，發現程式碼需要修改，再次提交時直接使用已存在的 Change-Id： git commit -a --amendgit review","categories":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/categories/OpenStack/"}],"tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"},{"name":"Git","slug":"Git","permalink":"https://k2r2bai.com/tags/Git/"},{"name":"Python","slug":"Python","permalink":"https://k2r2bai.com/tags/Python/"}]},{"title":"透過官方 Ansible 部署 Kubernetes","slug":"kubernetes/deploy/official-ansible","date":"2016-02-24T09:08:54.000Z","updated":"2019-12-02T01:49:42.394Z","comments":true,"path":"2016/02/24/kubernetes/deploy/official-ansible/","link":"","permalink":"https://k2r2bai.com/2016/02/24/kubernetes/deploy/official-ansible/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方 Ansible Playbook 來部署 Kubernetes 到 CentOS 7 系統上，其中 Kubernetes 將額外部署 Dashboard 與 DNS 等 Add-ons。其他更多平台的部署可以參考 Creating a Kubernetes Cluster。 本次安裝版本為： Kubernetes v1.5.2 Etcd v3.1.0 Flannel v0.5.5 Docker v1.12.6","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方 Ansible Playbook 來部署 Kubernetes 到 CentOS 7 系統上，其中 Kubernetes 將額外部署 Dashboard 與 DNS 等 Add-ons。其他更多平台的部署可以參考 Creating a Kubernetes Cluster。 本次安裝版本為： Kubernetes v1.5.2 Etcd v3.1.0 Flannel v0.5.5 Docker v1.12.6 節點資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統採用CentOS 7.x： IP Address Role CPU Memory 172.16.35.12 master1 2 4G 172.16.35.10 node1 2 4G 172.16.35.11 node2 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 預先準備資訊首先安裝前要確認以下幾項都已將準備完成： 所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。 所有主機擁有 Sudoer 權限。 所有節點需要設定/etc/host解析到所有主機。 master1或部署節點需要安裝 Ansible 與相關套件： $ sudo yum install -y epel-release$ sudo yum install -y ansible python-netaddr git 部署 Kubernetes首先透過 Git 工具來取得 Kubernetes 官方的 Ansible Playbook 專案，並進入到目錄： $ git clone \"https://github.com/kubernetes/contrib.git\"$ cd contrib/ansible 編輯inventory/hosts檔案(inventory)，並加入以下內容： [masters]master1[etcd:children]masters[nodes]node[1:2] 然後利用 Ansible ping module 來檢查節點是否可以溝通： $ ansible -i inventory/hosts all -m pingmaster1 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;node2 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;node1 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125; 編輯inventory/group_vars/all.yml檔案，並修改以下內容： source_type: packageManagercluster_name: cluster.kairennetworking: flannelcluster_logging: truecluster_monitoring: truekube_dash: truedns_setup: truedns_replicas: 1 其他參數可自行選擇是否啟用。 (Option)編輯roles/flannel/defaults/main.yaml檔案，修改以下內容： flannel_options: --iface=enp0s8 這邊主要解決 Vagrant 預設抓 NAT 網卡問題。 完成後進入到scripts目錄，並執行以下指令進行部署： $ INVENTORY=../inventory/hosts ./deploy-cluster.sh...PLAY RECAP *********************************************************************master1 : ok=229 changed=93 unreachable=0 failed=0node1 : ok=126 changed=58 unreachable=0 failed=0node2 : ok=122 changed=58 unreachable=0 failed=0 經過一段時候就會完成，若沒有發生任何錯誤的話，就可以令用 kubectl 查看節點資訊： $ kubectl get nodesNAME STATUS AGEnode1 Ready 3mnode2 Ready 3m 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get svc --all-namespacesNAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes 10.254.0.1 &lt;none&gt; 443/TCP 3hkube-system elasticsearch-logging 10.254.164.5 &lt;none&gt; 9200/TCP 3hkube-system heapster 10.254.213.162 &lt;none&gt; 80/TCP 3hkube-system kibana-logging 10.254.176.124 &lt;none&gt; 5601/TCP 3hkube-system kube-dns 10.254.0.10 &lt;none&gt; 53/UDP,53/TCP 3hkube-system kubedash 10.254.68.80 80/TCP 3hkube-system kubernetes-dashboard 10.254.84.138 &lt;none&gt; 80/TCP 3hkube-system monitoring-grafana 10.254.193.233 &lt;none&gt; 80/TCP 3hkube-system monitoring-influxdb 10.254.135.115 &lt;none&gt; 8083/TCP,8086/TCP 3h 完成後，透過瀏覽器進入 Dashboard。 Targeted runsAnsible 提供 Tag 來指定執行或者忽略，這邊腳本也提供了該功能，如以下只部署 Etcd： $ ./deploy-cluster.sh --tags=etcd","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"}]},{"title":"Vagrant CoreOS 部署 Kubernetes 測試叢集","slug":"kubernetes/deploy/vagrant-multi-install","date":"2016-02-23T09:08:54.000Z","updated":"2019-12-02T01:49:42.394Z","comments":true,"path":"2016/02/23/kubernetes/deploy/vagrant-multi-install/","link":"","permalink":"https://k2r2bai.com/2016/02/23/kubernetes/deploy/vagrant-multi-install/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本節將透過 Vagrant 與 CoreOS 來部署單機多節點的 Kubernetes 虛擬叢集，並使用 Kubernetest CLI 工具與 API 進行溝通。 本次安裝版本為： CoreOS alpha. Kubernetes v1.5.4.","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本節將透過 Vagrant 與 CoreOS 來部署單機多節點的 Kubernetes 虛擬叢集，並使用 Kubernetest CLI 工具與 API 進行溝通。 本次安裝版本為： CoreOS alpha. Kubernetes v1.5.4. 事前準備首先必須在主機上安裝Vagrant工具，點選該 Vagrant downloads 頁面抓取當前系統的版本，並完成安裝。 接著在主機上安裝kubectl，該程式是主要與 Kubernetes API 進行溝通的工具，透過 Curl 工具來下載。如果是 Linux 作業系統，請下載以下： $ curl -O \"https://storage.googleapis.com/kubernetes-release/release/v1.5.4/bin/linux/amd64/kubectl\"$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 如果是 OS X，請取代 URL 為以下： $ curl -O \"https://storage.googleapis.com/kubernetes-release/release/v1.5.4/bin/darwin/amd64/kubectl\"$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 安裝 Kubernetes首先透過 Git 工具來下載 CoreOS 的 Kubernetes 專案，裡面包含了描述 Vagrant 要建立的檔案： $ git clone https://github.com/coreos/coreos-kubernetes.git$ cd coreos-kubernetes/multi-node/vagrant 接著複製config.rb.sample並改成config.rb檔案： $ cp config.rb.sample config.rb 編輯config.rb設定檔，並修改成以下內容： $update_channel=\"alpha\"$controller_count=1$controller_vm_memory=1024$worker_count=2$worker_vm_memory=1024$etcd_count=1$etcd_vm_memory=512 (Option)若 CNI 想使用 Calico 網路與安裝不同版本 Kubernetes 的話，需要修改../generic/controller-install.sh與./generic/worker-install.sh檔案以下內容： export K8S_VER=v1.5.4_coreos.0export USE_CALICO=true 設定好後，即可透過以下指令來建立 SSL CA Key 與更新 Box 資訊： $ sudo ln -sf /usr/local/bin/openssl /opt/vagrant/embedded/bin/openssl$ vagrant box update 確認完成後，執行以下指令開始建立叢集： $ vagrant up P.S. 這邊建置起來裡面虛擬機還要下載一些東西，要等一下子才會真正完成。 設定 Kubernetes Config當完成部署後，需要配置 kubectl 連接 API，這邊可以選擇以下兩種的其中一種進行： 使用一個 Custom Kubernetes Config$ export KUBECONFIG=\"$&#123;KUBECONFIG&#125;:$(pwd)/kubeconfig\"$ kubectl config use-context vagrant-multi 更新與使用本地的 Config$ kubectl config set-cluster vagrant-multi-cluster --server=\"https://172.17.4.101:443\" --certificate-authority=$&#123;PWD&#125;/ssl/ca.pem$ kubectl config set-credentials vagrant-multi-admin --certificate-authority=$&#123;PWD&#125;/ssl/ca.pem --client-key=$&#123;PWD&#125;/ssl/admin-key.pem --client-certificate=$&#123;PWD&#125;/ssl/admin.pem$ kubectl config set-context vagrant-multi --cluster=vagrant-multi-cluster --user=vagrant-multi-admin$ kubectl config use-context vagrant-multi Kubernetes 系統驗證完成設定後，即可使用 kubectl 來查看節點資訊： $ kubectl get nodesNAME STATUS AGE172.17.4.101 Ready,SchedulingDisabled 3m172.17.4.201 Ready 3m172.17.4.202 Ready 3m 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system heapster-v1.2.0-4088228293-4vv12 2/2 Running 0 28mkube-system kube-apiserver-172.17.4.101 1/1 Running 0 29mkube-system kube-controller-manager-172.17.4.101 1/1 Running 0 29mkube-system kube-dns-782804071-w6w12 4/4 Running 0 29mkube-system kube-dns-autoscaler-2715466192-q1k18 1/1 Running 0 29mkube-system kube-proxy-172.17.4.101 1/1 Running 0 28mkube-system kube-proxy-172.17.4.201 1/1 Running 0 29mkube-system kube-proxy-172.17.4.202 1/1 Running 0 29mkube-system kube-scheduler-172.17.4.101 1/1 Running 0 28mkube-system kubernetes-dashboard-3543765157-vk0mt 1/1 Running 0 29m","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"CoreOS","slug":"CoreOS","permalink":"https://k2r2bai.com/tags/CoreOS/"},{"name":"Vagrant","slug":"Vagrant","permalink":"https://k2r2bai.com/tags/Vagrant/"}]},{"title":"Ansible Playbooks","slug":"devops/cm/ansible-playbook","date":"2016-02-18T04:23:01.000Z","updated":"2019-12-02T01:49:42.385Z","comments":true,"path":"2016/02/18/devops/cm/ansible-playbook/","link":"","permalink":"https://k2r2bai.com/2016/02/18/devops/cm/ansible-playbook/","excerpt":"Playbooks 是 Ansible 的設定、部署與編配語言等。可以被用來描述一個被遠端的主機要執行的指令方案，或是一組 IT 行程執行的指令集合。 在基礎層面上，Playbooks 可以被用來管理部署到遠端主機的組態檔案，在更高階層上 Playbooks 可以循序對多層式架構上的伺服器執行線上的 Polling 更新內部的操作，並將操作委派給其他主機，包含過程中發生的監視器服務、負載平衡伺服器等。","text":"Playbooks 是 Ansible 的設定、部署與編配語言等。可以被用來描述一個被遠端的主機要執行的指令方案，或是一組 IT 行程執行的指令集合。 在基礎層面上，Playbooks 可以被用來管理部署到遠端主機的組態檔案，在更高階層上 Playbooks 可以循序對多層式架構上的伺服器執行線上的 Polling 更新內部的操作，並將操作委派給其他主機，包含過程中發生的監視器服務、負載平衡伺服器等。 Playbooks 被設計成易懂與基於 Text Language 的二次開發，有許多方式可以組合 Playbooks 與其附屬的檔案。建議在閱讀 Playbooks 時，同步閱讀 Example Playbooks。 Playbooks 與 ad-hoc 相比是一種完全不同的 Ansible 應用方式，該方式也是 Ansible 強大之處。簡單來說 Playbooks 是一種組態管理系統與多機器部署系統基礎，與現有系統不同之處在於非常適合複雜的部署。若想參考範例，可以參閱 ansible-examples repository。 Playbook Language ExamplePlaybook 採用 YAML 語法來表示。playbook 由一或多個plays組成的內容為元素的列表。在play中一組機器會被映射成定義好的角色，在 Ansible 中play內容也被稱為tasks。 以下是一個簡單的範例： ---- name: Configure cluster with apache hosts: cluster sudo: yes remote_user: ubuntu tasks: - name: install apache2 apt: name=apache2 update_cache=yes state=latest - name: enabled mod_rewrite apache2_module: name=rewrite state=present notify: - restart apache2 - name: apache2 listen on port 8081 lineinfile: dest=/etc/apache2/ports.conf regexp=&quot;^Listen 80&quot; line=&quot;Listen 8081&quot; state=present notify: - restart apache2 - name: apache2 virtualhost on port 8081 lineinfile: dest=/etc/apache2/sites-available/000-default.conf regexp=&quot;^&lt;VirtualHost \\*:80&gt;&quot; line=&quot;&lt;VirtualHost *:8081&gt;&quot; state=present notify: - restart apache2 handlers: - name: restart apache2 service: name=apache2 state=restarted 上面範例，會依序地執行 tasks，然而當每個 task 執行時有發生改變的話，就會接著執行notify中的 handlers。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"Ansible Dynamic Inventory","slug":"devops/cm/ansible-dynamic-inventory","date":"2016-02-17T06:23:01.000Z","updated":"2019-12-02T01:49:42.384Z","comments":true,"path":"2016/02/17/devops/cm/ansible-dynamic-inventory/","link":"","permalink":"https://k2r2bai.com/2016/02/17/devops/cm/ansible-dynamic-inventory/","excerpt":"在預設情況下，我們所使用的都是一個靜態的 Inventory 檔案，編輯主機、群組以及變數時都需要固定手動編輯完成。 Ansible 提供了 Dynamic Inventory 檔案，這個檔案是透過呼叫外部腳本或程式來產生指定的格式的 JSON 字串。這樣做的好處就是可以透過這個外部腳本與程式來管理系統（如 API）抓取最新資源訊息。","text":"在預設情況下，我們所使用的都是一個靜態的 Inventory 檔案，編輯主機、群組以及變數時都需要固定手動編輯完成。 Ansible 提供了 Dynamic Inventory 檔案，這個檔案是透過呼叫外部腳本或程式來產生指定的格式的 JSON 字串。這樣做的好處就是可以透過這個外部腳本與程式來管理系統（如 API）抓取最新資源訊息。 Ansible 使用者通常會互動於大多數的物理硬體，因此會有許多人可能也是Cobbler的使用者。 Cobbler 是一個透過網路部署 Linux 的服務，而且經過調整更能夠進行 Windows 部署。該工具是使用 Python 開發，因此輕巧便利，使用簡單指令就可以完成 PXE 網路安裝環境。 比如說以下這個範例就是透過腳本程式產生的： &#123; \"production\": [\"delaware.example.com\", \"georgia.example.com\", \"maryland.example.com\", \"newhampshire.example.com\", \"newjersey.example.com\", \"newyork.example.com\", \"northcarolina.example.com\", \"pennsylvania.example.com\", \"rhodeisland.example.com\", \"virginia.example.com\" ], \"staging\": [\"ontario.example.com\", \"quebec.example.com\"], \"vagrant\": [\"vagrant1\", \"vagrant2\", \"vagrant3\"], \"lb\": [\"delaware.example.com\"], \"web\": [\"georgia.example.com\", \"newhampshire.example.com\", \"newjersey.example.com\", \"ontario.example.com\", \"vagrant1\" ] \"task\": [\"newyork.example.com\", \"northcarolina.example.com\", \"ontario.example.com\", \"vagrant2\" ], \"redis\": [\"pennsylvania.example.com\", \"quebec.example.com\", \"vagrant3\"], \"db\": [\"rhodeisland.example.com\", \"virginia.example.com\", \"vagrant3\"]&#125; 使用方式如下： 加上執行(x)的權限給 script 將 script 與 inventory file 放在同一目錄 如此一來 ansible 就會自動讀取 inventory file 取得靜態的 inventory 資訊，並執行 script 取得動態的 inventory 資訊，將兩者 merge 後並使用。 目前官方已有提供幾個 Dynamic Inventory 的範例教學，如以下： Cobbler External Inventory Script AWS EC2 External Inventory Script OpenStack External Inventory Script","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"Ansible Ad-Hoc 指令與 Modules","slug":"devops/cm/ansible-adhoc","date":"2016-02-17T04:23:01.000Z","updated":"2019-12-02T01:49:42.384Z","comments":true,"path":"2016/02/17/devops/cm/ansible-adhoc/","link":"","permalink":"https://k2r2bai.com/2016/02/17/devops/cm/ansible-adhoc/","excerpt":"ad-hoc command（特設指令）簡單說就是直接執行指令，這些指令不需要要被保存在日後使用。在進行 Ansible 的 Playbook 語言之前，了解 ad-hoc 指令也可以幫助我們做一些快速的事情，不一定要寫出一個完整的 Playbooks 指令。 模組（也被稱為Task plugins或是Library plugins）是 Ansible 中實際執行的功能，它們會在每個 Playbook 任務中被執行，也可以透過 ansible 直接呼叫使用。目前 Ansible 已經擁有許多模組，可參閱 Module Index。","text":"ad-hoc command（特設指令）簡單說就是直接執行指令，這些指令不需要要被保存在日後使用。在進行 Ansible 的 Playbook 語言之前，了解 ad-hoc 指令也可以幫助我們做一些快速的事情，不一定要寫出一個完整的 Playbooks 指令。 模組（也被稱為Task plugins或是Library plugins）是 Ansible 中實際執行的功能，它們會在每個 Playbook 任務中被執行，也可以透過 ansible 直接呼叫使用。目前 Ansible 已經擁有許多模組，可參閱 Module Index。 首先我們先編輯/etc/ansible/hosts，加入以下內容： [cluster]ansible-slave-1 ansible_host=172.16.1.206ansible-slave-2 ansible_host=172.16.1.207ansible-slave-3 ansible_host=172.16.1.208 Parallelism and Shell Commands接下來我們將透過範例來說明 Ansible 的平行性與 Shell 指令，一開始我們需要將 ssh-agent 加入私有金鑰管理： $ ssh-agent bash$ ssh-add ~/.ssh/id_rsa 如果不想要透過 ssh-agent 的金鑰登入，可以在 ansible 指令使用--ask-pass（-k）參數，但是建議使用 ssh-agent。 剛剛我們在 Inventroy 檔案建立了一個群組（Cluster），裡面擁有三台主機，接下來我們透過執行一個簡單的指令與參數來實現並行執行： $ ansible cluster -a \"sleep 2\" -f 1 上面的指令會隨機執行一台主機，完成後接下執行下一台，然而-f參數可以改變一次執行的 bash，好比改成： $ ansible cluster -a \"sleep 2\" -f 3 會發現 bash 是平行執行的。 我們除了使用預設的 user 登入以外，也可以指定要登入的使用者： $ ansible cluster -a \"echo $USER\" -u ubuntu 如果想透過特權（sudo）執行指令，可以透過以下方式： $ ansible cluster -a \"apt-get update\" -u ubuntu --become 若該使用者沒有設定 sudo 不需要密碼的話，可以加入--ask-sudo-pass（-k）來驗證密碼。也可以使用--become-method來改變權限使用方法（預設為 sudo）。 也可以透過--become-user來切換使用者： $ ansible cluster -a \"echo $USER\" -u ubuntu --become-user root 若有密碼，可以使用以上是基本的幾個指令，但當使用 ansible ad-hoc 指令時，會發現無法使用`shell 變數`以及`pipeline 等相關`，這是因為預設的 ansible ad-hoc 指令不支援，故要改用 shell 模組來執行：```sh$ ansible cluster -m shell -a &apos;echo $(hostname) | grep -o &quot;[0-9]&quot;&apos; 以上指令的-m表示要使用的模組。但要注意！使用 ansible 指令時要留意&quot;cmd&quot;與&#39;comd&#39;的差別，比如使用&quot;cmd&quot;會是抓取當前系統的資訊。 File TransferAnsible 能夠以平行的方式同時scp大量的檔案到多台主機上，如以下範例： $ ansible cluster -m copy -a \"src=/etc/hosts dest=~/hosts\" 也可以使用file模組做到修改檔案的權限與屬性（這邊可以將copy替換成file）： $ ansible cluster -m file -a \"dest=~/hosts mode=600\"$ ansible cluster -m file -a \"dest=~/hosts mode=600 owner=ubuntu group=ubuntu\" file模組也能夠建立目錄： $ ansible cluster -m file -a \"dest=~/data mode=755 owner=ubuntu group=ubuntu state=directory\" 若要刪除可以使用以下方式： $ ansible cluster -m file -a \"dest=~/data state=absent\" Managing Packages目前 Ansible 已經支援了yum與apt的模組，以下是一個apt 確認指定軟體名稱是否已安裝，並且不升級： $ ansible cluster -m apt -a \"name=ntp state=present\" 也可以在name=ntp後面加版本號，如name=ntp-{version}。 若要確認是否為最新版本，可以使用以下指令： $ ansible cluster -m apt -a \"name=ntp state=latest\" 若要確認一個軟體套件沒有安裝，可以使用以下指令： $ ansible cluster -m apt -a \"name=ntp state=absent\" --become 更多的指令資訊可以查看 About Modules。 Users and Groups若想要建立系統使用者與群組，可以使用user模組，如以下範例： $ ansible all -m user -a \"name=food password=food\" --become 刪除則如以下： $ ansible all -m user -a \"name=food state=absent\" -b --become與-b是等效的。 Deploying From Source ControlAnsible 不只可以透過apt與ad-hoc 指令來安裝與部署應用程式，也能用git模組來安裝： $ ansible cluster -m git -a \"repo=https://github.com/imac-cloud/Spark-tutorial.git dest=~/spark-tutorial\" -f 3 Managing ServicesAnsible 也可以透過service模組來確認指定主機是否已啟動服務： $ ansible cluster -m service -a \"name=ssh state=started\" 也可以改變state來執行對應動作，如state=restarted就會重新啟動服務。 Time Limited Background Operations有些操作需要長時間執行於後台，在指令開始執行後，可以持續檢查執行狀態，但是若不想要獲取該資訊可以使用以下指令： $ ansible ansible-slave-1 -B 3600 -P 0 -a \"/usr/bin/long_running_operation --do-stuff\" 若要檢查執行狀態的話，可以使用async_status來傳入一個jid查看： $ ansible cluster -m async_status -a \"jid=488359678239.2844\" 獲取狀態指令如下： $ ansible ansible-slave-1 -B 1800 -P 60 -a \"/usr/bin/long_running_operation --do-stuff\" -B表示最常執行時間，-P表示每隔60秒回傳狀態。 Gathering Facts在 Playboooks 中有對 Facts 做一些描述，他表示的是一些系統已知的變數，若要查看所有 Facts，可以使用以下指令： $ ansible cluster[0] -m setup 接下來可以針對 Playbooks 與 Variables 進行研究。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"Ansible Inventory","slug":"devops/cm/ansible-inventory","date":"2016-02-17T04:23:01.000Z","updated":"2019-12-02T01:49:42.384Z","comments":true,"path":"2016/02/17/devops/cm/ansible-inventory/","link":"","permalink":"https://k2r2bai.com/2016/02/17/devops/cm/ansible-inventory/","excerpt":"Ansible 在同一時間能夠工作於多個系統，透過在 inventory file 所列舉的主機與群組來執行對應的指令，該檔案預設存於/etc/ansible/hosts。 IT 人員不只能夠使用預設的檔案，也能夠在同一時間使用多個檔案，甚至來抓取來至雲端的 inventory 檔案，這是一個是動態的 inventory ，這部分可以參考 Dynamic Inventory。","text":"Ansible 在同一時間能夠工作於多個系統，透過在 inventory file 所列舉的主機與群組來執行對應的指令，該檔案預設存於/etc/ansible/hosts。 IT 人員不只能夠使用預設的檔案，也能夠在同一時間使用多個檔案，甚至來抓取來至雲端的 inventory 檔案，這是一個是動態的 inventory ，這部分可以參考 Dynamic Inventory。 Hosts and GroupsInventory 是一個INI-like格式的檔案，如以下範例所示： mail.example.com[webservers]foo.example.combar.example.com[dbservers]one.example.comtwo.example.comthree.example.com 如果 SSH 不是標準 Port 的話，可以使用:來對應要使用的 Port。但在 SSH config 檔案所列出來的主機將不會與 paramiko 進行連線，但是會與 OpenSSH 進行連接使用。 badwolf.example.com:5309 雖然可以使用以上方式達到不同 Port 連接，但是還是建議使用預設 Port。 假設只有靜態 IP，但又希望透過一些別名（aliases）來表示主機，或透過不同 Port 連接的話，可以表示如以下： jumper ansible_port=5555 ansible_host=192.168.1.50 若要一次列出多個主機可以使用以下 Pattern： [webservers]www[01:50].example.com 在數字 Pattern，前導的 0 可以根據需求刪除或加入。不只可以定義數字型，還能定義英文字母範圍： [databases]db-[a:f].example.com 也可以為每台主機的設定基礎連線類型與使用者資訊： [targets]localhost ansible_connection=localother1.example.com ansible_connection=ssh ansible_user=mpdehaanother2.example.com ansible_connection=ssh ansible_user=mdehaan Host Variables如上述範例，我們可以很容易將變數分配給將在 Playbooks 使用的主機： [atlanta]host1 http_port=80 maxRequestsPerChild=808host2 http_port=303 maxRequestsPerChild=909 Group Variables變數也能夠被應用到整個群組裡： [atlanta]host1host2[atlanta:vars]ntp_server=ntp.atlanta.example.comproxy=proxy.atlanta.example.com Groups of Groups, and Group Variables另外，也可以用:children 來建立群組中的群組，並使用:vars來設定變數： [atlanta]host1host2[raleigh]host2host3[southeast:children]atlantaraleigh[southeast:vars]some_server=foo.southeast.example.comhalon_system_timeout=30self_destruct_countdown=60escape_pods=2[usa:children]southeastnortheastsouthwestnorthwest Splitting Out Host and Group Specific Data該部分說明想要儲存 list 與 hash table 資料，或者從 Inventory 檔案保持分離主機與群組的特定變數。在 Ansible 的第一優先作法實際上是不儲存變數於主 Inventort 檔案。 除了直接在 INI 檔案儲存變數外，主機與群組變數也可以儲存在個人相對的 Inventory 檔案。這些變數檔案格式為 YAML。有效的副檔名如.yml、.yaml，以及.json或沒有副檔名。 一般當 remote host 數量不多時，把變數定義在 inventory 中是 ok 的；但若 remote host 的數量越來越多時，將變數的宣告定義在外部的檔案中會是比較好的方式。 假設 Inventory 檔案路徑為： /etc/ansible/hosts 如果主機被命名為foosball以及在raleigh與webservers的群組，以下位置的 YAML 檔案變數將提供給主機使用： # can optionally end in '.yml', '.yaml', or '.json'/etc/ansible/group_vars/raleigh/etc/ansible/group_vars/webservers/etc/ansible/host_vars/foosball ansible 會自動尋找 playbook 所在的目錄中的host_vars目錄 以及group_vars目錄 中所包含的檔案，並使用定義在這兩個目錄中的變數資訊。 舉例來說，inventory / playbook / host_vars / group_vars 可以用類似以下的方式進行配置： inventory：/home/vagrant/ansible/playbooks/inventory playbook：/home/vagrant/ansible/playbooks/myplaybook host_vars：/home/vagrant/ansible/playbooks/host_vars/prod1.example.com.tw group_vars：/home/vagrant/ansible/playbooks/group_vars/production 變數定義的方式有兩種方式： db_primary_host: prod1.example.com.twdb_replica_host: prod2.example.com.twdb_name: widget_productiondb_user: widgetuserdb_password: lastpasswordredis_host: redis_stag.example.com.tw 也可以用 YAML 的方式定義： ---db: user: widgetuser password: lastpassword name: widget_production primary: host: prod1.example.com.tw port: 5432 replica: host: prod2.example.com.tw port: 5432redis: host: redis_stag.example.com.tw port: 6379 甚至可以在繼續細分，定義檔案../playbooks/group_vars/production/db： ---db: user: widgetuser password: lastpassword name: widget_production primary: host: prod1.example.com.tw port: 5432 replica: host: prod2.example.com.tw port: 5432 List of Behavioral Inventory Parameters正如上述提到，設定以下變數可以定義 Ansible 該如何控制以及遠端主機。如主機連線： ansible_connection Connection type to the host. Candidates are local, smart, ssh or paramiko. The default is smart. SSH connection： ansible_host The name of the host to connect to, if different from the alias you wish to give to it.ansible_port The ssh port number, if not 22ansible_user The default ssh user name to use.ansible_ssh_pass The ssh password to use (this is insecure, we strongly recommend using --ask-pass or SSH keys)ansible_ssh_private_key_file Private key file used by ssh. Useful if using multiple keys and you don&apos;t want to use SSH agent.ansible_ssh_common_args This setting is always appended to the default command line for sftp, scp, and ssh. Useful to configure a ``ProxyCommand`` for a certain host (or group).ansible_sftp_extra_args This setting is always appended to the default sftp command line.ansible_scp_extra_args This setting is always appended to the default scp command line.ansible_ssh_extra_args This setting is always appended to the default ssh command line.ansible_ssh_pipelining Determines whether or not to use SSH pipelining. This can override the ``pipelining`` setting in ``ansible.cfg``. 權限提升（可參閱Ansible Privilege Escalation）： ansible_become Equivalent to ansible_sudo or ansible_su, allows to force privilege escalationansible_become_method Allows to set privilege escalation methodansible_become_user Equivalent to ansible_sudo_user or ansible_su_user, allows to set the user you become through privilege escalationansible_become_pass Equivalent to ansible_sudo_pass or ansible_su_pass, allows you to set the privilege escalation password 遠端主機環境參數： ansible_shell_type The shell type of the target system. Commands are formatted using &apos;sh&apos;-style syntax by default. Setting this to &apos;csh&apos; or &apos;fish&apos; will cause commands executed on target systems to follow those shell&apos;s syntax instead.ansible_python_interpreter The target host python path. This is useful for systems with more than one Python or not located at &quot;/usr/bin/python&quot; such as \\*BSD, or where /usr/bin/python is not a 2.X series Python. We do not use the &quot;/usr/bin/env&quot; mechanism as that requires the remote user&apos;s path to be set right and also assumes the &quot;python&quot; executable is named python, where the executable might be named something like &quot;python26&quot;.ansible\\_\\*\\_interpreter Works for anything such as ruby or perl and works just like ansible_python_interpreter. This replaces shebang of modules which will run on that host. 一個主機檔案範例： some_host ansible_port=2222 ansible_user=manageraws_host ansible_ssh_private_key_file=/home/example/.ssh/aws.pemfreebsd_host ansible_python_interpreter=/usr/local/bin/pythonruby_module_host ansible_ruby_interpreter=/usr/bin/ruby.1.9.3","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"Ansible 介紹與使用","slug":"devops/cm/ansible-basic","date":"2016-02-16T04:23:01.000Z","updated":"2019-12-02T01:49:42.384Z","comments":true,"path":"2016/02/16/devops/cm/ansible-basic/","link":"","permalink":"https://k2r2bai.com/2016/02/16/devops/cm/ansible-basic/","excerpt":"Ansible 是最近越來越夯多 DevOps 自動化組態管理軟體，從 2013 年發起的專案，由於該架構為無 agent 程式的架構，以部署靈活與程式碼易讀而受到矚目。Ansible 除了有開源版本之外，還針對企業用戶推出 Ansible Tower 版本，已有許多知名企業採用，如 Apple、Twitter 等。 Ansible 架構圖如下所示，使用者透過 Ansible 編配操控公有與私有雲或 CMDB（組態管理資料庫）中的主機，其中 Ansible 編排是由Inventory(主機與群組規則)、API、Modules(模組)與Plugins(插件)組合而成。","text":"Ansible 是最近越來越夯多 DevOps 自動化組態管理軟體，從 2013 年發起的專案，由於該架構為無 agent 程式的架構，以部署靈活與程式碼易讀而受到矚目。Ansible 除了有開源版本之外，還針對企業用戶推出 Ansible Tower 版本，已有許多知名企業採用，如 Apple、Twitter 等。 Ansible 架構圖如下所示，使用者透過 Ansible 編配操控公有與私有雲或 CMDB（組態管理資料庫）中的主機，其中 Ansible 編排是由Inventory(主機與群組規則)、API、Modules(模組)與Plugins(插件)組合而成。 Ansible 與其他管理工具最大差異在於不需要任何 Agent，預設使用 SSH 來做遠端操控與配置，並採用 YAML 格式來描述配置資訊。 Ansible 提供了一個 Playbook 分享平台，可以讓管理與開發者上傳自己的功能與角色配置的 Playbook，該網址為 Ansible Galaxy。 優點： 開發社群活躍。 playbook 使用的 yaml 語言，很簡潔。 社群相關文件容易理解。。 沒有 Agent 端。 安裝與執行的速度快 配置簡單、功能強大、擴展性強 可透過 Python 擴展功能 提供用好的 Web 管理介面與 REST API 介面（AWX 平台） 缺點： Web UI 需要收費。 官方資料都比較淺顯。 Ansible 安裝與基本操作Ansible 有許多種安裝方式，如使用 Github 來透過 Source Code 安裝，也可以透過 python-pip 來安裝，甚至是使用作業系統的套件管理系統安裝，以下使用 Ubuntu APT 來進行安裝： $ sudo apt-get install software-properties-common$ sudo apt-add-repository ppa:ansible/ansible$ sudo apt-get update$ sudo apt-get install ansible 也可以使用 Python-pip 來進行安裝： $ sudo easy_install pip$ sudo pip install -U pip$ sudo pip install ansible 節點準備首先我們要在各節點先安裝 SSH Server ，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定特權模式不需要輸入密碼： $ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ubuntu$ sudo chmod 440 /etc/sudoers.d/ubuntu 這邊 User 為ubuntu，若使用者不一樣請更換。 建立 SSH Key，並複製 Key 使之不用密碼登入： $ ssh-keygen -t rsa$ ssh-copy-id localhost 新增各節點 Domain name 至/etc/hosts檔案： 172.16.1.205 ansible-master172.16.1.206 ansible-slave-1172.16.1.207 ansible-slave-2172.16.1.208 ansible-slave-3 並在 Master 節點複製所有 Slave 的 SSH Key： $ ssh-copy-id ubuntu@ansible-slave-1$ ssh-copy-id ubuntu@ansible-slave-2... 設定 Invetory FileAnsible 能夠在同一時間工作於多個基礎設施的系統中。透過作用於 Ansible 的 Inventory 檔案所列出的主機與群組，該檔案預設被存在/etc/ansible/hosts。 /etc/ansible/hosts 是一個 INI-like 的檔案格式，如以下內容： ansible-slave-1ansible-slave-2ansible-slave-3 也可以建立成 Groups，如以下內容： [openstack]ansible-slave-1ansible-slave-2ansible-slave-3 若要參考更多資訊，可看 Invetory File。 基本功能操作Ansible 基本操作如以下指令： $ ansible &lt;pattern_goes_here&gt; -m &lt;module_name&gt; -a &lt;arguments&gt; &lt;pattern_goes_here&gt;部分可以參考 Patterns。 比如我們可以用 Ping 模組來測試是否連線成功： $ ansible all -m pingansible-slave-2 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;ansible-slave-3 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;ansible-slave-1 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125; 其中all為所有 Invetory 的主機，-m為使用的模組。若使用指定的 Inventory 檔案可以使用-i。 也可以執行指定指令： $ ansible all -a \"/bin/echo hello\" -a 後面為要執行的指令。 若要指定登入的使用者，且執行特權模式，可以使用以下指令： $ ansible all -a \"apt-get update\" -u vagrant -b -u為登入使用者，-b 為切換成特權模式（root），早期版本為--sudo。 主機的 SSH Key 檢查在 Ansible 1.2.1 與之後的版本預設都需要做主機 SSH key 檢查。 如果一台主機重新安裝或者在 ‘known_hosts’ 有不同的 SSH Key 的話，將會導致錯誤發生，但不希望這樣的問題影響 Ansible 使用，可以在 /etc/ansible/ansible.cfg 或者~/.ansible.cfg檔案關閉檢查。 [defaults]host_key_checking = False 也可以代替為設定環境變數： $ export ANSIBLE_HOST_KEY_CHECKING=False 還要注意在 paramiko 模式主機金鑰檢查緩慢是合理的，因此建議切換使用 SSH。 Ansible 會在遠端系統上記錄有關模組參數的一些資訊存於 syslog，除非該執行任務有標示 ‘no_log: True’。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"透過 Ansible 部署 Ceph 叢集","slug":"ceph/deploy/ceph-ansible","date":"2016-02-15T09:08:54.000Z","updated":"2019-12-02T01:49:42.377Z","comments":true,"path":"2016/02/15/ceph/deploy/ceph-ansible/","link":"","permalink":"https://k2r2bai.com/2016/02/15/ceph/deploy/ceph-ansible/","excerpt":"本節將介紹如何透過 ceph-ansible 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor與三台 OSD。而 MDS 則是當使用到 CephFS 的時候才需要部署。","text":"本節將介紹如何透過 ceph-ansible 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor與三台 OSD。而 MDS 則是當使用到 CephFS 的時候才需要部署。 節點資訊本安裝將使用四台虛擬機器作為部署主機，虛擬機器採用 OpenStack，其規格為以下： Role RAM CPUs Disk IP Address mon 2 GB 1vCPU 20 GB 172.16.1.200 osd1 2 GB 1vCPU 20 GB 172.16.1.201 osd2 2 GB 1vCPU 20 GB 172.16.1.202 osd3 2 GB 1vCPU 20 GB 172.16.1.203 其中若是虛擬機，要額外建立 3 顆虛擬硬碟來作為 OSD 使用，如以下： Dev path Disk Size Description /dev/vdb 25 GB osd1 掛載 /dev/vdb 25 GB osd2 掛載 /dev/vdb 25 GB osd3 掛載 事前準備首先在mon節點設定 SSH 到其他節點不需要密碼，請依照以下執行： $ ssh-keygen -t rsa$ ssh-copy-id osd1... 若虛擬機的話，建立金鑰後可以直接上傳公有金鑰提供給其他節點。 接著在每一個節點設定 sudo 不需要密碼： $ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 一般虛擬機映像檔預設就有設定。 然後在每一台節點新增以下內容到/etc/hosts： 127.0.0.1 localhost10.21.20.99 ceph-deploy172.16.1.200 mon172.16.1.201 osd1172.16.1.202 osd2172.16.1.203 osd3 回到mon節點安裝部署將使用到的 ansible 工具： $ sudo apt-get install -y software-properties-common git cowsay$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible 在mon節點編輯/etc/ansible/hosts，加入以下內容： [mons]mon[osds]osd[1:3] (option)若要安裝 rgw 與 mds 的話，可再添加以下： [rgws]mon[mdss]mon 完成後透過以下指令檢查節點是否可以溝通： $ ansible all -m ping172.16.1.200 | success &gt;&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;... 部署 Ansible Ceph 叢集首先在mon節點透過 git 來下載 ceph-ansible 專案： $ git clone \"https://github.com/ceph/ceph-ansible.git\"$ cd ceph-ansible$ cp site.yml.sample site.yml$ cp group_vars/all.sample group_vars/all$ cp group_vars/mons.sample group_vars/mons$ cp group_vars/osds.sample group_vars/osds 若要部署 rgw 與 mds 的話，需再執行以下指令： $ cp group_vars/mdss.sample group_vars/mdss$ cp group_vars/rgws.sample group_vars/rgws 接著編輯group_vars/all檔案，修改以下內容： ceph_origin: 'upstream'ceph_stable: truemonitor_interface: eth0journal_size: 5000public_network: 172.16.1.0/24 其他版本可以參考官方的說明 ceph-ansible。 完成後再編輯group_vars/osds檔案，修改以下內容： journal_collocation: truedevices: - /dev/vdb 這邊使用 journal，也可以選擇其他使用。若有多顆 OSD則修改devices。 上述都確認無誤後，編輯site.yml檔案，並修改一下內容： ---- hosts: mons become: True roles: - ceph-mon- hosts: osds become: True roles: - ceph-osd (option)若要部署 rgw 與 mds 的話，需再加入以下內容： - hosts: mdss become: True roles: - ceph-mds- hosts: rgws become: True roles: - ceph-rgw 完成後就可以透過以下指令來進行部署： $ ansible-playbook site.yml","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Ansible","slug":"Ansible","permalink":"https://k2r2bai.com/tags/Ansible/"}]},{"title":"透過 Foreman 管理 Puppet","slug":"devops/cm/puppet-foreman","date":"2016-02-14T04:23:01.000Z","updated":"2019-12-02T01:49:42.385Z","comments":true,"path":"2016/02/14/devops/cm/puppet-foreman/","link":"","permalink":"https://k2r2bai.com/2016/02/14/devops/cm/puppet-foreman/","excerpt":"Foreman 是一個 Puppet 的生命周期管理系統，類似 puppet-dashboard，通過它可以很直觀的查看 Puppet 所有客戶端的同步狀態與 facter 參數。","text":"Foreman 是一個 Puppet 的生命周期管理系統，類似 puppet-dashboard，通過它可以很直觀的查看 Puppet 所有客戶端的同步狀態與 facter 參數。 事前準備由於 foreman 是取決於 puppet 執行主機的組態管理，他需要部署一個 puppet master 與 agent 環境。下面的列表為在安裝之前需要備設定的項目： Root 權限：所有伺服器能夠使用sudo。 私人網路 DNS：Forward 與 reverse 的 DNS 必須被設定，可參考How To Configure BIND as a Private Network DNS Server on Ubuntu 14.04。 防火牆有開啟使用的 port： Puppet master 必須可以被存取8140埠口。 安裝 Foreman安裝 Foreman 最簡單的方法是使用 Foreman 安裝程式。Foreman 安裝程式與配置必要的元件來執行 Foreman，包含以下內容： Foreman Puppet master and agent Apache Web Server with SSL and Passenger module 下載 Foreman 可以依照以下指令進行： $ sudo sh -c 'echo \"deb http://deb.theforeman.org/ trusty 1.5\" &gt; /etc/apt/sources.list.d/foreman.list'$ sudo sh -c 'echo \"deb http://deb.theforeman.org/ plugins 1.5\" &gt;&gt; /etc/apt/sources.list.d/foreman.list'$ wget -q http://deb.theforeman.org/pubkey.gpg -O- | sudo apt-key add -$ sudo apt-get update &amp;&amp; sudo apt-get install foreman-installer 安裝完成後，要執行 Foreman Installer 可以使用以下指令： $ sudo foreman-installer 完成後會看到以下資訊： Success!* Foreman is running at https://puppet-master.com Default credentials are 'admin:changeme'* Foreman Proxy is running at https://puppet-master.com:8443* Puppetmaster is running at port 8140The full log is at /var/log/foreman-installer/foreman-installer.log 之後修改puppet.conf檔案，開啟diff選項： $ sudo vim /etc/puppet/puppet.confshow_diff = true 新增 Foreman Host 到 Foreman 資料庫要新增 Host 可以使用以下指令： $ sudo puppet agent --test 完成後登入 Web，並輸入admin/changeme。 驗證 Foreman$ sudo puppet module install -i /etc/puppet/environments/production/modules puppetlabs/ntpNotice: Preparing to install into /etc/puppet/environments/production/modules ...Notice: Downloading from https://forge.puppetlabs.com ...Notice: Installing -- do not interrupt .../etc/puppet/environments/production/modules└─┬ puppetlabs-ntp (v4.1.2) └── puppetlabs-stdlib (v4.10.0) 參考資源 How To Use Foreman To Manage Puppet Nodes on Ubuntu 14.04","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"},{"name":"Puppet","slug":"Puppet","permalink":"https://k2r2bai.com/tags/Puppet/"}]},{"title":"Puppet 介紹與使用","slug":"devops/cm/puppet-basic","date":"2016-02-13T04:23:01.000Z","updated":"2019-12-02T01:49:42.385Z","comments":true,"path":"2016/02/13/devops/cm/puppet-basic/","link":"","permalink":"https://k2r2bai.com/2016/02/13/devops/cm/puppet-basic/","excerpt":"Puppet 是一個開放原始碼專案，基於 Ruby 的系統組態管理工具，採用 Client/Server 的部署架構。是一個為了實現資料中心自動化管理，而被設計的組態管理軟體，它使用跨平台語言規範，管理組態檔案、使用者、軟體套件與系統服務等。用戶端預設每個半小時會與伺服器溝通一次，來確定是否有更新。當然也可以配置主動觸發來強制用戶端更新。這樣可以把平常的系統管理工作程式碼化，透過程式碼化的好處是可以分享、保存與避免重複勞動，也可以快速恢復以及快速的大規模環境部署伺服器。","text":"Puppet 是一個開放原始碼專案，基於 Ruby 的系統組態管理工具，採用 Client/Server 的部署架構。是一個為了實現資料中心自動化管理，而被設計的組態管理軟體，它使用跨平台語言規範，管理組態檔案、使用者、軟體套件與系統服務等。用戶端預設每個半小時會與伺服器溝通一次，來確定是否有更新。當然也可以配置主動觸發來強制用戶端更新。這樣可以把平常的系統管理工作程式碼化，透過程式碼化的好處是可以分享、保存與避免重複勞動，也可以快速恢復以及快速的大規模環境部署伺服器。 優點： 成熟的組態管理軟體。 應用廣泛。 功能很完善。 提供許多資源可以配置 擁有許多的支持者。 缺點： 無法批次處理。 語言採用 DSL 與 Ruby。 缺少錯誤回報與檢查。 要透過程式定義先後順序。 基本概念介紹基礎設施即程式碼(Infrastructure as Code)在官方可以了解到 puppet 是一個概念為Infrastructure as Code的工具。Infrastructure as Code 與一般撰寫的 shell scrip 類似，但是比後者更高一個層次，將這一層虛擬化，使管理者只需要定義 Infrastructure 的狀況即可。這樣除了可以模組化、reuse外，也可以清楚透過 code 了解環境安裝了什麼與設定了什麼，因此 code 就是一個 infrastructure。 資源(Resource)Puppet 中一個基礎元素為resource，一個 resource 可以是file、package或者是service等，透過 resource 我們可以查看環境上檔案、套件、服務狀態等。更多資訊可以參考 Resource 列表與使用方式。 P.S resource type 要注意大小寫，當作 metaparameters 的時候寫作 Type[title] Type 要大寫。 相依性(Dependencies)在使用 Puppet 時，通常會撰寫 manifest 檔案來定義 resource。而這些 resource 在執行時會以同步的方式完成。 P.S 因為是同步(Sync)執行，故會有相依性的問題產生。這時候就可以用 Puppet 提供的 before / require 關鍵字來配置先後順序。 Puppet 安裝與基本操作環境建置我們將使用兩台 Ubuntu 14.04 主機來進行操作，一台為主控節點，另一台為Agent 節點。下面是我們將用到的伺服器的基礎資訊： puupet 主控節點 IP：10.21.20.10 主機名稱：puppetmaster 完整主機名稱：puppetmaster.example.com puupet agent 節點 IP：10.21.20.8 主機名稱：puppetslave 完整主機名稱：puppetslave.example.com 在每台節點完成以下步驟： $ sudo apt-get update &amp;&amp; sudo apt-get -y install ntp$ sudo vim /etc/ntp.confserver 1.tw.pool.ntp.org iburstserver 3.asia.pool.ntp.org iburstserver 2.asia.pool.ntp.org iburst Puppet 主控節點部署首先我們要先安裝 puppet 套件，透過wget下載puppetlabs-release.deb資源庫套件： $ wget https://apt.puppetlabs.com/puppetlabs-release-trusty.deb$ sudo dpkg -i puppetlabs-release-trusty.deb$ sudo apt-get update 完成後，我們就可以下載puppetmaster-passenger： $ sudo apt-get install puppetmaster-passenger 安裝過程中會發現錯誤，這部分可以忽略： Warning: Setting templatedir is deprecated. See http://links.puppetlabs.com/env-settings-deprecations (at /usr/lib/ruby/vendor_ruby/puppet/settings.rb:1139:in `issue_deprecation_warning&apos;) 安裝完後，可以透過以下指令查看版本： $ puppet --version3.8.4 這時我們可以透過resource指令來查看可用資源： $ puppet resource [type]$ puppet resource serviceservice &#123; 'acpid': ensure =&gt; 'running', enable =&gt; 'true',&#125;service &#123; 'apache2': ensure =&gt; 'running',&#125; 更多的 resource 可以查看 Type Reference。 在開始之前，我們先將 apache2 關閉，來讓 puppet 主控伺服器關閉： $ sudo service apache2 stop 接著我們要建立一個檔案/etc/apt/preferences.d/00-puppet.pref來鎖定 APT 自動更新套件，因為套件更新會造成組態檔的混亂： $ sudo vim /etc/apt/preferences.d/00-puppet.prefPackage: puppet puppet-common puppetmaster-passengerPin: version 3.8*Pin-Priority: 501 Puppet 主控伺服器是一個認證推送機構，需要產生自己的認證，用於簽署所有 agent 的認證要求。首先要刪除所有該套件安裝過程建立的 ssl 憑證。預設憑證放在 /var/lib/puppet/ssl底下。 $ sudo rm -rf /var/lib/puppet/ssl 接著我們要修改puppet.conf 檔案，來配置節點之前認證溝通，這邊要註解templatedir這行。然後在檔案的[main]增加以下資訊。 $ sudo vim /etc/puppet/puppet.conf[main]...server = puppetmasterenvironment = productionruninterval = 1hstrict_variables = truecertname = puppetmasterdns_alt_names = puppetmaster, puppetmaster.example.com 詳細的檔案可以參閱Main Config File (puppet.conf) 修改完後，透過puppet指令建立新的憑證： $ puppet master --verbose --no-daemonizeInfo: Creating a new certificate revocation listInfo: Creating a new SSL key for puppetmasterInfo: csr_attributes file loading from /etc/puppet/csr_attributes.yamlInfo: Creating a new SSL certificate request for puppetmasterInfo: Certificate Request fingerprint (SHA256): 9B:C5:45:F8:C5:8F:C2:B1:4D:15:E3:64:5F:DB:19:AB:06:C4:60:99:48:F3:BA:8F:D3:03:7E:35:BE:BC:4E:B1Notice: puppetmaster has a waiting certificate requestNotice: Signed certificate request for puppetmasterNotice: Removing file Puppet::SSL::CertificateRequest puppetmaster at '/var/lib/puppet/ssl/ca/requests/puppetmaster.pem'Notice: Removing file Puppet::SSL::CertificateRequest puppetmaster at '/var/lib/puppet/ssl/certificate_requests/puppetmaster.pem'Notice: Starting Puppet master version 3.8.4 當看到Notice: Starting Puppet master version 3.8.4代表完成，這時候可用 CTRL-C離開。 檢查新產生的 SSL 憑證，可以使用以下指令： $ puppet cert list -all+ \"puppetmaster\" (SHA256) 8C:5E:39:A7:81:94:2B:09:7E:20:B8:F2:46:59:60:D9:FA:5D:4A:9E:BF:27:D7:C1:1A:A4:3E:97:12:D3:BE:21 (alt names: \"DNS:puppet-master\", \"DNS:puppet-master.example.com\", \"DNS:puppetmaster\") 設定一個 Puppet manifests預設的 manifests 為/etc/puppet/manifests/site.pp。這個主要 manifests 檔案包括了用於在 Agent 節點執行的組態定義： $ sudo vim /etc/puppet/manifests/site.pp# execute 'apt-get update'exec &#123; 'apt-update': # exec resource named 'apt-update'command =&gt; '/usr/bin/apt-get update' # command this resource will run&#125;# install apache2 packagepackage &#123; 'apache2':require =&gt; Exec['apt-update'], # require 'apt-update' before installingensure =&gt; installed,&#125;# ensure apache2 service is runningservice &#123; 'apache2':ensure =&gt; running,&#125; 上面幾行用來部署 apache2 到 agent 節點。 完成後，修改/etc/apache2/sites-enabled/puppetmaster.conf檔，修改SSLCertificateFile與SSLCertificateKeyFile對應到新的憑證： SSLCertificateFile /var/lib/puppet/ssl/certs/puppetmaster.pemSSLCertificateKeyFile /var/lib/puppet/ssl/private_keys/puppetmaster.pem 然後重新開啟服務： $ sudo service apache2 restart Puppet agent 節點部署首先在 agent 節點上使用以下指令下載 puppet labs 的套件，並安裝： $ wget https://apt.puppetlabs.com/puppetlabs-release-trusty.deb$ sudo dpkg -i puppetlabs-release-trusty.deb$ sudo apt-get update$ sudo apt-get install -y puppet 由於 puppet 預設是不會啟動的，所以要編輯/etc/default/puppet檔案來設定： $ sudo vim /etc/default/puppetSTART=yes 之後一樣設定防止 APT 更新到 puppet，修改/etc/apt/preferences.d/00-puppet.pref檔案： $ sudo vim /etc/apt/preferences.d/00-puppet.prefPackage: puppet puppet-commonPin: version 3.8*Pin-Priority: 501 設定 puppet agent編輯/etc/puppet/puppet.conf檔案，將templatedir這行註解掉，並移除[master]部分的相關設定： $ sudo vim /etc/puppet/puppet.conf[main]logdir=/var/log/puppetvardir=/var/lib/puppetssldir=/var/lib/puppet/sslrundir=/var/run/puppetfactpath=$vardir/lib/facter# templatedir=$confdir/templates[agent]server = puppetmaster.example.comcertname = puppetslave.example.com 完成後啟動 puppet： $ sudo service puppet start 在主控伺服器上對憑證要求進行簽證當完成 master 節點與 slave 節點後，可以在主控伺服器上使用以下指令來列出當前憑證請求： $ puppet cert list\"puppetnode.example.com\" (SHA256) 52:43:4C:ED:16:34:A3:EA:E7:5D:B0:97:FF:66:4F:C8:E0:51:AD:80:E6:32:95:53:FC:24:AE:15:17:17:3A:C0 接著使用以下指令進行簽證： $ puppet cert sign puppetnode.example.comNotice: Signed certificate request for puppetnode.example.comNotice: Removing file Puppet::SSL::CertificateRequest puppetnode.example.com at '/var/lib/puppet/ssl/ca/requests/puppetnode.example.com.pem' 也可以使用puppet cert sign --all來一次簽署多個。 若想要移除可以使用puppet cert clean hostname。 簽署成功後，可以用以下指令查看： $ puppet cert list --all+ \"puppetmaster\" (SHA256) 8C:5E:39:A7:81:94:2B:09:7E:20:B8:F2:46:59:60:D9:FA:5D:4A:9E:BF:27:D7:C1:1A:A4:3E:97:12:D3:BE:21 (alt names: \"DNS:puppet-master\", \"DNS:puppet-master.example.com\", \"DNS:puppetmaster\")+ \"puppetnode.example.com\" (SHA256) EF:D6:E5:7E:45:B0:5D:EC:D4:17:E6:31:A2:97:F6:C2:31:2A:19:B9:0E:9D:31:77:9A:02:93:BC:73:B9:5E:58 部署主節點的 manifests當配置並完成 puppet manifests，現在需要部署 manifests 到 slave 節點上。要載入 puppet manifests 可以使用以下指令： $ puppet agent --testInfo: Retrieving pluginfactsInfo: Retrieving pluginInfo: Caching catalog for puppetnode.example.comInfo: Applying configuration version '1452086629'Notice: /Stage[main]/Main/Exec[apt-update]/returns: executed successfullyNotice: Finished catalog run in 17.31 seconds 之後我們可以使用puppet apply來提交 manifests： $ puppet apply /etc/puppet/manifests/site.pp 若要指定節點，可以建立如以下的*.pp檔： $ sudo vim /etc/puppet/manifests/site-example.ppnode 'puppetslave1', 'puppetslave2' &#123;# execute 'apt-get update'exec &#123; 'apt-update': # exec resource named 'apt-update'command =&gt; '/usr/bin/apt-get update' # command this resource will run&#125;# install apache2 packagepackage &#123; 'apache2':require =&gt; Exec['apt-update'], # require 'apt-update' before installingensure =&gt; installed,&#125;# ensure apache2 service is runningservice &#123; 'apache2':ensure =&gt; running,&#125;&#125; Puppet 是一個很成熟的工具，已有許多模組被貢獻，我們可以透過以下方式下載模組： $ puppet module install puppetlabs-apache 注意，不要在一個已經部署 Apache 的環境上使用該模組，否則會清空為沒有被 puppet 管理的 apache 配置。 接著我們修改site.pp來配置 apache： $ sudo vim /etc/puppet/manifest/site.ppnode 'puppetslave' &#123;class &#123; 'apache': &#125; # use apache moduleapache::vhost &#123; 'example.com': # define vhost resourceport =&gt; '8080',docroot =&gt; '/var/www/html'&#125;&#125; 參考資源 Modules Search InfoQ Puppet 介紹 Puppet 學習 Puppet 筆記 puppet學習筆記：puppet資源file詳細介紹 How To Install Puppet To Manage Your Server Infrastructure","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"SaltStack 介紹","slug":"devops/cm/saltstack-basic","date":"2016-02-12T04:23:01.000Z","updated":"2019-12-02T01:49:42.385Z","comments":true,"path":"2016/02/12/devops/cm/saltstack-basic/","link":"","permalink":"https://k2r2bai.com/2016/02/12/devops/cm/saltstack-basic/","excerpt":"Saltstack 是一套基礎設施管理開發套件、簡單易部署、可擴展到管理成千上萬的伺服器、控制速度佳(以 ms 為單位)。Saltstack 提供了動態基礎設施溝通總線用於編配、遠端執行、配置管理等等。Saltstack 是從 2011 年開始的專案，已經是很成熟的開源專案。該專案簡單的兩大基礎功能就是配置管理與遠端指令執行。 Saltstack 採用集中化管理，我們一般可以理解為 Puppet 的簡化版本與 Func的加強版本。Saltstack 是基於 Python 語言開發的，結合輕量級訊息佇列（ZeroMQ）以及 Python 第三方模組（Pyzmq、PyCrypto、Pyjinja2、python-msgpack與PyYAML等）。","text":"Saltstack 是一套基礎設施管理開發套件、簡單易部署、可擴展到管理成千上萬的伺服器、控制速度佳(以 ms 為單位)。Saltstack 提供了動態基礎設施溝通總線用於編配、遠端執行、配置管理等等。Saltstack 是從 2011 年開始的專案，已經是很成熟的開源專案。該專案簡單的兩大基礎功能就是配置管理與遠端指令執行。 Saltstack 採用集中化管理，我們一般可以理解為 Puppet 的簡化版本與 Func的加強版本。Saltstack 是基於 Python 語言開發的，結合輕量級訊息佇列（ZeroMQ）以及 Python 第三方模組（Pyzmq、PyCrypto、Pyjinja2、python-msgpack與PyYAML等）。 優點： 部署簡單與方便。 支持大部分 UNIX/Liunx 及 Windows 環境。 主從集中化管理。 配置簡單、功能強大與擴展性強。 主控端（Master）與被控制端（Minion）基於憑證認證。 支援 API 以及自定義模組，透過 Python 輕鬆擴展。 社群活躍。 缺點： Web UI 雖然有，但是沒有報表功能。 需要 Agent 透過 Saltstack 環境，我們可在成千上萬的伺服器進行批次的指令執行，根據不同的集中化管理配置、分散檔案、收集伺服器資料、作業系統基礎環境以及軟體套件等。 參考資源 SaltStack介紹和架構解析","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://k2r2bai.com/tags/Automation-Engine/"}]},{"title":"Docker 快速部署 Ceph 測試叢集","slug":"ceph/deploy/ceph-docker","date":"2016-02-11T09:08:54.000Z","updated":"2019-12-02T01:49:42.378Z","comments":true,"path":"2016/02/11/ceph/deploy/ceph-docker/","link":"","permalink":"https://k2r2bai.com/2016/02/11/ceph/deploy/ceph-docker/","excerpt":"本節將介紹如何透過 ceph-docker 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要1 Monitor與3 OSD。另外部署 MDS 與 RGW 來進行簡單測試。","text":"本節將介紹如何透過 ceph-docker 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要1 Monitor與3 OSD。另外部署 MDS 與 RGW 來進行簡單測試。 節點配置本安裝採用一台虛擬機器來提供部署，可使用 VBox 或 OpenStack 等建立，其環境資源大小如下： hostname CPUs RAM ceph-aio 2vCPU 4GB 若使用 Vagrant + VBox 的話，可以使用 Vagrantfile 腳本。 而該虛擬機要額外建立三顆虛擬區塊裝置，如下所示： Dev path Disk Description /dev/sdb 20 GB osd-1 使用 /dev/sdc 20 GB osd-2 使用 /dev/sdd 20 GB osd-3 使用 事前準備首先在主機安裝 Docker Engine，可以透過以下指令進行安裝： $ curl -fsSL https://get.docker.com/ | sh 部署 Ceph 測試叢集首先為了不與預設 Docker 網路共用，這邊額外建立一網路來提供給 Ceph 使用： $ docker network create --driver bridge ceph-net$ docker network inspect ceph-net&#123; \"Subnet\": \"172.18.0.0/16\", \"Gateway\": \"172.18.0.1/16\"&#125; 建立 Monitor完成網路建立後，就可以開始部署 Ceph 叢集了。一開始我們必須先建立 Monitor Container： $ cd ~ &amp;&amp; DIR=$(pwd)$ sudo docker run -d --net=ceph-net \\-v $&#123;DIR&#125;/ceph:/etc/ceph \\-v $&#123;DIR&#125;/lib/ceph/:/var/lib/ceph/ \\-e MON_IP=172.18.0.2 \\-e CEPH_PUBLIC_NETWORK=172.18.0.0/16 \\--name mon1 \\ceph/daemon mon 若發生錯誤請刪除以下目錄。如以下指令： $ sudo rm -rf $&#123;DIR&#125;/etc/ceph/$ sudo rm -rf $&#123;DIR&#125;/var/lib/ceph/ 檢查是否正確部署： $ docker exec -ti mon1 ceph -vceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)$ docker exec -ti mon1 ceph -scluster 2c254496-e948-4abb-a6dc-9aea41bbb56a health HEALTH_ERR no osds monmap e1: 1 mons at &#123;1068f41de69a=172.18.0.2:6789/0&#125; election epoch 3, quorum 0 1068f41de69a osdmap e1: 0 osds: 0 up, 0 in flags sortbitwise pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects 0 kB used, 0 kB / 0 kB avail 64 creating 建立 OSD上面可以看到 Monitor 建立完成，但是會有錯誤，因為目前沒有 OSD。因此這邊將建立三個 OSD Container 來模擬叢集做實際儲存的功能，透過以下方式部署： $ cd ~ &amp;&amp; DIR=$(pwd)$ sudo docker run -d --net=ceph-net \\--privileged=true --pid=host \\-v $&#123;DIR&#125;/ceph:/etc/ceph \\-v $&#123;DIR&#125;/lib/ceph/:/var/lib/ceph/ \\-v /dev/:/dev/ \\-e OSD_DEVICE=/dev/sdb \\-e OSD_TYPE=disk \\-e OSD_FORCE_ZAP=1 \\--name osd1 \\ceph/daemon osd 若要建立多個 OSD，只需要修改OSD_DEVICE與name即可，這邊建議建立三個 OSD。因為預設 pool 採用三份副本，若節點數過少需要自行修改副本數或 CRUSH Map。 完成後，可以透過以下指令檢查 Device 被使用： $ docker exec -ti osd1 df | grep \"osd\"/dev/sdb1 20857836 34924 20822912 1% /var/lib/ceph/osd/ceph-0 也可以直接透過 Monitor 來查看叢集安全狀態，如 PG 是否有誤等： $ docker exec -ti mon1 ceph -scluster 23fa3f2c-a401-46e0-abc1-d71b4625b348 health HEALTH_OK monmap e2: 1 mons at &#123;0b7ff674673f=172.18.0.2:6789/0&#125; election epoch 4, quorum 0 0b7ff674673f mgr no daemons active osdmap e15: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds,require_kraken_osds pgmap v29: 64 pgs, 1 pools, 0 bytes data, 0 objects 101 MB used, 61005 MB / 61106 MB avail 64 active+clean 建立 RGW當完成一個 RAODS(MON+OSD)叢集後，即可建立物件儲存閘道(RAODS Gateway)提供 S3 與 Swift 相容的 API，來儲存檔案到叢集中，一個 RGW Container 建立如下所示： $ cd ~ &amp;&amp; DIR=$(pwd)$ sudo docker run -d --net=ceph-net \\-v $&#123;DIR&#125;/lib/ceph/:/var/lib/ceph/ \\-v $&#123;DIR&#125;/ceph:/etc/ceph \\-p 8080:8080 \\--name rgw1 \\ceph/daemon rgw 完成後，透過 curl 工具來測試是否正確部署： $ curl -H \"Content-Type: application/json\" \"http://127.0.0.1:8080\"&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt; 透過 Python Client 進行檔案儲存，首先下載程式： $ wget \"https://gist.githubusercontent.com/kairen/e0dec164fa6664f40784f303076233a5/raw/33add5a18cb7d6f18531d8d481562d017557747c/s3client\"$ chmod u+x s3client$ sudo pip install boto 接著透過以下指令建立一個使用者： $ docker exec -ti rgw1 radosgw-admin user create --uid=\"test\" --display-name=\"I'm Test account\" --email=\"test@example.com\"\"keys\": [ &#123; \"user\": \"test\", \"access_key\": \"PFMKGXCFD77L8X4CF0T4\", \"secret_key\": \"SA8RpGO7SoN4TIdRxYtxloc5kRSLQvhOihJdDGG3\" &#125; ], 建立一個放置環境參數的檔案s3key.sh： export S3_ACCESS_KEY=\"PFMKGXCFD77L8X4CF0T4\"export S3_SECRET_KEY=\"SA8RpGO7SoN4TIdRxYtxloc5kRSLQvhOihJdDGG3\"export S3_HOST=\"127.0.0.1\"export S3_PORT=\"8080\" 然後 source 檔案，並嘗試執行列出 bucket 指令： $ . s3key.sh$ ./s3client list---------- Bucket List ---------- 建立一個 Bucket，並上傳檔案： $ ./s3client create filesCreate [files] success ...$ ./s3client upload files s3key.sh /Upload [s3key.sh] success ... 完成後，即可透過 list 與 download 來查看與下載： $ ./s3client list files---------- [files] ----------s3key.sh 157 2016-07-26T06:48:14.327Z$ ./s3client download files s3key.shDownload [s3key.sh] success ... 建立 MDS當系統需要使用到 CephFS 時，我們將必須建立 MDS(Metadata Server) 來提供詮釋資料的儲存，一個 MDS 容器部署如下： $ cd ~ &amp;&amp; DIR=$(pwd)$ sudo docker run -d --net=ceph-net \\-v $&#123;DIR&#125;/lib/ceph/:/var/lib/ceph/ \\-v $&#123;DIR&#125;/ceph:/etc/ceph \\-e CEPHFS_CREATE=1 \\--name mds1 \\ceph/daemon mds 透過以下指令檢查是否建立無誤： $ docker exec -ti mds1 ceph mds state5: 1/1/1 up &#123;0=mds-aea2f53de13a=up:active&#125;$ docker exec -ti mds1 ceph fs lsname: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"利用 Graphite 監控系統資料","slug":"devops/monitoring/graphite","date":"2016-02-11T04:23:01.000Z","updated":"2019-12-02T01:49:42.385Z","comments":true,"path":"2016/02/11/devops/monitoring/graphite/","link":"","permalink":"https://k2r2bai.com/2016/02/11/devops/monitoring/graphite/","excerpt":"Graphite 是一款開源的監控繪圖工具。Graphite 可以實時收集、存儲、顯示時間序列類型的數據（time series data）。它主要有三個部分構成： Carbon：基於 Twisted 的行程，用來接收資料。 Whisper：專門儲存時間序列類型資料的小型資料庫。 Graphite webapp：基於 Django 的網頁應用程式。","text":"Graphite 是一款開源的監控繪圖工具。Graphite 可以實時收集、存儲、顯示時間序列類型的數據（time series data）。它主要有三個部分構成： Carbon：基於 Twisted 的行程，用來接收資料。 Whisper：專門儲存時間序列類型資料的小型資料庫。 Graphite webapp：基於 Django 的網頁應用程式。 安裝 Graphite在開始配置 Graphite 之前，需要先安裝系統相依套件： $ sudo apt-get install build-essential graphite-web graphite-carbon python-dev apache2 libapache2-mod-wsgi libpq-dev python-psycopg2 在安裝期間graphite-carbon會詢問是否要刪除 whisper database files，這邊回答YES。 配置 Carbon透過增加[test]到 Carbon 的/etc/carbon/storage-schemas.conf 檔案，這部分單純用於測試使用，如果不需要可以直接跳過： [carbon]pattern = ^carbon\\.retentions = 60:90d[test]pattern = ^test\\.retentions = 5s:3h,1m:1d[default_1min_for_1day]pattern = .*retentions = 60s:1d 更多如何配置 Carbon storage 的資訊，可以參考 storage-schemas.con。 之後複製預設的聚合組態到/etc/carbon： $ sudo cp /usr/share/doc/graphite-carbon/examples/storage-aggregation.conf.example /etc/carbon/storage-aggregation.conf 設定在開機時，啟動 Carbon 快取，編輯/etc/default/graphite-carbon： CARBON_CACHE_ENABLED=true 啟動 Carbon 服務： $ sudo service carbon-cache start 安裝與配置 PostgreSQL安裝 PostgreSQL 讓 graphite-web 應用程式使用： $ sudo apt-get install postgresql 切換到postgres使用者，並建立資料庫使用者給 Graphite： $ sudo su - postgrespostgres# createuser graphite --pwprompt 建立graphite與grafana資料庫： postgres# createdb -O graphite graphitepostgres# createdb -O graphite grafana 切換graphite來檢查配置是否成功： $ sudo su - graphite 設定 Graphite更新 Graphite web 使用的後端資料庫與其他設定，編輯/etc/graphite/local_settings.py，加入以下： DATABASES = &#123;'default': &#123; 'NAME': 'graphite', 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'USER': 'graphite', 'PASSWORD': 'graphiteuserpassword', 'HOST': '127.0.0.1', 'PORT': '' &#125;&#125;USE_REMOTE_USER_AUTHENTICATION = TrueTIME_ZONE = 'UTC'SECRET_KEY = 'some-secret-key' TIME_ZONE 可以查詢 Wikipedia’s timezone database SECRET_KEY可以使用openssl rand -hex 10指令來建立。 初始化資料庫： $ sudo graphite-manage syncdb 設定 Graphite 使用 Apache首先複製 Graphite 的 Apache 配置樣板到 Apache sites-available 目錄： $ sudo cp /usr/share/graphite-web/apache2-graphite.conf /etc/apache2/sites-available 編輯/etc/apache2/sites-available/apache2-graphite.conf，修改預設監聽的 port： &lt;VirtualHost *:8080&gt; 編輯/etc/apache2/ports.conf加入監聽的 port： Listen 80Listen 8080 取消預設 Apache 的 site： $ sudo a2dissite 000-default 啟用 Graphite 的虛擬 site，並重新載入： $ sudo a2ensite apache2-graphite$ sudo service apache2 reload 重新啟動 apache 服務： $ sudo service apache2 restart 完成後，即可登入example_domain.com:8080。 測試一個簡單資料： $ for i in 4 6 8 16 2; do echo \"test.count $i `date +%s`\" | nc -q0 127.0.0.1 2003; sleep 6; done 參考連結 Deploy Graphite with Grafana on Ubuntu 14.04 How To Install and Use Graphite on an Ubuntu 14.04 Server Grafana＋collectd＋InfluxDB","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Data Collect","slug":"Data-Collect","permalink":"https://k2r2bai.com/tags/Data-Collect/"}]},{"title":"kube-up 腳本部署 Kubernetes 叢集(Deprecated)","slug":"kubernetes/deploy/kubeup-deploy","date":"2016-01-16T09:08:54.000Z","updated":"2019-12-02T01:49:42.393Z","comments":true,"path":"2016/01/16/kubernetes/deploy/kubeup-deploy/","link":"","permalink":"https://k2r2bai.com/2016/01/16/kubernetes/deploy/kubeup-deploy/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方腳本kube-up.sh來部署 Kubernetes 到 Ubuntu 14.04 系統上。其他更多平台的部署可以參考 Creating a Kubernetes Cluster。 本環境安裝資訊： Kubernetes v1.5.4 Etcd v2.3.0 Flannel v0.5.5 Docker v1.13.1","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方腳本kube-up.sh來部署 Kubernetes 到 Ubuntu 14.04 系統上。其他更多平台的部署可以參考 Creating a Kubernetes Cluster。 本環境安裝資訊： Kubernetes v1.5.4 Etcd v2.3.0 Flannel v0.5.5 Docker v1.13.1 節點資訊本次安裝作業系統採用Ubuntu 14.04 Server，測試環境為 OpenStack VM 與實體主機： IP Address Role CPU Memory 172.16.35.12 master1 2 4G 172.16.35.10 node1 2 4G 172.16.35.11 node2 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 事前準備安裝前需要確認叢集滿足以下幾點： 目前官方只測試過 Ubuntu 14.04，官方說法是 15.x 也沒問題，但 16.04 上我測試無法自動完成，要自己補上各種服務的 Systemd 腳本。 部署節點可以透過 SSH 與其他節點溝通，並且是無密碼登入，以及有 Sudoer 權限。 所有節點需要安裝Docker或rtk引擎。安裝方式為以下： $ curl -fsSL \"https://get.docker.com/\" | sh$ sudo iptables -P FORWARD ACCEPT 部署 Kubernetes 叢集首先下載官方 Release 的原始碼程式： $ curl -sSL \"https://github.com/kubernetes/kubernetes/archive/v1.5.4.tar.gz\" | tar zx$ mv kubernetes-1.5.4 kubernetes 接著編輯kubernetes/cluster/ubuntu/config-default.sh設定檔，修改以下內容： export nodes=$&#123;nodes:-\"ubuntu@172.16.35.12 ubuntu@172.16.35.10 ubuntu@172.16.35.11\"&#125;export role=\"ai i i\"export NUM_NODES=$&#123;NUM_NODES:-3&#125;export SERVICE_CLUSTER_IP_RANGE=192.168.3.0/24export FLANNEL_NET=172.16.0.0/16SERVICE_NODE_PORT_RANGE=$&#123;SERVICE_NODE_PORT_RANGE:-\"30000-32767\"&#125; 設定要部署的 Kubernetes 版本環境參數： export KUBE_VERSION=1.5.4export FLANNEL_VERSION=0.5.5export ETCD_VERSION=2.3.0export KUBERNETES_PROVIDER=ubuntu 然後進入到kubernetes/cluster目錄，並執行以下指令： $ sudo sed -i 's/verify-kube-binaries//g' kube-up.sh$ ./kube-up.sh...Cluster validation succeededDone, listing cluster services:Kubernetes master is running at http://172.16.35.12:8080 當看到上述資訊即表示成功部署，這時候進入到cluster/ubuntu/binaries目錄複製 kubectl 工具： $ sudo cp kubectl /usr/local/bin/ 最後透過 kubectl 工具來查看叢集節點是否成功加入： $ kubectl get nodesNAME STATUS AGE172.16.35.12 Ready 2m172.16.35.10 Ready 2m172.16.35.11 Ready 2m (Option)部署 Add-ons若要部署 kubernetes Dashboard 與 DNS 等額外服務的話，要修改kubernetes/cluster/ubuntu/config-default.sh設定檔，修改一下內容： ENABLE_CLUSTER_MONITORING=\"$&#123;KUBE_ENABLE_CLUSTER_MONITORING:-true&#125;\"ENABLE_CLUSTER_UI=\"$&#123;KUBE_ENABLE_CLUSTER_UI:-true&#125;\"ENABLE_CLUSTER_DNS=\"$&#123;KUBE_ENABLE_CLUSTER_DNS:-true&#125;\"DNS_SERVER_IP=$&#123;DNS_SERVER_IP:-\"192.168.3.10\"&#125;DNS_DOMAIN=$&#123;DNS_DOMAIN:-\"cluster.local\"&#125; 通常基本款大概為 Dashboard、DNS、Monitoring 與 Logging，。 修改完成後，進入到kubernetes/cluster/ubuntu目錄，並執行以下指令： $ KUBERNETES_PROVIDER=ubuntu ./deployAddons.sh 透過 kubectl 查看資訊，這邊服務屬於系統的，所以預設會被分到kube-system命名空間： $ kubectl get pods --namespace=kube-system 最後就可以透過瀏覽器查看 Dashboard。 建立 Nginx 應用程式Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將使用兩種方式建立一個簡單的 Nginx 服務。 利用 ad-hoc 指令建立kubectl 提供了 run 指令來快速建立應用程式部署，如下建立 Nginx 應用程式： $ kubectl run nginx --image=nginx$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-701339712-w5wlq 1/1 Running 0 26m 172.16.86.2 172.16.35.11 而當應用程式(deploy)被建立後，我們還需要透過 Kubernetes Service 來提供給外部網路存取應用程式，如下指令： $ kubectl expose deploy nginx --port 80 --type NodePort$ kubectl get svc -o wide 完成後要接著建立 svc（Service）來提供外部網路存取應用程式，使用以下指令建立： $ kubectl expose rc nginx --port=80 --type=NodePort$ kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 192.168.3.1 &lt;none&gt; 443/TCP 37mnginx 192.168.3.199 &lt;nodes&gt; 80:31764/TCP 30m 這邊採用NodePort，即表示任何節點 IP 位址的31764 Port 都會 Forward 到 Nginx container 的80 Port。 若想刪除應用程式與服務的話，可以透過以下指令： $ kubectl delete deploy nginx$ kubectl delete svc nginx 撰寫 YAML 檔案建立Kubernetes 支援了 JSON 與 YAML 來描述要部署的應用程式資訊，這邊撰寫nginx-dp.yaml來部署 Nginx 應用： apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 接著建立 Service 來提供存取服務，這邊撰寫nginx-svc.yaml來建立服務： apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP nodePort: 30000 selector: app: nginx 然後透過 kubectl 指令來指定檔案建立： $ kubectl create -f nginx-dp.yamldeployment \"nginx\" created$ kubectl create -f nginx-svc.yamlservice \"nginx-service\" created 完成後，可以查看一下資訊： $ kubectl get svc,pods,rc -o wideNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORsvc/kubernetes 192.168.3.1 &lt;none&gt; 443/TCP 51m &lt;none&gt;svc/nginx-service 192.168.3.155 &lt;nodes&gt; 80:30000/TCP 1m app=nginxNAME READY STATUS RESTARTS AGE IP NODEpo/nginx-4087004473-0wrbs 1/1 Running 0 2m 172.16.101.2 172.16.35.10 最後要刪除的話，直接將 create 改成使用delete即可： $ kubectl delete -f nginx-dp.yaml$ kubectl delete -f nginx-svc.yaml 其他 Kubernetes 網路技術Kubernetes 支援多種網路整合，若 Flannel 用不爽可以改以下幾種： OpenVSwitch with GRE/VxLAN Linux Bridge L2 networks Weave Calico(使用 BGP Routing)","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://k2r2bai.com/tags/Ubuntu/"}]},{"title":"hyperkube 建立多節點 Kubernetes","slug":"kubernetes/deploy/docker-multi","date":"2016-01-14T09:08:54.000Z","updated":"2019-12-02T01:49:42.392Z","comments":true,"path":"2016/01/14/kubernetes/deploy/docker-multi/","link":"","permalink":"https://k2r2bai.com/2016/01/14/kubernetes/deploy/docker-multi/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本篇將說明如何透過 Docker 來部署一個多節點的 kubernetes 叢集。其架構圖如下所示： 本環境安裝資訊： Kubernetes v1.5.5 Docker v17.03.0-ce","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本篇將說明如何透過 Docker 來部署一個多節點的 kubernetes 叢集。其架構圖如下所示： 本環境安裝資訊： Kubernetes v1.5.5 Docker v17.03.0-ce 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為 Vagrant with Libvirt 或 Vbox： IP Address Role CPU Memory 172.16.35.12 master1 2 4G 172.16.35.10 node1 2 4G 172.16.35.11 node2 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 事前準備安裝前需要確認叢集滿足以下幾點： 所有節點需要安裝Docker或rtk引擎。安裝方式為以下： $ curl -fsSL \"https://get.docker.com/\" | sh$ sudo iptables -P FORWARD ACCEPT Kubernetes 部署這邊將分別部署 Master 與 Node(Worker) 節點。 建立 Master 節點首先下載官方 Release 的原始碼程式： $ git clone \"https://github.com/kubernetes/kube-deploy\" 接著進入部署目錄來進行部署動作，Master 執行以下指令： $ export IP_ADDRESS=\"172.16.35.12\"$ cd kube-deploy/docker-multinode$ ./master.sh...Master done! 執行後，透過 Docker 指令查看是否成功： $ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbfb6461499fb gcr.io/google_containers/hyperkube-amd64:v1.5.5 \"/hyperkube kubele...\" 4 minutes ago Up 4 minutes kubelet... 這邊會隨時間開啟其他 Component 的 Docker Container。 確認完成後，就可以下載 kubectl 來透過 API 管理叢集： $ curl -O \"https://storage.googleapis.com/kubernetes-release/release/v1.5.5/bin/linux/amd64/kubectl\"$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 安裝好 kubectl 後就可以透過以下指令來查看資訊： $ kubectl get nodesNAME STATUS AGE172.16.35.12 Ready 11s 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system k8s-proxy-v1-bfdml 1/1 Running 0 1mkube-system kube-dns-4101612645-fb1rn 4/4 Running 0 1mkube-system kubernetes-dashboard-3543765157-999p2 1/1 Running 0 1m 建立 Node(Worker) 節點首先下載官方 Release 的原始碼程式： $ git clone \"https://github.com/kubernetes/kube-deploy\" 接著進入部署目錄來進行部署動作，Node 執行以下指令： $ export MASTER_IP=\"172.16.35.12\"; export IP_ADDRESS=\"172.16.35.11\"$ cd kube-deploy/docker-multinode$ ./worker.sh...+++ [0324 07:23:06] Done. After about a minute the node should be ready 驗證安裝完成後可以查看所有節點狀態，執行以下指令： $ kubectl get nodesNAME STATUS AGE172.16.35.10 Ready 3m172.16.35.11 Ready 4m172.16.35.12 Ready 1m 接著我們透過部署簡單的 Nginx 應用程式來驗證系統是否正常： $ kubectl run nginx --image=nginx --port=80deployment \"nginx\" created$ kubectl expose deploy nginx --port=80service \"nginx\" exposed 透過指令檢查 Pods： $ kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-3449338310-ttqp2 1/1 Running 0 32s 10.1.1.2 172.16.35.11 透過指令檢查 Service： $ kubectl get svc -o wideNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes 10.0.0.1 &lt;none&gt; 443/TCP 47m &lt;none&gt;nginx 10.0.0.149 &lt;none&gt; 80/TCP 37s run=nginx 取得應用程式的 Service ip，並存取服務： $ IP=$(kubectl get svc nginx --template=&#123;&#123;.spec.clusterIP&#125;&#125;)$ curl $&#123;IP&#125;","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"Hyperkube 建立單機 Kubernetes","slug":"kubernetes/deploy/docker-singal","date":"2016-01-13T09:08:54.000Z","updated":"2019-12-02T01:49:42.392Z","comments":true,"path":"2016/01/13/kubernetes/deploy/docker-singal/","link":"","permalink":"https://k2r2bai.com/2016/01/13/kubernetes/deploy/docker-singal/","excerpt":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本篇將說明如何透過 Docker 來部署一個單機的 kubernetes。其架構圖如下所示：","text":"本文章已被棄用，請不要參考。(This post is being deprecated. Don’t refer it.) 本篇將說明如何透過 Docker 來部署一個單機的 kubernetes。其架構圖如下所示： 事前準備在開始安裝前，我們必須在部署的主機或虛擬機安裝與完成以下兩點： 確認安裝 Docker Engine 於主機作業系統。 $ curl -fsSL \"https://get.docker.com/\" | sh$ sudo iptables -P FORWARD ACCEPT 定義要使用的 Kubernetes 版本，目前支援 1.2.0+ 版本。 $ export K8S_VERSION=\"1.5.4\" 部署 Kuberentes 元件完成上述後，透過執行以下指令進行部署： $ sudo docker run -d \\--volume=/:/rootfs:ro \\--volume=/sys:/sys:ro \\--volume=/var/lib/docker/:/var/lib/docker:rw \\--volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\--volume=/var/run:/var/run:rw \\--net=host \\--pid=host \\--privileged=true \\--name=kubelet \\gcr.io/google_containers/hyperkube-amd64:v$&#123;K8S_VERSION&#125; \\/hyperkube kubelet \\--containerized \\--hostname-override=\"127.0.0.1\" \\--address=\"0.0.0.0\" \\--api-servers=\"http://localhost:8080\" \\--config=/etc/kubernetes/manifests \\--cluster-dns=10.0.0.10 \\--allow-privileged=true --v=2 執行後，透過 Docker 指令查看是否成功： $ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbfb6461499fb gcr.io/google_containers/hyperkube-amd64:v1.5.4 \"/hyperkube kubele...\" 4 minutes ago Up 4 minutes kubelet... 這邊會隨時間開啟其他 Component 的 Docker Container。 確認完成後，就可以下載 kubectl 來透過 API 管理叢集： $ curl -O \"https://storage.googleapis.com/kubernetes-release/release/v$&#123;K8S_VERSION&#125;/bin/linux/amd64/kubectl\"$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 接著設定 kubectl config 來使用測試叢集： $ kubectl config set-cluster test-doc --server=http://localhost:8080$ kubectl config set-context test-doc --cluster=test-doc$ kubectl config use-context test-doc 驗證安裝當完成所有步驟後，就可以檢查節點狀態： $ kubectl get nodesNAME STATUS AGE127.0.0.1 Ready 6m 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get po --all-namespaceskubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system k8s-etcd-127.0.0.1 1/1 Running 0 15mkube-system k8s-master-127.0.0.1 4/4 Running 2 15mkube-system k8s-proxy-127.0.0.1 1/1 Running 0 15mkube-system kube-addon-manager-127.0.0.1 2/2 Running 0 15m 接著我們透過部署簡單的 Nginx 應用程式來驗證系統是否正常： $ kubectl run nginx --image=nginx --port=80deployment \"nginx\" created$ kubectl expose deploy nginx --port=80service \"nginx\" exposed 透過指令檢查 Pods： $ kubectl get po -o wideNAME READY STATUS RESTARTS AGE NODEnginx-198147104-u9lt6 1/1 Running 0 3m 127.0.0.1 透過指令檢查 Service： $ kubectl get svc -o wideNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes 10.0.0.1 &lt;none&gt; 443/TCP 11m &lt;none&gt;nginx 10.0.0.133 &lt;none&gt; 80/TCP 3m run=nginx 取得應用程式的 Service ip，並存取服務： $ IP=$(kubectl get svc nginx --template=&#123;&#123;.spec.clusterIP&#125;&#125;)$ curl $&#123;IP&#125;","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"學習 Docker Network 之間的差別","slug":"container/docker-network","date":"2016-01-05T09:08:54.000Z","updated":"2019-12-02T01:49:42.381Z","comments":true,"path":"2016/01/05/container/docker-network/","link":"","permalink":"https://k2r2bai.com/2016/01/05/container/docker-network/","excerpt":"Docker 的網路是透過 Linux 的網路命名空間與虛擬網路裝置(Veth pair)實現而成。然而 Docker 的網路支援了不同類型功能，每一種都有其用意，本篇將針對以下幾項 Docker network mode 進行實作與介紹： Bridge Mode (default) Host Mode None Mode Container Mode","text":"Docker 的網路是透過 Linux 的網路命名空間與虛擬網路裝置(Veth pair)實現而成。然而 Docker 的網路支援了不同類型功能，每一種都有其用意，本篇將針對以下幾項 Docker network mode 進行實作與介紹： Bridge Mode (default) Host Mode None Mode Container Mode Bridge ModeBridge Mode 是 Docker 預設使用的 network mode 。若你在已安裝 Docker 的環境中使用 ifconfig 指令，你可以看到有一個名為 docker0 的 network interface： 此時，你可以在這個 Docker 環境中執行一個 docker container ： $ docker run -ti busybox sh 一樣使用 ifconfig 指令，你也會看到一個 eth0 network interface： 這就是為什麼在 host 上可以直接使用 ping 指令與 container 進行通訊，因為 veth796c087 讓 docker0(172.17.0.1) 與 eth0(172.17.0.2) 位於同一個區網。 我們用一張簡單的圖來表示這三者的關係： 由上圖你可以發現，Container 1 可透過 eth0 經過 veth4fd8759 與 docker0 進行溝通，Container 2 也是如此，且 Container 1 與 Container 2 也可以進行溝通。 Host ModeHost Mode 可以把他想像成建立一個與 Host 擁有同樣的 network interface 的 Container ，使用方式： $ docker run --net=host -ti busybox sh None ModeNone Mode 是建置最簡潔的 Container ，也就是沒有任何 network interface 的 Container。使用方式是在建立 Container 的同時給與 --net=none 的參數： $ docker run --net=none -ti busybox sh 此時，你若使用 ifconfig 指令，會發現這個 Container 沒有任何 network interface。 Container Mode首先，我們要先啟動一個 Container，並且使用這個 Container 的 Container ID 建立另外一個 Container： 建置第二個 Container 的方式，將第一個 Container ID 當參數進行建置： docker run -ti --net=container:d16d87a29be3 busybox sh 此時，你會發現兩個 Container 的 IP 都是 172.17.0.2 ，雖然他們是不同的 Container 但是被放置同一個 Namespace 內，三者的關係如下：","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"用 ELK 做監控系統","slug":"devops/logging/elk","date":"2016-01-03T04:23:01.000Z","updated":"2019-12-02T01:49:42.385Z","comments":true,"path":"2016/01/03/devops/logging/elk/","link":"","permalink":"https://k2r2bai.com/2016/01/03/devops/logging/elk/","excerpt":"ELK 是由三個套件的開頭英文組成的縮寫，其 E 表示Elasticsearch，L 表示Logstash，K 表示Kibana，作為收集資料、資料索引以及資料視覺化的工具集合，以下分別簡單介紹三個套件。","text":"ELK 是由三個套件的開頭英文組成的縮寫，其 E 表示Elasticsearch，L 表示Logstash，K 表示Kibana，作為收集資料、資料索引以及資料視覺化的工具集合，以下分別簡單介紹三個套件。 LogstashLogstash 可以簡單、有效、快速的處理Log資料，不過Logstash的主要功能是處理時間類型的Log，也就是在Log檔中有時間戳記（TimeStamp）的資料，而分析Log資料主要就是分析事件發生的時間和內容 Logstash Forwarder可傳送所收集到的 Log 訊息到 Logstash。 ElasticsearchElasticsearch 是一個開源的資料搜尋分析系統，它可以解決現在 Web 去做資料庫的搜尋的種種問題，嚴格來說也不只是 web，(有可能是為了撈資料的效能，或是 schema free, real-time 等等)。 KibanaKibana 是一個開源和免費的工具，他可以幫助您匯總、分析和搜索重要數據日志並提供友好的web界面 系統與安裝版本 OS: Ubuntu 14.04 Elasticsearch 1.4.4 Logstash 1.5.0 Kibana 4 進行安裝首先安裝 Java Oraclesudo apt-get purge openjdk*sudo apt-get -y autoremovesudo add-apt-repository -y ppa:webupd8team/javasudo apt-get updateecho debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selectionsecho debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selectionssudo apt-get -y install oracle-java7-installer 安裝 Elasticsearch匯入 Elasticsearch public GPG key 到 apt wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add - 建立 Elasticsearch source list： echo &apos;deb http://packages.elasticsearch.org/elasticsearch/1.4/debian stable main&apos; | sudo tee /etc/apt/sources.list.d/elasticsearch.list 更新套件： sudo apt-get update 安裝 elasticsearch 1.4.4： sudo apt-get -y install elasticsearch=1.4.4 安裝完成，開啟配置檔： sudo vi /etc/elasticsearch/elasticsearch.yml 如果想限制給外界存取 Elasticsearch，可找到 network.host ，將內容取代成”localhost”，如下： network.host: localhost 開啟 Elasticsearch： sudo service elasticsearch restart 重開機立即啟動 Elasticsearch ： sudo update-rc.d elasticsearch defaults 95 10 安裝 Kibana下載 Kibana 4 到 opt 資料夾 cd /opt 使用wget下載 Kibana 套件壓縮檔： wget https://download.elasticsearch.org/kibana/kibana/kibana-4.0.1-linux-x64.tar.gz 解壓縮檔案： tar xvf kibana-*.tar.gz 開啟 Kibana 配置檔： vi ~/kibana-4*/config/kibana.yml 配置檔中找到host將 IP address “0.0.0.0” 取代成 “localhost”，此設定讓 Kibana 只能被 localhost 存取，如下： host: &quot;localhost&quot; 將下載完的 kibana 資料夾名稱改成 kibana： sudo mv kibana-4.0.1-linux-x64 kibana Kibana 執行 /opt/kibana/bin/kibana 來開啟，但我們想用 service 的方式開啟。 下載 Kibana 4 init 腳本: cd /etc/init.d &amp;&amp; sudo wget https://gist.githubusercontent.com/thisismitch/8b15ac909aed214ad04a/raw/bce61d85643c2dcdfbc2728c55a41dab444dca20/kibana4 開啟 Kibana service： sudo chmod +x /etc/init.d/kibana4sudo update-rc.d kibana4 defaults 96 9sudo service kibana4 start 安裝 Logstash建立 Logstash source list： echo &apos;deb http://packages.elasticsearch.org/logstash/1.5/debian stable main&apos; | sudo tee /etc/apt/sources.list.d/logstash.list 更新套件： sudo apt-get update 安裝 Logstash： sudo apt-get install logstash 產生 SSL 認證因為我們將使用 Logstash Forwarder 收集 logs並傳送到 Logstash Server ，所以我們必須建立一對SSL 認證的 key： sudo mkdir -p /etc/pki/tls/certssudo mkdir /etc/pki/tls/private 設定 openssl 配置： sudo vi /etc/ssl/openssl.cnf 配置檔中找到 [ v3_ca ] ，並新增以下內容： subjectAltName = IP:logstash_server_private_ip 產生 SSL 認證和 private key 到 /etc/pki/tls/ ，如下： cd /etc/pki/tls 設定 SSL 驗證： sudo openssl req -config /etc/ssl/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt 配置 Logstash新增配置檔 01-lumberjack-input.conf： sudo vi /etc/logstash/conf.d/01-lumberjack-input.conf 新增以下配置內容： input &#123; lumberjack &#123; port =&gt; 5000 type =&gt; &quot;logs&quot; ssl_certificate =&gt; &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot; ssl_key =&gt; &quot;/etc/pki/tls/private/logstash-forwarder.key&quot; &#125;&#125; 新增配置檔 10-syslog.conf： sudo vi /etc/logstash/conf.d/10-syslog.conf 新增以下配置內容： filter &#123; if [type] == &quot;syslog&quot; &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGTIMESTAMP:syslog_timestamp&#125; %&#123;SYSLOGHOST:syslog_hostname&#125; %&#123;DATA:syslog_program&#125;(?:\\[%&#123;POSINT:syslog_pid&#125;\\])?: %&#123;GREEDYDATA:syslog_message&#125;&quot; &#125; add_field =&gt; [ &quot;received_at&quot;, &quot;%&#123;@timestamp&#125;&quot; ] add_field =&gt; [ &quot;received_from&quot;, &quot;%&#123;host&#125;&quot; ] &#125; syslog_pri &#123; &#125; date &#123; match =&gt; [ &quot;syslog_timestamp&quot;, &quot;MMM d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ] &#125; &#125;&#125; 新增配置檔 30-lumberjack-output.conf： sudo vi /etc/logstash/conf.d/30-lumberjack-output.conf 新增以下配置內容： output &#123; elasticsearch &#123; host =&gt; localhost &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 重啟 Logstash： sudo service logstash restart 完成後就可以設置Logstash Forwarder（簡單說就是加入 Client）。 複製 SSL Certificate 與 Logstash Forwarder 套件 (On Logstash Server)scp /etc/pki/tls/certs/logstash-forwarder.crt user@client_server_private_address:/tmp 安裝 Logstash Forwarder 套件 (On Client)Logstash Forwarder source list： echo &apos;deb http://packages.elasticsearch.org/logstashforwarder/debian stable main&apos; | sudo tee /etc/apt/sources.list.d/logstashforwarder.list 一樣可使用Elasticsearch的 GPG key 來安裝： wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add - 安裝 Logstash Forwarder package： sudo apt-get updatesudo apt-get install logstash-forwarder 複製 Logstash server’s SSL認證到 /etc/pki/tls/certs： sudo mkdir -p /etc/pki/tls/certssudo cp /tmp/logstash-forwarder.crt /etc/pki/tls/certs/ 配置 Logstash Forwarder設定Logstash Forwarder 配置檔(On Client Server)： sudo vi /etc/logstash-forwarder.conf 配置檔中找到 network ，底下加入以下內容： &quot;servers&quot;: [ &quot;logstash_server_private_address:5000&quot; ], &quot;timeout&quot;: 15, ssl ca&quot;: &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot; 配置檔中找到 files ，底下加入以下內容： &#123; &quot;paths&quot;: [ &quot;/var/log/syslog&quot;, &quot;/var/log/auth.log&quot; ], &quot;fields&quot;: &#123; &quot;type&quot;: &quot;syslog&quot; &#125; &#125; 重啟 Logstash Forwarder： sudo service logstash-forwarder restart 完成後，即可開啟瀏覽器，網址列輸入locahost:5601。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/categories/DevOps/"}],"tags":[{"name":"Monitoring","slug":"Monitoring","permalink":"https://k2r2bai.com/tags/Monitoring/"},{"name":"Logging","slug":"Logging","permalink":"https://k2r2bai.com/tags/Logging/"},{"name":"DevOps","slug":"DevOps","permalink":"https://k2r2bai.com/tags/DevOps/"},{"name":"Visualization","slug":"Visualization","permalink":"https://k2r2bai.com/tags/Visualization/"}]},{"title":"建立自己的 Docker Registry","slug":"container/docker-registry","date":"2016-01-02T09:08:54.000Z","updated":"2019-12-02T01:49:42.381Z","comments":true,"path":"2016/01/02/container/docker-registry/","link":"","permalink":"https://k2r2bai.com/2016/01/02/container/docker-registry/","excerpt":"Docker Registry 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 Docker Hub、QUAY 與 GCP registry，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 Deploying a registry server。","text":"Docker Registry 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 Docker Hub、QUAY 與 GCP registry，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 Deploying a registry server。 預先準備資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Role CPU Memory 172.16.35.13 docker-registry 1 2G 安裝首先進入到docker-registry節點，安裝 Docker engine： $ curl -fsSL \"https://get.docker.com/\" | sh 完成安裝後，接著透過以下指令建立一個 Docker registry 容器： $ docker run -d -p 5000:5000 --restart=always --name registry \\-v $(pwd)/data:/var/lib/registry \\registry:2 -v 為 host 與 container 要進行同步的目錄，主要存放 docker images 資料 接著為了方便檢視 Docker image，這邊另外部署 Docker registry UI： $ docker run -d -p 5001:80 \\-e ENV_DOCKER_REGISTRY_HOST=172.16.35.13 \\-e ENV_DOCKER_REGISTRY_PORT=5000 \\konradkleine/docker-registry-frontend:v2 完成後就可以透過瀏覽器進入 Docker registry UI 查看資訊。也可以透過以下指令檢查是否部署成功： $ docker pull ubuntu:14.04$ docker tag ubuntu:14.04 localhost:5000/ubuntu:14.04$ docker push localhost:5000/ubuntu:14.04The push refers to a repository [localhost:5000/ubuntu]447f88c8358f: Pusheddf9a135a6949: Pushed... 其他 Docker registry 列表： Portus Atomic Registry Private Registries in RancherOS VMware Harbor","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Container Registry","slug":"Container-Registry","permalink":"https://k2r2bai.com/tags/Container-Registry/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"Docker Machine Driver 使用","slug":"container/docker-machine-driver","date":"2015-12-06T09:08:54.000Z","updated":"2019-12-02T01:49:42.381Z","comments":true,"path":"2015/12/06/container/docker-machine-driver/","link":"","permalink":"https://k2r2bai.com/2015/12/06/container/docker-machine-driver/","excerpt":"Docker machine 是 Docker 早期提供 Windows 與 Mac OS X 建立 Docker 環境的工具，其採用 VirtualBox 來提供一個 Container OS，再利用 Docker client 來進行操作。然而 Docker Machine 除了使用 VirtualBox 外，還可以使用Generic Driver與OpenStack Driver來建立雲端平台上的 Docker 環境。","text":"Docker machine 是 Docker 早期提供 Windows 與 Mac OS X 建立 Docker 環境的工具，其採用 VirtualBox 來提供一個 Container OS，再利用 Docker client 來進行操作。然而 Docker Machine 除了使用 VirtualBox 外，還可以使用Generic Driver與OpenStack Driver來建立雲端平台上的 Docker 環境。 使用 Generic Driver以下範例使用 Generic Driver 來連接一個已存在的 OpenStack instance： $ docker-machine create --driver generic \\--generic-ssh-user ubuntu \\--generic-ssh-key ~/.ssh/id_rsa \\--generic-ip-address 10.26.1.82 \\docker-engine-1 --generic-ssh-user為要遠端的使用者。 --generic-ssh-key為要使用的 SSH Key。 --generic-ip-address為要使用的虛擬機 IP。 使用 OpenStack DriverDocker Machine 提供了使用者可以建立 Docker 容器於 OpenStack 虛擬機。以下範例是使用 OpenStack Driver 來連接 OpenStack 並建立虛擬機： $ docker-machine create --driver openstack \\--openstack-username \"&lt;KEYSTONE_USERNAME&gt;\" \\--openstack-password \"&lt;KEYSTONE_PASSWD&gt;\" \\--openstack-tenant-name \"&lt;KEYSTONE_PROJECT_NAME&gt;\" \\--openstack-auth-url \"&lt;KEYSTONE_URL&gt;\" \\--openstack-flavor-name \"m1.medium\" \\--openstack-image-name \"Ubuntu-14.04-Server-Cloud\" \\--openstack-net-name \"admin-net\" \\--openstack-floatingip-pool \"internal-net\" \\--openstack-ip-version 4 \\--openstack-ssh-user \"ubuntu\" \\--openstack-sec-groups \"ALL_PASS\" \\openstack-docker --openstack-username為 Keystone 使用者帳號。 --openstack-password為 Keystone 使用者密碼。 --openstack-tenant-name為要使用的 project 名稱。 --openstack-auth-url為 Keystone URL。 --openstack-flavor-name為要使用的 Flavor。 --openstack-image-name為要使用的 Image 名稱。 --openstack-net-name為要使用的私有網路名稱。 --openstack-floatingip-pool為要使用的 Floating 網路名稱。 --openstack-ip-versionl為要使用的網路版本。 --openstack-ssh-user為要遠端的使用者名稱。 --openstack-sec-groups為要使用的 Security Grous 名稱，可多個如以下ALL_PASS, SSH。","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"}]},{"title":"利用 HAProxy 建立服務負載平衡","slug":"linux/ubuntu/haproxy","date":"2015-11-28T08:23:01.000Z","updated":"2019-12-02T01:49:42.398Z","comments":true,"path":"2015/11/28/linux/ubuntu/haproxy/","link":"","permalink":"https://k2r2bai.com/2015/11/28/linux/ubuntu/haproxy/","excerpt":"HAProxy 提供了高可靠性、負載平衡（Load Balancing）、基於 TCP 以及 HTTP 的應用程式代理，更支援了虛擬機的使用。HAProxy 是一個開放式原始碼，免費、快速以及非常可靠，根據官方測試結果，該軟體最高能夠支援到 10G 的並行傳輸，因此特別適合使用在負載很大的 Web 伺服器，且這些伺服器通常需要保持 Session 或者 Layer 7 網路的處理，但這些都可以使用 HAProxy 來完成。 HAProxy 具有以下幾個優點： 開放式原始碼，因此免費，且穩定性高 能夠負荷 10G 網路的並行傳輸 支援連線拒絕功能 支援全透明化的代理 擁有內建的監控狀態儀表板 支援虛擬機的使用","text":"HAProxy 提供了高可靠性、負載平衡（Load Balancing）、基於 TCP 以及 HTTP 的應用程式代理，更支援了虛擬機的使用。HAProxy 是一個開放式原始碼，免費、快速以及非常可靠，根據官方測試結果，該軟體最高能夠支援到 10G 的並行傳輸，因此特別適合使用在負載很大的 Web 伺服器，且這些伺服器通常需要保持 Session 或者 Layer 7 網路的處理，但這些都可以使用 HAProxy 來完成。 HAProxy 具有以下幾個優點： 開放式原始碼，因此免費，且穩定性高 能夠負荷 10G 網路的並行傳輸 支援連線拒絕功能 支援全透明化的代理 擁有內建的監控狀態儀表板 支援虛擬機的使用 HAProxy 安裝本教學會使用到一台 Proxy 節點與兩台 Web 節點，如下： IP Address Role 172.17.0.2 proxy 172.17.0.3 web-1 172.17.0.4 web-2 本篇採用 Ubuntu 作業系統，因此可透過 apt 直接安裝，以下範例是在 Ubuntu Server 環境中操作： $ sudo apt-get install software-properties-common python-software-properties$ sudo apt-add-repository ppa:vbernat/haproxy-1.5$ sudo apt-get update$ sudo apt-get install haproxy 若要安裝其他版本，可以修改成以下： &gt; sudo apt-add-repository ppa:vbernat/haproxy-1.6&gt; HAProxy 設定完成安裝後，要透過編輯/etc/haproxy/haproxy.cfg設定檔來配置 Proxy： global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon maxconn 1024 ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.httpfrontend nginxs_proxy bind 172.17.0.2:80 mode http default_backend nginx_serversbackend nginx_servers mode http balance roundrobin option forwardfor http-request set-header X-Forwarded-Port %[dst_port] http-request add-header X-Forwarded-Proto https if &#123; ssl_fc &#125; option httpchk HEAD / HTTP/1.1\\r\\nHost:localhost server web1 172.17.0.3:80 check cookie s1 server web2 172.17.0.4:80 check cookie s2listen haproxy_stats bind 0.0.0.0:8080 stats enable stats hide-version stats refresh 30s stats show-node stats auth username:password stats uri /stats 完成設定後，需重啟服務： $ sudo service haproxy restart","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"Load Balancer","slug":"Load-Balancer","permalink":"https://k2r2bai.com/tags/Load-Balancer/"}]},{"title":"NFS 簡單安裝與使用","slug":"linux/ubuntu/nfs","date":"2015-11-24T04:23:01.000Z","updated":"2019-12-02T01:49:42.399Z","comments":true,"path":"2015/11/24/linux/ubuntu/nfs/","link":"","permalink":"https://k2r2bai.com/2015/11/24/linux/ubuntu/nfs/","excerpt":"網路檔案系統(Network FileSystem，NFS)是早期由 SUN 公司所開發出來的分散式檔案系統協定。主要透過 RPC Service 使檔案能夠共享於網路中，NFS 的好處是它支援了不同系統與機器的溝通能力，使資料能夠很輕易透過網路共享給別人。","text":"網路檔案系統(Network FileSystem，NFS)是早期由 SUN 公司所開發出來的分散式檔案系統協定。主要透過 RPC Service 使檔案能夠共享於網路中，NFS 的好處是它支援了不同系統與機器的溝通能力，使資料能夠很輕易透過網路共享給別人。 安裝與設定首先在 NFS 節點安裝以下套件： $ sudo apt-get -y install nfs-kernel-server 編輯/etc/idmapd.conf設定檔，然後設定 Domain： Domain = kyle.bai.example 接著編輯/etc/exports檔案，加入以下內容： /var/nfs/images 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check)/var/nfs/vms 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check)/var/nfs/volumes 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check) 然後重新啟動 NFS Server，如以下指令： $ sudo /etc/init.d/nfs-kernel-server restart 接著到 Client 端，安裝 NFS 工具： $ sudo apt-get -y install nfs-common 編輯/etc/idmapd.conf設定檔，然後設定 Domain： Domain = kyle.bai.example 然後透過以下指令來掛載使用： $ sudo mount -t nfs kyle.bai.example:/var/nfs/images /var/nfs/images 完成後，透過以下指令來檢查： $ df -hTFilesystem Type Size Used Avail Use% Mounted onudev devtmpfs 7.9G 8.0K 7.9G 1% /devtmpfs tmpfs 1.6G 776K 1.6G 1% /run/dev/sda1 ext4 459G 8.3G 427G 2% /none tmpfs 4.0K 0 4.0K 0% /sys/fs/cgroupnone tmpfs 5.0M 0 5.0M 0% /run/locknone tmpfs 7.9G 0 7.9G 0% /run/shmnone tmpfs 100M 0 100M 0% /run/user10.0.0.61:/var/nfs/images nfs4 230G 5.1G 213G 3% /var/nfs/images 編輯/etc/fstab檔案來提供開機掛載： 10.0.0.61:/var/nfs/vms /var/lib/nova/instances nfs defaults 0 0 也可以安裝自動掛載工具，透過以下指令安裝： $ sudo apt-get -y install autofs 編輯/etc/auto.master檔案，加入以下內容到最後面： /- /etc/auto.mount 然後編輯/etc/auto.mount檔案，設定以下內容： # create new : [mount point] [option] [location] /mntdir -fstype=nfs,rw kyle.bai.example:/home 建立掛載用目錄： $ sudo mkdir /mntdir 啟動 auto-mount 服務： $ sudo initctl restart autofs 完成後透過以下方式檢查： $ cat /proc/mounts | grep mntdir Cinder 使用 NFSOpenStack Cinder 也支援了 NFS 的驅動，因此只需要在/etc/cinder/cinder.conf設定以下即可： [DEFAULT]...enabled_backends = nfs[nfs]nfs_shares_config = /etc/cinder/nfs_sharesvolume_driver = cinder.volume.drivers.nfs.NfsDrivervolume_backend_name = nfs-backendnfs_sparsed_volumes = True 建立 Cinder backend 來提供不同的 Backend 的使用： $ cinder type-create TYPE$ cinder type-key TYPE set volume_backend_name=BACKEND","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"File System","slug":"File-System","permalink":"https://k2r2bai.com/tags/File-System/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://k2r2bai.com/tags/OpenStack/"},{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"}]},{"title":"Ceph FS 基本操作","slug":"ceph/cephfs","date":"2015-11-21T09:08:54.000Z","updated":"2019-12-02T01:49:42.377Z","comments":true,"path":"2015/11/21/ceph/cephfs/","link":"","permalink":"https://k2r2bai.com/2015/11/21/ceph/cephfs/","excerpt":"Ceph FS 底層的部分同樣是由 RADOS(OSDs + Monitors + MDSs) 提供，在上一層同樣與 librados 溝通，最上層則是有不同的 library 將其轉換成標準的 POSIX 檔案系統供使用。","text":"Ceph FS 底層的部分同樣是由 RADOS(OSDs + Monitors + MDSs) 提供，在上一層同樣與 librados 溝通，最上層則是有不同的 library 將其轉換成標準的 POSIX 檔案系統供使用。 建立一個 Ceph File System首先將一個叢集建立完成，並提供 Metadata Server Node 與 Client，建立 Client 可以透過以下指令： $ ceph-deploy install &lt;myceph-client&gt; 建立 MDS 節點可以透過以下指令： $ ceph-deploy mds create mds-node 當 Ceph 叢集已經提供了MDS後，可以建立 Data Pool 與 Metadata Pool： $ ceph osd pool create cephfs_data 128$ ceph osd pool create cephfs_metadata 128 How to judge PG number： Less than 5 OSDs set pg_num to 128 Between 5 and 10 OSDs set pg_num to 512 Between 10 and 50 OSDs set pg_num to 4096 If you have more than 50 OSDs, you need to understand the tradeoffs and how to calculate the pg_num value by yourself 完成 Pool 建立後，我們將儲存池拿來給 File System 使用，並建立檔案系統： $ ceph fs new cephfs cephfs_metadata cephfs_data 取得 Client 驗證金鑰： $ cat /etc/ceph/ceph.client.admin.keyring[client.admin] key = AQC/mo9VxqsXDBAAQ/LQtTmR+GTPs65KBsEPrw== 建立，並儲存到檔案admin.secret： AQC/mo9VxqsXDBAAQ/LQtTmR+GTPs65KBsEPrw== 檢查 MDS 與 FS： $ ceph fs ls$ ceph mds stat 建立 Mount 用目錄，並且 Mount File System： $ sudo mkdir /mnt/mycephfs$ sudo mount -t ceph &#123;ip-address-of-monitor&#125;:6789:/ /mnt/mycephfs/ -o name=admin,secretfile=admin.secret 檢查系統 DF 與 Mount 結果： $ sudo df -l$ sudo mount 使用CEPH檔案系統時，要注意是否安裝了元資料伺服器(Metadata Server)。且請確認CEPH版本為是0.84之後的版本。 Ceph Filesystem FUSE (File System in User Space)首先在MDS節點上安裝ceph-fuse 套件： $ sudo apt-get install -y ceph-fuse 完成後，我們就可以Mount起來使用： $ sudo mkdir /mnt/myceph-fuse$ sudo ceph-fuse -m &#123;ip-address-of-monitor&#125;:6789 /mnt/myceph-fuse 當 Mount 成功後，就可以到該目錄檢查檔案。 FUSE：使用者空間檔案系統（Filesystem in Userspace，簡稱FUSE）是作業系統中的概念，指完全在使用者態實作的檔案系統。目前Linux通過內核模組對此進行支援。一些檔案系統如ZFS，glusterfs和lustre使用FUSE實作。","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"File System","slug":"File-System","permalink":"https://k2r2bai.com/tags/File-System/"}]},{"title":"使用 ceph-deploy 工具部署 Ceph 叢集","slug":"ceph/deploy/ceph-deploy","date":"2015-11-20T09:08:54.000Z","updated":"2019-12-02T01:49:42.378Z","comments":true,"path":"2015/11/20/ceph/deploy/ceph-deploy/","link":"","permalink":"https://k2r2bai.com/2015/11/20/ceph/deploy/ceph-deploy/","excerpt":"本節將介紹如何透過 ceph-deploy 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台Monitor與三台OSD。而 MDS 則是當使用到 CephFS 的時候才需要部署。","text":"本節將介紹如何透過 ceph-deploy 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台Monitor與三台OSD。而 MDS 則是當使用到 CephFS 的時候才需要部署。 環境準備在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有 6 台節點，叢集拓樸圖如下所示： +------------------+ | [ Deploy Node ] | | 10.21.20.99 | | Ceph deploy tool | +--------+---------+ | | +------------------+ | +-----------------+ | [ Admin Node ] | | |[ Monitor Node ]| | 10.21.20.100 |-----------+-----------| 10.21.20.101 | | Ceph admin ops | | | Ceph mon Node | +------------------+ | +-----------------+ | +---------------------------+--------------------------+ | | | | | |+-------+----------+ +--------+---------+ +--------+---------+| [ OSD Node 1 ] | | [ OSD Node 2 ] | | [ OSD Node 3 ] || 10.21.20.121 +-------+ 10.21.20.122 +-------+ 10.21.20.123 || Object Storage | | Object Storage | | Object Storage || Disk * 2 | | Disk * 2 | | Disk * 2 |+------------------+ +------------------+ +------------------+ P.S. 上面磁碟分為兩個，因為這邊教學不將 journal 分開來，故一顆當作系統使用，一顆為資料儲存與 journal 使用。P.S. 這邊網路建議設定為static，若有支援 Jumbo frame 也可以開啟。 首先在每一台節點新增以下內容到/etc/hosts： 127.0.0.1 localhost10.21.20.99 ceph-deploy10.21.20.100 ceph-admin10.21.20.101 ceph-mon110.21.20.121 ceph-osd110.21.20.122 ceph-osd210.21.20.123 ceph-osd3 然後在每台節點執行以下指令來使sudo不需要輸入密碼： $ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 上面 ubuntu 是節點的使用者名稱，這邊都是一樣。P.S. 當然若要注意安全考量，而不讓該使用者直接使用有權限的資源，可以使用 root user。 接著在設定Deploy節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行： $ ssh-keygen -t rsa$ ssh-copy-id ceph-mon1$ ssh-copy-id ceph-mds... 若 Deploy 節點的使用者與其他不同的話，編輯~/.ssh/config加入以下內容： Host ceph-admin Hostname ceph-admin User ubuntuHost ceph-mds Hostname ceph-mds User mds... 之後在Deploy節點安裝部署工具，首先安裝基本相依套件，使用apt-get來進行安裝，再透過python-pip進行安裝部署工具： $ sudo apt-get install -y python-pip$ sudo pip install ceph-deploy P.S. ceph-deploy 所安裝的 ceph 版本，會受到 ceph-deply 工具版本不同而有所差異。 完成後即可開始部署 Ceph 環境。 環境部署首先建立一個名稱為mycluster的目錄，並進入該目錄： $ sudo mkdir mycluster$ cd mycluster 採用 ceph-deploy 工具部署環境時，需要依照以下步驟進行，首先建立要當任 Monitor 的節點，透過以下方式： $ ceph-deploy new ceph-mon1 &lt;other_nodes&gt; 當執行該指令時，不是直接讓 ceph-mon1 節點安裝成為 Monitor，而只是新增一個conf，並標示誰是 Monitor，當在初始化階段時，才將該設定檔給對應節點，讓它啟動是設定為 Monitor 角色。 接著我們需要先讓每個節點（ceph-deploy除外）安裝 ceph-common 套件，透過以下方式安裝： $ ceph-deploy install ceph-admin ceph-mds &lt;other_nodes&gt; 當完成安裝後，才能開始真正的部署節點的角色，第一先將 Monitor 都完成部署，才能讓叢集先正常被運作，透過以下指令來將 Monitors 初始化： $ ceph-deploy mon create-initial 上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行： $ ceph-deploy osd prepare ceph-osd1:/dev/sdb &lt;other_nodes&gt;:&lt;data_disk&gt; 若要將 journal 分離，可以使用以下方式： $ ceph-deploy osd prepare ceph-osd1:/dev/sdb:/dev/sdc &lt;other_nodes&gt;:&lt;data_disk&gt;:&lt;journal_disk&gt; 部署沒有問題的話，即可啟用該 OSD： $ ceph-deploy osd activate ceph-osd1:/dev/sdb &lt;other_nodes&gt;:&lt;data_disk&gt; P.S. 較新的版本該指令可以省略，因為在準備期間就會幫你直接啟動。 這樣一個簡單的叢集就部署完成了，這時候我們可以隨需求加入admin與MDS節點，可以透過以下方式進行： $ ceph-deploy admin ceph-admin$ ceph-deploy mds create ceph-mds 完成後，可以透過以下指令檢查 ceph 叢集狀態： $ ceph healthHEALTH_OK$ ceph statuscluster e2432059-e219-4555-8d37-c32d5b16e4a4 health HEALTH_OK monmap e1: 1 mons at &#123;ceph-mon1=10.21.20.101:6789/0&#125; election epoch 6, quorum 0, ceph-mon1 osdmap e119: 3 osds: 3 up, 3 in flags sortbitwise pgmap v813: 128 pgs, 2 pools, 91289 kB data, 22856 objects 691 MB used, 2152 GB / 2152 GB avail 128 active+clean 如果出現ERROR: missing keyring, cannot use cephx for authentication，請注意這個檔案/etc/ceph/ceph.client.admin.keyring是否有權限讀取。 如果出現too few PGs per，修改pg_num與pgp_num。範例如下： $ ceph osd pool set rbd pg_num 128$ ceph osd pool set rbd pgp_num 128 若要檢查 mds 可以使用以下指令： $ ceph mds stat 若想檢查 OSDs 的目前狀態可以使用以下幾個指令： $ ceph osd statosdmap e119: 3 osds: 3 up, 3 in flags sortbitwise$ ceph osd dump$ ceph osd tree 最後如果進行多台 Monitor 的部署的話，要注意讓這些節點的時間同步。Ceph 使用多 Monitort 來避免單點故障問題，部署的比例可自行定義，比如 1 台、3:1 台、5:3 台等等。","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"}]},{"title":"Ceph 分散式儲存系統介紹","slug":"ceph/introduction","date":"2015-11-19T09:08:54.000Z","updated":"2019-12-02T01:49:42.378Z","comments":true,"path":"2015/11/19/ceph/introduction/","link":"","permalink":"https://k2r2bai.com/2015/11/19/ceph/introduction/","excerpt":"Ceph 提供了Ceph 物件儲存以及Ceph 區塊儲存，除此之外 Ceph 也提供了自身的Ceph 檔案系統，所有的 Ceph 儲存叢集的部署都開始於 Ceph 各節點，透過網路與 Ceph 的叢集溝通。最簡單的 Ceph 儲存叢集至少要建立一個 Monitor 與兩個 OSD（Object storage daemon），但是當需要運行 Ceph 檔案系統時，就需要再加入Metadata伺服器。","text":"Ceph 提供了Ceph 物件儲存以及Ceph 區塊儲存，除此之外 Ceph 也提供了自身的Ceph 檔案系統，所有的 Ceph 儲存叢集的部署都開始於 Ceph 各節點，透過網路與 Ceph 的叢集溝通。最簡單的 Ceph 儲存叢集至少要建立一個 Monitor 與兩個 OSD（Object storage daemon），但是當需要運行 Ceph 檔案系統時，就需要再加入Metadata伺服器。 Ceph 目標開發檔案系統是一種複雜的投入，但是如果能夠準確地解決問題的話，則擁有著不可估量的價值。我們可以把 Ceph 的目標可以簡單定義為以下： 容易擴充到 PB 級別的儲存容量 在不同負載情況下的高效能（每秒輸入/輸出操作數[IPOS]、帶寬） 高可靠性 但這些目標彼此間相互矛盾(例如:可擴充性會減少或阻礙效能，或影響可靠性)。 Ceph 開發了一些有趣的概念(例如動態 metadata 分區、資料分散、複製)。 Ceph 的設計也實作了容錯性來防止單一節點故障問題（SOPF），並假設，大規模（PB 級）中儲存的故障是一種常態，而非異常。最後，它的設計沒有假設特定的工作負荷，而是包含了可變的分散式工作負荷的適應能力，從而提供最佳的效能。它以POSIX 兼容為目標完成這些工作，允許它透明的部署於那些依賴於 POSIX 語義上現有的應用(通過Ceph增強功能)。最後，Ceph 是開源分散式儲存和 Linux 主流核心的一部分。 其中最底層的 RADOS 是由 OSD、Monitor 與 MDS 三種所組成。 Ceph 架構現在，讓我們先在上層探討 Ceph 架構及其核心元素。之後深入到其它層次，來解析 Ceph 的一些主要方面，從而進行更詳細的分析。 Ceph 生態系統可以大致劃分為四部分（圖1）： 客戶端 (資料使用者) metadata 伺服器 (快取及同步分散的metadata) 物件儲存叢集 (以物件方式儲存資料與 metadata，實現其它主要職責) 叢集監控 (實現監控功能) 如圖所示，客戶使用 metadata 伺服器，執行 metadata 操作(來確定資料位置)。metadata 伺服器管理資料位置，以及在何處儲存取新資料。值得注意的是，metadata 儲存在一個儲存叢集上（標記為 “metadata I/O”）。實際的檔案 I/O 發生在客戶和物件儲存叢集之間。這樣一來，提供了更高層次的 POSIX 功能(例如，開啟、關閉、重新命名)就由 metadata 伺服器管理，不過 POSIX 功能（例如讀和寫）則直接由物件儲存叢集管理。 上面分層視圖說明了，一系列伺服器透過一個客戶端介面存取 Ceph 生態圈系統，這就明白了 metadata 伺服器和物件級儲存之間的關係。分散式儲存系統可以在一些層面中查看，包括一個儲存設備的格式（Extent and B-tree-based Object File System [EBOFS]或者一個備選），還有一個設計用於管理資料複製、故障檢測、復原，以及隨後資料遷移的覆蓋管理層，叫做Reliable Autonomic Distributed Object Storage（RADOS）。最後，監視器用於區別元件中的故障，包括隨後的通知。 Ceph 元件成員簡單的 Ceph 生態系統了解 Ceph 概念架構後，我們來探討另一個層次，了解在 Ceph 中實現的主要元件。 Ceph 和傳統檔案系統之間的重要差異之一，就是智能部分都用在了生態環境，而不是檔案系統本身。 圖中顯示了一個簡單的Ceph生態系統。Ceph Client 是 Ceph 檔案系統的用戶。Ceph Metadata Daemon 提供了 metadata 伺服器，而 Ceph Object Storage Daemon 提供了實際儲存（對資料和 metadata 兩者）。最後 Ceph Monitor 提供了叢集管理。要注意的是 Ceph 客戶，物件儲存端點，metadata 伺服器（根據檔案系統的容量）可以有許多，而且至少有一對冗餘的監視器。那麼這個檔案系統是如何分散的呢？ Ceph Client因為Linux顯示檔案系統的一個共有介面（通過虛擬檔案系統交換機[VFS]），Ceph 的用戶透視圖就是透明的。然而管理者的透視圖肯定是不同的，考慮到很多伺服器會包含儲存系統這一潛在因素（要查看更多建立的 Ceph 叢集的資訊，可看參考資料部分）。從用戶的角度來看，存取大容量的儲存系統，卻不知道下面聚合成一個大容量的儲存池的 metadata 伺服器、監視器、還有獨立的對象儲存裝置。用戶只是簡單地看到一個安裝點，在這點上可以執行標准檔案 I/O。 P.S 核心或使用者空間 : 早期版本的 Ceph 利用在 User SpacE（FUSE）的 Filesystems，它把文件系統推入到用戶空間，還可以很大程度上簡化其開發。但是今天，Ceph 已經被集成到主線內核，使其更快速，因為用戶空間上下文交換機對文件系統 I/O 已經不再需要。 Ceph Metadata Server（MDS）作為處理 Ceph File System 的 metadata 之用，若僅使用 Block or Object storage，就不會用到這部分功能。 P.S 若只使用 Ceph Object Storage 與 Ceph Block Device 的話，將不需要部署 MDS 節點。 Ceph Monitor（MON）Ceph 包含實施叢集映射管理的監控者，但是故障管理的一些要素是在物件儲存本身執行的。當物件儲存裝置發生故障或者新裝置加入時，監控者就會檢測和維護一個有效的叢集映射。這個功能會按一種分散式的方法執行，這種方式中映射升級可以和當前的流量溝通。Ceph 使用Paxos，它是一系列分散式共識演算法。 在每一個 Ceph 儲存中，都有一到多個 Monitor 存在，為了有效的在分散式的環境下存取檔案，每一個 Ceph Monitor daemon 維護著很多份與 叢集相關的映射資料(map)，包含： Monitor map：包含叢集的 fsid 、位置、名稱、IP 位址和 Port，也包括目前 epoch、此狀態圖何時建立與最近修改時間。 OSD map：包含叢集 fsid 、此狀態圖何時建立、最近修改時間、儲存池（Pools）列表 、副本數量、放置群組（PG）數量、 OSD 列表與其狀態（如 up 、 in ） Placement Group map：包含放置群組版本、其時間戳記、最新的 OSD epoch、佔用率以及各放置群組詳細，如放置群組 ID 、 up set 、 acting set 、 PG 狀態（如 active+clean），和各儲存池的資料使用情況統計。 CRUSH map： 包含儲存裝置列表、故障域樹狀結構（如裝置、主機、機架、row、機房等等），以及儲存資料時如何利用此樹狀結構的規則。 MDS map：包含當前 MDS map 的 epoch、建立於何時與最近修改時間，還包含了儲存 metadata 的儲存池、metadata 伺服器列表、還有哪些 metadata 伺服器是 up 且in 的。 主要是用來監控 Ceph 儲存叢集上的即時狀態，確保 Ceph 可以運作無誤。 Ceph Object Storage Daemon（OSD）與傳統的物件儲存類似，Ceph 儲存節點不僅包括儲存，還包含了智能容錯與恢復。傳統的驅動是只響應來自啟動者下達的命令中簡單目標。但是物件儲存裝置是智能裝置，它能作為目標和啟動者，支持與其他物件儲存裝置的溝通與合作。 實際與 Client 溝通進行資料存取的即為 OSD。每個 object 被儲存到多個 PG（Placement Group）中，每個 PG 再被存放到多個 OSD 中。為了達到 Active + Clean 的狀態，確認每份資料都有兩份的儲存，每個 Ceph 儲存叢集中至少都必須要有兩個 OSD。 主要功能為實際資料（object）的儲存，處理資料複製、恢復、回填（backfilling, 通常發生於新 OSD 加入時）與重新平衡（發生於新 OSD 加入 CRUSH map 時），並向 Monitor 提供鄰近 OSD 的心跳檢查資訊。 CRUSHCeph 透過 CRUSH（Controlled, Scalable, Decentralized Placement of Replicated Data） 演算法來計算資料儲存位置，來確認如何儲存和檢索資料，CRUSH 授權 Ceph client 可以直接連接 OSD，而不再是透過集中式伺服器或者中介者來儲存與讀取資料。該演算法與架構特性使 Ceph 可以避免單一節點故障問題、效能瓶頸與擴展的限制。 CRUSH 是一種偽隨機資料分散算法，能夠再階層結構的儲存叢集很好地分散物件的副本，該演算法實作了偽隨機(確定性)的函式，會透過輸入o​​bject id、object group id等參數，來回傳一組儲存裝置（即儲存物件的副本）。CRUSH 有以下兩個重要的資訊： Cluster Map：被用來描述儲存叢集的階層結構。 CRUSH Rule：被用來描述副本的分散策略。 CRUSH 主要提供了以下功能： 將資料有效率的存放在不同的儲存裝置中。 即使移除整個 cluster中的儲存裝置，也不會影響到資料正常的存取。 不需要有任何主要的管理裝置(or 節點)來做為控管之用。 可依照使用者所定義的規則來處理資料分散的方式。 透過以上資訊，CRUSH 可以將資料分散存放在不同的儲存實體位置，並避免單點錯誤造成資料無法存取的狀況發生。 PoolsPool 是儲存物件邏輯分區。Ceph Client 從 Monitor 取得 Cluster map，並把物件寫入 Pool。Pool 的副本數、CRUSH Ruleset 和 PG 數量決定著 Ceph 該如何放置資料。 一個 Pool 可以設定許多參數，但最少需要正確設定以下資訊： 物件擁有權與存取權限 Placement Group 數量 CRUSH Ruleset 設定與使用 Placement GroupCeph 把物件映射到放置群組（PG），PG 是一種邏輯的物件 Pool 片段，這些物件會組成一個群組後，再存到 OSD 中。PG 減少了各物件存入對應的 OSD 時 metadata 的數量，更多的 PG（如：每個OSD 有 100 個群組）可以使負載平衡更好。 P.S 多個 PG 也可以對應到同一個 OSD，因此 PG 與 OSD 其實是種多對多的關係。 PG ID 計算當 Client 在綁定某個 Monitor 時，會先取得最新的 Cluster map 副本，該 map 可讓 Client 端知道叢集有多少 Monitor、OSD 與 MDS。但是無法知道要存取的物件位置。 在 Ceph 中，物件的位置是透過計算得知的。因此 Client 只需要傳入 Object id 與 Pool 即可知道物件位置。當 Client 需要有名稱物件（如 mysql、archive 等）時，Ceph 會用物件名稱來計算 PG（是一個 Hash value）、 OSD 編號與 Pool。流程如下： Client 輸入 pool id 與 object id（e.g., pool=’vms’, object id=’instance-1’） CRUSH 取得 object id，並進行 Hash 取得值 CRUSH 在以 OSD 數量對 Hash value 進行 mod 運算，來取得 pg id（e.g., 58） CRUSH 再透過取得 pool name 來取得 pool id（e.g., ‘vms’ = ‘4’）5* CRUSH 在把 pool id 加到 pg id 前面（e.g, 4.58） 透過計算物件位置的方式，改善了傳統查詢定位的效能問題。CRUSH 演算法讓 Client 計算物件應該存放到哪裡，並連接該 Primary OSD 進行物件儲存與檢索。 Ceph 三大儲存服務系統特性與功能Ceph 是一個統一的儲存系統，提供了物件、區塊與檔案系統功能，且擁有高可靠、高擴展。該三大儲存服務功能與特性如下： CEPH 物件儲存 CEPH 區塊裝置 CEPH 檔案系統 RESTful 介面 精簡空間配置 與 POSIX 語意相容 與 S3 和 Swift 相容的 API 映像檔大小最大支援 16 EB（exabytes） Metadata 與資料分離 S3 風格的子域名 可組態的等量化（Configurable striping） 動態重新平衡(Dynamic rebalancing) 統一的 S3/Swift 命名空間 記憶體快取 子目錄快照 使用者管理 快照 可組態的等量化（Configurable striping） 利用率追蹤 寫入時複製 cloning 核心驅動的支援 等量化物件（Striped objects） 支援 KVM 與 libvirt 支援使用者空間檔案系統（FUSE） 雲端解決方案整合 可作為雲端解決方案的後端 可作為 NFS/CIFS 部署 多站點部署 累積備份 可用於 Hadoop 上（替代 HDFS ） 災難復原 參考 Ceph中文文件 小信豬 Ceph Ceph 官方文件 Ceph Development Ceph 整合案例 Ceph Book","categories":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/categories/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://k2r2bai.com/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://k2r2bai.com/tags/Storage/"},{"name":"Distribution System","slug":"Distribution-System","permalink":"https://k2r2bai.com/tags/Distribution-System/"}]},{"title":"Apache Cassandra 分散式資料庫","slug":"data-engineer/apache-cassandra","date":"2015-11-17T09:08:54.000Z","updated":"2019-12-02T01:49:42.382Z","comments":true,"path":"2015/11/17/data-engineer/apache-cassandra/","link":"","permalink":"https://k2r2bai.com/2015/11/17/data-engineer/apache-cassandra/","excerpt":"Cassandra 是最初由 Facebook 開發，之後貢獻給 Apache 基金會維護的分散式 NoSQL 資料庫系統，一般被認為 Amazon Dyname 與 Google BigTable 的結合體，主要是分散性像 Dynamo，然而資料模型卻如 BigTable。目前有多家公司採用，可運行上千台節點來提供超過 200 TB 的資料。 Cassandra 擁有幾個特點，也因為這些特點讓許多人選擇使用該資料庫，以下幾個項目簡單列出其特點： 完全去中心化，且不是主從架構的備份 統一類型的節點 以 P2P 協定串連起網路，清除 SPOF（Single Point Of Failure）問題 高擴展性，新增與刪除節點容易 可呼叫的一致性，並支援強一致性與弱一致性 支援跨區域的叢集架構 每個區域儲存一份完整資料，來提供存取局部性、容錯與災難復原 寫入效能理論上比讀取好（但近期有證實現在讀取也很不錯），適合串流資料儲存 比 HBase 的隨機存取效能要好上許多，但不擅長區間掃描 可作為 HBase 的即時查詢快取。","text":"Cassandra 是最初由 Facebook 開發，之後貢獻給 Apache 基金會維護的分散式 NoSQL 資料庫系統，一般被認為 Amazon Dyname 與 Google BigTable 的結合體，主要是分散性像 Dynamo，然而資料模型卻如 BigTable。目前有多家公司採用，可運行上千台節點來提供超過 200 TB 的資料。 Cassandra 擁有幾個特點，也因為這些特點讓許多人選擇使用該資料庫，以下幾個項目簡單列出其特點： 完全去中心化，且不是主從架構的備份 統一類型的節點 以 P2P 協定串連起網路，清除 SPOF（Single Point Of Failure）問題 高擴展性，新增與刪除節點容易 可呼叫的一致性，並支援強一致性與弱一致性 支援跨區域的叢集架構 每個區域儲存一份完整資料，來提供存取局部性、容錯與災難復原 寫入效能理論上比讀取好（但近期有證實現在讀取也很不錯），適合串流資料儲存 比 HBase 的隨機存取效能要好上許多，但不擅長區間掃描 可作為 HBase 的即時查詢快取。 若要瞭解更多 Cassandra 可以閱讀 Cassandra Wiki 。 部署多節點叢集本節將安裝一個簡單的 Cassandra 叢集，來提供 NoSQL 資料庫以存取資料，以下為節點配置： IP Address HostName 172.17.0.2 cassandra-1 172.17.0.3 cassandra-2 172.17.0.4 cassandra-3 首先需在每個節點安裝 Java，這邊採用 Oracle 的 Java 來進行安裝： $ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections$ sudo apt-get -y install oracle-java7-installer 若要安裝java8請修改成oracle-java8-installer。 接著在各節點安裝 Cassandra 套件，這邊採用apt-get安裝，首先加入 Repository： $ echo \"deb http://www.apache.org/dist/cassandra/debian 22x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list 這邊安裝2.2.x版本，若要其他版本則修改22x，如改為21x。 為了避免軟體套件更新軟體時有簽證警告，需加入 Apache 基金會與套件資源庫相關的三個公有金鑰： $ gpg --keyserver pgp.mit.edu --recv-keys F758CE318D77295D$ gpg --export --armor F758CE318D77295D | sudo apt-key add -$ gpg --keyserver pgp.mit.edu --recv-keys 2B5C1B00$ gpg --export --armor 2B5C1B00 | sudo apt-key add -$ gpg --keyserver pgp.mit.edu --recv-keys 0353B12C$ gpg --export --armor 0353B12C | sudo apt-key add - 完成後更新 apt-get Repository： $ sudo apt-get update 安裝 Cassandra NoSQL 於每個節點上： $ sudo apt-get install -y cassandra 完成後，我們必須開始配置各節點來組成一個叢集，先把每個節點的 Cassandra 服務關閉： $ sudo service cassandra stop 關閉後，在每個節點編輯/etc/cassandra/cassandra.yaml檔案，並修改一下內容： cluster_name: 'examples' num_tokens: 256 seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"172.17.0.2, 172.17.0.3, 172.17.0.4\"listen_address: 172.17.0.2 broadcast_address: 172.17.0.2rpc_address: 0.0.0.0 broadcast_rpc_address: 172.17.0.2 P.S. 這邊要注意不同節點會有不同，如listen_address與seeds等。 都完成設定檔後，就可以重啟每台的 Cassandra 服務： $ sudo service cassandra restart 當確認所有節點重新啟動後，在其中一個節點建立 keyspaces 來做 replication： $ cqlshcqlsh&gt; create keyspace Spark_smack WITH REPLICATION = &#123; 'class' : 'SimpleStrategy', 'replication_factor' : '3' &#125;; exit Spark_smack 該值是可以依個人名稱修改。 建立完成後，可以使用nodetool指令來檢查是否成功： $ nodetool status spark_smackDatacenter: datacenter1=======================Status=Up/Down|/ State=Normal/Leaving/Joining/Moving-- Address Load Tokens Owns (effective) Host ID RackUN 172.17.0.3 255.68 KB 256 100.0% 9c9eb117-f787-47bd-825f-3daf49eba489 rack1UN 172.17.0.2 252.03 KB 256 100.0% 7a4adb77-42d1-402f-b57b-4a40ad013e2c rack1UN 172.17.0.4 127.2 KB 256 100.0% 5823fd78-45f2-4328-9470-f1053bb3fc3b rack1 驗證系統我們透過 Client 程式來連到cassandra-1節點來驗證叢集是否建立成功： $ cqlshcqlsh&gt; use &lt;keyspace name&gt;; 這邊&lt;keyspace name&gt;範例為 spark_smack。 然後建立一個資料表： CREATE TABLE emp( emp_id int PRIMARY KEY, emp_name text, emp_city text, emp_sal varint, emp_phone varint ); 接著插入一筆資料到資料表： INSERT INTO emp (emp_id, emp_name, emp_city ) VALUES(1, 'Kyle', 'Taichung' ); 若都沒有任何錯誤的話，現在連接到其他節點來檢查是否有同步資料： $ cqlshcqlsh&gt; use spark_smack;cqlsh&gt; select * from emp; emp_id | emp_city | emp_name | emp_phone | emp_sal--------+----------+----------+-----------+--------- 1 | Taichung | Kyle | null | null P.S 若發生如以下錯誤，請依照指示解決： Connection error: ('Unable to connect to any servers', &#123;'127.0.0.1': error(111, \"Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\")&#125;) 首先檢查設定檔/etc/cassandra/cassandra.yaml裡面的rpc_address是否為0.0.0.0。 最後，若要復原與備份可以閱讀 Cassandra Backup and Recovery。 參考資源 Cassandra简介 Install and Configure a 3 node Cassandra Cluster on Ubuntu 14.04 關於 Cassandra 的錯誤觀點","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Database","slug":"Database","permalink":"https://k2r2bai.com/tags/Database/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://k2r2bai.com/tags/NoSQL/"}]},{"title":"Kuberentes 是什麼？","slug":"cncf/kubernetes","date":"2015-11-10T09:08:54.000Z","updated":"2019-12-02T01:49:42.379Z","comments":true,"path":"2015/11/10/cncf/kubernetes/","link":"","permalink":"https://k2r2bai.com/2015/11/10/cncf/kubernetes/","excerpt":"Kubernetes 是 Google 的開源專案，主要用於管理跨主機容器化叢集系統。該專案脫胎於 Google 內部叢集管理工具 Borg，早期主要貢獻者更是參與 Borg 專案的人員，大概基本上都認為 Kubernetes 許多概念與架構是來至 Google 十餘年的設計、部署、管理大規模容器的經驗。 Kubernetes 在 Docker 叢集管理中是很重要的一員，其實作了許多功能，包含應用程式部署、叢集節點擴展、自動容錯機制等，並且能夠統一管理跨主機的管理。其架構如下圖所示。","text":"Kubernetes 是 Google 的開源專案，主要用於管理跨主機容器化叢集系統。該專案脫胎於 Google 內部叢集管理工具 Borg，早期主要貢獻者更是參與 Borg 專案的人員，大概基本上都認為 Kubernetes 許多概念與架構是來至 Google 十餘年的設計、部署、管理大規模容器的經驗。 Kubernetes 在 Docker 叢集管理中是很重要的一員，其實作了許多功能，包含應用程式部署、叢集節點擴展、自動容錯機制等，並且能夠統一管理跨主機的管理。其架構如下圖所示。 使用 Kubernetes 的好處，網路上已有許多相關資訊，這邊列舉幾個： 輕易的 Scale out/in 容器 自動化容錯與擴展，如容器的部署與副本 以多個容器組成群組，並提供容器負載平衡 容易擴展節點 以叢集的方式來管理跨機器的容器。 基於 Docker 的應用程式封裝、實例化與執行。 兩種節點角色： Node：是運行 k8s worker 的實體或者虛擬機器，一般稱為 Minion 節點。該節點會運作幾個 k8s 關鍵元件： kubelet：為 master 節點的 agent。 kube-proxy： Sevice 使用其將服務請求連接路由到 Pod 的容器中。 docker engine（或 rocket）：主要建立容器的引擎。 Master：每一個 k8s 叢集都會有一個或多個 Master，主要提供 REST APIs、管理 k8s 工具、運行 Etcd 、Scheduler 以及 Pod 的 Replication Controller 等服務。 如架構圖所示，Kubernetes 由多個元件組合而成，然而在了解 Kubernetes 元件之前，我們需要先知道 Kubernetes 的生態圈中的一些重要概念，這些概念將影響對於 Kubernetes 進一步的探索，其主要概念如下： Pods（po）：是 k8s 的最基本執行單元，即包含一組容器與 Volumes。在同一個 Pod 裡的容器會共享使用一個網路命名空間，並利用 localhost 溝通，Pod 生命週期是短暫的。 Services（svc）：由於 k8s 的 Pods 中的容器 IP 會隨著排程、故障等因素而改變，因此 k8s 利用 Service 來定義一系列存取這些 Pod 應用程式服務的抽象層。Service 透過 Proxy 的 port 與 Selector 來決定服務請求要傳送給後端哪些 Pods 中的容器，因此對外是單一的存取介面，。 Replication Controllers（rc）：主要確保 k8s 叢集所指定 Pod 中的容器被正常運作的副本數量，當正在執行的容器發生故障，而少於副本數時，Replication Controller 會在任一節點啟動一個新的容器，然後若是多於副本數量則會自動移除一個容器。 Lable：被用來區分 Pod、Service 與 Replication Controllers 的 Key/Vaule 標籤，用來傳遞使用者定義的屬性。Label 是 rc 與 svc 的執行基礎，rc 透過標示容器 label 來讓 svc 服務請求正確選擇要存取容器。","categories":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/categories/CNCF/"}],"tags":[{"name":"CNCF","slug":"CNCF","permalink":"https://k2r2bai.com/tags/CNCF/"},{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://k2r2bai.com/tags/Kubernetes/"}]},{"title":"Docker 基礎介紹","slug":"container/docker-introduction","date":"2015-11-09T09:08:54.000Z","updated":"2019-12-02T01:49:42.380Z","comments":true,"path":"2015/11/09/container/docker-introduction/","link":"","permalink":"https://k2r2bai.com/2015/11/09/container/docker-introduction/","excerpt":"Docker 是一個開源的專案，主要提供容器化應用程式的部署與自動化的管理，透過部署 Docker engine 於 Linux 作業系統上，提供軟體抽象層以及作業系統層虛擬化自動管理機制。Docker 使用一些 Linux Kernel 中的軟體，如 Control Groups（cgroups）、namespaces（核心命名空間）、Another UnionFS 等來達到建置獨立的環境與存取 CPU、Memory 以及網路資源等，最終提供一個輕量級的虛擬化。Docker 的發展非常迅速，從 0.9.0 版本後，採用自己開發的 Libcontainer 函式庫作為直接存取由 Linux Kernel 提供的虛擬化基礎設施。Docker 也支援了多平台，從個人電腦到私有、公有雲都能夠進行使用與部署輕量級虛擬化。 Docker 並非一個新興技術，而是過往的 Linux Container 技術的基礎上進行封裝。因此使用者只需要關注應用程式如何被部署與執行就好，而不用管容器是如何被建置出來。","text":"Docker 是一個開源的專案，主要提供容器化應用程式的部署與自動化的管理，透過部署 Docker engine 於 Linux 作業系統上，提供軟體抽象層以及作業系統層虛擬化自動管理機制。Docker 使用一些 Linux Kernel 中的軟體，如 Control Groups（cgroups）、namespaces（核心命名空間）、Another UnionFS 等來達到建置獨立的環境與存取 CPU、Memory 以及網路資源等，最終提供一個輕量級的虛擬化。Docker 的發展非常迅速，從 0.9.0 版本後，採用自己開發的 Libcontainer 函式庫作為直接存取由 Linux Kernel 提供的虛擬化基礎設施。Docker 也支援了多平台，從個人電腦到私有、公有雲都能夠進行使用與部署輕量級虛擬化。 Docker 並非一個新興技術，而是過往的 Linux Container 技術的基礎上進行封裝。因此使用者只需要關注應用程式如何被部署與執行就好，而不用管容器是如何被建置出來。 使用 Docker 有以下幾項好處： 更快速的交付和部署 高效能的虛擬化環境 易遷移和擴展服務 管理簡單化 更有效的使用實體主機資源 可建置任何語言的 Dockerized 應用程式 跨平台的管理、部署與使用 微軟過往許多需要相依於 Windows 語言應用程式也慢慢被支援到 Docker 了。 Docker Container vs 傳統虛擬化傳統虛擬化技術一般是透過在 Host OS 上安裝 Hypervisor（VMM），然後由 Hypervisor 來管理不同虛擬主機，每個虛擬主機都需要安裝不同的作業系統，因此每個環境被獨立的完全隔離。 然而 Docker 提供應用程式在獨立的 Container 中執行，這些Container 並不需要像虛擬化一樣額外依附在 Hypervisor 甚至 Guest OS 上，而是透過 Docker engine 來進行管理。 Docker 基本概念Docker 擁有幾個基本概念，其中包含 Docker 三大重要部分與元件。我們將針對這些概念進行概述，也會在其他章節進一步說明。 Docker images：Image（映像檔）被用來啟動容器的實際執行得應用程式環境。這概念類似 VM 的映像檔，VM 透過映像檔來啟動作業系統，並執行許多服務，但 Docker 的映像檔則只是檔案系統的儲存狀態，是一個唯讀的板模。 Docker containers：Container（容器）是一個應用程式執行的實例，Docker 會提供獨立、安全的環境給應用程式執行，容器是從映像檔建立，並運作於主機上。 P.S 盡量不要在一個 Container 執行過多的服務。 Docker registries：Registries 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 Docker Hub，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 Deploying a registry server。 Docker 的推出與發展非常迅速，相關的部署工具與資源相繼出現，更因此讓原名為 dotcloud 變成 Docker, Inc。Docker 也在 2014 - 2015 年推出了以下三大工具： docker-machine：Docker machine 是可以透過指令來安裝 Docker engine 的工具。該工具可以讓使用者不需要學習一堆安裝指令來部署容器環境，目前已經支援了許多驅動程式，例如：OpenStack、Amazon EC2、Google Cloud Engine 與 Microsoft Azure等，更可以被用來建立混合環境。 docker-compose：Compose 是 Docker 的編配工具，可以用來建置 Swarm 上的多節點容器化叢集與單一節點的應用程式。Compose 的前身是 Fig，使用者可以透過定義 YAML 檔案來描述與維護所有應用程式服務定義與部署，如多個服務之間如何連接等，使用 Compose 部署的應用程式可以在不影響其他服務情況下自動更新。 docker-swarm：Swarm 是 Docker 的原生叢集與調度工具，它基於應用程式生命週期、容器使用、效能需求自動優化分散式應用程式的基礎架構。且 Swarm 可以透過許多服務發現（Service Discovery）套件來打造 HA 的叢集，Swarm 也提供很高的靈活性，使應用程式可以簡單的分散部署在多主機環境上。 使用者操作 Docker 的方法Docker 主機上會執行一個 Docker daemon，就能夠開啟許多 Container。如果要對 Docker 進行操作的話，可以使用 Docker client 軟體，如 docker client、docker-py、Kitematic，這些工具會分別採用以下兩種方式來對部署 Docker daemon 進行管理： UNIX Sockets RESTful API 溝通方式如下圖所示，其中 Docker daemon 可以同時安裝 Docker client 來直接進行 Docker 使用（一般安裝都會有），詳細資訊可以參閱 Docker Remote API - Docker Documentation。 最後有興趣看每週 Docker 的新聞可以訂閱 Docker Weekly，參閱 Docker Newsletter 參考資源 Docker 官方 Docker Blog InfoQ Docker 深入淺出 Docker —— 從入門到實踐","categories":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/categories/Container/"}],"tags":[{"name":"Container","slug":"Container","permalink":"https://k2r2bai.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"https://k2r2bai.com/tags/Docker/"}]},{"title":"簡單設定 Apache2 Proxy 與 VirtualHost","slug":"linux/ubuntu/apache2-proxy","date":"2015-11-04T09:08:54.000Z","updated":"2019-12-02T01:49:42.397Z","comments":true,"path":"2015/11/04/linux/ubuntu/apache2-proxy/","link":"","permalink":"https://k2r2bai.com/2015/11/04/linux/ubuntu/apache2-proxy/","excerpt":"Apache2 是一套經過測試與用於生產環境的 HTTP 伺服器，在許多網頁伺服器中被廣泛的採用，Apache2 除了本身能力強大外，其也整合了許多的額外模組來提供更多的擴展功能。","text":"Apache2 是一套經過測試與用於生產環境的 HTTP 伺服器，在許多網頁伺服器中被廣泛的採用，Apache2 除了本身能力強大外，其也整合了許多的額外模組來提供更多的擴展功能。 Apache2 安裝與設定要安裝 Apache 伺服器很簡單，只需要透過 APT 進行安裝即可： $ sudo apt-get update$ sudo apt-get install -y libapache2-mod-proxy-html libxml2-dev apache2 build-essential 啟用 Proxy Modules這邊可以透過以下指令來逐一啟動模組： a2enmod proxya2enmod proxy_httpa2enmod proxy_ajpa2enmod rewritea2enmod deflatea2enmod headersa2enmod proxy_balancera2enmod proxy_connecta2enmod proxy_html 設定 Default conf 來啟用編輯/etc/apache2/sites-available/000-default.conf設定檔，加入 Proxy 與 VirtualHost 資訊： # 簡單 Proxypass 範例&lt;VirtualHost *:80&gt; ErrorLog $&#123;APACHE_LOG_DIR&#125;/laravel-error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/laravel-access.log combined ProxyPass / http://192.168.20.10/ ProxyPassReverse / http://192.168.20.10/ ProxyPreserveHost On ServerName laravel.kairen.com ServerAlias laravel.kairen.com ServerAlias *.laravel.kairen.com&lt;/VirtualHost&gt;# 簡單 Load balancer 範例&lt;Proxy balancer://api-gateways&gt; # Server 1 BalancerMember http://192.168.20.11:8080/ # Server 2 BalancerMember http://192.168.20.12:8080/&lt;/Proxy&gt;&lt;VirtualHost *:*&gt; ProxyPass / balancer://api-gateways&lt;/VirtualHost&gt; 完成後重新啟動服務即可： $ sudo service apache2 restart 使用 SSL Reverse-Proxy如果需要設定 SSL 連線與認證的話，可以透過以下設定方式來提供： Listen 443NameVirtualHost *:443&lt;VirtualHost *:443&gt; SSLEngine On SSLCertificateFile /etc/apache2/ssl/file.pem ProxyPass / http://192.168.20.11:8080/ ProxyPassReverse / http://192.168.20.11:8080/&lt;/VirtualHost&gt; 完成後重新啟動服務即可： $ sudo service apache2 restart","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"HTTP Server","slug":"HTTP-Server","permalink":"https://k2r2bai.com/tags/HTTP-Server/"}]},{"title":"Ubuntu Metal as a Service 安裝","slug":"linux/ubuntu/ubuntu-maas-install","date":"2015-11-04T04:23:01.000Z","updated":"2019-12-02T01:49:42.399Z","comments":true,"path":"2015/11/04/linux/ubuntu/ubuntu-maas-install/","link":"","permalink":"https://k2r2bai.com/2015/11/04/linux/ubuntu/ubuntu-maas-install/","excerpt":"Ubuntu MAAS 提供了裸機服務，將雲端的語言帶到物理伺服器中，其功能可以讓企業一次大量部署伺服器硬體環境的作業系統與基本設定等功能。","text":"Ubuntu MAAS 提供了裸機服務，將雲端的語言帶到物理伺服器中，其功能可以讓企業一次大量部署伺服器硬體環境的作業系統與基本設定等功能。 安裝與設定首先為了方便系統操作，將sudo設置為不需要密碼： $ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu ubuntu 為 User Name。 安裝相關套件： $ sudo apt-get install software-properties-common 新增 MAAS 與 JuJu 的資源庫： sudo add-apt-repository -y ppa:juju/stablesudo add-apt-repository -y ppa:maas/stablesudo add-apt-repository -y ppa:cloud-installer/stablesudo apt-get update 安裝 MAAS接著安裝 MAAS： $ sudo apt-get install -y maas 安裝完成後，建立一個 admin 使用者帳號： $ sudo maas-region createadmin 這時候就可以登入 MAAS。 登入後，需要設定 PXE 網路與下載 Image。當完成後就可以開其他主機來測試 PXE-boot。 安裝 JuJu安裝 JuJu quick start： $ sudo apt-get update &amp;&amp; sudo apt-get install juju-quickstart 完成後，透過以下指令來部署 JuJu GUI： $ juju-quickstart --gui-port 4242 上傳客製化 Images首先安裝 Maas Image Builder 來建立映像檔： $ sudo add-apt-repository -y ppa:blake-rouse/mib-daily$ sudo apt-get install maas-image-builder 安裝完成後，我們可以使用以下指令建立映像檔，如以下為建立一個CentOS 7的映像檔： $ sudo maas-image-builder -a amd64 -o centos7-amd64-root-tgz centos --edition 7 目前 CentOS 支援了6.5與7.0版本。 當映像檔建立完成後，就可以上傳到 MAAS 了，在這之前需要登入 MAAS，才有權限上傳： $ maas login &lt;user_name&gt; &lt;maas_url&gt; &lt;api_key&gt; user_name&gt;為使用者帳號名稱。&lt;maas_url&gt;為 MAAS 網址，如 http://192.168.1.2/MAAS&lt;api_key&gt;為帳號 API Key。r00tme 登入後，就可以使用以下指令進行上傳客製化的映像檔了： $ maas &lt;user_name&gt; boot-resources create name=centos/centos7 architecture=amd64/generic content@=centos7-amd64-root-tgz 登入時，要輸入指令需加&lt;user_name&gt;。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"PXE","slug":"PXE","permalink":"https://k2r2bai.com/tags/PXE/"},{"name":"Bare Metal","slug":"Bare-Metal","permalink":"https://k2r2bai.com/tags/Bare-Metal/"}]},{"title":"利用 Keepalived 提供 VIP","slug":"linux/ubuntu/keepalived","date":"2015-10-29T04:23:01.000Z","updated":"2019-12-02T01:49:42.398Z","comments":true,"path":"2015/10/29/linux/ubuntu/keepalived/","link":"","permalink":"https://k2r2bai.com/2015/10/29/linux/ubuntu/keepalived/","excerpt":"Keepalived 是一種基於 VRRP 協定實現的高可靠 Web 服務方案，用於防止單點故障問題。因此一個 Web 服務運作至少會擁有兩台伺服器執行 Keepalived，一台作為 master，一台作為 backup，並提供一個虛擬 IP（VIP），master 會定期發送特定訊息給 backup 伺服器，當 backup 沒收到 master 訊息時，表示 master 已故障，這時候 backup 會接管 VIP，繼續提供服務，來確保服務的高可靠性。","text":"Keepalived 是一種基於 VRRP 協定實現的高可靠 Web 服務方案，用於防止單點故障問題。因此一個 Web 服務運作至少會擁有兩台伺服器執行 Keepalived，一台作為 master，一台作為 backup，並提供一個虛擬 IP（VIP），master 會定期發送特定訊息給 backup 伺服器，當 backup 沒收到 master 訊息時，表示 master 已故障，這時候 backup 會接管 VIP，繼續提供服務，來確保服務的高可靠性。 VRRPVRRP（Virtual Router Redundancy Protocol，虛擬路由器備援協定），是一個提供備援路由器來解決單點故障問題的協定，該協定有兩個重要概念： VRRP 路由器與虛擬路由器：VRRP 路由器是表示運作 VRRP 的路由器，是一個實體裝置，而虛擬路由器是指由 VRRP 建立的邏輯路由器。一組 VRRP 路由器協同運作，並一起構成一台虛擬路由器，該虛擬路由對外提供一個唯一固定的 IP 與 MAC 位址的邏輯路由器。 主控制路由器（master）與備援路由器（backup）：主要是在一組 VRRP 中的兩種互斥角色。一個 VRRP 群組中只能擁有一台是 master，但可以有多個 backup 路由器。 VRRP 協定使用選擇策略從路由器群組挑選一台作為 master 來負責 ARP 與轉送 IP 封包，群組中其他路由器則作為 backup 的角色處理等待狀態。當由於某種原因造成 master 故障時，backup 會在幾秒內成為 master 繼續提供服務，該階段不用改變任何 IP 與 MAC 位址。 Keepalived 節點配置本教學將使用以下主機數量與角色： IP Address Role 172.16.1.101 vip 172.16.1.102 master 172.16.1.103 backup 安裝與設定這 ubuntu 14.04 LTS Server 中已經內建了 Keepalived 可以透過 apt-get 來安裝： $ sudo apt-get install -y keepalived 也可以透過 source code 進行安裝，流程如下： $ sudo apt-get install build-essential libssl-dev$ wget http://www.keepalived.org/software/keepalived-1.2.2.tar.gz$ tar -zxvf keepalived-1.2.2.tar.gz$ cd keepalived-1.2.2$ ./configure --prefix=/usr/local/keepalived$ make &amp;&amp; make install 完成後，要將需要的設定檔進行複製到/etc/: $ cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived$ cp /usr/local/keepalived/sbin/keepalived /usr/sbin/$ cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/$ mkdir -p /etc/keepalived/$ cp /usr/local/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf 安裝完成後編輯/etc/keepalived/keepalived.conf檔案進行設定，在master節點加入以下內容： global_defs &#123; notification_email &#123; user@example.com &#125; notification_email_from mail@example.org smtp_server 172.16.1.100 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state MASTER # Tag 為 MASTER interface eth0 virtual_router_id 51 priority 101 # MASTER 權重高於 BACKUP advert_int 1 mcast_src_ip 172.16.1.102 # VRRP 實體主機的 IP authentication &#123; auth_type PASS # Master 驗證方式 auth_pass 1111 &#125; #VIP virtual_ipaddress &#123; 172.16.1.101 # 虛擬 IP &#125;&#125; Master 完成後，接著編輯backup節點的/etc/keepalived/keepalived.conf，加入以下內容： global_defs &#123; notification_email &#123; user@example.com &#125; notification_email_from mail@example.org smtp_server 172.16.1.100 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state BACKUP # Tag 為 BACKUP interface eth0 virtual_router_id 51 priority 100 # 權重要低於 MASTER advert_int 1 mcast_src_ip 172.16.1.103 # vrrp 實體主機 IP authentication &#123; auth_type PASS auth_pass 1111 &#125; # VIP virtual_ipaddress &#123; 172.16.1.101 # 提供的 VIP &#125;&#125; 最後在兩台節點啟動 Keepalived： $ sudo service keepalived start","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"High Availability","slug":"High-Availability","permalink":"https://k2r2bai.com/tags/High-Availability/"}]},{"title":"Apache Kafka 叢集","slug":"data-engineer/kafka-install","date":"2015-10-13T09:08:54.000Z","updated":"2019-12-02T01:49:42.383Z","comments":true,"path":"2015/10/13/data-engineer/kafka-install/","link":"","permalink":"https://k2r2bai.com/2015/10/13/data-engineer/kafka-install/","excerpt":"Apache Kafka 是一個分散式的訊息佇列框架，是由 LinkedIn 公司使用 Scala 語言開發的系統，被廣泛用來處理高吞吐量與容易水平擴展，目前許多巨量資料運算框架以都有整合 Kafka，諸如：Spark、Cloudera、Apache Storm等， Kafka 是基於Publish/Subscribe的訊息系統，主要設計由以下特點： 在 TB 級以上資料也能確保常數時間複雜度的存取效能，且時間複雜度為O(1)的訊息持久化。 高吞吐量，在低階的商業電腦上也能提供單機100k/sec條以上的訊息傳輸。 支援 Kafka Server 之間的訊息分區(Partition)以及分散式發送，並保證每個分區內的訊息循序傳輸。 同時支援離線資料處理與即時資料處理 容易的服務不中斷水平擴展。","text":"Apache Kafka 是一個分散式的訊息佇列框架，是由 LinkedIn 公司使用 Scala 語言開發的系統，被廣泛用來處理高吞吐量與容易水平擴展，目前許多巨量資料運算框架以都有整合 Kafka，諸如：Spark、Cloudera、Apache Storm等， Kafka 是基於Publish/Subscribe的訊息系統，主要設計由以下特點： 在 TB 級以上資料也能確保常數時間複雜度的存取效能，且時間複雜度為O(1)的訊息持久化。 高吞吐量，在低階的商業電腦上也能提供單機100k/sec條以上的訊息傳輸。 支援 Kafka Server 之間的訊息分區(Partition)以及分散式發送，並保證每個分區內的訊息循序傳輸。 同時支援離線資料處理與即時資料處理 容易的服務不中斷水平擴展。 Kafka 從架構上來看，Kafka 會擁有以下幾個角色： Producer：主要為 Publish 訊息到 Topic。 Consumer：主要為 Subscribe Topic 來取得訊息。 Broker：訊息的中介者，可看錯是一台訊息 Server，可部署單機至多台叢集。 Topic：拿來做訊息的分類。 Zookeeper：Zookeeper 不算是 Kafka 一員，但 Kafka 依賴 Zookeeper 來做到 Sync。 Apache Kafka 的一個簡單應用架構可以參考下圖，透過 Spark Streaming 來進行串接做快速的串流資料收集，並利用 Spark 框架進行分析後取得結果存於 Cassandra 資料庫叢集，最後在由應用程式或前端網頁來顯示處理過的資料。 安裝 Apache Kafka一個簡單的節點配置如下： IP Address Role zookeeper id broker.id 172.17.0.2 kafka-1 1 0 172.17.0.3 kafka-2 2 1 172.17.0.4 kafka-3 3 2 首先要在每台節點安裝 Java，這邊採用 Oracle 的 Java 來進行安裝： $ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections$ sudo apt-get -y install oracle-java7-installer 完成後，接著在每台節點安裝 Zookeeper 服務，這邊採用apt-get來進行安裝： $ sudo apt-get install -y zookeeperd 完成安裝後，zookeeper 會自動開啟服務於 port 2181。若沒啟動使用以下指令： $ sudo service zookeeper restart 若想部署多節點 Zookeeper，請修改每台節點的/etc/zookeeper/conf/zoo.cfg檔案，加入以下內容：(Option) server.1=172.17.0.2:2888:3888server.2=172.17.0.3:2888:3888server.3=172.17.0.4:2888:3888 並設定 ID，如下指令： $ echo \"1\" | sudo tee /etc/zookeeper/conf/myid 測試 Zookeeper 是否啟動，可以透過 Telnet 來進行： $ telnet localhost 2181 當節點上述完成後就可以下載 Kafka 套件，並解壓縮到/opt/底下： $ sudo curl -s \"http://ftp.tc.edu.tw/pub/Apache/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz\" | sudo tar -xz -C /opt/$ sudo mv /opt/kafka_2.10-0.9.0.1 /opt/kafka 下載完成後，編輯/opt/kafka/config/server.properties，並加入以下內容： broker.id=0host.name=172.17.0.2zookeeper.connect=172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181 這邊的broker.id需跟著節點數變動，從 0 開始計數。若使用OpenStack或者Docker這些虛擬化的話，需在設定檔加入： advertised.host.name=&lt;Advertised IP&gt;advertised.port=9092 編輯完以後就分別啟動這三台 Broker： $ cd /opt/kafka$ bin/kafka-server-start.sh config/server.properties &amp; 驗證服務當所有 Server 啟動完成後，就可以透過建立 Topic 來確認是否成功部署完成： $ /opt/kafka/bin/kafka-topics.sh --create \\--zookeeper localhost:2181 \\--replication-factor 3 \\--partitions 1 \\--topic test 可以試著將--replication-factor改為 4，若成功會看到以下錯誤訊息： replication factor: 4 larger than available brokers: 3 原因是我們只有建立 3 台叢集。 建立完成後，可以用以下指令查看： $ /opt/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test# 會看到類似以下資訊Topic:test PartitionCount:1 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1 接下來透過 Publish 來傳送訊息： $ /opt/kafka/bin/kafka-console-producer.sh \\--broker-list localhost:9092 \\--topic test# 輸入ggeederggeeder 接著就要讀取訊息，透過 Subscribe 來訂閱收取資料： $ /opt/kafka/bin/kafka-console-consumer.sh \\--zookeeper 172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181 \\--topic test \\--from-beginning 這時透過手動方式關閉該 broker，來測試 replication 是否有正確運作： $ jps83 Kafka$ sudo kill -9 83 接著可以先去看該 Topic 的 Leader 是否有變化： $ /opt/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 此時看 Consume message，會發現訊息應該還是會保存的完整無缺： $ /opt/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic test","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Kafka","slug":"Kafka","permalink":"https://k2r2bai.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"https://k2r2bai.com/tags/Message-Queue/"}]},{"title":"Spark on Mesos 多節點部署","slug":"data-engineer/spark-mesos","date":"2015-10-12T09:08:54.000Z","updated":"2019-12-02T01:49:42.383Z","comments":true,"path":"2015/10/12/data-engineer/spark-mesos/","link":"","permalink":"https://k2r2bai.com/2015/10/12/data-engineer/spark-mesos/","excerpt":"Spark + Mesos 叢集是由多個主節點與工作節點組合而成，它實作了兩層的排程（Scheduler）來提供粗/細粒度的排程。在 Mesos 中主節點（Master）主要負責資料的分配與排程，然而從節點（Slave）則是主要執行任務負載的角色。Mesos 也提供了高可靠的部署模式，可利用多個主節點的 ZooKeeper 來做服務發現。","text":"Spark + Mesos 叢集是由多個主節點與工作節點組合而成，它實作了兩層的排程（Scheduler）來提供粗/細粒度的排程。在 Mesos 中主節點（Master）主要負責資料的分配與排程，然而從節點（Slave）則是主要執行任務負載的角色。Mesos 也提供了高可靠的部署模式，可利用多個主節點的 ZooKeeper 來做服務發現。 在 Mesos 上所執行的應用程式都被稱為框架（Framework），該框架會被 Mesos 以 API 方式處理資源的提供，並將任務提交給 Mesos。其任務執行流程有以下幾個步驟構成： Slave 提供可用資源給 Master Master 向 Framework 的資源供應，並說明 Slave 資源 Framework Scheduler 回應任務以及每個任務的資源需求 Master 將任務發送到適當的 Slave 執行器（Executor） 事前準備以下為節點配置： IP Address HostName 192.168.1.10 mesos-master 192.168.1.11 mesos-slave-1 192.168.1.12 mesos-slave-2 首先我們要在各節點先安裝 ssh-server 與 Java JDK，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定(hadoop)不用需打 sudo 密碼： $ echo \"hadoop ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop P.S 要注意 hadoop 要隨著現在使用的 User 變動。 建立ssh key，並複製 key 使之不用密碼登入： $ ssh-keygen -t rsa$ ssh-copy-id localhost 安裝Java 1.8 JDK： $ sudo apt-get purge openjdk*$ sudo apt-get -y autoremove$ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections$ sudo apt-get -y install oracle-java8-installer 新增各節點 Hostname 至/etc/hosts檔案： 127.0.0.1 localhost192.168.1.10 mesos-master192.168.1.11 mesos-slave-1192.168.1.12 mesos-slave-2 並在Master節點複製所有Slave的 ssh key： $ ssh-copy-id ubuntu@mesos-slave-1$ ssh-copy-id ubuntu@mesos-slave-2 Mesos 安裝首先要安裝 Mesos 於系統上，可以採用以下方式獲取最新版本的 Respository： $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E56151BF$ DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')$ CODENAME=$(lsb_release -cs)$ echo \"deb http://repos.mesosphere.com/$&#123;DISTRO&#125; $&#123;CODENAME&#125; main\" | sudo tee /etc/apt/sources.list.d/mesosphere.list 加入 key 與 repository 後，即可透過apt-get安裝： sudo apt-get updatesudo apt-get -y install mesos P.S Master需要再安裝 Marathon Master 節點設定首先設定 Zookeeper ID： $ echo 1 | sudo tee /etc/zookeeper/conf/myid 設定 Zookeeper configuration： HOST_IP=$(ip route get 8.8.8.8 | awk '&#123;print $NF; exit&#125;')echo server.1=$HOST_IP:2888:3888 | sudo tee -a /etc/zookeeper/conf/zoo.cfg 若要部署 HA 需要加入多個 Master 節點的 Zookeeper。 完成後，重新啟動 Zookeeper 服務： $ sudo service zookeeper restart 接著設定 Mesos zk configuration： $ echo zk://$HOST_IP:2181/mesos | sudo tee /etc/mesos/zk 設定 Mesos quorum 參數： $ echo 1 | sudo tee /etc/mesos-master/quorum 若是 OpenStack VM 需要設定 Host IP 和 EXENTAL_IP 為 區網 IP 而非 Flaot IP：（Optional） EXENTAL_IP='192.168.1.10'echo $EXENTAL_IP | sudo tee /etc/mesos-master/hostnameecho $HOST_IP | sudo tee /etc/mesos-master/ipecho 'mesos-cluster' | sudo tee /etc/mesos-master/cluster 接著設定advertise_ip： $ echo $HOST_IP | sudo tee /etc/mesos-master/advertise_ip 當設定完成，要接著設定 Marathon，首先建立組態目錄： sudo mkdir /etc/marathon/sudo mkdir /etc/marathon/conf 設定 hostname： $ echo $EXENTAL_IP | sudo tee /etc/marathon/conf/hostname 設定 master ip ： $ echo zk://$HOST_IP:2181/mesos | sudo tee /etc/marathon/conf/master 設定 master zookeeper ： $ echo zk://$HOST_IP:2181/marathon | sudo tee /etc/marathon/conf/zk 關閉 Master 節點的mesos-slave service： sudo service mesos-slave stopsudo sh -c \"echo manual &gt; /etc/init/mesos-slave.override\" 重新啟動 Mesos 與 Marathon 服務： sudo service mesos-master restartsudo service marathon restart Slave 節點設定由於我們是使用 ubuntu 套件，Zookeeper 會以相依套件被自動下載至環境上，故我們要手動停止服務： sudo service zookeeper stopsudo sh -c \"echo manual &gt; /etc/init/zookeeper.override\" 設定 Mesos 與 Marathon： 若使用 OpenStack VM，需要將 MASTER_IP 和 PUBlIC_IP 設定為區網 IP MASTER_IP=\"192.168.1.10\"PUBlIC_IP=\"192.168.1.11\"HOST_IP=$(ip route get 8.8.8.8 | awk '&#123;print $NF; exit&#125;')echo zk://$MASTER_IP:2181/mesos | sudo tee /etc/mesos/zk 設定 Hostname 可以使用 OpenStack Float IP（Optional）： echo $PUBlIC_IP | sudo tee /etc/mesos-slave/hostname 設定 slave ip： $ echo $HOST_IP | sudo tee /etc/mesos-slave/ip 關閉 mesos-master 服務，並取消自動開機啟動： sudo service mesos-master stopsudo sh -c \"echo manual &gt; /etc/init/mesos-master.override\" 重新啟動 Mesos slave 服務： $ sudo service mesos-slave restart 驗證安裝結果當安裝完成，我們要驗證系統是否正常運行，可以透過以下指令運行： MASTER=$(mesos-resolve `cat /etc/mesos/zk`)mesos-execute --master=$MASTER --name=\"cluster-test\" --command=\"sleep 5\" 若要查看細節資訊，可以用瀏覽器開啟 Mesos Console、Marathon console 安裝 Spark Driver首先下載 Spark，並修改權限： $ curl -s \"http://archive.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz\" | sudo tar -xz -C /opt/$ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark$ sudo chown $USER:$USER -R /opt/spark 之後到spark/conf目錄，將spark-env.sh.template複製為park-env.sh： $ cp spark-env.sh.template spark-env.sh 在spark-env.sh這內容最下方增加這幾筆環境參數： export MESOS_NATIVE_JAVA_LIBRARY=\"/usr/lib/libmesos.so\"export MASTER=\"mesos://192.168.1.10:5050\"export SPARK_EXECUTOR_URI=\"/opt/spark-1.5.2.tgz\"export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:jre/bin/java::\")export SPARK_LOCAL_IP=$(ifconfig eth0 | awk '/inet addr/&#123;print substr($2,6)&#125;')export SPARK_LOCAL_HOSTNAME=$(ifconfig eth0 | awk '/inet addr/&#123;print substr($2,6)&#125;') 接著下載一個新的spark-1.5.2-bin-hadoop2.6.tgz，並解壓縮： $ cd ~/$ wget \"http://archive.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz\"$ tar -xvf spark-1.5.2-bin-hadoop2.6.tgz$ sudo mv spark-1.5.2-bin-hadoop2.6 spark-1.5.2$ sudo vim spark-1.5.2/conf/spark-env.shexport MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.soexport SPARK_EXECUTOR_URI=\"/opt/spark-1.5.2.tgz\"export MASTER=mesos://192.168.1.10:5050export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:jre/bin/java::\") 完成後壓縮資料夾： $ sudo tar -czvf spark-1.5.2.tgz spark-1.5.2/ 並在Master節點複製到所有Slave並解壓縮： $ scp spark-1.5.2.tgz mesos-slave-1:~/ &amp;&amp; ssh mesos-slave-1 sudo mv ~/spark-1.5.2.tgz /opt$ scp spark-1.5.2.tgz mesos-slave-2:~/ &amp;&amp; ssh mesos-slave-2 sudo mv ~/spark-1.5.2.tgz /opt$ sudo tar -xvf /opt/spark-1.5.2.tgz 設定使用者環境參數： $ echo \"export SPARK_HOME=/opt/spark\" | sudo tee -a ~/.bashrc$ echo \"export PATH=\\$SPARK_HOME/bin:\\$PATH\" | sudo tee -a ~/.bashrc 執行spark-shell，來驗證 Spark 可否正常執行： $ spark-shell --master mesos://192.168.1.34:5050val data = 1 to 10000val distData = sc.parallelize(data)distData.filter(_&lt; 10).collect() 或使用範例程式提交 Job： $ spark-submit --class org.apache.spark.examples.SparkPi \\--master mesos://192.168.1.10:5050 \\--num-executors 1 \\--executor-memory 1g \\--executor-cores 1 \\lib/spark-examples*.jar \\1","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Mesos","slug":"Mesos","permalink":"https://k2r2bai.com/tags/Mesos/"},{"name":"HDFS","slug":"HDFS","permalink":"https://k2r2bai.com/tags/HDFS/"}]},{"title":"Ubuntu PXE 安裝與設定","slug":"linux/ubuntu/ubuntu-pxe","date":"2015-10-11T04:23:01.000Z","updated":"2019-12-02T01:49:42.399Z","comments":true,"path":"2015/10/11/linux/ubuntu/ubuntu-pxe/","link":"","permalink":"https://k2r2bai.com/2015/10/11/linux/ubuntu/ubuntu-pxe/","excerpt":"預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。","text":"預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。 PXE 伺服器必須要提供至少含有 DHCP 以及 TFTP : DHCP 服務必須要能夠提供用戶端的網路參數之外，還得要告知用戶端 TFTP 所在的位置為何才行 TFTP 則是提供用戶端 boot loader 及 kernel file 下載點的重要服務 Kickstart我們在手動安裝作業系統時，會針對需求安裝作業系統的相關套件、設定、disk切割等，當我們重複的輸入這些資訊時，隨著需安裝的電腦越多會越裝越阿雜，如果有人可以幫你完成這樣一套輸入資訊的話，就可以快速的自動化部署多台電腦，除了方便外，心情也格外爽快。 kickstart是Red Hat公司針對自動化安裝Red Had、Fedora、CentOS而制定的問題回覆規範，透過這個套件可以指定回覆設定問題，更能夠指定作業系統安裝其他套裝軟體，也可以執行Script(sh, bash)，通常kickstart設定檔(.cfg)是透過system-config-kickstart產生。也可以利用GUI的CentOS下產生安裝用的cfg檔案。 Preseed相對於kickstart，preseed是Debain/Ubuntu的自動化安裝回覆套件。 其他工具 Stacki 3 Ubuntu MAAS Foreman LinMin OpenStack Ironic Crowbar PXE 安裝與設定首先安裝相關軟體，如 TFTP、DHCP等： sudo apt-get install -y tftpd-hpa isc-dhcp-server lftp openbsd-inetd DHCP 設定首先編輯 /etc/dhcp/dhcpd.conf檔案，在下面配置 DHCP： ddns-update-style none;default-lease-time 600;max-lease-time 7200;log-facility local7;subnet 10.21.10.0 netmask 255.255.255.0 &#123; range 10.21.10.200 10.21.10.250; option subnet-mask 255.255.255.0; option routers 10.21.10.254; option broadcast-address 10.21.10.255; filename &quot;pxelinux.0&quot;; next-server 10.21.10.240;&#125; 完成後，重新啟動 DHCP 服務： $ sudo service isc-dhcp-server restart * Stopping ISC DHCP server dhcpd [fail] * Starting ISC DHCP server dhcpd [ OK ] 檢查 DHCP 是否正確被啟動： $ sudo netstat -lu | grep bootudp 0 0 *:bootps *:* TFTP Server 設定編輯/etc/inetd.conf檔案，在最下面加入以下內容： tftp dgram udp wait root /usr/sbin/in.tftpd /usr/sbin/in.tftpd -s /var/lib/tftpboot 接著設定 Boot 時啟動服務，以及重新啟動相關服務： $ sudo update-inetd --enable BOOT$ sudo service openbsd-inetd restart * Restarting internet superserver inetd [ OK ]$ sudo service tftpd-hpa restart 檢查 TFTP Server 是否正確啟動： $ netstat -lu | grep tftpudp 0 0 *:tftp *:* 建立開機選單完成後安裝 syslinux: sudo apt-get -y install syslinux 複製 syslinux 設定檔至/var/lib/tftpboot目錄中： sudo cp /usr/lib/syslinux/menu.c32 /var/lib/tftpbootsudo cp /usr/lib/syslinux/vesamenu.c32 /var/lib/tftpbootsudo cp /usr/lib/syslinux/pxelinux.0 /var/lib/tftpbootsudo cp /usr/lib/syslinux/memdisk /var/lib/tftpbootsudo cp /usr/lib/syslinux/mboot.c32 /var/lib/tftpbootsudo cp /usr/lib/syslinux/chain.c32 /var/lib/tftpboot 建立/var/lib/tftpboot/pxelinux.cfg目錄： $ sudo mkdir /var/lib/tftpboot/pxelinux.cfg 接著編輯/var/lib/tftpboot/pxelinux.cfg/default檔案，設定開機選單，以下為簡單設定範例： UI vesamenu.c32TIMEOUT 100MENU TITLE Welcom to KaiRen.Lab PXE Server SystemLABEL local MENU LABEL Boot from local drive MENU DEFAULT localboot 0LABEL Custom CentOS 6.5 MENU LABEL Install Custom CentOS 6.5 kernel ./centos/vmlinuz append initrd=./centos/initrd.img ksdevice=bootif ip=dhcp ks=http://10.21.10.240/centos-ks/default_ks.cfgLABEL Hadoop CentOS 6.5 MENU LABEL Install Hadoop CentOS 6.5 kernel ./centos/vmlinuz append initrd=./centos/initrd.img ksdevice=bootif ip=dhcp ks=http://10.21.10.240/centos-ks/hdp_ks.cfgLABEL Ubuntu Server 14.04 MENU LABEL Install Ubuntu Server 14.04 kernel ./ubuntu/server/14.04/linux append initrd=./ubuntu/server/14.04/initrd.gz method=http://10.21.10.240/ubuntu/server/14.04/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"PXE","slug":"PXE","permalink":"https://k2r2bai.com/tags/PXE/"},{"name":"Bare Metal","slug":"Bare-Metal","permalink":"https://k2r2bai.com/tags/Bare-Metal/"}]},{"title":"CentOS 6.5 PXE 安裝與設定","slug":"linux/centos/centos-pxe","date":"2015-10-03T04:23:01.000Z","updated":"2019-12-02T01:49:42.397Z","comments":true,"path":"2015/10/03/linux/centos/centos-pxe/","link":"","permalink":"https://k2r2bai.com/2015/10/03/linux/centos/centos-pxe/","excerpt":"預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。","text":"預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。 安裝環境 CentOS 6.5 Minimal Install Intel(R) Core(TM)2 Quad CPU Q8400 @ 2.66GHz 500 GB 4G RAM Two Eth Card Inner eth = PEX DHCP Outer eth = Public network PXE 安裝與設定首先安装 Setuptool 於 CentOS 上 $ sudo yum install -y setuptool ntsysv iptables system-config-network-tui 關閉防火牆與 SElinux，避免驗證時被阻擋： $ sudo service iptables stop$ sudo setenforce 0 接著編輯/etc/selinux/config，修改以下內容: SELINUX=disabled 然後編輯/etc/sysconfig/network-scripts/ifcfg-ethx設定與確認 IP Address 是否正確： DEVICE=ethxHWADDR=C4:6E:1F:04:60:24 #依照個人ethTYPE=EthernetUUID=ada7e5dc-a2e9-4a89-9c93-e1f559cd05f2ONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPADDR=192.168.28.130 #依照網路NETMASK=255.255.255.0USERCTL=no DHCP Server 安裝與設定DHCP是「 動態主機配置協定」(Dynamic Host Configuration Protocol)。DHCP是可自動將IP位址指派給登入TCP/IP網路的用戶端的一種軟體(這種IP位址稱為「動態IP位址」)。這邊安裝方式為以下： $ sudo yum -y install dhcp 完成後編輯/etc/dhcp/dhcpd.conf，並修改以下設定: ddns-update-style none;ignore client-updates;allow booting;allow bootp;option ip-forwarding false;option mask-supplier false;option broadcast-address 192.168.28.255;subnet 192.168.28.0 netmask 255.255.255.0 &#123; option routers 192.168.28.130 range 192.168.28.50 192.168.28.60; #option subnet-mask 255.255.255.0; #option domain-name &quot;i4502.dic.ksu&quot;; option domain-name-servers 10.21.20.1; next-server 192.168.28.130; filename &quot;pxelinux.0&quot;;&#125; 設定完後，重新啟動 DHCP 服務： $ sudo service dhcpd start$ sudo chkconfig dhcpd on TFTP Server 安裝與設定簡單文件傳輸協議或稱小型文件傳輸協議（英文：Trivial File Transfer Protocol，縮寫TFTP），是一種簡化的文件傳輸協議。小型文件傳輸協議非常簡單，通過少量存儲器就能輕鬆實現——這在當時是很重要的考慮因素。所以TFTP被用於引導計算機，例如沒有大容量存儲器的路由器。安裝方式為以下： $ sudo yum -y install tftp-server tftp 安裝完成後編輯/etc/xinetd.d/tftp，修改以下內容： service tftp&#123; socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /install/tftpboot disable = yes per_source = 11 cps = 100 2 flags = IPv4&#125; P.S 如果不修改 server_args，預設為 /var/lib/tftpboot/。 接著建立/install/tftpboot來存放 Boot 映像檔： sudo mkdir -p /install/tftpbootsudochcon --reference /var /installsudo service xinetd restartsudo chkconfig xinetd onsudo chkconfig tftp on 安裝 syslinu如果要使用 PXE 的開機管理程式與開機選單的話，那就得要安裝 CentOS 內建提供的 syslinux 軟體，從裡面撈出兩個檔案即可。當然啦，這兩個檔案得要放置在 TFTP 的根目錄下才好！整個實作的過程如下。 yum -y install syslinuxcp /usr/share/syslinux/menu.c32 /install/tftpboot/cp /usr/share/syslinux/vesamenu.c32 /install/tftpboot/cp /usr/share/syslinux/pxelinux.0 /install/tftpboot/mkdir /install/tftpboot/pxelinux.cfgll /install/tftpboot/ 掛載CentOS 映像檔已CentOS 6.5 Minimal為範例。 mount -o loop CentOS-6.5-x86_64-minimal.iso /mntmkdir -p /install/tftpboot/kernel/centos6.5cp /mnt/isolinux/vmlinuz /install/tftpboot/kernel/centos6.5cp /mnt/isolinux/initrd.img /install/tftpboot/kernel/centos6.5cp /mnt/isolinux/isolinux.cfg /install/tftpboot/pxelinux.cfg/demoumount /mnt vmlinuz：就是安裝軟體的核心檔案 (kernel file) initrd.img：就是開機過程中所需要的核心模組參數 isolinux.cfg –&gt; demo：作為未來 PXE 所需要的開機選單之參考 設定開機選單vim /install/tftpboot/pxelinux.cfg/default 修改： UI vesamenu.c32TIMEOUT 300DISPLAY ./boot.msgMENU TITLE Welcome to KAIREN's PXE Server SystemLABEL local MENU LABEL Boot from local drive MENU DEFAULT localboot 0LABEL ubuntu MENU LABEL Install CentOS 6.5 kernel ./kernel/centos6.5/vmlinuz append initrd=./kernel/centos6.5/initrd.img 修改額外開機選單訊息vim /install/tftpboot/boot.msg 訊息： Welcome to KAI-REN's PXE Server System.The 1st menu can let you system goto hard disk menu.The 2nd menu can goto interactive installation step. 提供NFS Server 提供映像檔NFS 就是 Network FileSystem 的縮寫，最早之前是由 Sun 這家公司所發展出來的。 它最大的功能就是可以透過網路，讓不同的機器、不同的作業系統、可以彼此分享個別的檔案 (share files)。這個 NFS 伺服器可以讓你的 PC 來將網路遠端的 NFS 伺服器分享的目錄，掛載到本地端的機器當中， 在本地端的機器看起來，那個遠端主機的目錄就好像是自己的一個磁碟分割槽一樣 (partition)。 mkdir -p /install/nfs_share/centos6.5vim /etc/fstab 在最底下加入： /root/CentOS-6.5-x86_64-minimal.iso /install/nfs_share/centos6.5 iso9660 defaults,loop 0 0 安裝並提供分享目錄 mount -adfyum -y install nfs-utilsvim /etc/exports 加入： /install/nfs_share/ 192.168.28.0/24(ro,async,nohide,crossmnt) localhost(ro,async,nohide,crossmnt) 修改System nfs conf vim /etc/sysconfig/nfs 如下(P.S 找到上面這幾個設定值，我們得要設定好固定的 port 來開放防火牆給用戶處理)： RQUOTAD_PORT=901LOCKD_TCPPORT=902LOCKD_UDPPORT=902MOUNTD_PORT=903STATD_PORT=904 修改NFS 不需要對映帳號 vim /etc/idmapd.conf 如下： [General]Domain = \"kairen.pxe.com\"[Mapping]Nobody-User = nfsnobodyNobody-Group = nfsnobody 重開服務 service rpcbind restartservice nfs restartservice rpcidmapd restartservice nfslock restartchkconfig rpcbind onchkconfig nfs onchkconfig rpcidmapd onchkconfig nfslock onrpcinfo -pshowmount -e localhost 如果看到Export list for localhost:/install/nfs_share 192.168.28.0/24,localhost就是成功了。 提供 HTTP ServerApache HTTP Server（簡稱Apache）是Apache軟體基金會的一個開放原始碼的網頁伺服器軟體，可以在大多數電腦作業系統中運行，由於其跨平台和安全性。 yum -y install httpdservice httpd startchkconfig httpd on 建立CentOS 6.5目錄 mkdir -p /var/www/html/install/centos6.5vim /etc/fstab 加入到最下方： /root/CentOS-6.5-x86_64-minimal.iso /var/www/html/install/centos6.5 iso9660 defaults,loop 0 0 掛載起來 mount -adf 提供 FTP Serveryum -y install vsftpdservice vsftpd startchkconfig vsftpd onmkdir -p /var/ftp/install/centos6.5vim /etc/fstab 一樣加入Mount : /root/CentOS-6.5-x86_64-minimal.iso /var/ftp/install/centos6.5 iso9660 defaults,loop,context=system_u:object_r:public_content_t:s0 0 0 掛載起來 mount -adf HTTP FTP","categories":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://k2r2bai.com/tags/Linux/"},{"name":"PXE","slug":"PXE","permalink":"https://k2r2bai.com/tags/PXE/"},{"name":"Bare Metal","slug":"Bare-Metal","permalink":"https://k2r2bai.com/tags/Bare-Metal/"}]},{"title":"Spark on Hadoop YARN 單機安裝","slug":"data-engineer/spark-yarn","date":"2015-09-19T09:08:54.000Z","updated":"2019-12-02T01:49:42.384Z","comments":true,"path":"2015/09/19/data-engineer/spark-yarn/","link":"","permalink":"https://k2r2bai.com/2015/09/19/data-engineer/spark-yarn/","excerpt":"本教學為安裝 Spark on Hadoop YARN 的 all-in-one 版本，將 Spark 應用程式執行於 YARN 上，來讓應用程式執行於不同的工作節點上。","text":"本教學為安裝 Spark on Hadoop YARN 的 all-in-one 版本，將 Spark 應用程式執行於 YARN 上，來讓應用程式執行於不同的工作節點上。 事前準備首先我們要先安裝 ssh-server 與 Java JDK，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定(hadoop)不用需打 sudo 密碼： $ echo \"hadoop ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop P.S 要注意 hadoop 要隨著現在使用的 User 變動。 建立ssh key，並複製 key 使之不用密碼登入： $ ssh-keygen -t rsa$ ssh-copy-id localhost 安裝Java 1.7 JDK： $ sudo apt-get purge openjdk*$ sudo apt-get -y autoremove$ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections$ sudo apt-get -y install oracle-java7-installer 安裝 Hadoop YARN首先我們須先將 Hadoop YARN 安裝完成，詳細步驟如下所示。下載Hadoop 2.6.0 or laster version： $ curl -s \"https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz\" | sudo tar -xz -C /opt/$ sudo mv /opt/hadoop-2.6.0 /opt/hadoop 若要下載不同版本可以到官方查看。 到 hadoop 底下的 /etc/hadoop 設定所有conf檔與evn.sh檔： $ cd /opt/hadoop/etc/hadoop$ sudo vim hadoop-env.sh# 修改裡面的Java_Homeexport JAVA_HOME=/usr/lib/jvm/java-7-oracle 修改mapred-site.xml.template檔案： $ sudo mv mapred-site.xml.template mapred-site.xml$ sudo vim mapred-site.xml# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 修改hdfs-site.xml檔案： $ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode$ sudo chown -R $USER:$USER /usr/local/hadoop_store$ sudo vim hdfs-site.xml# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;&lt;/property&gt; 修改core-site.xml檔案： $ sudo mkdir -p /app/hadoop/tmp$ sudo chown $USER_NAME:$USER_NAME /app/hadoop/tmp$ sudo vim core-site.xml# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; 修改yarn-site.xml檔案： $ sudo vim yarn-site.xml# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 進行 Namenode 格式化： $ cd /opt/hadoop/bin$ ./hadoop namenode -format 沒出錯的話，就可以開啟Hadoop對應服務： $ cd /opt/hadoop/sbin$ ./start-yarn.sh$ ./start-dfs.sh 檢查是否開啟以下服務： $ jps3457 ResourceManager7087 Jps3593 NodeManager4190 DataNode4025 NameNode4383 SecondaryNameNode 開啟 Website YARN Dashboard 與 HDFS Dashboard 設定環境變數： $ cd$ sudo vim .bashrc# 加入以下到最後一行export HADOOP_HOME=\"/opt/hadoop\"export PATH=PATH:$HADOOP_HOMEexport HADOOP_BIN=\"/opt/hadoop/bin\"export PATH=$PATH:$HADOOP_BIN 透過source指令引用環境變數： $ source .bashrc 驗證系統為了驗證系統是否建置成功，可執行一個範例程式來看看是否能夠正常執行，如下所示：首先上傳資料到HDFS上： $ sudo vim words.txt# 加入以下，可以自行在多加AACDBBDEAAAA# 加入以上$ hadoop fs -mkdir /example$ hadoop fs -put words.txt /example 執行範例程式： $ cd /opt/hadoop/share/hadoop/mapreduce$ hadoop jar hadoop-mapreduce-examples-2.6.0.jar wordcount /example/words.txt /example/output Spark 安裝不管單機或叢集，安裝 Spark 只需要在 Master 節點上進行即可，步驟如下： 首先下載 Spark，並修改權限： $ curl -s https://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz | sudo tar -xz -C /opt/$ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark$ sudo chown $USER:$USER -R /opt/spark 其他 Hadoop 版本可以到這邊Spark-Hadoop查看。 設定 Spark 環境參數： $ echo \"export HADOOP_CONF_DIR=\\$HADOOP_HOME/etc/hadoop\" | sudo tee -a /opt/spark/conf/spark-env.sh$ echo \"export YARN_CONF_DIR=\\$HADOOP_HOME/etc/hadoop\" | sudo tee -a /opt/spark/conf/spark-env.sh$ echo \"export SPARK_HOME=/opt/spark\" | sudo tee -a /opt/spark/conf/spark-env.sh$ echo \"export SPARK_JAR=/opt/spark/lib/spark-assembly-1.5.2-hadoop2.6.0.jar\" | sudo tee -a /opt/spark/conf/spark-env.sh$ echo \"export PATH=\\$SPARK_HOME/bin:\\$PATH\" | sudo tee -a /opt/spark/conf/spark-env.sh 設定使用者環境參數： $ echo \"export SPARK_HOME=/opt/spark\" | sudo tee -a ~/.bashrc$ echo \"export PATH=\\$SPARK_HOME/bin:\\$PATH\" | sudo tee -a ~/.bashrc 驗證系統為了驗證 Spark 是否成功安裝，可以透過執行一個範例程式來看看結果，如下所示： $ cd /opt/spark$ spark-submit --class org.apache.spark.examples.SparkPi \\--master yarn-cluster \\--num-executors 1 \\--executor-memory 1g \\--executor-cores 1 \\lib/spark-examples*.jar \\1","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"},{"name":"Mesos","slug":"Mesos","permalink":"https://k2r2bai.com/tags/Mesos/"}]},{"title":"Spark Standalone 模擬分散式運算","slug":"data-engineer/spark-standalone","date":"2015-09-18T09:08:54.000Z","updated":"2019-12-02T01:49:42.384Z","comments":true,"path":"2015/09/18/data-engineer/spark-standalone/","link":"","permalink":"https://k2r2bai.com/2015/09/18/data-engineer/spark-standalone/","excerpt":"本教學為安裝 Spark Standalone 的叢集版本，將 Spark 應用程式執行於自己的分散式機制與各台機器連結上，來讓應用程式執行於不同的工作節點上。","text":"本教學為安裝 Spark Standalone 的叢集版本，將 Spark 應用程式執行於自己的分散式機制與各台機器連結上，來讓應用程式執行於不同的工作節點上。 事前準備首先我們要在各節點先安裝 ssh-server 與 Java JDK，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定(hadoop)不用需打 sudo 密碼： $ echo \"hadoop ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop P.S 要注意 hadoop 要隨著現在使用的 User 變動。 建立ssh key，並複製 key 使之不用密碼登入： $ ssh-keygen -t rsa$ ssh-copy-id localhost 安裝Java 1.7 JDK： $ sudo apt-get purge openjdk*$ sudo apt-get -y autoremove$ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections$ sudo apt-get -y install oracle-java7-installer 新增各節點 Hostname 至 /etc/hosts 檔案： 127.0.0.1 localhost192.168.1.10 hadoop-master192.168.1.11 hadoop-slave1192.168.1.11 hadoop-slave2 並在ssh key：```sh$ ssh-copy-id ubuntu@hadoop-slave1$ ssh-copy-id ubuntu@hadoop-slave2 安裝 Spark首先下載 Spark，並修改權限： $ curl -s https://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz | sudo tar -xz -C /opt/$ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark$ sudo chown $USER:$USER -R /opt/spark 之後到spark/conf目錄，將spark-env.sh.template複製為spark-env.sh： $ cp spark-env.sh.template spark-env.sh 在spark-env.sh這內容最下方增加這幾筆環境參數： export SPARK_MASTER_IP=\"hadoop-master\" export SPARK_MASTER_PORT=\"7077\"export SPARK_MASTER_WEBUI_PORT=\"8090\" SPARK_MASTER_IP為主節點（Master）的 IP。SPARK_MASTER_PORT為主節點（Master）的 Port。SPARK_MASTER_WEBUI_PORT為 WebUI 的 Port，預設為 8080。 接著複製slaves.template為slaves： $ cp slaves.template slaves 在最下方增加每台機器的 hostname： hadoop-slave1hadoop-slave2 完成後將設定檔複製給其他台機器： scp -r /opt/spark ubuntu@hadoop-slave1:/optscp -r /opt/spark ubuntu@hadoop-slave2:/opt 啟動 Spark ： /opt/spark/sbin/start-all.sh 這樣 Spark 就啟動完成了，開啟 Web UI 來檢查狀態。 設定使用者環境參數： $ echo \"export SPARK_HOME=/opt/spark\" | sudo tee -a ~/.bashrc$ echo \"export PATH=\\$SPARK_HOME/bin:\\$PATH\" | sudo tee -a ~/.bashrc 驗證系統為了驗證 Spark 是否成功安裝，可以透過執行一個範例程式來看看結果，如下所示： $ cd /opt/spark$ spark-submit --class org.apache.spark.examples.SparkPi \\--master spark://hadoop-master:7077 \\--num-executors 1 \\--executor-memory 1g \\--executor-cores 1 \\lib/spark-examples*.jar \\1","categories":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://k2r2bai.com/tags/Spark/"}]},{"title":"基本物件導向概念","slug":"msic/oop","date":"2014-06-03T04:23:01.000Z","updated":"2019-12-02T01:49:42.399Z","comments":true,"path":"2014/06/03/msic/oop/","link":"","permalink":"https://k2r2bai.com/2014/06/03/msic/oop/","excerpt":"物件導向程式設計（英語：Object-oriented programming，縮寫：OOP） 物件導向程式設計推廣了程式的靈活性和可維護性，在大型的專案被廣泛的應用。此外，支持者聲稱物件導向程式設計要比以往的做法更加便於學習，因為它能夠讓人們更簡單地設計並維護程式，使得程式更加便於分析、設計、理解。學習物件導向須了解幾點與幾個特性： Object 是 Class 的 Instance。 Class 是 Object 的定義。描述 Object 的組成與功能。 Object 使用前必須建立 Class 的 Object(Instance) (使用New)。 真正被用來處理問題的是 Object，而 Class 決定了 Object 的行為。","text":"物件導向程式設計（英語：Object-oriented programming，縮寫：OOP） 物件導向程式設計推廣了程式的靈活性和可維護性，在大型的專案被廣泛的應用。此外，支持者聲稱物件導向程式設計要比以往的做法更加便於學習，因為它能夠讓人們更簡單地設計並維護程式，使得程式更加便於分析、設計、理解。學習物件導向須了解幾點與幾個特性： Object 是 Class 的 Instance。 Class 是 Object 的定義。描述 Object 的組成與功能。 Object 使用前必須建立 Class 的 Object(Instance) (使用New)。 真正被用來處理問題的是 Object，而 Class 決定了 Object 的行為。 類別類別（Class）定義了一件事物的抽象特點。類別定義了事物的屬性與可做到的行為。Ex:「人」包含了一切的基本特徵與行為，頭髮、樣貌等等，類別可以為程式提供模版和結構。一個類別的方法和屬性被稱為「成員」。如以下： 類別 人 : &#123; 公有成員： 走(); 講話(); 吃飯() 私有成員： 頭髮顏色; 身體膚色;&#125; 這邊解釋了一個人的類別，其中公有(public)與私有(private)會在封裝性提到。 物件物件 (Object)是類別的例項(instance)。Ex:「人」類別列舉出人的特點，這個類別定義了世界上所有的人。而台灣人這個物件則是一種具體的人種，他的屬性也是具體的。人有皮膚顏色，而台灣人的皮膚顏色是白-黃之間。因此，台灣人就是人這個類別的一個例項(或稱實例)。 系統給物件分配記憶體空間，而不會給類別分配記憶體空間。這很好理解，類別是抽象的系統不可能給抽象的東西分配空間，而物件則是具體的。 我們無法讓人這個類別去走動，但是我們可以讓物件「台灣人」去走動，正如人可以走動一樣，但沒有具體的人就無法動作。 宣告或定義：台灣人是一種人台灣人.走();台灣人.身體顏色() = 白; 訊息傳遞物件通過接受訊息、處理訊息、傳出訊息或使用其他類別的方法來實作一定功能，就叫訊息傳遞機制(Message Passing)。 如：台灣人可以通過”講話”與其他人的溝通，從而導致一系列的事發生。 繼承繼承性(Inheritance)是指，在某種情況下，一個類別會有「子類別」。子類別會比原本的類別(稱為父類別)要更加具體化。Ex:人有分台灣人、日本人等。在這種情況柯p可能就是台灣人的例項，並包含柯p自己的特性與行為。繼承的好處在於可以將相同事情做一次歸類，便可讓子類別擁有該屬性與行為。如下： 類別 台灣人:繼承於人 &#123; 公有成員： 早餐專吃牛排();&#125;定義阿文是台灣人阿文.講話();阿文.早餐專吃牛排(); 當一個類別從多個父類別繼承時，我們稱之為「多重繼承」。好比混血兒從分別兩個人種繼承而來。但多重繼承有時矛盾，讓人無法理解，且有些程式也不支援。 封裝性具備封裝性（Encapsulation）的物件導向程式設計隱藏了某一方法的具體執行步驟，取而代之的是通過訊息傳遞機制傳送訊息給它。好比一個人”吃飯”，吃飯的這個人才知道具體吃下了什麼，而其他人並不知道，例如。 一個程序導向如下： 定義柯p柯p.拆除違建();柯p.講話() 一個物件導向如下： 定義小文是人小文.吃飯()小文.吃飯()小文.吃飯() 封裝是通過限制只有特定類別的物件可以存取這一特定類別的成員，而它們通常利用介面實作訊息的傳入傳出。通常來說，成員會依它們的存取許可權被分為3種：公有成員、私有成員以及保護成員 公有成員(public)：任何例項成員都可以被其他人使用。 私有成員(private)：只有類別本身可以使用。 保護成員(protected)：只有繼承的子類別與類別本身可以使用。 多型多型（Polymorphism）是指由繼承而產生的相關的不同的類別，其物件對同一訊息會做出不同的響應。好比『台灣人』與『美國人』都有”講話”的行為，但實際卻用不同語言講話。如下： 類別 台灣人 : &#123; 公有成員： 講話();&#125;類別 美國人 : &#123; 公有成員： 講話();&#125;定義 歐罵馬 是美國人定義 馬陰酒 是台灣人歐罵馬.講話(); // 講美語馬陰酒.講話(); // 講國語 多型性的概念也可以用在運算子過載(overload)上。多型是建立在繼承的基礎之上的，沒有繼承，就不會有多型。繼承的目的有兩種： 繼承程式碼：達到程式碼再用性（reuse） 繼承介面（interface）。達到介面再用性（reuse），為多型預作準備 抽象性抽象(Abstraction)是簡化複雜的現實問題的途徑，它可以為具體問題找到最恰當的類別定義，並且可以在最恰當的繼承級別解釋問題。Ex:柯p在一般時候都被當作一個人，但是如果想要讓他做柯p做的事，你可以呼叫柯p的方法。如果台灣人類別還有人的父類別，那麼你完全可以視柯p為一個人。 參考 Wiki OOP Oreilly","categories":[{"name":"Programming","slug":"Programming","permalink":"https://k2r2bai.com/categories/Programming/"}],"tags":[{"name":"Programming","slug":"Programming","permalink":"https://k2r2bai.com/tags/Programming/"},{"name":"OOP","slug":"OOP","permalink":"https://k2r2bai.com/tags/OOP/"}]},{"title":"NYTime Objective-C 程式規範","slug":"msic/ios-code-style","date":"2013-11-03T04:23:01.000Z","updated":"2019-12-02T01:49:42.399Z","comments":true,"path":"2013/11/03/msic/ios-code-style/","link":"","permalink":"https://k2r2bai.com/2013/11/03/msic/ios-code-style/","excerpt":"Objective-C 程式規範，參考於紐約時報所規範之程式風格。","text":"Objective-C 程式規範，參考於紐約時報所規範之程式風格。 點語法 始终使用點語法來存取屬性，存取其他實例時應用中括號[]。 建議： view.backgroundColor = [UIColor orangeColor];[UIApplication sharedApplication].delegate; 反對： [view setBackgroundColor:[UIColor orangeColor]];UIApplication.sharedApplication.delegate; 間距 永遠不要使用制表符號（tab）間隔。確保在Xcode 中設定了此偏好。 方法的大括號和其他的大括號（if/else/switch/while 等等）始終和宣告在同一行開始，在新的一行結束。 建議： if (user.isHappy) &#123;// Do something&#125;else &#123;// Do something else&#125; 方法之間空格一行，這樣有助於視覺的清晰度和程式碼組織性。在方法中的功能區塊之間應該使用空白分開，但往往可能建立一個新的方法。 @synthesize 和 @dynamic 在實現中都須佔一個新行。 條件判斷條件判斷主要部分不需使用大括號來防止出錯。這些錯誤包含添加第二行(程式碼)，並希望它是if 語法的一部分時。還有另外一種更危險的，當 if 語法裡面的一行被註解掉，下一行就會在不經意間成為了這個 if 語法的一部分。此外，這種風格也更符合所有其他的條件判斷，因此也更容易檢查。 建議： if (!error) &#123; return success;&#125; 反對： if (!error) return success; 或 if (!error) return success; 三元運算式三元運算式以 (條件) ? 成立 : 不成立; 只有當它可以增加程式碼清晰度或整潔時才使用。單一的條件都應該優先考虑使用。多條件時通常使用 if 會更好懂，或者重構成實體變數。 建議： result = (a &gt; b) ? x : y; 反對： result = a &gt; b ? x = c &gt; d ? c : d : y; Switch case由於使用Switch case會針對所有條件是進行處理，需針對所有條件實現，以及用區塊對case做工作區別，確保區塊的正確需給予一項約束： 建議： MyEnum types = MyEnumValueA;switch (types) &#123; case MyEnumValueA:&#123; break; &#125; case MyEnumValueB: &#123; break; &#125; case MyEnumValueC: &#123; break; &#125; default: &#123; &#125;&#125; 反對： int types = 0;switch (types) &#123; case 0: break; case 1: break; case 2: break; default:&#125; 錯誤處理當引用一個回傳錯誤參數（error parameter）的方法時，應該針對回傳值，而非錯誤變數。 建議： NSError *error;if (![self trySomethingWithError:&amp;error]) &#123; // 錯誤處理&#125; 反對： NSError *error;[self trySomethingWithError:&amp;error];if (error) &#123; // 錯誤處理&#125; 一些Apple的API在成功的情况下會寫一些垃圾值给錯誤參數(如果非空)，所以針對錯誤變數可能會造成虛假结果 (或者崩溃)。 方法方法宣告中，在 -/+ 符號後應該有一個空格。方法片段之間也應該有一個空格。 建議： - (void)setExampleText:(NSString *)text image:(UIImage *)image; 變數變數名稱應該盡可能命名具有描述性。除了 for() 迴圈外，其他情况都應該避免使用單字母的變數名。星號表示指標變數，例如：NSString *text 不要寫成 NSString* text 或者 NSString * text ，常數除外。盡量定義屬性來取代直接使用實體變數。除了初始化方法（init， initWithCoder:等）， dealloc 方法和自定義的setters和 getters内部，應避免直接存取實體變數。更多有關在初始化方法和 dealloc方法中使用存取器方法的資訊，可參考這邊。 建議： @interface NYTSection: NSObject@property (nonatomic) NSString *headline;@end 反對： @interface NYTSection : NSObject &#123; NSString *headline;&#125;@property (nonatomic) NSString* string; 變數屬性限制當涉及到ARC變數屬性限制時，限制保留字 (__strong, __weak, __unsafe_unretained, __autoreleasing) 應該位於Class與變數名稱之間，如：NSString * __weak text。 命名盡可能遵照Apple的命名法則，尤其那些涉及到記憶體管理規範，（NonARC）的。 長一點和描述性的方法名稱和變數名稱都不错。 建議： UIButton *settingsButton; 反對： UIButton *setBut; 類別名稱和常數名稱應該使用三個字母當開頭（例如 KRT），但 Core Data 實體名稱可以省略。為了程式碼的乾淨，常數應該使用相關類別的名字作為開頭，並使用駝峰式命名法。 建議： static const NSTimeInterval NYTArticleViewControllerNavigationFadeAnimationDuration = 0.3; 反對： static const NSTimeInterval fadetime = 1.7; 屬性和區域變數應該使用駝峰式命名法，並且首字母小寫。 為了保持一致，實體變數應該使用駝峰式命名，且首字母小寫，以下底線為開頭。這是LLVM的自動合成的實體變數一致。如果LLVM可以自動合成變數，那就讓它自動合成吧。 建議： @synthesize descriptiveVariableName = _descriptiveVariableName; 反對： id varnm; 註解當需要的时候，註解應該被用來解释 為什麼 特定程式做了某些事情。所使用之任何註解必須保持最新，否則就刪掉。 通常應避免一大塊註解，程式碼應該盡量作為自身的檔案，只需要隔幾行寫幾句說明。這並不適用於那些用來生成檔案的註解。 init 和 deallocdealloc 方法應該放在實作檔最上面，並且剛好在 @synthesize 和 @dynamic 保留字的後面。在任何類別中，init 都應該直接放在 dealloc 方法的下方。 init 方法的結構應該像這樣： - (instancetype)init &#123; self = [super init]; // 呼叫指定的初始化方法 if (self) &#123; // Custom initialization &#125; return self;&#125; 字面變數當創建 NSString， NSDictionary， NSArray，和 NSNumber 物件的不可變實體時，都應該使用字面量。要注意 nil 值不能傳給 NSArray 和 NSDictionary 字面變數，這樣做會導致崩潰。 建議： NSArray *names = @[@\"Brian\", @\"Matt\", @\"Chris\", @\"Alex\", @\"Steve\", @\"Paul\"];NSDictionary *productManagers = @&#123;@\"iPhone\" : @\"Kate\", @\"iPad\" : @\"Kamal\", @\"Mobile Web\" : @\"Bill\"&#125;;NSNumber *shouldUseLiterals = @YES;NSNumber *buildingZIPCode = @10018; 反對： NSArray *names = [NSArray arrayWithObjects:@\"Brian\", @\"Matt\", @\"Chris\", @\"Alex\", @\"Steve\", @\"Paul\", nil];NSDictionary *productManagers = [NSDictionary dictionaryWithObjectsAndKeys: @\"Kate\", @\"iPhone\", @\"Kamal\", @\"iPad\", @\"Bill\", @\"Mobile Web\", nil];NSNumber *shouldUseLiterals = [NSNumber numberWithBool:YES];NSNumber *buildingZIPCode = [NSNumber numberWithInteger:10018]; CGRect函式當存取一個 CGRect 的 x， y， width， height 时，應該使用CGGeometry 函式取代直接存取結構體成員。Apple的 CGGeometry 参考中說到： All functions described in this reference that take CGRect data structures as inputs implicitly standardize those rectangles before calculating their results. For this reason, your applications should avoid directly reading and writing the data stored in the CGRect data structure. Instead, use the functions described here to manipulate rectangles and to retrieve their characteristics. 建議： CGRect frame = self.view.frame;CGFloat x = CGRectGetMinX(frame);CGFloat y = CGRectGetMinY(frame);CGFloat width = CGRectGetWidth(frame);CGFloat height = CGRectGetHeight(frame); 反對： CGRect frame = self.view.frame;CGFloat x = frame.origin.x;CGFloat y = frame.origin.y;CGFloat width = frame.size.width;CGFloat height = frame.size.height; 常數常數使用字元、字串 字面變數或數值變數，因為常數可以輕易重用，且可以快速改變而不需要查找和替換。常數應該宣告為 static 常數而不是 #define ，除非很明確的要當做marco來使用。 建議： static NSString * const NYTAboutViewControllerCompanyName = @\"The New York Times Company\";static const CGFloat NYTImageThumbnailHeight = 50.0; 反對： #define CompanyName @\"The New York Times Company\"#define thumbnailHeight 2 枚舉類型當使用 enum 時，建議使用新的基礎類型規範，因為它具有更强的類型檢查和程式碼補全功能。現在的SDK 包含了一個marco來建議使用者利用新的基礎類型 - NS_ENUM() 建議： typedef NS_ENUM(NSInteger, NYTAdRequestState) &#123; NYTAdRequestStateInactive, NYTAdRequestStateLoading&#125;; 位元碼當用到位元碼時，使用 NS_OPTIONS marco。 舉例： typedef NS_OPTIONS(NSUInteger, NYTAdCategory) &#123; NYTAdCategoryAutos = 1 &lt;&lt; 0, NYTAdCategoryJobs = 1 &lt;&lt; 1, NYTAdCategoryRealState = 1 &lt;&lt; 2, NYTAdCategoryTechnology = 1 &lt;&lt; 3&#125;; 私有屬性私有屬性應該宣告在類別實作檔的延展(匿名的類目)中。有名字的類目(例如 NYTPrivate 或 private) 永遠都不應該使用，除非要擴展其他類別。 建議： @interface NYTAdvertisement ()@property (nonatomic, strong) GADBannerView *googleAdView;@property (nonatomic, strong) ADBannerView *iAdView;@property (nonatomic, strong) UIWebView *adXWebView;@end 圖片命名圖片名稱應該被統一命名，以保持組織的完整。它們應該被命名為一個說明它們用途的駝峰式字串，其次是自定義類別或屬性的無前綴名字（如果有的話），然後進一步說明顏色與對應的解析度識別(ex:iPhone 6 and iPhone 5為@2x)或 圖片展示位置，最後是它們的狀態。 建議： RefreshBarButtonItem / RefreshBarButtonItem@2x 和 RefreshBarButtonItemSelected / RefreshBarButtonItemSelected@2x ArticleNavigationBarWhite / ArticleNavigationBarWhite@2x 和 ArticleNavigationBarBlackSelected / ArticleNavigationBarBlackSelected@2x. 對應的解析度識別 圖片目錄中被用於類似目地的圖片應歸入各自組中。最後確保圖片存放於Images.xcassets。 布林因為 nil 解析為 NO，所以没有必要在條件中與它進行比較。永遠不要直接和 YES 進行比較，因為 YES 被定義為 1，而 BOOL 可以多達八位元。 這使得整個檔案有更多的一致性和更大的視覺清晰度。 建議： if (!someObject) &#123;&#125; 反對： if (someObject == nil) &#123;&#125; 對於 BOOL 來說, 這有兩種用法: if (isAwesome)if (![someObject boolValue]) 反對： if ([someObject boolValue] == NO)if (isAwesome == YES) // 絕對不要這麼做 如果一個 BOOL 屬性名稱是一個形容詞，屬性可以省略 “is” 前缀，但為 get 存取器指定一個慣用的名稱，例如： @property (assign, getter=isEditable) BOOL editable; 內容和範例來自 Cocoa 命名指南 。 單一實例單例對象應該使用執行緒安全的模式創建共享的實例。 + (instancetype)sharedInstance &#123; static id sharedInstance = nil; static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^&#123; sharedInstance = [[self alloc] init]; &#125;); return sharedInstance;&#125; 這將會預防有時可能產生的許多崩潰。 Xcode为了避免檔案複雜，實體檔案與目錄應該保持與Xcode同步。Xcode 建立的任何群組（group）都必须在檔案系统有相應的映射。為了更清晰，程式碼不僅應該按照類型進行分组，也可以根據功能進行分组。 如果可以的話，盡可能一直打開 target Build Settings 中 “Treat Warnings as Errors” 以及一些額外的警告。如果你需要忽略指定的警告,使用 Clang 的編譯特性 。 其他Objective-C風格指南 Google GitHub Adium Sam Soffes CocoaDevCentral Luke Redpath Marcus Zarra","categories":[{"name":"iOS","slug":"iOS","permalink":"https://k2r2bai.com/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://k2r2bai.com/tags/iOS/"},{"name":"Programming","slug":"Programming","permalink":"https://k2r2bai.com/tags/Programming/"}]}]}